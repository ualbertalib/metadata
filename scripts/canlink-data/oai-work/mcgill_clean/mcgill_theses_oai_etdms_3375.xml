<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/assets/blacklight_oai_provider/oai2-b0e501cadd287c203b27cfd4f4e2d266048ec6ca2151d595f4c1495108e36b88.xsl"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd"><responseDate>2020-07-24T23:02:57Z</responseDate><request resumptionToken="oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):3375" verb="ListRecords">https://escholarship.mcgill.ca/catalog/oai</request><ListRecords><record><header><identifier>oai:escholarship.mcgill.ca:kw52jb221</identifier><datestamp>2020-03-21T05:00:21Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Multiple sclerosis (MS) is an inflammatory, demyelinating, and neurodegenerative disease that affects the white (WM) and grey matter (GM) of the central nervous system. The cortical GM is heavily affected by MS lesions, which could explain clinical symptoms currently unaccounted for by the number and volume of WM lesions. However, the cortex remains a challenging anatomical region to visualize in vivo with current imaging technology.   This thesis presents a series of studies that use various magnetic resonance imaging (MRI) contrasts to determine how frequent cortical lesions (CL) are in multiple sclerosis (MS), how these lesions change over time, and what is their impact on symptoms. To this end, we first present a multi-contrast MRI reading protocol using 3T scans, developed to gain sensitivity in CL detection using clinically-available MRI scanners (3T scanners). The comparison of our multi-contrast protocol to double inversion recovery (DIR), the current MRI contrast of choice to detect CLs, shows a higher sensitivity of our multi-contrast approach. We then used the multi-contrast protocol to assess the frequency of CLs in pediatric-onset MS, a population where CLs have been described as scarce and having little impact on clinical symptoms. Our study found a high frequency of CLs in this population and a specific impact of frontal CLs on upper limb dexterity. We also used the multi-contrast protocol to determine the number of CLs in a cohort of recently diagnosed MS patients who were scanned using triple-dose gadolinium. This contrast agent allows the detection of acute lesions that have a compromise of the blood-brain-barrier (BBB). We determined the frequency of gadolinium enhancing lesions that affect the cortex of these young adult patients and followed them over time with monthly scans acquired for two years. We assessed the evolution of these lesions in terms of resolution and persistence on different MRI contrasts, and estimated the temporal patterns of change of a myelin-sensitive MRI marker (magnetization transfer ratio = MTR) in the cortical and white matter (WM) portions of these lesions. Our analysis found a significant drop of the MTR values after the gadolinium enhancement event in the grey and WM portions of both the lesions that resolve and the lesions that do not resolve on other MRI contrasts. Finally, we compared the 3T multi-contrast MRI reading protocol against a 7T multi-contrast protocol, to determine the types of CLs detected at each field strength, the sensitivity of a 3T multi-contrast detection approach, the association of specific CL types to other MRI outcome measures (e.g. WM lesion volume), and the proportion of the different CLs types across clinical subtypes. This last study showed that: 1) multi-contrast 3T and 7T scans detect leukocortical lesions with similar accuracy; 2) subpial lesions, although detected with a lower sensitivity on 3T scans, show a high correlation with the counts obtained through 7T images, suggesting that the subpial lesions detected at 3T may provide a clinically-useful sample of the total population of subpial lesions, (the proverbial "tip of the iceberg"); 3) the total number of 3T CLs is a significant predictor of the number of subpial lesions detected at 7T; 4) the number of CLs increases in secondary progressive MS, compared to relapsing-remitting MS; and 5) the proportion of the different lesion types does not change across clinical subtypes.In summary, this work highlights the relevance of multi-contrast 3T MRI assessment of cortical pathology in vivo to characterize the evolution of all CL types.</description><description>La sclérose en plaques est une maladie inflammatoire, démyélinisante, et dégénérative de la matière blanche et grise du système nerveux central. La matière grise du cortex est atteinte par des nombreuses lésions qui pourraient expliquer des symptômes qui ne sont pas associés aux lésions de la matière blanche. Cependant, le cortex est une région anatomique de difficile visualisation in vivo avec les méthodes actuelles d'imagerie médicale. Cette dissertation présente une série d'études qui se servent de plusieurs contrastes d'imagerie par résonance magnétique (IRM) pour établir : la fréquence des lésions corticales causées par la SP, leur évolution dans le temps, et leur impact sur les symptômes des patients. Dans ce but, on présente un protocole de lecture d'imagerie par résonance magnétique (IRM) cliniques 3T qui emploie multiples contrastes et qui a été développé pour augmenter la sensibilité de dépistage des lésions corticales. La comparaison de notre protocole avec la séquence par double récupération d'inversion (DIR), considérée comme le contraste de choix en IRM pour dépister les lésions corticales, montre une sensibilité supérieure de notre approche.Puis, le protocole a été utilisé pour évaluer la fréquence des lésions corticales chez les patients pédiatriques atteints de SP, un groupe ou celles-ci sont considérées rares et sans impact sur les symptômes. Notre étude montre un grand nombre de lésions corticales dans ce groupe, et un impact des lésions corticales frontales sur la dextérité des membres supérieurs. On a également employé le protocole pour établir la fréquence des lésions corticales chez les patients adultes atteints de SP récemment diagnostiqués (dans les trois derniers mois) qui ont passé une étude d'IRM avec triple dose d'agent de contraste (gadolinium). Ce contraste montre les lésions aigües quand le fonctionnement de la barrière hématoencéphalique est compromis. On a établi la fréquence des lésions corticales se rehaussant avec gadolinium et on a suivi leur comportement grâce aux IRM mensuelles faites pendant deux ans. Plus tard, on a étudié l'évolution de ces lésions en termes de régression ou persistance selon les différents contrastes d'IRM, ainsi qu'en termes semi-quantitatifs grâce à la technique par transfert de magnétisation (ITM). Notre analyse montre une chute significative des valeurs d'ITM après le rehaussement avec gadolinium, dans la matière grise et blanche des lésions qui régressent et aussi celles qui ne le font pas. Enfin, on a comparé le protocole d'IRM 3T multi-contraste avec celui de 7T afin d'établir le type de lésions corticales visibles avec chacun d'eux, la sensibilité du protocole 3T, l'association de chaque type de lésion avec d'autres mesures par IRM (par ex. : lésions de la matière blanche), et la proportion de chaque type de lésion dans chaque phénotype. Cette etude nous a permis de tirer les conclusions suivantes : 1) les protocoles multi-contrastes d'IRM 3T et 7T s'avèrent également efficaces pour dépister les lésions leucocorticales; 2) les IRM 3T sont moins sensibles pour le dépistage des lésions sous-piales mais le nombre des lésions a une corrélation élevée avec celui détecté par les IRM 7T. En conséquence, les lésions dans l'IRM 3T peuvent être considérées comme un échantillon clinique utile du nombre total des lésions sous-piales; 3) le nombre total des lésions corticales dans les IRM 3T est un prédicteur significatif du nombre des lésions sous-piales détectées par 7T; 4) le nombre de lésions corticales augmente chez les patients en phase secondaire progressive de la maladie; et 5) la proportion de chaque type de lésion ne change pas entre la phase rémittente-récurrente et celle secondaire progressive. En somme, ce travail met en évidence l'importance de l'évaluation des lésions corticales avec un protocole multi-contraste d'IRM 3T, y compris l'évaluation des lésions sous-piales</description><creator>Maranzano, Josefina</creator><contributor>Sridar Narayanan (Supervisor)</contributor><date>2019</date><subject>Neuroscience</subject><title>Elucidating cortical lesion evolution in Multiple Sclerosis</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/0v8382763.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/kw52jb221</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Integrated Program in Neuroscience</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:h415pc79g</identifier><datestamp>2020-03-21T05:00:22Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les microglies, les cellules de l'immunité innée, résidentes du cerveau, sont impliquées dans la réponse inflammatoire cérébrale et le modelage des réseaux neuronaux au cours du développement. La perturbation de leurs activités par des stimuli environnementaux pouvant conduire à des altérations psychopathologiques. Nous avons étudié le rôle des microglies dans les effets neurobiologiques et comportementaux d'un stimulus inflammatoire.Les travaux précédents ont révélé que l'administration de lipopolysaccharide (LPS), une endotoxine bactérienne, provoque des comportements de type dépressifs. Le rôle des microglies dans ces altérations a fait l'objet de la première étude de cette thèse (Chapitre 2). Afin de dépléter les microglies du cerveau, des souris adultes ont reçu par voie d'administration intra hippocampique des liposomes contenant du clodronate, provoquant ainsi l'apoptose des microglies phagocytaires. L'administration de LPS active dans l'hippocampe la synthèse de cytokines pro-inflammatoires [interleukin (IL)-1b et facteur de nécrose tumorale (TNF)a] et anti-inflammatoires (IL-10), et de l'indoleamine 2,3- dioxygénase, une enzyme impliquée dans le métabolisme du tryptophane aux activités prodépressives. La déplétion des microglies phagocytaires atténue les effets du LPS, à l'exception de l'IL-1b, dont l'expression est exacerbée. De plus, l'administration de clodronate prévient les effets du LPS sur les comportements de type dépressif. Ces résultats ont révélé que les microglies phagocytaires sont impliquées dans les effets inflammatoires et comportementaux de type dépressif induits par le LPS. Nous avons ensuite étudié le rôle des microglies dans les effets comportementaux d'une inflammation maternelle précoce provoquée lors de la colonisation du cerveau foetal par les microglies (Chapitres 3 &amp; 4). Ainsi, nous avons administré au jour gestationnel (JG)9.5 du LPS à des souris gestantes et évalué la trajectoire développementale pré- et post-natale des microglies et du comportement de la progéniture (Chapitre 3). L'administration de LPS à JG9.5 provoque une réduction du pourcentage représenté par les microglies matures aux JG14.5 et 18.5 et des déficits comportementaux persistants à l'âge adulte avec undimorphisme sexuel prononcé. Nous avons alors recherché à identifier les mécanismes moléculaires impliqués dans les effets du LPS administré à JG9.5, en étudiant la piste de l'action des cytokines inflammatoires (Chapitre 4). Nous nous sommes focalisés sur l'IL-1b, la cytokine inflammatoire effectrice principale de l'activité microgliale. L'expression de l'IL-1b et des cytokines associées (IL-6, TNFa et IL-10) augmente dans le plasma maternel, le placenta et le cerveau foetal, 2 et 4 heures après l'administration de LPS. Ces changements sont accompagnés d'un phénotype microglial immature à JG18.5 et d'une réduction de la population microgliale totale au jour postnatal (JPN)9. Enfin, les souris adultes, prénatalement traitées au LPS, développent des altérations des comportements de type sociaux et des comportements répétitifs. Les altérations du nombre de microglies induites par le LPS sont corrélées aux troubles comportementaux, et ce, de façon spécifique en fonction du sexe des souris. Enfin, la co-administration de l'antagoniste du récepteur de l'IL-1 et de LPS chez les femelles gestantes au JG9.5 réduit, voire prévient les effets inflammatoires et comportementaux du LPS.En conclusion, ce travail de thèse montre l'implication des microglies dans les effets comportementaux d'une inflammation périphérique à court terme chez des souris adultes, ou à long terme chez la descendance de souris ayant subi une activation immunitaire durant la gestation. </description><description>Recent research on microglia has uncovered a multitude of activities that extends the role of these cells well beyond their traditional function as immune sentinels. The most prominent of these newly described activities is an intricate role in neuronal network remodeling notably upon environmental challenge or during brain development, the disruption of which can result in long lasting consequences relevant to several psychopathologies. We sought, in the current thesis, to identify some of the mechanisms involved. Our approach was to target the immune function of microglia, based on our previous findings linking systemic immunogenic challenge with lipopolysaccharide (LPS) in mice with the development of despair-like behaviour. Here, we sought to identify immune mediators activated in microglia following a single systemic challenge with LPS (Chapter 2). These studies were conducted in adult mice in which phagocytic microglia were depleted using an injection of liposomal clodronate in the CA3 region. LPS challenge significantly upregulated the expression of both pro-inflammatory [interleukin (IL)-1b and tumor necrosis factor (TNF)a] and anti-inflammatory (IL-10) cytokines compared to saline treated animals. In addition, LPS highly increased the expression of indoleamine 2,3-dioxygenase (IDO), an important rate limiting enzyme for metabolizing tryptophan in the brain and an indicator of the activation of this depression mediating pathway. Clodronate-mediated depletion attenuated all of these effects apart from IL-1b expression which was further exacerbated. Behavioural assessment of the mice demonstrated a significant LPS-induced increase of immobility in the forced swim test (FST), which was prevented by clodronate. This experimental approach provided a snapshot of the role of inflammation in the development of brain dysfunction mediated by microglia. In subsequent studies (chapter 3 &amp; 4), we utilized a prenatal infection model using LPS to activate maternal immunity at a relatively early [Gestational Day (GD)9.5] time point when microglia colonize the fetal brain to assess the impact on microglial population during development and the subsequent behaviour of the progeny (Chapter 3). The results demonstrated LPS reduced the percentage of mature microglial population at GD14.5 and GD18.5. In addition, prenatal LPS had a significant effect on the offspring's neonatal as well as adult behaviour, with a clear divergence along sex lines in adulthood.In the final study (Chapter 4), we sought to investigate the mechanisms underlying the changes we noted in microglial development and behavioural deficits. For this, we focused on the role played by pro-inflammatory cytokines, particularly IL-1b which represents the main effector of microglial activation following infection or injury. Detailed analysis of the expression of IL-1b and other related cytokines (IL-6, TNFa and IL-10) revealed an increased expression of these mediators in maternal plasma, placenta and fetal brain, 2 and 4 hours after the prenatal LPS treatment. These changes were accompanied with a decreased percentage of mature microglia in the brain of embryos at GD18.5 and of total microglia population at post-natal day (PND)9. In the adult offspring (PND65), we detected an increased density in specific higher-order structures implicated in complex behaviours, as well as altered social preference and memory and increased repetitive actions. Interestingly, changes in the number of microglia correlated significantly with the LPS-induced behavioural impairments, most of which were sexually dimorphic with a bias towards males. Strikingly, co-administration of the IL-1 receptor antagonist (ra) in mice at GD9.5 either prevented or significantly attenuated the effects induced by LPS at all the developmental time points tested.In conclusion, the work presented in this thesis shows the role of microglia in mediating short- and long-term LPS-induced behavioural deficits. </description><creator>Lacabanne, Chloé</creator><contributor>Giamal Luheshi (Supervisor1)</contributor><contributor>Sophie Layé (Supervisor2)</contributor><date>2019</date><subject>Neuroscience</subject><title>Inflammation and immune-mediated neurobehavioral alterations: a critical role for microglia</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/br86b5628.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/h415pc79g</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Integrated Program in Neuroscience</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:5m60qv188</identifier><datestamp>2020-03-21T05:00:23Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>System identification is the process of building dynamical models from measured data in order to determine and quantify the underlying relationships between them. Ideally, the resulting mathematical models ought to imitate precisely the observed behavior of the system under examination. In this doctoral dissertation, we focus on developing effective methodologies for quantifying dynamic interrelationships in physiological systems using parametric, nonparametric and connectionist approaches. Due to the complex nature of physiological functions, standard system identification methods, which usually assume linear and time-invariant interrelationships, fail. Thus, this work describes fast and reliable modeling schemes that are capable of dealing with (a) multiple input systems (b) nonlinear dynamics (c) nonstationarities in system dynamics and (d) binary responses. These schemes were applied in combination with Laguerre-Volterra (LV) models, which can capture a wide range of nonlinear dynamic input-output causal interrelationships and Multivariate Autoregressive models (MVAR), which are used to detect couplings and causality between time series. The performance of the abovementioned methodologies was assessed using both simulations and experimental data. Specifically, we examined,•The time-varying (TV) characteristics of Cerebral Autoregulation (CA) in patients suffering from Vasovagal Syncope (VVS) during Head-Up Tilt (HUT) testing.  •Exercise-induced cardiovascular and cerebrovascular changes in healthy subjects and stroke survivors.•Neuronal responses to subthalamic nucleus (STN) Local Field Potentials (LFP) in Parkinson's Disease (PD) patients undergoing Deep Brain Stimulation (DBS).</description><description>L'identification des systèmes est le processus de construction de modèles dynamiques à partir des données mesurées afin de déterminer et de quantifier les relations sous-jacentes entre eux. Idéalement, les modèles mathématiques qui en résultent doivent imiter précisément le comportement observé du système examiné. Dans cette thèse de doctorat, nous nous efforçons d'élaborer des méthodologies efficaces pour quantifier les interrelations dynamiques dans les systèmes physiologiques à l'aide d'approches paramétriques, non paramétriques et connexionnistes. En raison de la nature complexe des fonctions physiologiques, les méthodes standard d'identification du système, qui supposent habituellement des interrelations linéaires et temporelles, échouent. Ainsi, ce travail décrit des schémas de modélisation rapides et fiables qui sont capables de traiter (a) des systèmes d'entrée multiples (b) des dynamiques non linéaires (c) des systèmes non stationnaires et (d) des réponses binaires. Ces schémas ont été appliqués en combinaison avec les modèles Laguerre-Volterra (LV), qui peuvent capturer une large gamme d'interrelations causales d'entrée-sortie dynamiques non linéaires et de modèles autogressifs multivariés (MVAR), qui sont utilisés pour détecter les couplages et la causalité entre les séries temporelles. La performance des méthodes susmentionnées a été évaluée en utilisant des simulations et des données expérimentales. Plus précisément, nous avons examiné,•Les caractéristiques de l'Autoregulation Cérébral (AC) variant dans le temps (TV) dans les patients souffrant du Syncope Vasovagale (VVS) pendant le test de Head-Up Tilt (HUT).•Changements cardiovasculaires et cérébrovasculaires induits par l'exercice chez des sujets sains et des survivants d'accident vasculaire cérébral.•Réponses neuronales aux potentiels de terrain locaux (LFP) du noyau subthalamique (NST) chez les patients atteints de la maladie de Parkinson (PD) subissant une chirurgie de Stimulation Cérébrale Profonde (SCP).</description><creator>Kostoglou, Kyriaki</creator><contributor>Georgios Mitsis (Supervisor)</contributor><date>2018</date><subject>Electrical and Computer Engineering</subject><title>Identification of multiple-input, linear and nonlinear, time-varying systems and binary response systems for biomedical applications</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/0v838277c.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/5m60qv188</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:hq37vq70k</identifier><datestamp>2020-03-21T05:00:24Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Chaque année, l'industrie des jus génère des milliers de tonnes de résidus de pressage, appelés "marc", qui sont riches en polyphénols. Compte tenu du tonnage annuel généré par l'industrie, la gestion du marc de fruits représente un défi majeur même si cette biomasse est sous-utilisée, peu couteuse, et présente un potentiel industriel de valorisation. Une part des composés phénoliques, qui sont des pigments naturels responsables de la protection contre le stress oxydatif, reste piégée dans le marc (peau et graines) après le pressage du jus. Leur activité antioxydante élevée et leur couleur naturelle pourraient offrir de nouvelles perspectives d'utilisation commerciale dans les industries alimentaire, cosmétique et pharmaceutique pour leurs effets protecteurs, colorant et stabilisants.Les polyphénols comprennent plus de 10000 molécules, avec de nombreux sous-groupes très différents de part sur leur structure chimique. Cette recherche globale de la diversité des polyphénols, en termes de solubilité, de perméabilité et de circulation dans le corps, contribue à leur adoption par l'industrie de la santé et de l'alimentation.Les procédés d'extraction assistée par micro-ondes (MAE) et à l'eau sous pression faiblement polaire (PLPW) sont considérés comme des technologies simples et peu coûteuses, étant de bonnes alternatives aux méthodes d'extraction conventionnelles pour la valorisation des marcs de fruits. Le PLPW utilise uniquement de l'eau tandis que MEA utilise de l'éthanol aqueux généralement accepté pour l'extraction de produits naturels. Les deux technologies permettent d'extraire directement les pomaces humide, sans le besoin de les sécher. En fournissant des données optimales sur les rendements d'extraction de marc de fruits avec ces technologies vertes, il est possible d'accélérer leur adoption dans l'industrie. Le rapport coût / bénéfice global de l'extraction des marcs est spécifique au produit. Ainsi, des efforts ont été déployés dans cette étude pour adapter ces technologies d'extraction aux marcs de bleuets, aux marcs de pomme et aux marcs de canneberges, jumelés à des analyses de criblage haut débit pour en déterminer le profil en polyphénols, leur bioactivité et leurs couleurs, déterminant ainsi la valeur commerciale des extraits. Afin de contribuer à la popularité et la valeur commerciale des polyphénols, une revue de la littérature sur leur biodisponibilité et leur chimie a été réalisée pour rationaliser leur utilisation potentielle en tant que principe actif dans de multiples secteurs.</description><description>Every year the juice industry generates thousands of tons of press residue, called "pomace," which is rich in polyphenols. Considering its annual processed tonnage, the fruit pomace represents a serious disposal challenge for the industry, yet is a low-cost under-utilized material with industrial potential. Phenolic compounds, the pigmented molecules responsible for protecting organisms against oxidative stress, are trapped in the pomace (skin, pulp and seeds) after the juice is pressed out. Their high antioxidant activity and natural colours could provide new perspectives for commercial use in the food, cosmetic and pharmaceutical industries for their protective, attractive and stabilizing effects. Polyphenols comprise over 10,000 molecules, with many sub-groups very different on their chemical structure. This comprehensive research on polyphenol diversity, in terms of solubility, permeability, and circulation in the body, contributes to their adoption by the health and food industry.Microwave assisted extraction (MAE) and pressurized low polarity water (PLPW) are regarded as simple and low-cost technology, alternatives to conventional extraction methods, for the valorisation of fruit pomace. The PLPW uses only water while MAE uses aqueous ethanol, well accepted for the extraction of natural product. Both technologies are convenient for reprocessing the wet pomace, without the need for prior dehydration. By providing optimized processing data using these green technologies for the extraction of valuable compounds from fruit pomace, it is possible to accelerate their adoption in industry. The overall cost/benefit ratio of reprocessing the pomace is product-specific. Thus, special efforts in this study were taken to adapt these extraction technologies to blueberry pomace, cranberry pomace and apple pomace and to use high-throughput analysis to shorten the time for screening the polyphenolic profiles, their bioactivity and color properties. For enhancing the marketing value of polyphenols, a comprehensive review of their bioavailability was done to better explain their potential use as active ingredient for use/application in multiple sectors.</description><creator>Bélair, Viviane</creator><contributor>Valerie Orsat (Supervisor)</contributor><date>2019</date><subject>Bioresource Engineering</subject><title>Bioavailability of polyphenols extracted from fruit pomace using green technologies</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/t722hc08v.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/hq37vq70k</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Bioresource Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:mp48sf89k</identifier><datestamp>2020-03-21T05:00:25Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Contexte: le tractus gastro-intestinal humain héberge près d'un trillion de microorganismes, organisés dans un écosystème complexe appelé microbiote intestinal. Notre diversité génétique bactérienne détermine la digestion des glucides alimentaires complexes (glycanes). En effet, le génome humain code peu de Carbohydrate-Active Enzymes (CAZymes) capables de catalyser le métabolisme des sucres complexes présents dans les légumes, les plantes et les céréales. En conséquence, la plupart de ces glycanes atteignent le gros intestin intact, où ils sont digérés par le microbiote intestinal, qui code un répertoire beaucoup plus large de CAZymes. Dans des maladies telles que l'obésité et la colite ulcéreuse, l'équilibre entre les différents phyla est perturbé. Cependant, cette dysbiose pourrait être corrigée avec des prébiotiques, des composés non digestibles qui modifient la composition du microbiote, et ont des effets bénéfiques sur la santé de l'hôte. La modulation ciblée de la communauté bactérienne par des régimes enrichis en prébiotiques constitue donc une voie thérapeutique importante pour le traitement de plusieurs maladies. Objectif: un casse-tête reste sans solution: quelles bactéries et quels gènes sont responsables du métabolisme d'un prébiotique distinct? Par conséquent, nous visons à lier des structures spécifiques de glycanes aux bactéries qui ont le pouvoir de les métaboliser et à comprendre comment la composition de l'écosystème global change et s'adapte après l'administration de repas riches en ces glycanes. Méthodes et résultats: une banque de glycanes alimentaires conjugués à un colorant fluorescent a été synthétisée et purifiée. En utilisant des échantillons de selles comme réservoir du microbiote intestinal, les bactéries consommant nos sondes ont été séparées par cytométrie en flux (FACS) et identifiées à l'aide d'un séquençage d'ADNr 16S pour révéler la diversité des bactéries consommant le glycane donné. Conclusion: notre méthode, indépendante de la culture bactérienne, a permis de révéler le métabolisme des bactéries intestinales spécifique à la structure des glycane administrés.</description><description>Background: the human gastro-intestinal tract hosts almost a trillion microorganisms, organized in a complex ecosystem known as the gut microbiota. Our bacterial genetic diversity dictates the digestion of complex dietary carbohydrates (glycans). Indeed, the human genome only encodes few carbohydrate-active enzymes (CAZymes) able to catalyse the metabolism of complex sugars found in vegetables, plants, and grains. Consequently, most of these glycans reach the large intestine intact, where they are digested by the gut microbiota, which encodes a much wider repertoire of CAZymes. In diseases such as obesity and ulcerative colitis, the balance between the different phyla is disturbed. However, this dysbiosis might be corrected with prebiotics, non-digestible compounds that alter the microbiota composition with beneficial outcomes to the host's health. The purposeful modulation of bacterial community by prebiotic-enriched diets is therefore an important therapeutic avenue for the treatment of numerous disorders. Aim: one puzzle remains unsolved: which bacteria, and which genes, are responsible for the metabolism of a given prebiotic? Therefore, we aim to link specific glycan structures to the metabolizing gut bacteria, and to understand how the composition of the overall ecosystem changes and adapts after the administration of meals that are rich in those glycans. Methods and results: a library of dietary glycans conjugated to a fluorescent dye was synthetized and purified. Using stool samples as proxies for the gut microbiota, the bacteria consuming our probes were sorted by Fluorescence-Activated Cell Sorting (FACS) and identified with 16S rDNA sequencing to uncover the diversity of bacteria consuming the given glycan. Conclusion: our culture-independent method proved successful in revealing structure-specific metabolism of gut bacteria.</description><creator>Altamura, Fernando</creator><contributor>Bastien Castagner (Internal/Supervisor)</contributor><date>2019</date><subject>Pharmacology &amp; Therapeutics</subject><title>Revealing gut microbiota metabolism with dietary glycan-fluorophore conjugates</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/3t945t177.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/mp48sf89k</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Pharmacology and Therapeutics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:ks65hf44t</identifier><datestamp>2020-03-21T05:00:25Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Au cours des dernières années, de nouvelles méthodes d'optimisation mathématique stochastique permettant d'optimiser simultanément un vaste ensemble de décisions devant être prises lors la planification de la production d'un complexe minier ont été développées. Un complexe minier est un ensemble d'opérations qui constituent une chaîne d'approvisionnement minière, en commençant par l'extraction du minerai du gisement jusqu'à sa transformation finale en produit mis sur le marché. Cette structure se différencie des approches antérieures où les diverses opérations du complexe minier sont optimisées indépendamment, ce qui ne permet pas de tirer profit des fortes dépendances entre les différents processus. Les données d'entrée essentielles à cette optimisation simultanée sont des simulations géostatistiques qui quantifient la variabilité et l'incertitude des attributs d'un gisement de minerai donné. Jusqu'à maintenant, les méthodes de simulation les plus avancées permettent de reproduire des géométries complexes ainsi que la connectivité multipoints des valeurs extrêmes. Il demeure néanmoins que les réalisations obtenues avec ces méthodes sont générées à l'échelle de points, ce qui requiert une étape de post-traitement pour générer les modèles de gisement à l'échelle des blocs. Cette étape de post-traitement est nécessaire pour représenter le gisement pour à des fins d'opérations d'ingénierie. Ainsi, un modèle à plusieurs millions de blocs nécessite une discrétisation de l'ordre de plusieurs centaines de millions de points à simuler. Cela représente un défi computationnel de taille en termes de post-traitement de ces modèles. La première partie de ce mémoire présente la méthode de simulation d'ordre supérieur proposée permettant de générer des réalisations directement à l'échelle des blocs avec les données disponibles à l'échelle des points. Suivant le paradigme de la simulation séquentielle, la méthode estime, pour chaque emplacement de bloc, la fonction de densité de probabilité commune à support croisé en utilisant des splines de type Legendre comme ensemble de fonctions de base nécessaires. Les blocs précédemment simulés sont ajoutés à l'ensemble des données conditionnelles, qui contient initialement les données de forage disponibles. Un motif spatial, défini par la configuration du bloc à simuler et les données conditionnelles correspondantes, est utilisé pour déterminer des statistiques additionnelles d'ordre supérieur venant d'une image d'entraînement. La méthode est d'abord testée dans un environnement contrôlé. Les résultats de ces tests démontrent que les réalisations simulées présentent des résultats consistants reproduisant les structures majeures ainsi que les relations d'ordre supérieur des données. Ensuite, la méthode est utilisée pour simuler un gisement d'or. Cette étude confirme l'efficacité et la capacité de la méthode à reproduire les statistiques spatiales jusqu'au 4e ordre, tout en étant conforme à ces mêmes statistiques venant des données de forage. Le temps de calcul nécessaire pour générer une réalisation avec l'approche proposée est réduit d'un facteur de 5 comparativement à la version de l'algorithme considérant l'échelle du point.La deuxième partie du mémoire présente une étude de cas où les simulations obtenues via l'approche décrite ci-haut sont utilisées comme données d'entrée pour l'optimisation simultanée d'un complexe minier. Les résultats de cette optimisation indiquent qu'une valeur présente nette de 5 à 16% plus élevée est obtenue comparativement à une optimisation prenant comme données d'entrée des simulations générées avec une méthode traditionnelle basée sur des statistiques du 2e ordre. La comparaison montre aussi qu'utiliser des simulations présentant des connectivités de blocs à fortes teneurs plus réalistes permet d'obtenir une planification de production mieux informée. Cela montre que l'optimisation peut capitaliser sur une meilleure compréhension de la connectivité des fortes teneurs.</description><description>Over recent years, new methods have developed the mining production schedule stochastic optimization into a framework that includes, in one mathematical formulation, all the components of a mining complex and optimizes it simultaneously. A mining complex is a set of operations that integrate all aspects in a mineral value chain, starting from the materials extracted from the ground culminating with its transformations into a final product delivered to the mineral market. The framework diverges from past methods that optimize each operation of the mining complex separately, which do not benefit from the coexisting harmony between connected processors. Core inputs of this all-inclusive optimization are the geostatistical simulations quantifying variability and uncertainty of relevant attributes in a given mineral deposit. To date, the state-of-the-art simulation methods can reproduce complex geometries and multi-point connectivity of extreme values. However, the generated realizations are performed at the point-support which requires a post-processing step to generate block-support orebody models, as needed to represent the mineral deposit due to engineering purposes. For example, a multimillion block model requires discretization of the magnitude of hundreds of millions of nodes to simulate. This presents computational challenges in post-processing such a massive model. The first part of this thesis presents the high-order simulation method that generates realizations directly at the block support conditioned to the available data at point support scale. Following the sequential simulation paradigm, the method estimates, at each block location, the cross-support joint probability density function using Legendre-like splines as the set of basis functions needed. The previously simulated blocks are added to the set of conditioning data, which initially contains the available drillhole data. A spatial template, defined by the configuration of the block to be simulated and related conditioning values in both support scales, is used to infer additional high-order statistics from a training image. First, the method is tested in a controlled environment, and the simulated realizations show consistent results reproducing major structures and high-order relations of data. Second, the method is used to simulate a gold deposit, and its efficiency is demonstrated by reproducing spatial statistics up to a fourth-order, coinciding with the ones present in the available drillhole data. The running time of generating one realization with the proposed approach is reduced by a factor of 5 when compared to the point-support version of the algorithm.The second part of the thesis presents a case study where the simulations of the gold deposit mentioned above are incorporated into the simultaneous optimization of a gold mining complex. The resulting life-of-mine (LOM) production schedule yields 5 to 16% higher net present value when compared to the case, where the same mining complex is optimized, but the deposit is modelled through a traditional simulation method based on two-points statistics. The comparison shows that incorporating simulations with more realistic connectivities of high-grade blocks, through the use of high-order direct block simulations, into the optimization results in a more informed LOM production schedule. This shows that the optimization can capitalize on the better understanding of the connectivity of high-grades.</description><creator>de Carvalho, João Pedro</creator><contributor>Roussos G Dimitrakopoulos (Internal/Supervisor)</contributor><date>2019</date><subject>Mining and Materials</subject><title>High-order direct block support simulation and application at a gold mining complex</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/7w62fb58n.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/ks65hf44t</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Mining and Materials</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:j3860913b</identifier><datestamp>2020-03-21T05:00:27Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Introduction: Enhanced Recovery Pathways (ERP) include multiple evidence-based interventions in the preoperative, intraoperative and postoperative periods that together reduce morbidity and length of stay after colorectal surgery. Increased adherence to these interventions is associated with better postoperative outcomes, but adherence is lower in the postoperative period. Some postoperative elements require patient participation and educating patients is a recommended strategy to improve engagement and adherence. Mobile digital applications are increasingly used in health care education. The objective of this research is to estimate the extent to which a novel mobile app affects adherence to an ERP for colorectal surgery in comparison to standard written material. Methods: This was a superiority, parallel-group, assessor-blind, sham-controlled randomized trial involving patients undergoing colorectal resection in a single institution. All participants received standard preoperative education during a visit with a specialized ERP nurse, including an illustrated brochure designed by patient education specialists. On the day of surgery, participants were randomly assigned with a 1:1 ratio into one of two groups: (1) iPad including a novel mobile device app for postoperative education and self-assessment of recovery, or (2) iPad without the app. The primary outcome measure was mean adherence (%) to a bundle of 5 postoperative ERP elements requiring patient participation on postoperative days 1 and 2: mobilization, chewing gum for gastrointestinal motility stimulation, breathing exercises, consumption of oral liquids and consumption of nutritional drinks. Secondary outcomes included: (1) length of hospital stay, (2) postoperative complications, and (3) patient satisfaction. Results: Ninety-seven patients completed the study, 50 in the intervention group and 47 in the control group. There was high utilization of the app; 94% of the patients used the app on POD 0 and 82% on POD 1. Median (IQR) hospital stay was 4 days (2-6) in the intervention group and 3 days (2-5) for the control (p=0.33). There was no difference in mean overall adherence to the bundle between the intervention and the control groups (59% vs 62%, p= 0.5). After adjustment for confounders, the impact of the app on overall adherence remained not significant (Coefficient 2.4 (95%CI -5 to 10) p= 0.53). Patient experience was similar in both groups with an overall median score of 4 out of 5 in degree of feeling well-informed, motivation, confidence and satisfaction with care. Conclusions: In this randomized trial, use of a mobile health application did not improve adherence to a well-established ERP in colorectal surgery patients, when compared to standard written patient education.  App usage was high in the first 2 postoperative days suggesting an interest in the use of the technology. Future research should evaluate the impact of applications integrating novel behavioral change techniques that create a more personalized approach to patients and its use should be particularly assessed in contexts where adherence is low.</description><description>Introduction: Les voies de rétablissement améliorées (ERP) comprennent de multiples interventions fondées sur des preuves dans les périodes préopératoires, peropératoires et postopératoires, qui, ensemble, réduisent la morbidité et la durée du séjour après une chirurgie colorectale. Une adhérence accrue à ces interventions est associée à de meilleurs résultats postopératoires, mais l'adhérence est moindre dans la période postopératoire. Certains éléments postopératoires nécessitent la participation du patient et l'éducation des patients est une stratégie recommandée pour améliorer l'engagement et l'adhérence. Les applications numériques mobiles sont de plus en plus utilisées dans l'enseignement des soins de santé. L'objectif de cette recherche est d'estimer dans quelle mesure une nouvelle application mobile affecte l'adhésion à un ERP pour la chirurgie colorectale en comparaison à un document écrit standard. Méthodes: Un essai randomisé, contrôlé par un évaluateur à l'aveugle, et en groupes parallèles de supériorité, impliquant des patients subissant une résection colorectale dans un seul établissement. Tous les participants ont reçu une formation préopératoire standard lors d'une visite avec une infirmière spécialisée en ERP, y compris une brochure illustrée conçue par des spécialistes de l'éducation des patients. Le jour de la chirurgie, les participants ont été assignés au hasard avec un ratio de 1: 1 dans l'un des deux groupes suivants: (1) iPad incluant une nouvelle application mobile pour l'éducation postopératoire et l'auto-évaluation de la récupération, ou (2) iPad sans l'application . Le résultat primaire était l'observance moyenne (%) à un ensemble de 5 éléments ERP postopératoires nécessitant la participation du patient  aux jours 1 et 2 postopératoires: mobilisation, gomme à mâcher pour la stimulation de la motilité gastro-intestinale, exercices respiratoires, consommation de liquides oraux et consommation de boissons nutritionnelles. Les critères secondaires comprenaient: (1) la durée de l'hospitalisation, (2) les complications postopératoires et (3) la satisfaction des patients. Résultats: Quatre-vingt-dix-sept patients ont terminé l'étude, 50 dans l'intervention et 47 dans le groupe témoin. Il y avait une utilisation élevée de l'application, avec 94% des patients utilisant l'application sur POD 0 et 82% sur POD 1. La durée d'hospitalisation médiane (IQR) était de 4 jours (2-6) dans le groupe d'intervention et de 3 jours (2 -5) pour le contrôle (p = 0,33). Il n'y a pas eu de différence dans l'observance globale moyenne au groupe entre l'intervention et les groupes de contrôle (59% vs 62%, p = 0,5). Après ajustement pour les facteurs de confusion, l'impact de l'application sur l'observance globale n'a pas été significatif (coefficient 2,4 (IC à 95% -5 à 10) p = 0,53). L'expérience des patients était similaire dans les deux groupes avec un score médian global de 4 sur 5 en ce qui concerne le degré de bien-être, de motivation, de confiance et de satisfaction à l'égard des soins. Conclusions: Dans cet essai randomisé, l'utilisation d'une application mobile de santé n'a pas amélioré l'adhésion à une voie de rétablissement améliorée bien établie chez les patients ayant subi une chirurgie colorectale, par rapport à la formation standard écrite des patients. L'utilisation des applications était élevée au cours des deux premières journées postopératoires, suggérant un intérêt pour l'utilisation de la technologie. Les recherches futures devraient évaluer l'impact des applications intégrant de nouvelles techniques de changement comportemental qui créent une approche plus personnalisée avec les patients, et son utilisation devrait être particulièrement évaluée dans des contextes où l'adhésion est faible.</description><creator>Mata Gutierrez, Juan</creator><contributor>Liane S Feldman (Internal/Supervisor)</contributor><contributor>Julio Flavio Fiore Junior (Internal/Cosupervisor2)</contributor><date>2019</date><subject>Surgery</subject><title>A mobile application to improve adherence to an Enhanced Recovery Pathway after colorectal surgery</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/ht24wm471.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/j3860913b</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Surgery</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:47429c578</identifier><datestamp>2020-03-21T05:00:28Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les virus constituent une menace permanente et en constante évolution. Bien que le virus de l'immunodéficience humaine 1 (VIH-1) soit répandu depuis des décennies, le virus n'a été identifié que pour la première fois au début des années 1980. D'autre part, le virus Zika (ZIKV) a été identifié à la fin des années 1940, mais il n'a été révélé au public que lors des récentes épidémies. Bien que ces virus soient très distincts, leur point commun est le manque de traitements curatifs efficaces. L'infection au VIH-1 est traitée avec des médicaments antirétroviraux, mais ces derniers sont incapables d'éradiquer le virus des réservoirs latents. Une stratégie proposée pour une guérison consiste à réactiver l'expression du VIH-1 par les cellules infectées de manière latente et de cibler et tuer ces cellules par des médicaments conventionnels et / ou une forte réponse immunitaire. Ces cellules latentes sont réactivées à l'aide d'agents de réversion de la latence appelés LRA. Pour notre premier objectif, nous avons caractérisé l'efficacité et la toxicité de divers LRA dans un nouveau modèle de la latence du VIH-1 à la fois dans les lymphocytes CEM, les monocytes THP-1 et les macrophages dérivés des monocytes THP-1. Ensuite, en utilisant ce modèle, nous avons séparé les cellules réactivées des cellules non réactivées et isolé l'ARN total de ces deux populations cellulaires. Ces ARN ont été envoyé pour séquençage (RNA-seq) afin d'obtenir un profil d'expression différentielle des ARNm, des microARN et des longs ARN non codants. Ces ARN seront étudiés pour leur implication dans le maintien ou la perturbation de la latence du VIH-1. Alors que le ZIKV est généralement associé à des symptômes non spécifiques, des épidémies récentes ont impliqué le virus dans le développement du syndrome de Guillain-Barré chez l'adulte et la microcéphalie chez les bébés nés de mères infectées. De plus, certaines de ces épidémies ont infecté de larges portions de la population. Ceci suggère que des mutations récentes du virus ont conduit à augmenter sa pathogénicité et sa propagation. En outre, il n'existe actuellement aucun vaccin ou traitement approuvé pour ce virus. De petits ARN interférents (siARN) ont été utilisés pour inhiber la réplication de nombreux virus. Récemment, l'administration d'un siARN ciblant le virus Ebola a conféré une protection contre le virus chez les macaques rhésus, cependant aucun siARN n'a été développé contre le virus Zika. Dans La deuxième partie de ce projet, nous avons étudié deux souches de ZIKV: une souche brésilienne issue d'une épidémie récente, et une souche thaïlandaise dont la lignée est distincte de celles associées à des complications neurologiques. Nous avons comparé leurs effets cytopathiques et leurs capacités à générer de l'ARN viral et des virus infectieux. Par la suite, nous avons conçu et sommes toujours en train de valider des siARN ciblant le génome du ZIKV qui pourraient prévenir la propagation du virus.</description><description>Viruses are an ongoing and evolving threat to public health. Although Human Immunodeficiency Virus 1 (HIV-1) has been widespread for decades, the virus was only first identified in the early 1980s. Zika virus (ZIKV), on the other hand, was identified in the late 1940s, but was only brought to the public eye in recent outbreaks. While these viruses are very distinct, they both lack effective curative strategies. HIV-1 is treated with antiretroviral drugs, but these drugs are unable to eradicate the virus from latent reservoirs. A proposed strategy for a cure involves reactivating latently infected cells with the use of Latency Reversing Agents (LRA) so that these viruses can be targeted by conventional drugs and/or a strong immune response. For our first aim, we have characterized a new model of HIV-1 latency in CEM lymphocytes, THP-1 monocytes, and THP-1 monocyte-derived macrophages to test the efficacy and toxicity of various LRA. Then, using this model, we have separated reactivated cells from non-reactivated cells and isolated the corresponding RNA, which has been sent for RNA sequencing to build a differential expression pattern of mRNA, miRNA, and long non-coding RNAs. These RNAs will be studied for their implication in the maintenance or disruption of HIV-1 latency. While ZIKV is generally associated with non-specific symptoms, recent outbreaks have implicated the virus in the development of Guillain-Barré syndrome in adults and microcephaly in babies born from infected mothers. Furthermore, several of these outbreaks have infected large portions of the population. This would suggest that recent mutations in the virus have led to increased pathogenicity and spread. Furthermore, there is currently no approved vaccine or treatment for this virus. Small interfering (si)RNAs have been used to inhibit the replication of many viruses. Recently, the delivery of a siRNA targeting Ebola virus has been shown to be protective against the virus in rhesus macaques, but there has been no siRNA developed against ZIKV. In the second aim of this project, we have studied two strains of ZIKV: a Brazilian strain from a recent outbreak, and a Thai strain whose lineage is distinct from those associated with neurological complications. We compare their cytopathic effect and ability to generate viral RNA and viral titers. Next, we have designed and are in the process of validating siRNA(s) targeting the ZIKV genome that could be used in to prevent the spread of the virus.</description><creator>McCullogh, Craig</creator><contributor>Anne Gatignol (Internal/Supervisor)</contributor><date>2019</date><subject>Microbiology &amp; Immunology</subject><title>Exploring cure strategies for viruses</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/2v23vw30t.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/47429c578</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Microbiology and Immunology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:5m60qv19j</identifier><datestamp>2020-03-21T05:00:28Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>La recherche qui fait l'objet de ce mémoire a pour but d'explorer l'estimation des paramètres de dépendance d'un modèle de copule Gaussienne pour données dis- crètes. Les liens entre les modèles de copule Gaussienne et les modèles graphiques sont analysés. En s'appuyant sur les travaux antérieurs de Nešlehová (2007) et Popovic et al. (2018), trois nouveaux algorithmes sont proposés. La première méthode se base sur les bandit manchots, la deuxième utilises une approximation à la limite variationnelle supérieure et la troisième apprend le modèle de copule à l'aide d'un modèle génératif profond (Kingma and Welling, 2013). Les trois algorithmes utilisent une nouvelle factorization de matrice et il est ensuite démontré que cette décomposition a des propriétés théoriques intéressantes. Finalement, la performance des méthodes est comparée à celle des algorithmes existants sur des données deux et trois dimensionnelles simulées.</description><description>The purpose of this thesis is to investigate parameter estimation in a multivariate Gaussian copula model with discrete marginal distributions. Connections between Gaussian copula models and probabilistic graphical models are analysed. Building upon theoretical results in Nešlehová (2007) and Popovic et al. (2018), three novel algorithms are proposed. The first method relies on multi-armed bandits, the second uses a variational upper bound approximation and the third leverages recent advances in deep generative models (Kingma and Welling, 2013). All three methods rely on a novel full rank decomposition of square real matrices which is shown to have desirable theoretical properties. The performance of all three algorithms is compared to existing algorithms on two and three dimensional simulated datasets.</description><creator>Mazoure, Bogdan</creator><contributor>Johanna Neslehova (Internal/Supervisor)</contributor><date>2019</date><subject>Mathematics and Statistics</subject><title>Estimating the structure of probabilistic graphical models through a Gaussian copula with discrete marginals</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/6h440v923.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/5m60qv19j</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Mathematics and Statistics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:sx61dp62m</identifier><datestamp>2020-03-21T05:00:29Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>La chute des prix des nouvelles technologies de séquençage (NGS) s'est accompagnée de leur utilisation accrue, en recherche et en clinique. L'interprétation toujours meilleure des génomes humains peut permettre le développement de meilleures stratégies de prévention, de diagnostic et de traitement des maladies. Des investissements significatifs ont vu le jour dans de nombreux pays industrialisés en vue de réaliser les promesses de la médecine personnalisée. Cependant, le séquençage du génome complet de patients n'est offert en tant que test clinique que dans un nombre très limité d'établissements de santé dans le monde. La France et le Québec ont investi de manière considérable dans la recherche en génomique. Cependant, des décisions stratégiques doivent encore être prises quant à l'implémentation clinique des technologies NGS dans ces deux juridictions. Dès lors, l'objectif de ce projet est de contribuer à l'ensemble des preuves et faits à la disposition des décideurs publics. Nous avons focalisé notre attention sur deux technologies, le séquençage de l'exome (whole-exome sequencing, WES) et du génome complet (whole-genome sequencing, WGS). Notre objectif était d'établir si l'utilisation efficace et responsable du WES/WGS pouvait être mise en péril par des lacunes dans les politiques publiques ou cadres règlementaires et normatifs applicables. A l'heure actuelle, l'interprétation clinique de la séquence génomique ou exomique d'un patient nécessite l'intervention de nombreuses parties prenantes, y compris des chercheurs qui utilisent des outils, procédés et normes développés dans le cadre de la recherche pour analyser les données NGS. En parallèle, les cadres normatifs existants ont été construits pour accommoder les données génétiques, mais n'abordent pas la question des données génomiques. Notre hypothèse est que ces éléments créent un besoin de standardisation, qui pourrait requérir des adaptations du cadre normatif. Nous avons répondu à trois questions de recherches : (1) Quels enjeux les utilisateurs de technologies NGS soulèvent-il à propos de leur utilisation en clinique ? Pour répondre nous avons fait une étude systématique de la littérature. (2) Comment les données NGS de patients sont-elles à l'heure actuelle par des institutions de santé en France et au Québec ? Pour répondre nous avons réalisé une étude de cas multiples.(3) Y a-t-il des lacunes dans les cadres normatifs qui devraient être comblées pour assurer l'utilisation responsable, efficace et standardisée des données NGS en clinique ? Pour répondre nous avons fait une revue narrative des cadres applicables en France et au Quebec. Dans notre étude systématique de la littérature, nous avons identifié 23 enjeux différents liés l'utilisation de données NGS de patients. Nous avons aussi trouvé que de nombreux utilisateurs des technologies NGS appelaient à ce que les pratiques soient standardisées avant l'introduction de WES/WGS en clinique. De plus, de nombreux ajustements infrastructurels devront être fait pour que des institutions de santé puissent accommoder le stockage, et l'interprétation de données massives et complexes en génomique. A travers notre étude de cas, nous avons découvert qu'en plus de la gestion de nombreux niveaux de complexité des données NGS, il est nécessaire d'obtenir l'appui de nombreuses parties-prenantes avant de pouvoir offrir le WES/WGS aux patients. Organiser cela à l'échelle nationale ne peut pas se faire sans un engagement politique fort aux plus hauts niveaux de l'Etat. Enfin, notre étude des cadres normatifsa montré qu'ils étaient très protecteurs des personnes, et pourraient nécessiter des ajustements mineurs pour accommoder les tests génomiques. En revanche, nous avons aussi pu conclure que la médecine génomique ne pourrait pas être mise en place sans un engagement politique, ainsi que des investissements monétaires et infrastructurels forts, qui ne sont que partiellement présents actuellement en France et au Québec.</description><description>The decreasing cost of next-generation sequencing (NGS) technologies has resulted in their increased use in research, and in the clinical context. Indeed, the correct interpretation of a human genome can enable better prevention, diagnosis and treatment strategies. Significant public investments in NGS have been made in developed nations to realise the promise of personalized medicine. Yet, today the sequencing and analysis of a patient's exome or genome is only offered as a clinical test in a few clinics around the world. France and Quebec have made sizable investments in genomics research, and France announced the launch of a genomic medicine plan in 2016. However, policy decisions still have to be made on the nation-wide clinical implementation of NGS technologies in both jurisdictions. Therefore, this project's objective was to contribute to the body of evidence available to policymakers in France and Quebec on the clinical implementation of NGS technologies. We focused our attention on two specific NGS technologies, namely Whole Genome Sequencing (WGS), and Whole Exome Sequencing (WES). We specifically aimed to assess if the responsible and efficient use of WES/WGS data in the context of clinical care could be impeded by policy gaps. Currently, the clinical interpretation of a patient's genome sequence data is done through the intervention of many stakeholders including basic science researchers. These researchers use bioinformatics tools, processes and norms developed for research to filter and analyse patients NGS data. In parallel, existing regulatory and normative frameworks have been developed for the use of genetic data, and include no clear definition of genomic data or genomic technologies. We hypothesised that these elements create a strong need for standardization of practices, and may require adaptations of current regulatory and normative frameworks to the context of NGS. We therefore aimed to answer three research questions: (1) What issues do technology users experience and foresee when using WES data to inform patient care? To answer this, we performed a systematic review of the literature.(2) How are patients' NGS data currently managed (produced, analysed, interpreted and shared) in clinical institutions in Quebec and in France? We answered this by performing a case studies analysis, interrogating key stakeholders directly involved in managing patients' NGS data in France and Quebec. (3) Are there gaps in the current regulatory and normative frameworks which should be addressed to enable a responsible and efficient standardized use of NGS data in the clinic? To answer this, we performed a narrative review of the currently applicable normative frameworks in France and in Quebec.In our systematic literature review, we identified 23 distinct challenges linked to the production, analysis, reporting and sharing of patients' WES data. We also found that technology users were calling for practices to be more standardized before NGS was offered as a clinical test, and that numerous infrastructural adjustments had to be made in order for healthcare institutions to accommodate the vase amounts of highly complex NGS data. Through our case study analysis, we showed that in addition to managing the various levels of complexities of producing, analysing and sharing complex NGS data, a significant buy-in from numerous stakeholders was necessary in order to offer clinical genomics to patients. At the National level, this cannot be done without a strong political will. Finally, through our normative frameworks analysis, we concluded that existing frameworks were highly protective of patients and research participants, and could need marginal adjustments in order to accommodate for NGS tests. However, we also concluded that clinical genomics could not be realized without political will, and sustained monetary and infrastructural investments, which are only partly present at the moment in France and Quebec.</description><creator>Bertier, Gabrielle</creator><contributor>Anne Cambon-Thomsen (Supervisor2)</contributor><contributor>Yann Joly (Supervisor1)</contributor><date>2019</date><subject>Human Genetics</subject><title>Clinical implementation of next-generation sequencing technologies in France and Quebec: A multidisciplinary analysis of policy implications</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/wm117r18k.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/sx61dp62m</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Human Genetics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:5138jg94k</identifier><datestamp>2020-03-21T05:00:31Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Face aux pénuries en eau et une rapide croissance démographique, chacun problématique dans plusieurs pays en voie de développement, la réutilisation des eaux usées pour l'irrigation offre une alternative de gestion permettant de conserver les ressources en eau. Cependant, l'irrigation avec des eaux usées insuffisamment ou non traitées présente un risque à la santé humaine puisque plusieurs contaminants, dont les métaux lourds, peuvent se retrouver dans ou sur les parties comestibles de diverses cultures tels les pommes de terre (Solanum tuberosum L.) et épinards (Spinacia oleracea L.).La présente étude visa à éclaircir le rôle des sorbants biologiques dans la restauration des sols et cultures contaminés avec des métaux lourds par voie de leur irrigation avec des eaux usagées non-traitées. Des cultures comestibles furent choisis afin de mieux évaluer l'effet du système racinaire sur l'assimilation de métaux lourds par une culture. Les pommes de terre furent plantées, irrigués aux 10 jours, des échantillions de lixiviat prélevés, puis des échantillons de sol prélevés 2 jours après chaque irrigation. Tous les métaux lourds s'accumulèrent dans la couche supérieure du sol: le Fe, Pb, et Zn étant détectés à une profondeur de 0.1 m, puis le Fe détecté jusqu'à une profondeur de 0.3 m. Aucun métal lourd ne fut détecté dans le lixiviat. Les pommes de terre furent récoltées à maturité et séparés en chair, pelure, feuille, tige et racine. On retrouva des métaux lourds à travers toutes les plants de pomme de terre, mais à des niveaux plus bas pour les plants irrigués avec de l'eau douce plutôt que des eaux usées. Par rapport au témoin sans biochar, l'ajout de biochar au sol diminua de façon significative (p&lt;0.05) les teneurs en Cd et Zn de la chaire des tubercules (69% et 33%, respectivement) ainsi que  celle  des pelures. Après la seconde saison de culture, l'ajout de biochar diminua (p&lt;0.05) les teneurs en Cd, Cu, Cr, Pb and Zn de la chair comestible.Comme exemple de culture en surface, des épinards furent plantés dans les lysimètres. Cette culture fut irriguée aux 10 jours et les feuilles échantillonnées à deux occasions pour une analyse de métaux lourds. L'ajout de biochar au sol améliora son CEC et augmenta son pH, donnant lieu à une réduction de 42% du Zn dans les feuilles. L'effet d'un ajout de biochar au sol n'eut qu'un effet minime sur le mouvement des autres métaux lourds (Cd, Cu, Cr, Fe, and Pb) vers les feuilles d'épinards, le résultat probable de leur compétition avec d'autres cations dans la solution du sol.L'effet d'un ajour de biochar sur le rendement des pommes de terre fut évalué sur une période de deux saisons. Il est à noter que ce manque de différence en rendements (avec/sans biochar) eu lieu même si les différentes parties des plants ayant reçu des eaux usées montrèrent tous une accumulation de métaux lourds significativement (p&lt;0.05) plus élevé que dans les plants irrigués avec de l'eau douce. Ceci pourrait être préoccupant, puisqu'un producteur pourrait atteindre le rendement attendu, tout en ayant produit des pommes de terre insalubres sans le savoir. Il est à conclure que l'ajout de biochar de pelures de plantain au sol à un taux de 1% n'eut aucun effet significatif, ni sur l'aspect santé des pommes de terre, ni sur leur rendement. L'ajout du biochar, augmenta le pH et la CEC du sol, mais ces impacts positifs furent masqués par une suffisance d'éléments nutritifs dans le sol.En général, l'ajout de biochar démontra un potentiel élevé pour immobiliser des métaux lourds dans le sol, réduisant ainsi leur assimilation par les plantes. L'accumulation de métaux lourds dans le sol et leur assimilation et distribution à travers la plante différa selon l'espèce cultivée. Potentiellement, l'ajout de biochar au sol permettrait donc une utilisation plus sécuritaire des eaux usées pour l'irrigation de certaines cultures. </description><description>In many developing countries, water scarcity and the growing population are becoming problematic. Therefore, the reuse of wastewater for irrigation provides an alternative management option. Irrigation with poorly treated or untreated wastewater  could, however,  pose risk to human health due to the presence of a wide range of contaminants, including heavy metals, which can move into the edible parts of various crops such as  potatoes (Solanum tuberosum L.) and spinach (Spinacia oleracea L.).The study aimed to investigate biosorbent role in the remediation of heavy metals in soil and crops irrigated with untreated wastewater. Both aboveground and belowground crops were selected to better assess the effect of rooting system on the plant uptake of heavy metals.To achieve this goal, a field lysimeter experiment was undertaken to elucidate the fate and transport of six water-borne heavy metals (Cd, Cr, Cu, Fe, Pb and Zn) in irrigation water applied to potatoes (cv. Russet Burbank) and spinach grown on a sandy soil. Plantain peel biochar (1% w/w) was incorporated in the top 0.1 m of soil. All the control and biochar treatments were replicated three times in a completely randomized design carried out on nine outdoor PVC lysimeters (1.0 m height × 0.45 m diameter).In a two-year study, potatoes were planted, irrigated at 10-day intervals, leachate samples were collected, followed by soil samples collected two days after each irrigation. Results showed that all heavy metals accumulated in the top soil; Fe, Pb and Zn were detected at 0.1 m depth; while only Fe was detected at 0.3 m depth. No heavy metals were detected in the leachate. Matured potatoes were harvested and separated into flesh, peel, leaf, stem and root. Results indicated that heavy metals translocated to all parts of the potato plant. The heavy metals were relatively low in the potato parts under freshwater (vs. wastewater). Biochar-amended-soil significantly (p&lt;0.05) reduced only Cd and Zn in tuber flesh (69% and 33%, respectively) and peels compared to the non-amended wastewater control. Interestingly, biochar amendment, after the second season significantly (p&lt;0.05) reduced Cd, Cu, Cr, Pb and Zn in the edible flesh.As an example of aboveground crops, spinach was planted in lysimeters, irrigated every 10 days and harvested twice for heavy metal analysis. Results showed that biochar amendment improved CEC and increased the pH of the soil, which resulted in a 42% reduction of Zn in spinach leaves. The impact of biochar on translocation of other heavy metals (Cd, Cu, Cr, Fe, and Pb) to spinach leaves was at minimum, possibly due to competition with other compounds in the soil solution.The effect of biochar on potato yield was also studied for the two seasons. It was found that: (1) In the first season, the yield was significantly less in the biochar treatment, possibly due to germination delay in the biochar amended lysimeters. (2) In the second season with no germination delay observed, the yield was similar in the presence and absence of biochar. Of note, yields were not affected even though significantly higher (p&lt;0.05) heavy metals were taken up by different parts of the potato plants under wastewater irrigation (vs. freshwater). This can be alarming to some degree as the farmer may be getting the expected yield but with unhealthy potatoes, and not realizing this at all. It was concluded that effect of plantain peel biochar on the plant health parameters and yield of potatoes was not significant.Overall, biochar amendment, with improved pH and CEC, showed high potential in the immobilization of heavy metals in soil, thereby reducing their uptake by plants. Therefore, the application of biochar as a soil amendment could result in a safer use of wastewater irrigation for crops. The accumulation of heavy metals in soil and uptake by plant parts, however, were crop dependent.</description><creator>Nzediegwu, Christopher</creator><contributor>Shiv Prasher (Supervisor)</contributor><date>2019</date><subject>Bioresource Engineering</subject><title>The role of biochar in enhancing safe use of untreated wastewater in agriculture</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/p8418q49z.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/5138jg94k</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Bioresource Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:nc580p871</identifier><datestamp>2020-03-21T05:00:31Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Introduction:  Cardiovascular disease (CVD) is one of the leading causes of death in women. Loss of ovarian function and endogenous estrogen deficiency predisposes postmenopausal women to CVD and osteoporosis. Calcium has been recommended for the prevention and treatment of osteoporosis. However, the effect of calcium intake, both from supplements as well as dietary sources, on cardiovascular (CV) health remains uncertain and largely dependent on study design and population. Carotid intima-media thickness (cIMT), arterial stiffness and hemodynamic parameters can detect CVD at a very early stage with high predictive value. Therefore, in this study, we examined the association between dietary calcium intake and CV markers, including cIMT, arterial stiffness and hemodynamics, and serum lipids, in healthy postmenopausal women. Methods: Healthy postmenopausal women without CV risk factors and not taking calcium or vitamin D supplements were included in this study. Ninety-six postmenopausal women were included for vascular assessment (Main Study), whereas 80 participants were included for assessment of serum lipids (Secondary Study). Dietary calcium (dCa) and dietary vitamin D (dvitD) intake were evaluated by a validated food frequency questionnaire to estimate usual intake in the previous month. All participants underwent cIMT, as well as arterial stiffness measurements, including carotid to femoral pulse wave velocity (cfPWV) and other hemodynamic measurements in the early morning. Fasting blood samples were collected for assessment of serum biomarkers, including lipids. CV markers were compared across &lt;600, 600-1000 and &gt;1000 mg/d dCa intake. We performed an exploratory analysis of 600-1000 mg/d dCa group as this group had the most favourable vascular markers. Furthermore, we conducted a subgroup analysis comparing CV parameters across &lt;800, 800-1000 and &gt;1000 mg/d dCa intake as the group with 800-1000 mg/d dCa showed the best CV marker values in the exploratory analysis. Results:  The mean (±standard deviation) age and body mass index of our study population were 60.2±6.3 years and 25.6±3.9 kg/m2, respectively. Although statistically non-significant, we noted favorable values of CV markers in 600-1000 mg/d dCa group compared to the extreme groups (&lt;600 mg/d or &gt;1000 mg/d) in the primary analysis.Although there was no significant associations, we noted a tendency for improved CV marker values in those with &lt;1000 mg/d vs. &gt;1000 mg/d dCa, as well those in the middle groups of (600-1000 mg/d or 800-1000 mg/d) compared to the extreme groups (&lt;600 mg/d, &lt;800 mg/d or &gt;1000 mg/d in further analyses. Conclusion:  Although the present study does not suggest significant associations between dCa and CV markers in healthy postmenopausal women, our results indicate that mid-spectrum levels of dCa intake, as compared to the lower and higher extremes, might be associated with better values of CV markers. Our ongoing randomized controlled trial will allow for a better evaluation of the effect of dCa on CV health.</description><description>Introduction: La maladie cardiovasculaire (MCV) est l'une des principales causes de décès chez les femmes. La perte de la fonction ovarienne et la carence en œstrogènes endogènes prédisposent les femmes ménopausées aux maladies cardiovasculaires et à l'ostéoporose. Le calcium a été recommandé pour la prévention et le traitement de l'ostéoporose. Cependant, l'effet de l'apport en calcium, provenant à la fois des suppléments et des sources alimentaires, sur la santé cardiovasculaire (CV) demeure incertain et dépend largement de la conception de l'étude et de la population. L'épaisseur de l'intima-média carotidienne (EIMc), la rigidité artérielle et les paramètres hémodynamiques, ainsi que certains lipides sériques peuvent détecter la MCV à un stade très précoce avec une valeur prédictive élevée. Par conséquent, dans cette étude, nous avons examiné l'association entre l'apport alimentaire en calcium et les marqueurs CV, y compris la EIMc, la rigidité artérielle et les paramètres hémodynamique, et les lipides sériques, chez les femmes ménopausées en bonne santé.Méthodes: Des femmes ménopausées en bonne santé sans facteurs de risque CV et ne prenant pas de suppléments de calcium ou de vitamine D ont été incluses dans cette étude. Quatre-vingt-seize femmes ménopausées ont été incluses pour l'évaluation vasculaire (étude principale), tandis que 80 participantes ont été incluses pour l'évaluation des lipides sériques (étude secondaire). L'apport en calcium alimentaire (CaA) et en vitamine D alimentaire (vitDA) a été évalué à l'aide d'un questionnaire de fréquence alimentaire validé pour estimer l'apport habituel au cours du mois précédent. Toutes les participantes ont subi une évaluation de l'EIMc, ainsi que des mesures de rigidité artérielle, y compris la vitesse de prpopagation l'onde de pouls carotido-fémorale  et d'autres mesures hémodynamiques tôt le matin. Des échantillons de sang à jeun ont été prélevés pour l'évaluation des biomarqueurs sériques, y compris les lipides. Les marqueurs CV ont été comparés entre &lt;600, 600-1000 et &gt;1000 mg/jour CaA. Nous avons effectué une analyse exploratoire de 600-1000 mg / j groupe dCa car ce groupe avait les marqueurs vasculaires les plus favorables. De plus, nous avons effectué une analyse en sous-groupes comparant les paramètres CV pour des apports en dCa &lt;800, 800-1000 et&gt; 1000 mg / j, le groupe présentant 800-1000 mg / j dCa présentant les meilleures valeurs de marqueur CV dans l'analyse exploratoire. Résultats: L'âge moyen (± écart-type) et l'indice de masse corporelle de notre population étudiée étaient de 60,2 ± 6,3 ans et de 25,6 ± 3,9 kg/m2, respectivement. Bien que statistiquement non significatif, nous avons noté des valeurs favorables de marqueurs CV dans 600-1000 mg / j groupe dCa par rapport aux groupes extrêmes (&lt;600 mg / j ou&gt; 1000 mg / j) dans l'analyse primaire. Bien qu'il n'y ait pas d'associations significatives, nous avons noté une tendance à l'amélioration des valeurs des marqueurs CV chez les sujets ayant &lt;1000 mg / j vs&gt; 1000 mg / j dCa, ainsi que ceux des groupes du milieu (600-1000 mg / j ou 800 -1000 mg / j) par rapport aux groupes extrêmes (&lt;600 mg / j, &lt;800 mg / j ou&gt; 1000 mg / j dans d'autres analyses. Conclusion: Bien que la présente étude ne suggère pas d'associations significatives entre le CaA et les marqueurs CV chez les femmes ménopausées en bonne santé, nos résultats indiquent que des niveaux moyens de l'apport en CaA, comparativement aux extrêmes inférieurs et supérieurs, pourraient être associés à de meilleures valeurs des marqueurs CV. Notre essai randomisé contrôlé présentement en cours permettra une meilleure évaluation de l'effet du CaA sur la santé cardiovasculaire.</description><creator>Das, Shubhabrata</creator><contributor>Styliani Stella Daskalopoulou (Supervisor1)</contributor><contributor>Suzanne Morin (Supervisor2)</contributor><date>2019</date><subject>Medicine</subject><title>Association of dietary calcium intake and cardiovascular markers in healthy postmenopausal women</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/rx913s12r.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/nc580p871</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Medicine</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:vx021h44b</identifier><datestamp>2020-03-21T05:00:32Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les modèles orientés buts permettent d'obtenir, de spécifier, d'analyser et de valider des exigences parce qu'ils décrivent les représentations hiérarchiques des exigences système, des solutions possibles, des objectifs des parties prenantes et leurs relations. Lors de la phase initiale des exigences, les modèles orientés buts aident les ingénieurs en exigences à comprendre les buts des parties prenantes et à explorer des solutions alternatives en fonction de leur impact sur ces buts. Malgré ces avantages de la modélisation orientée buts, la réutilisation des modèles de buts a reçu peu d'attention dans la communauté de modélisation de buts pour diverses raisons. Nous soutenons que les exigences minimales à satisfaire pour la réutilisation des modèles de buts sont les suivantes: (i) assurer que l'analyse de compromis via l'évaluation du modèle orienté buts soit possible via la hiérarchie de réutilisation, (ii) fournir les moyens de reporter les décisions à un point ultérieur de la hiérarchie de réutilisation lorsque des informations plus complètes sont disponibles, tout en permettant à l'artefact réutilisable d'être analysé; (iii) tenir compte des contraintes imposées par d'autres notations de modélisation lors de l'évaluation de modèles orientés buts réutilisables; (iv) permettre à l'information dépendante du contexte d'être modélisée de manière à ce que le modèle orienté buts puisse être utilisé et analysé dans divers contextes d'application; et (v) enfin et surtout, offrir une interface bien définie pour la réutilisation. Cette thèse soutient que les approches de modélisation orientée buts existantes ne répondent pas entièrement à ces cinq capacités requises pour la réutilisabilité. Pour résoudre ce problème, cette thèse présente de nouveaux mécanismes d'évaluation de modèles orientés buts pour les évaluations ascendante et descendante basées sur un algorithme récursif paresseux. Ces nouveaux mécanismes permettent la conception, la modularisation, l'évaluation et l'analyse de modèles orientés buts réutilisables dans les hiérarchies de réutilisation en présence de contraintes externes prenant en compte les décisions différées et les informations contextuelles. Les extensions de métamodèle requises sont discutées et les outils de support démontrent la faisabilité de l'approche.</description><description>Goal models help elicit, specify, analyze, and validate requirements as they capture hierarchical representations of system requirements, possible solutions, stakeholder objectives, and their relationships. In the early requirements phase, goal models aid requirements engineers in understanding the goals of stakeholders and exploring solution alternatives based on their impact on these goals. Despite these strengths of goal modeling, reuse of goal models has received limited attention in the goal modeling community for various reasons. We argue that the requirements that need to be fulfilled at least for the reuse of goal models are as follows: (i) ensure that trade-off reasoning via goal model evaluation is possible through the reuse hierarchy, (ii) provide the means to delay decisions to a later point in the reuse hierarchy when more complete information is available, while still allowing the reusable artifact to be analyzed, (iii) take into account constraints imposed by other modeling notations when evaluating reusable goal models, (iv) allow context dependent information to be modeled so that the goal model can be used and analyzed in various application contexts, and (v) last but not the least, provide a well-defined interface for reuse. This thesis argues that existing goal modeling approaches do not fully address these five required capabilities for reusability. To address this issue, this thesis introduces novel goal model evaluation mechanisms for bottom-up and top-down evaluation based on a lazy, recursive algorithm. The novel mechanisms allow the design, modularization, evaluation, and analysis of reusable goal models in reuse hierarchies in the presence of external constraints taking delayed decisions and contextual information into account. Required metamodel extensions are discussed and tool support demonstrates feasibility.</description><creator>Duran, Mustafa Berk</creator><contributor>Gunter Mussbacher (Supervisor)</contributor><date>2019</date><subject>Electrical and Computer Engineering</subject><title>Reusable goal models</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/f1881p31r.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/vx021h44b</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:h128ng975</identifier><datestamp>2020-03-21T05:00:33Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>This thesis considers the utilization of numerical design optimization principles in biomedical engineering and vice versa to enrich the methodologies of the respective disciplines in order to obtain more efficient and effective problem solutions.Specifically, the first part of the thesis introduces the use of numerical optimization to generate optimal nanoparticle designs for cancer nanotherapy, as opposed to traditional empirical methods that are constrained by financial and temporal limitations. Computational models are used to evaluate treatment therapeutic and toxicological parameters. Since the gradients of the functions evaluated using computational models may be unavailable or cannot be approximated reliable, we employ derivative-free direct search optimization algorithms supported by convergence theory. Results revealed design solutions consisting of nanoparticle size, aspect ratio, and surface ligand density that maximize tumor targeting and minimize tumor diameter at the end of treatment. This part of the thesis shows the feasibility of applying optimization to achieve efficacious cancer treatments, and offers a quantitative paradigm to support clinical decision-making. In the second part of the thesis, a physiological mechanism is used to reduce the numerical complexity of an engineering design optimization problem.  In particular, we consider an air transportation system-of-systems design problem formulation that integrates aircraft sizing, fleet allocation, and route network configuration. The numerical complexity of the problem hinders its solution for large unstructured (as opposed to hub-spoke) networks. The bioinspired approach is used to eliminate parts of the optimization problem without loss of optimality, leading to significant numerical simplifications. The new approach is first validated for a 15-node network, which is the largest network size reported in the literature. We then obtain, report, and discuss results for a 20-node network, a size that is representative of the Canadian major airport landscape, for which results could not be obtained previously due to prohibitive computational cost and numerical difficulties. The presented bioinspired design methodology provides a paradigm for reducing complexity in the design of system-of-systems engineering with the potential of being useful in a wide range of applications.</description><description>La thèse suivante considère l'utilisation de principes d'optimisation de design numérique dans le domaine de l'ingénierie biomédicale et vice-versa, afin d'enrichir les méthodologies de ces disciplines, pour ensuite obtenir des solutions efficaces et effectives aux différents problèmes. Plus particulièrement, la première partie de la thèse présente l'utilisation de l'optimisation numérique afin de générer un design optimal de nanoparticules pour la nanothérapie dans le domaine du cancer, par opposition aux méthodes empiriques traditionnelles qui sont limitées par des contraintes financières et temporelles. Des modèles computationnels sont adoptés et utilisés pour évaluer les traitements thérapeutiques ainsi que les paramètres toxicologiques. Puisque les gradients des modèles computationnels sont indisponibles, et que leur obtention peut être non fiable, nous basons nos études d'optimisation sur une méthode sans dérivée; plus spécifiquement sur un algorithme direct de recherche avec des propriétésde convergence rigoureuses. Les résultats obtenus révèlent que la taille des nanoparticules, leur rapport de forme ainsi que la densité de surface du ligand maximisent le ciblage des tumeurs et minimisent le diamètre des tumeurs à la findu traitement. Cette partie de la thèse démontre la faisabilité de l'application de l'optimisation afin d'atteindre des traitements efficaces contre le cancer et offre une première étape vers un outil quantitatif pour soutenir la prise de décisions enclinique. Dans la deuxième partie de la thèse, un mécanisme physiologique est utilisé pour réduire la complexité numérique d'un problème d'optimisation de design en ingénierie. Plus particulièrement, nous prenons en considération la formulation d'un problème de design d'un système-de-systèmes de transports aériens qui intègre le dimensionnement de l'avion, l'allocation de la flotte et la configuration du réseau routier. La complexité numérique du problème empêche sa résolution pour de grands réseaux non-structurés (en opposition aux réseaux en étoile). Cette approche bio-inspirée est utilisée pour éliminer certaines parties du problème d'optimisation sans perte d'optimalité, conduisant à d'importantes simplifications numériques. La nouvelle approche est premièrement validée pour un réseau de 15 noeuds, qui représente le plus grand réseau décrit dans la littérature. Nous obtenons, rapportons, et discutons ensuite des résultats obtenus pour un réseau de 20 noeuds, une taille représentative du plus important paysage aéroportuaire canadien, pour lequel des résultats n'ont pu être obtenus précédemment, sans un cod¯t computationnel significatif et des difficultés numériques.</description><creator>Chamseddine, Ibrahim</creator><contributor>Michael Kokkolaras (Supervisor)</contributor><date>2019</date><subject>Mechanical Engineering</subject><title>Utilizing computational design optimization in cancer nanotherapy and biology system principles in air transportation: a successful demonstration of interdisciplinary research</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/6w924d90j.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/h128ng975</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:cr56n339b</identifier><datestamp>2020-03-21T05:00:34Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les systèmes de surveillance de l'état d'outil (TCM) sont essentiels pour réaliser l'avantage concurrentiel industriel souhaité, en termes de réduction des coûts, augmentation de la productivité, amélioration de la qualité et éviter d'endommager la pièce usinée. La condition de l'outil ne peut être prédite analytiquement en raison de la nature dynamique du processus dans un environnement industriel. Pour surmonter les limites des systèmes de TCM existants, un nouveau système TCM intelligent a été mis au point pour détecter avec précision les défaillances d'usure des outils, ainsi que la prédiction de brisure d'outils soudains avant d'endommager la pièce usinée. Le système analyse les caractéristiques du processus recueillies à partir de signaux des multi-capteurs en utilisant des méthodes avancées de traitement du signal et d'apprentissage automatique pour détecter l'état de l'outil dans les processus de coupe intermittents. Pour la détection de l'usure des outils, des algorithmes de traitement du signal et de prise de décision robustes et en temps réel ont été développés à l'aide de signaux de retour provenant du moteur d'entraînement de la broche. Le système extrait des caractéristiques générales descriptives, dans les domaines temporel et fréquentiel, détermine la condition de l'outil, puis communique une décision au contrôleur de la machine en présence de système de contrôle adaptif. L'approche proposée souligne l'effet de l'état de l'outil sur les caractéristiques extraites, tout en masquant les effets des paramètres de coupe tels que la vitesse de coupe, la vitesse d'avance, la profondeur de coupe, et la trajectoire et la géométrie de l'outil. Une telle capacité n'a jamais été atteinte auparavant. Les résultats ont indiqué la capacité de la technique de traitement à minimiser l'effort d'apprentissage du système d'au moins 75% et à détecter l'usure des outils au-dessus du seuil avec une précision supérieure à 95% et un niveau de confiance supérieur à 90%.Pour la prévision de défaillance soudaine, une nouvelle approche de traitement du signal a été développée pour la prédiction en ligne et la prévention de la brisure des outils lors de l'usinage intermittent. L'approche analyse les ondes d'émission acoustique associées à la génération de nouvelles surfaces lors de la propagation instable des fissures, qui précèdent la fracture de l'outil, dans le domaine temps-fréquence en utilisant la méthode de transformation de Hilbert-Huang. Les caractéristiques de la phase de pré-défaillance ont été identifiées à l'aide de la fonction de Teager-Kaiser Opérateur Énergétique et de la fonction de fenêtre Bartlett, qui distinguent les événements à haute énergie de la phase de pré-défaillance tout en abaissant toute autre variation de signal à basse énergie. Des résultats expérimentaux étendus, appuyés par une imagerie à grande vitesse des opérations de coupe et des brisures d'outils, ont démontré la précision du système développé pour prédire de manière cohérente la défaillance de l'outil de quatre à six engagements d'outils / pièces avant la brisure de l'outil. La sortie du système s'est également avérée indépendante des paramètres de coupe et du matériau de la pièce. Une corrélation entre la grandeur de fissure et les caractéristiques de pré-défaillance a été développée pour la prise de décision. Un processeur ultrarapide a été intégré au système TCM pour une prise de décision en temps réel dans un laps de temps très court (de l'ordre de 10 ms) pour les processus d'usinage à grande vitesse. Une communication du système développé avec le contrôleur de machine CNC a été mise en place. Le temps nécessaire au traitement du signal, à la prise de décision et à la communication avec le contrôleur de la machine permet d'arrêter le processus avant d'endommager une partie. Les résultats expérimentaux de la validation ont confirmé l'exactitude et la robustesse du système de MTC proposé. Aucun système similaire n'existe.</description><description>Tool condition monitoring (TCM) systems are essential to achieve the desired competitive advantage in manufacturing in terms of reducing cost, increasing productivity, improving quality, and preventing damage to the machined part. In this research work, a new intelligent TCM system has been developed for accurate detection of tool wear failure as well as prediction of sudden tool chipping/breakage before damaging the machined part. The system analyzes process-born features gathered from multi-sensor feedback signals using advanced signal processing and machine learning methods to detect the tool condition during cutting processes. Communication between the developed system and a CNC machine controller has been implemented. The time required for signal processing, decision making and communication with the machine controller allows stopping the operation before part damage. For tool wear detection, robust and real-time signal processing and decision-making algorithms were developed using feedback signals from the spindle drive motor. The proposed signal processing approach accentuates the tool condition effect on the extracted features while masking the effects of the cutting parameters. These features were employed in a machine learning algorithm to detect the tool condition. The results indicated the capability of the processing technique to minimize system learning effort by at least 75% and to detect tool wear with an accuracy above 95% and a confidence level above 90%. Such capability has never been achieved before.For sudden failure prediction, a novel signal processing approach for online prediction and prevention of tool chipping/breakage during intermittent machining was developed. The approach analyzes the acoustic emission waves associated with the generation of new surfaces during unstable crack propagation, which precede tool fracture. The features of the prefailure phase were identified using the Hilbert-Huang transformation method and the Teager-Kaiser Energy Operator, which can discriminate high energy/frequency events in the prefailure phase. Extensive experimental results demonstrated the accuracy of the developed system to consistently predict tool chipping. The system output has been shown to be independent of the cutting parameters and workpiece material. A correlation between the chipping size and the prefailure features was developed for decision making. No such system previously existed.</description><creator>Hassan, Mahmoud</creator><contributor>Jozsef Kovecses (Supervisor3)</contributor><contributor>Mahmoud Helmi Attia (Supervisor1)</contributor><contributor>Vincent Thomson (Supervisor2)</contributor><date>2019</date><subject>Mechanical Engineering</subject><title>Generalized sensor-based tool failure detection and prevention system for intermittent cutting operations</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/g732dc18b.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/cr56n339b</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:3f462767d</identifier><datestamp>2020-03-21T05:00:35Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The ventral hippocampus (vHC) to medial prefrontal cortex (mPFC) circuit is composed of monosynaptic long range projections originating in the vHC and terminating in the mPFC. This pathway plays a vital role in functions such as emotional processing and contextual memory, with vHC-mPFC circuit dysfunction associated with a large variety of neuropsychiatric disorders such as anxiety and schizophrenia. The identity and extent of long range projections from different subregions of the vHC (vCA1 and vSub) to the mPFC (infralimbic, IL and prelimbic, PL) and their functional role in a richer repertoire of behaviors has not been investigated in the mouse. The present study presents an attempt to characterize these features of the circuit in mice with injections of the retrograde tracer CTB into the PL and IL or anterograde AAV into the vHC. Our results indicate greater vHC innervation of the IL rather than PL, with a small population of neurons projecting to both. Moreover, we observed that the mPFC receives more numerous projections from the vCA1 than the vSub. In addition, we found a lack of GABAergic vHC projections to the mPFC. Lastly, we were able to use the molecular marker Neurotensin (Nts) to target a subpopulation of cells topographically restricted to distal vCA1 and proximal vSub with a unique pattern of PFC innervation. Optogenetic inhibition of this NtsvHC-IL circuit with NpHR did not alter anxiety-like behavior or social memory and had unclear effects on extinction learning and renewal. Taken together, our anatomical data from the mouse are largely aligned with the results of previous studies in rats and support translation of findings between both rodent species. The lack of changes with optogenetic inhibition are suggestive of the NtsvHC-IL circuit not having a functional role in the behaviors of emotional processing and memory tested and reveals previously unknown complexity within the vHC-mPFC pathway. Future studies should further delineate the pattern of anatomical connectivity and the functional role of this pathway in more nuanced behaviors, as well as assessing the significance of Neurotensin signaling and excitation/inhibition (E/I) balance within the circuit. Greater understanding of this pathway in rodent models could shed light on anatomical substrates of complex behaviors and how the functional roles of circuits are disrupted in diverse neuropsychiatric disorders.</description><description>Le circuit de l'hippocampe ventral (vHC) vers le cortex préfrontal médian (mPFC) est composé de projections monosynaptiques de longue distance provenant du vHC et se terminant dans le mPFC. Cette voie joue un rôle essentiel dans des fonctions telles que le traitement émotionnel ainsi que la mémoire contextuelle, avec un dysfonctionnement du circuit vHC-mPFC associé à une grande variété de troubles neuropsychiatriques tels que l'anxiété et la schizophrénie. L'identité et l'étendue des projections à longue distance de différentes sous- régions du vHC (vCA1 et vSub) au mPFC (infra-limbique, IL et pré-limbique, PL) et leur rôle fonctionnel dans un répertoire plus riche de comportements n'ont pas été étudiés chez la souris. La présente étude tente de caractériser ce circuit chez des souris avec des injections du traceur rétrograde CTB dans le PL et IL ou antérograde AAV dans le vHC. Nos résultats indiquent une plus grande innervation vHC de l'IL plutôt que PL, avec une petite population de neurones projetant à la fois. De plus, nous avons observé que le mPFC reçoit plus de projections du vCA1 que du vSub. En outre, nous avons trouvé un manque de projections GABA vHC pour le mPFC. Enfin, nous avons pu utiliser le marqueur moléculaire Neurotensin (Nts) pour cibler une sous- population de cellules topographiquement restreinte à vCA1 distale et vSub proximale avec un modèle unique d'innervation PFC. L'inhibition optogénétique de ce circuit NtsvHC-IL avec NpHR n'a pas altéré le comportement anxieux ou la mémoire sociale. Pris ensemble, nos données anatomiques de la souris sont en grande partie alignés avec les résultats d'études antérieures chez les rats et soutiennent la translation des résultats entre les deux espèces de rongeurs. L'absence de changements avec l'inhibition optogénétique suggère que le circuit NtsvHC-IL ne joue pas un rôle fonctionnel dans les comportements de traitement émotionnel et de mémoire testée et révèle une complexité jusqu'alors inconnue dans la voie vHC-mPFC. Des études futures devraient préciser davantage le modèle de connectivité anatomique et le rôle fonctionnel de cette voie dans des comportements plus nuancés, ainsi que l'évaluation de la signification de la signalisation Neurotensin et de l'équilibre excitation/inhibition (E/I) dans le circuit. Une meilleure compréhension de cette voie dans les modèles de rongeurs pourrait éclairer les substrats anatomiques des comportements complexes et comment les rôles fonctionnels des circuits sont perturbés dans divers troubles neuropsychiatriques.</description><creator>Ash, Polina</creator><contributor>Sylvain Williams (Internal/Supervisor)</contributor><contributor>Lalit K Srivastava (Internal/Cosupervisor2)</contributor><date>2019</date><subject>Neuroscience</subject><title>Investigating projections from the ventral hippocampus (vHC) to the prelimbic (PL) and infralimbic (IL) cortices in the mouse</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/mk61rk33d.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/3f462767d</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Integrated Program in Neuroscience</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:3b591b65m</identifier><datestamp>2020-03-21T05:00:36Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Historically, the unmanned aircraft has been an important component in military operations. The adaptation of unmanned aerial technology for civil purposes has rapidly captured the interest of the masses. Governments and hobbyists were the initial beneficiaries of the adaptation of Unmanned Aerial Vehicles (UAV) for civil use. With several industries such as agriculture, entertainment, real estate and delivery services using UAVs to perform essential functions, the proliferation of UAVs has become prominent in the commercial sector. The agile, affordable and accessible UAVs bring about many economic and social benefits. Nevertheless, the exponential growth of UAVs has also resulted in critical issues such as safety and privacy concerns. Due to the inherent differences between UAVs and manned aviation, the existing laws are inadequate to address the many issues that arise. Heeding to the rising problems, countries have resorted to strict and restrictive UAV regulations. UAVs are limited to segregated airspace and prevented from being fully integrated to a country's national airspace system. Strict regulations throttle innovation and discourage the nascent UAV industry. States are therefore burdened with demands to ensure aviation safety and uphold public rights to privacy on one hand, and demands to fully integrate UAVs to the national airspace on the other hand. The question is how national UAV regulations can address such a dichotomy. This thesis examines the development of national UAV regulations in selected countries and conducts a comparative analysis of the UAV regulations existing at the time of writing, to identify the best suited method to achieve regulatory balance. Upon the thesis findings it is recommended that sharing the regulatory responsibility between the government and the industry, increasing the involvement of the industry in drafting, implementing and enforcing regulations and thereby adopting the co-regulatory approach is the best way forward. </description><description>Historiquement, l'aéronef sans pilote a été un élément important dans les opérations militaires. L'adaptation de la technologie aérienne sans pilote à des fins civiles a rapidement éveillé l'intérêt des masses. Les gouvernements et les amateurs ont été les premiers bénéficiaires de l'adaptation des véhicules aériens sans pilote (UAV) à usage civil. Plusieurs industries telles que l'agriculture, le divertissement, l'immobilier et les services de livraison utilisent des UAVs pour remplir des fonctions critiques, ce qui a pour effet que la prolifération des UAVs est devenue prédominante dans le secteur commercial. Les drones agiles, abordables et accessibles apportent de nombreux avantages économiques et sociaux. Néanmoins, la croissance exponentielle des UAVs a également entraîné des problèmes critiques tels que la sûreté et la confidentialité. En raison des différences inhérentes entre les UAVs et l'aviation habitée, les lois existantes ne permettent pas de résoudre les nombreux problèmes qui surviennent. En tenant compte des problèmes croissants, les pays ont eu recours à des réglementations draconiennes strictes et restrictives. Les UAVs sont limités à un espace aérien séparé et ne peuvent être entièrement intégrés dans l'espace aérien national d'un pays. Les réglementations strictes étouffent l'innovation et découragent l'industrie naissante des UAVs. Les États ont donc le fardeau de maintenir la sécurité dans l'aviation et de protéger le public et leurs droits d'une part, et la demande d'intégration complète des drones afin que le plein potentiel de l'aviation sans pilote puisse être réalisé d'autre part. La question est de savoir comment les réglementations nationales sur les UAVs peuvent répondre à la dichotomie. La méthode la mieux adaptée pour atteindre l'équilibre requis est identifiée dans cette thèse en examinant l'élaboration de réglementations nationaux sur les UAVs et en procédant à une analyse comparative des réglementations actuelles sur les UAVs dans certains pays. Le partage de la responsabilité réglementaire entre le gouvernement et l'industrie, l'implication accrue de l'industrie des drones dans la rédaction, la mise en œuvre et l'application de la réglementation ainsi que l'adoption de l'approche de coréglementation constituent la meilleure voie à suivre.</description><creator>Pathirana, Dhananga</creator><contributor>Paul Stephen Dempsey (Internal/Supervisor)</contributor><contributor>Richard Gold (Internal/Cosupervisor2)</contributor><date>2019</date><subject>Air and Space Law</subject><title>Towards better regulation of unmanned aerial vehicles in national airspace: a comparative analysis of selected national regulations</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/d791sj54j.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/3b591b65m</identifier><degree><name>Master of Laws</name><grantor>McGill University</grantor><discipline>Institute of Air and Space Law</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:5t34sm66g</identifier><datestamp>2020-03-21T05:00:37Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Chronic Obstructive Pulmonary Disease (COPD) is a prevalent and complex respiratory disorder primarily caused by inhalational exposure to cigarette smoke (CS). However, only approximately 15% of cigarette smokers develop COPD, suggesting that genetic or epigenetic factors may contribute to disease susceptibility. One feature of COPD is emphysema, which is characterized by permanent alveolar wall destruction leading to airspace enlargement. The alveolar wall destruction in emphysema is a consequence of multiple pathogenic mechanisms including chronic inflammation, a protease: anti-protease imbalance and the upregulation of cell death programs such as apoptosis, autophagy and endoplasmic reticulum (ER) stress. However, molecular mechanism(s) that regulate these pathogenic events, and thus the development of emphysema, are poorly understood. This regulation may involve the aryl hydrocarbon receptor (AhR), which is a ligand-activated transcription factor whose involvement in COPD pathogenesis is unknown. Our lab has previously published that AhR-deficiency exacerbates CS-induced cell death and inflammation in lung structural cells. Thus, we hypothesized that AhR-deficiency would promote the development of CS-induced emphysema and COPD pathogenesis.Using a preclinical model of CS-exposure, we demonstrate that AhR deficiency worsens the development of a CS-induced emphysema-like phenotype in the murine lung. AhR ablation promoted the CS-induced: (1) upregulation of pathogenic mechanisms underlying emphysema development (i.e. inflammation, an antiprotease imbalance, and the activation of cell death machinery), (2) lung parenchymal destruction, and (3) declines in lung function. In COPD subjects, there was less pulmonary and systemic AHR expression. In humans, systemic AHR mRNA levels also positively correlated with lung function. There was no alteration in the frequency of AHR single nucleotide polymorphisms (SNPs) that could explain this decrease in AHR in COPD subjects. However, elevated expression of the AhR Repressor (AHRR), which is a negative regulator of the AHR, in the COPD lung may contribute to reduced AHR expression in these subjects.We also utilized an in vitro model of CS exposure to evaluate if the AhR attenuation of CS-induced cell death in lung structural cells involves its regulation of cell death modalities such as autophagy and/or ER stress. We show that the expression of the autophagy and ER stress protein LC3II was significantly elevated in cigarette smoke extract (CSE)-exposed AhR-deficient lung structural cells; these cells were primary mouse lung fibroblasts (MLFs), mouse lung epithelial cells (MLE12) and alveolar epithelial cells (A549). Heightened LC3II expression could not be explained by the transcriptional upregulation of key autophagy genes (Gabarapl1, Beclin1, Lc3b), upregulation of upstream autophagic machinery (ATG5-12, ATG3) or impaired autophagic flux. This suggested that elevated LC3II in CSE-treated AhR-deficient cells is likely mediated by an autophagy-independent mechanism. This was further supported by the absence of autophagosomes in transmission electron micrographs. However, CSE-treated Ahr-/- MLFs showed significantly reduced viability, widespread ER-dilation, elevated expression of ER stress markers (i.e. CHOP and GADD34) and the accumulation of ubiquitinated proteins. Thus, the AhR attenuates an ER stress response that is autophagy-independent yet associated with elevated LC3 expression and processing. Collectively, our data position the AhR as a central player in the homeostatic maintenance of lung health by demonstrating that loss of the AhR promotes the development of CS-induced emphysema. Given that no effective therapeutic options currently exist to stop or slow COPD progression, these findings could provide the basis for the development of new therapeutic agents or biomarkers for COPD.</description><description>La Maladie Pulmonaire Obstructive Chronique (MPOC) est une maladie respiratoire complexe à prévalence élevée, dont le principal facteur de risque est l'exposition à la fumée de cigarette. Cependant, seuls 15% des fumeurs développent une MPOC, suggérant l'implication de facteurs génétiques ou épigénétiques dans la susceptibilité à la maladie. Une des composantes de la MPOC est l'emphysème, défini par une destruction irréversible des parois alvéolaires aboutissant à un élargissement permanent des espaces aériens distaux. La destruction alvéolaire est la conséquence de multiples mécanismes physiopathologiques incluant l'inflammation chronique, le déséquilibre de la balance protéases : anti-protéases et l'activation de divers programmes de mort cellulaire tels que l'apoptose, l'autophagie ou le stress du réticulum endoplasmique (ER stress). Toutefois, les mécanismes moléculaires régulant ces événements, et par suite le développement de l'emphysème, sont encore imparfaitement connus. Cette régulation pourra impliquer l'aryl hydrocarbon receptor (AhR), qui est un facteur de transcription activé par un ligand et dont son implication dans le développement de MPOC est inconnue. Notre laboratoire a précédemment montré que le déficit en AhR dans les cellules pulmonaires exacerbe la mort cellulaire et l'inflammation induites par la fumée de cigarettes. C'est pour cela, nous avons émis l'hypothèse que la déficiance en AhR favoriserait le développement de l'emphysème et de la pathogenèse de la MPOC induits par la fumée de cigarettes.Dans notre modèle murin, nous démontrons que le déficit en AhR aggrave le développement d'un phénotype semblable à un emphysème induit par l'exposition à la fumée de cigarettes. L'ablation AhR a promu: 1) des mécanismes pathogéniques aboutissant au développement d'emphysème (ex : inflammation, déséquilibre protéases-antiprotéases au niveau pulmonaire et activation de programmes de mort cellulaire), (2) destruction de parenchyme pulmonaire, et (3) le déclin de la fonction respiratoire. L'expression pulmonaire et systémique de l'AHR était diminuée chez les patients atteints de MPOC. De plus, les niveaux d'ARN messagers systémiques de l'AHR étaient positivement corrélés à la fonction respiratoire. Nous avons observé que l'expression du répresseur d'AHR (AHRR), qui est un régulateur négatif d'AHR, est élevée dans les poumons de MPOC. Ceci pourra expliquer la diminution d'AHR observée chez ces sujets.De plus, nous avons utilisé un modèle in vitro d'exposition à la fumée de cigarette afin d'évaluer si AhR réduit le mort des cellules structurales en régulant les programmes de mort cellulaire, comme autophagie et/ou ER. Nous montrons que l'expression du marqueur d'autophagie et d'ER la protéine LC3II était significativement augmentée dans les cellules pulmonaires de structure déficientes pour l'AhR, exposées à de l'extrait de fumée de cigarette (CSE). L'augmentation de l'expression de LC3II n'était pas relative à une sur expression transcriptionnelle des gènes clés mis en jeu dans l'autophagie, une régulation positive de la machinerie autophagique d'amont, ou une altération du flux autophagique. Cependant, les MLFs Ahr-/- présentaient une viabilité significativement réduite, une dilatation diffuse du réticulum endoplasmique, et une élévation de l'expression du marqueur d'ER-stress. Ainsi, l'AhR semble atténuer une réponse de type ER stress de manière indépendante de l'autophagie.L'ensemble de ces données positionne l'AhR comme un facteur central dans la préservation de l'homéostasie pulmonaire en démontrant que la perte de l'AhR favorise le développement de l'emphysème induit par la fumée de cigarettes. Étant donnée l'absence actuelle de traitements efficaces pour arrêter ou ralentir la progression de la MPOC, ces résultats pourraient constituer une base de travail pour le développement de nouveaux traitements.</description><creator>Guerrina, Necola</creator><contributor>Carolyn Baglole (Supervisor)</contributor><date>2019</date><subject>Pathology</subject><title>Reduced aryl hydrocarbon receptor (AhR) expression drives the pathogenesis of cigarette smoke-induced emphysema</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/79408039v.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/5t34sm66g</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Pathology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:g445cg66r</identifier><datestamp>2020-03-21T05:00:38Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Influenza viruses cause significant morbidity and mortality worldwide. Vaccines are the best tools available to reduce the disease burden; however, vaccine effectiveness varies significantly between years, target populations and strains. Medicago Inc. has developed a highly efficient platform to produce plant-derived virus-like particle (VLP) vaccines bearing influenza hemagglutinin (HA) that have been shown to elicit strong humoral and CD4+ T cell responses in both pre-clinical and clinical studies. To better understand the immunogenicity of these vaccines, we studied the early interactions of VLPs with antigen-presenting cells (APC) in vitro. We demonstrated that VLPs bind to human monocytoid U-937 cells and monocyte-derived macrophages (MDMs) in a sialic acid-dependent manner. VLP attachment to the cell surface led to internalization, trafficking to acidic cell compartments and fusion of the VLP lipid envelope with endosomal membranes. Incubation of MDMs with VLPs bearing H1 (HA sequence from A/California/07/2009 (H1N1) strain) but not H5 (HA sequence from A/Indonesia/05/2005 (H5N1) induced proliferation of autologous lymphoid cells suggesting antigen processing by MDMs and stimulation of a memory T cell response. Pulse-exposure of MDMs with H1-VLPs resulted in a rapid and massive intracellular accumulation of HA that was driven by clathrin-mediated and clathrin-independent endocytosis as well as macropinocytosis/phagocytosis. The H1-VLPs endosomal distribution pattern suggested that HA delivered by VLP had entered both high-degradative late (supporting major histocompatibility complex (MHC) II-restricted antigen presentation) and low-degradative static early and/or recycling (favoring MHC I-restricted antigen cross-presentation) endosomal pathways. High-resolution tandem mass spectrometry identified a large number of HA-derived peptides associated with MHC I in the H1-VLP-treated MDMs. In addition, many host-derived MHC I peptides were identified in VLP-treated samples. These peptides were mainly processed by matrix metalloproteinases and cathepsins. The host proteins associated with these peptides were primarily involved in pathways modulating inflammation (i.e. stimulation and attenuation), innate and adaptive immunity, clathrin-mediated endocytosis, protein synthesis and endo-lysosomal degradation. Finally, tools we used while studying endosome-lysosome fusion led to the development of a novel serological assay for influenza based on 1,1'-dioctadecyl-3,3,3',3'-tetramethylindodicarbocyanine perchlorate (DiD) fluorescence dequenching. This assay measures 'functional' influenza antibody titers, is free from observer bias and has the potential to be fully automated. In summary, we demonstrated that HA delivery to APCs in a form of plant-derived VLPs facilitates antigen uptake, endosomal processing, presentation and cross-presentation. These observations may help to explain the broad and cross-reactive immune responses generated by VLP vaccines. The new DiD fluorescence dequenching assay we developed may give new insights into the spectrum of antibodies produced in response to influenza infection or vaccination.</description><description>Les virus grippaux entraînent une morbidité et une mortalité significatives dans le monde entier. Les vaccins sont les meilleurs outils disponibles pour réduire le fardeau de cette maladie; cependant, l'efficacité du vaccin varie de manière significative entre les années, les populations cibles et les souches. Medicago Inc. a développé une plate-forme hautement efficace pour produire des vaccins à particules pseudo-virales (VLP) d'origine hémagglutinine (HA) dérivées de plantes qui ont démontré une forte réponse humorale et lymphocytaire T CD4+ dans les études précliniques et cliniques. Pour mieux comprendre l'immunogénicité de ces vaccins, nous avons étudié les interactions précoces des VLP avec des cellules présentatrices d'antigènes (APC) in vitro. Nous avons démontré que les VLP se lient aux cellules monocytaires humaines U-937 et aux macrophages dérivés des monocytes (MDM) d'une manière dépendante de l'acide sialique. La fixation des VLP à la surface cellulaire a conduit à l'internalisation, au trafic vers les compartiments cellulaires acides et à la fusion de l'enveloppe lipidique VLP avec les membranes endosomales. L'incubation des MDM avec des VLP portant H1 (séquence HA de la souche A/California/07/2009 (H1N1) mais pas H5 (séquence HA de A/Indonesia/05/2005 (H5N1) induit une prolifération de cellules lymphoïdes autologues suggérant un traitement antigénique par MDMs et la stimulation d'une réponse lymphocytaire mémoire L'exposition pulsatile des MDM aux H1-VLP a entraîné une accumulation intracellulaire rapide et massive d'HA causée par l'endocytose induite par la clathrine et la clathrine ainsi que par la macropinocytose/phagocytose. La distribution endosomale de VLP suggérait que la HA délivrée par VLP était entrée à la fois dans les voies endosomiques tardives à haute dégradation (soutenant la présentation d'antigènes restreints au complexe majeur d'histocompatibilité (CMH) II) et dans les voies endosomiques statiques et/ou de recyclage statiques à faible dégradation (favorisant la présentation croisée de l'antigène du CMH I). La spectrométrie de masse en tandem à haute résolution a permis d'identifier un grand nombre de peptides dérivés de HA associés au CMH I dans les MDM H1-VLP traités. Les peptides du CMH I ont été identifiés dans des échantillons traités par VLP. Ces peptides ont été principalement traités par des métalloprotéinases matricielles et des cathepsines. Les protéines hôtes associées à ces peptides étaient principalement impliquées dans les voies modulant l'inflammation (c'est-à-dire la stimulation et l'atténuation), l'immunité innée et adaptative, l'endocytose médiée par la clathrine, la synthèse protéique et la dégradation endo-lysosomale. Enfin, les outils que nous avons utilisés lors de l'étude de la fusion endosome-lysosome ont conduit au développement d'un nouveau test sérologique de la grippe basé sur l'extinction de fluorescence 1,1'-dioctadécyl-3,3,3 ', 3'-tétraméthylindodicarbocyanine perchlorate (DiD). Ce test mesure les titres d'anticorps de la grippe 'fonctionnels', est exempt de biais observateur et a le potentiel d'être entièrement automatisé. En résumé, nous avons démontré que la délivrance de HA aux CPA dans une forme de VLP dérivée de plantes facilite l'absorption d'antigène, le traitement endosomal, la présentation et la présentation croisée. Ces observations peuvent aider à expliquer les réponses immunitaires larges et réactives croisées générées par les vaccins VLP. Le nouveau test d'extinction de fluorescence DiD que nous avons développé pourrait donner de nouvelles perspectives sur le spectre des anticorps produits en réponse à une infection grippale ou à la vaccination.</description><creator>Makarkov, Alexander</creator><contributor>Brian Ward (Supervisor)</contributor><date>2019</date><subject>Medicine</subject><title>Influenza vaccine development: immunogenicity and correlates of protection</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/mp48sf90b.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/g445cg66r</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Medicine</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:05741t984</identifier><datestamp>2020-03-21T05:00:39Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Additive manufacturing (AM) is an attractive solution to reduce processing cost of complex components. The interest of the aerospace industry into Ti-6Al-4V resides in its excellent strength to weight ratio and corrosion resistance properties. As of today, control of the process to structure to property relationship for AM technologies remains challenging. This research investigates the effects of some key laser wire deposition (LWD) parameters and various post deposition heat treatments on the developed microstructure and subsequent properties of the deposited Ti-6Al-4V materials. The effect of two different travel speeds on the structural development of thin Ti-6Al-4V deposits was investigated. A travel speed set at 1.4 mm/s promoted recrystallization of columnar prior β grains into horizontal prior β grains and a diffusion-controlled type of microstructure. A travel speed set at 7.2 mm/s resulted in higher cooling rates that produced a refined microstructure while no recrystallization of the prior β grains has been observed. Next the effect of five different post deposition heat treatments were investigated. A stress relief preserved the morphology of the deposited samples while annealing or HIP, followed or not by aging, coarsened the developed microstructure. The lower travel speed exhibited a strong anisotropy in elongation with lower values generated along the build direction as opposed to the isotropic results in elongation at higher travel speed. Moreover, samples processed at 7.2 mm/s produced high strength meeting the minimum wrought requirements as set by the AMS4911. Eventually, the grain boundary strengthening mechanism with regards to the α platelets thicknesses was found to be travel speed dependent. Thick samples were characterized by the deposition of multiple lateral beads within one layer. Thermal history was first investigated through the development of a FEM model reproducing the deposition process. Typical macrostructures and microstructures that developed post deposit were then investigated. Subsequent static tensile properties were found to be hardly meeting the cast minimum requirements as set by the ASTM F1108. No anisotropy in the impact toughness properties was observed. Eventually, HIP did not induce any major improvement of the mechanical properties for both thin and thick specimens and is not required for LWD applications. All the previous findings were eventually applied for the printing of a large Ti-6Al-4V aerospace component. The deposition strategy was presented. Destructive and non-destructive testing were done to check the printed part geometry and the process to structure to properties relationship. The printed part was characterized with a buy-to-fly ratio (BTF) of about 3.2:1 as opposed to a BTF ratio of 30.8:1 when using conventional subtractive processes.</description><description>La fabrication additive (FA) des alliages a été identifiée par l'industrie aérospatiale comme étant une solution attrayante pour réduire le coût de production de pièces complexes. L'intérêt que porte l'industrie aérospatiale pour cet alliage réside dans son excellent rapport entre résistance mécanique et faible densité ainsi que pour son excellente résistance à la corrosion. À ce jour, l'enjeu réside dans le contrôle de la relation qui existe entre le processus utilisé, la microstructure développée et les propriétés mécaniques qui en découlent au travers de l'utilisation des technologies de FA. Ce projet a pour but d'évaluer les effets de certains paramètres du processus de déposition par fil et laser (DFL) ainsi que les effets de plusieurs traitements thermiques post déposition sur le développement de la microstructure et des propriétés des pièces imprimées en Ti-6Al-4V. L'effet de deux vitesses de déplacement sur le développement structurel des dépôts minces de Ti-6Al-4V a été évalué. Une vitesse de déplacement fixée à 1.4 mm/s favorise la recristallisation des ex-grains β colonnaires en ex-grains β horizontaux ainsi que le développement d'une microstructure contrôlée par diffusion. Une vitesse de déplacement fixée à 7.2 mm/s aboutis à des taux de refroidissement plus élevés qui favorisent le développement d'une microstructure raffinée, alors qu'aucune recristallisation des ex-grains β n'a pas été observée. Un traitement d'adoucissement permis de préserver l'effet des paramètres de déposition sur les différentes microstructures développées alors qu'un cycle de recuit ou de compression isostatique à chaud (HIP), suivi ou non d'un cycle de revenu, firent croitre la microstructure développée post déposition. La vitesse de déplacement plus lente résulta en une déformation anisotropique avec des valeurs inférieures mesurées le long de la direction de déposition à l'opposé des déformations isotropiques générées pour les échantillons produits à 7.2 mm/s. De plus les échantillons produits à 7.2 mm/s développèrent des résistances en contraintes élevées dépassant les exigences minimales des matériaux forgés telles que définies par la norme AMS4911. Enfin, le mécanisme de renforcement par joints de grains associé à l'épaisseur des lamelles α s'est avéré être dépendant de la vitesse de déplacement.Des échantillons épais furent caractérisés par la déposition latérale de plusieurs cordons pour chacune des couches. Un modèle par la méthode des éléments finis (MÉF) reproduisant la stratégie de dépôt fut développé pour étudier l'historique des cycles thermiques. Les macrostructures et microstructures représentatives des échantillons post déposition furent étudiées. Les propriétés de résistances en contraintes développées n'ont que très difficilement atteint les exigences minimales associées aux matériaux coulés telles que définies par la norme ASTM F1108. Aucune anisotropie ne fut observée à la suite des essais Charpy. Enfin, un cycle de HIP n'engendre aucune amélioration concrète des propriétés mécaniques à la fois pour les échantillons épais et fins et n'est pas nécessaire pour les échantillons produits par DFL.Enfin, tous les enseignements précédents ont été mis à contribution aux travers d'une application concrète d'impression d'une grande pièce en Ti-6Al-4V pour le domaine aérospatiale. La stratégie de déposition fut présentée. Des tests destructifs et non destructifs ont été effectués pour vérifier la géométrie de la pièce imprimée ainsi que la relation de structure à propriété qui s'est développée. La pièce imprimée fut associée à un rapport « acheter-pour-voler » (APV) ou plus connu comme étant le rapport « buy-to-fly » (BTF) en anglais de 3.2:1 à l'opposé d'un rapport de 30.8:1 lorsque fabriquée par des méthodes conventionnelles.</description><creator>Chekir, Nejib</creator><contributor>Mathieu Brochu (Supervisor)</contributor><date>2019</date><subject>Mining and Materials</subject><title>Laser wire deposition additive manufacturing of Ti-6Al-4V for the aerospace industry</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/vx021h45m.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/05741t984</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mining and Materials</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:m613n0918</identifier><datestamp>2020-03-21T05:00:40Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Polar codes have received a great deal of attention in the past few years to the extent that they are selected to be included in the 5th Generation of Wireless Communications Standard (5G). Specifically, polar codes were selected as the coding scheme for the Enhanced Mobile Broadband (eMBB) control channel which requires codes of short length. The main bottleneck in the deployment of polar codes in 5G is the design of a decoder which can achieve good error-correction performance, with low hardware implementation cost and high throughput. Successive-Cancellation (SC) decoding was the first algorithm under which polar codes could achieve capacity when the code length is very high. However, for finite practical code lengths, SC decoding falls short in providing a reasonable error-correction performance because of its sub-optimality with respect to the Maximum-Likelihood (ML) decoder. Sphere Decoding (SD) is an algorithm that can achieve the performance of ML decoding with a very high complexity. In order to close the gap between SC and ML decoding, Successive-Cancellation List (SCL) decoding keeps a list of candidates and selects the one with the best Path Metric (PM). Although SCL provides a good error-correction performance, it comes at the cost of higher complexity and lower throughput. In this thesis, we first propose a low complexity SD algorithm which provides a good trade-off between the error-correction performance and the complexity of the decoder for polar codes of short lengths. We then propose algorithms to speed up the SCL decoders. We prove that while these algorithms have much higher throughput than the conventional SCL decoder, they incur no error-correction performance loss. We further propose several techniques to reduce the area occupation in the hardware implementation of SC and SCL decoders by reducing their memory requirements. We solve the flexibility issue of fast SC-based decoders and introduce a completely rate-flexible scheme. Hardware architectures for the proposed algorithms are presented and comparisons with state of the art are made. Finally, we evaluate the performance of polar codes in 5G and we show that polar codes can be used in practical applications by proposing a blind detection scheme with polar codes.</description><description>Les codes polaires occupent depuis quelques années l'attention de la communauté académique du codage de canal. Cet intérêt s'est étendu à l'industrie puisque les codes polaires prennent part au standard de communications mobiles de cinquième génération (5G). Plus précisément, ils sont sélectionnés comme schéma de codage pour le canal de contrôle du service mobile à large bande amélioré (Enhanced Mobile Broadband (eMBB)), requérant des codes de faible longueur. Le principal obstacle dans le déploiement des codes polaires pour la 5G est la conception d'un décodeur ayant un coût d'implantation matériel faible, tout en présentant à la fois de bonnes performances de correction d'erreurs et un débit élevé. Le décodage à annulation successive (Successive-Cancellation (SC)) est l'algorithme de décodage originel des codes polaires. Il permet d'atteindre la limite de capacité théorique, à condition que la taille du code polaire soit suffisamment grande. Cependant, pour des tailles de trame finies, l'algorithme SC présente des performances de décodage médiocres, provenant de sa sous-optimalité au regard du décodage à maximum de vraisemblance (Maximum-Likelihood (ML)). Le décodage par sphère (Sphere Decoding (SD)) atteint les performances ML au détriment d'une complexité calculatoire importante. Afin de réduire l'écart de performances entre les décodages ML et SC, le décodage à annulation successive par liste (Successive-Cancellation List (SCL)) a été proposé. Son principe réside dans le maintient d'une liste de mots candidats et de la sélection à l'issue du décodage du mot ayant la meilleure métrique. Bien que l'algorithme SCL présente de bonnes performances de décodage, sa complexité calculatoire est plus importante et son débit est réduit par rapport à l'algorithme SC. Dans ces travaux de thèse, nous proposons tout d'abord un algorithme SD à faible complexité calculatoire. Il permet d'obtenir un bon compromis entre sa performance de correction d'erreurs et sa complexité calculatoire pour des codes polaires de faibles tailles. Ensuite, nous proposons différents algorithmes accélérant les décodeurs SCL. Ces algorithmes permettent d'atteindre un débit bien plus élevé que le décoder SCL conventionnel, tout en assurant les mêmes performances de décodage. Ensuite, nous proposons différentes techniques réduisant l'occupation surfacique d'implémentations matérielles de décodeurs SC et SCL. Ces techniques sont basées sur la réduction des besoins mémoire de ces algorithmes. Nous résolvons les problèmes de flexibilité des décodeurs basés sur l'algorithme rapide SC et présentons une méthode compatible avec tout rendement de code. Des architectures matérielles adaptées aux algorithmes proposés sont présentées et des comparaisons avec l'état de l'art sont établies. Finalement, nous évaluons les performances des codes polaires du standard 5G et nous montrons que les codes polaires peuvent être utilisés dans des applications pratiques en proposant une méthode de détection aveugle.</description><creator>Hashemi, Seyyed Ali</creator><contributor>Warren Gross (Supervisor)</contributor><date>2019</date><subject>Electrical and Computer Engineering</subject><title>Fast, flexible, and area-efficient decoders for polar codes</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/n583xx153.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/m613n0918</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:0z709010f</identifier><datestamp>2020-03-21T05:00:41Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Part consolidation (PC) is an effective design technique to reduce part count and simplify product architecture. Through PC, it brings numerous benefits such as reduced assembly operation, decreased supply chain management cost, and increased structural reliability. Consolidation of parts, however, may lead to increased manufacturing difficulty in terms of complex geometry and material composition. Constrained by the capability of conventional manufacturing methods (e.g. machining and casting), PC is only applicable to components having simple geometries, no relative motion, no material variance, and no blockage of assembly access of others. As additive manufacturing (AM) evolves into an end-of-use product fabricating method, such constraints of PC have been largely relaxed, and PC has become one of the primary motivations of using AM. However, the rules and methods for exploring such emerging PC potentials are obsolete, and the understanding of how to utilize these PC potentials in a general product redesign process is very limited. To fill these gaps, three contributions are made in this thesis. First, a methodological framework enabling full exploitation of AM-enabled PC potentials in product redesign is proposed. The framework is highlighted by three design flows: core flow, complementary flow, and innovative flow. The core flow is dedicated to screening, consolidating, and refining parts that are highly feasible for consolidation. The complementary flow is supplementary to the core flow to further investigate the slight consolidation potential for parts that are rejected by the core flow. The innovative flow works in parallel with the prior flows. It aims at enlarging the design solution space by advocating design for function throughout the synthesis process of working principle, product layout, design space, material, and architecture. Amongst the three proposed flows, the core flow is the primary focus of this research and has been thoroughly investigated and implemented. Second, new candidacy rules, principles, strategies, and tools to support the systematic and automatic identification of PC candidates are developed. Third, a functional entity-based design method is proposed to aid the transition from the candidacy assembly design to the functionally-equivalent consolidated design. In the end, a computer-aided design tool is developed to implement the proposed core flow, which paves the way for better exploring and utilizing AM-enabled PC potentials in the redesign process of a complex product.</description><description>La consolidation de pièces (PC) est une technique de conception efficace pour réduire le nombre de pièces et simplifier l'architecture du produit. Grâce à la PC, de nombreux avantages sont apportés, notamment une réduction du nombre d'opérations assemblage, des coûts de gestion de la chaîne d'approvisionnement ainsi qu'une fiabilité structurelle accrue. Cependant, la consolidation de pièces peut entraîner une augmentation des difficultés de fabrication en termes de géométrie complexe et de choix de matériaux. Limitée par la capacité des méthodes de fabrication conventionnelles (par exemple usinage et moulage), la PC est uniquement applicable aux composants ayant des géométries simples, dont les pièces n'ont pas de mouvement relatif, pas de variance matérielle, et pas de blocage dans l'accès aux pièces pour l'assemblage. Comme la fabrication additive (AM) évolue vers une méthode de fabrication de produits finis, prêts à être employés, ces contraintes de PC ont été largement assouplies, et la PC est devenue l'une des principales motivations de l'utilisation de l'AM. Cependant, les règles et les méthodes pour explorer la PC, technique plutôt récente et émergente, sont obsolètes, et la compréhension de la façon d'utiliser les potentiels de la PC dans un processus général de reconception du produit est très limitée. Pour combler ces lacunes, trois contributions sont apportées dans cette thèse. Tout d'abord, un cadre méthodologique permettant une exploitation complète des potentiels de la PC pour l'AM dans un processus général de reconception du produit est proposé. Le cadre est mis en évidence par trois flux de conception: flux de base, flux complémentaire et flux innovant. Le flux de base est dédié au criblage, à la consolidation et à l'affinage de pièces pour la consolidation. Le flux complémentaire permet étudier plus en détail le potentiel de consolidation des pièces rejetées par le flux de base. Le flux innovant travaille en parallèle des deux flux précédents. Il vise à élargir l'espace des solutions de conception en préconisant la conception fonctionnelle tout au long du processus de synthèse, à travers le principe de fonctionnement, la mise en plan du produit, l'espace de conception, le choix du matériau et l'architecture. Parmi les trois flux proposés précédemment, le flux de base est l'objectif principal de cette recherche et a été minutieusement étudié et mis en oeuvre. Deuxièmement, de nouvelles règles, principes, stratégies et outils pour soutenir l'identification systématique et automatique des PC potentielles ont également été développés. Troisièmement, une méthode de conception basée sur l'entité fonctionnelle est proposée pour faciliter la transition de l'assemblage à la pièce consolidée, fonctionnellement équivalente. Finalement, un outil de conception assistée par ordinateur est développé pour mettre en oeuvre le flux de base proposé, ouvrant la voie à une meilleure exploration et utilisation des potentiels de la PC pour l'AM dans un processus général de reconception du produit complexe. </description><creator>Yang, Sheng</creator><contributor>Yaoyao Zhao (Supervisor)</contributor><date>2019</date><subject>Mechanical Engineering</subject><title>Rules and methods for exploring and utilizing additive manufacturing-enabled part consolidation potentials in product redesign</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/6t053j42w.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/0z709010f</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:2514nn79f</identifier><datestamp>2020-03-21T05:00:42Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>BACKGROUND: Donation after cardio-circulatory death (DCD) is a vital program to address the current deficit of transplantable organs. Uncertainty about the time to death when withdrawing life support therapy is a major barrier to DCD. The primary objective of this study is to develop a new model to predict death within 120 minutes of withdrawal of life sustaining therapy (WLST). METHODS: Prospective multicentre observational data from adult, DCD-eligible donors were analysed to develop prediction models using a priori selected potential predictors and two statistical approaches:  "classical" multivariable logistic regression and "ensemble" random forest classification. Models were internally validated in bootstrapped samples. Model performances and physician's prediction of outcome were compared for accuracy, discrimination and calibration. Models were re-analysed with the inclusion of physician's prediction as an additional potential predictor and post-hoc univariable analysis of the included predictors were conducted. RESULTS: Out of the included 307 eligible adult DCD donors, 57.7% died within 120 minutes of WLST. Based on the optimism adjusted area under the curve values, the "classical" models appeared to perform better than the "ensemble" models. The clinician's predictions appeared to be superior to both a priori models. The re-assessed classical model with physician's prediction and the a priori potential predictors, appeared to outperform all other models. CONCLUSION: Developing efficient models including commonly assessed objective predictors is possible. Including physician's prediction improved model performances. Further exploration of the models in larger sample sizes is required.</description><description>CONTEXTE : Le programme de don d'organes après décès cardiocirculatoire (DDC) représente un potentiel considérable d'accroître l'offre d'organes pour pallier la pénurie pour les greffes. Il nous permettra de réduire le temps d'attente pour les greffes, de sauver des vies et d'améliorer nettement la qualité de vie d'un grand nombre de Canadiens, notamment ceux vivant avec une maladie d'organes en phase terminale. L'incertitude liée au délai écoulé entre le retrait des thérapies de maintien des fonctions vitales (TMFV) et le décès est un obstacle majeur à l'adoption du DCD. L'objectif principal de cette étude est de développer un nouveau modèle permettant de prédire le délai de décès dans les 120 minutes suivant le retrait des TMFV. MÉTHODES : Il s'agit d'une étude analysant des données provenant d'une étude observationnelle prospective, multi-centres et multinationale, réalisée en milieu de soins intensifs auprès de donneurs DDC gravement malades, chez qui la décision du retrait des TMFV avait été prise. Les prédicteurs potentiels ont été identifiés a priori au début du développement des modèles et puis des deux techniques statistiques d'analyse suivantes ont été utilisées : une approche "classique", basée sur la méthode de régression logistique et une approche "ensemble" utilisant la méthode des forêts aléatoires. Ces modèles a priori développés ont été évalués et comparés en fonction de la précision, de la discrimination et de la calibration, et d'une validation interne réalisée à l'aide d'échantillons bootstrappés. Ils ont également été évalués en rapport aux prévisions des médecins. Les modèles ont été ensuite modifiés en incluant les prévisions des médecins comme prédicteur potentiel supplémentaire. Ces nouveaux modèles et des modèles a priori développées ont été réexaminés. Finalement, une analyse univariée post-hoc des prédicteurs potentiels a été faite. RÉSULTATS : Parmi les 307 donneurs de DCD adultes éligibles inclus, 57,7% sont décédés dans les 120 minutes suivant le retrait des TMFV. Sur la base de la discrimination estimée par l'aire sous la courbe ROC (AUC) avec un ajustement pour l'optimisme, le modèle "classique" s'est révélé plus performant que le "modèle d'ensemble" parmi les deux modèles a priori développés. Les prévisions des médecins semblaient supérieures à celles des deux modèles. Lorsque les résultats ont par la suite été comparés aux deux nouveaux modèles développés incluant les prévisions du médecin, le nouveau modèle classique semblait être supérieur à tous les autres modèles. CONCLUSION : Dans l'état actuel de notre analyse des modèles de prédiction développés dans cette étude, la conception de modèles efficaces comprenant des prédicteurs objectifs couramment évalués est possible. L'inclusion du prédicteur de la prévision des médecins améliore les performances des modèles développés. Une exploration plus approfondie des modèles développés ici dans des échantillons de plus grande taille serait nécessaire.</description><creator>Biswas, Sharmistha</creator><contributor>Jason Shahin (Internal/Cosupervisor2)</contributor><contributor>Andrea Benedetti (Internal/Supervisor)</contributor><date>2019</date><subject>Epidemiology and Biostatistics</subject><title>Predicting time to death after withdrawal of life-sustaining therapy in potential organ donors: a secondary analysis of a multicenter prospective observational study</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/p5547t68x.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/2514nn79f</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Epidemiology and Biostatistics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:1831cn189</identifier><datestamp>2020-03-21T05:00:42Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Thiol (SH)-terminated surfaces have a wide range of potential uses, including biomedical and catalytic applications. In the biomedical field, SH-terminated surfaces have shown great ability as anchoring sites for covalent immobilization of molecules representing an important criterion for diagnosis of different diseases or the formation of biosensors. For these applications, a target biomolecule holding a chemical group (e.g. maleimide, thiol or disulfide) reacts selectively with SH groups on the surface. Reproducible covalent attachment of bio-compounds is desired since the active sites of these compounds are generally more readily accessible, and a more even coverage of the surfaces can be achieved. The strong affinity between gold, Au, and SH groups also makes SH-terminated surfaces attractive as anchoring platforms for promoting Au adhesion or as a support for Au nanoparticles, crucial for low temperature catalysis applications. The present research first probes the creation and full characterization of SH-terminated organic coatings from binary gas mixtures (BGMs) comprised of a hydrocarbon gas (either ethylene or butadiene) and a sulfur-rich functional gas (hydrogen sulfide, H2S). The organic coatings were created using either a low-pressure radiofrequency plasma, or through vacuum ultraviolet (VUV, l&lt;200 nm) irradiation of the gas mixtures with a near-monochromatic krypton (Kr) lamp. Varying the gas mixture ratio, R, allowed control over the resulting films' properties, particularly the sulfur, [S], and thiol, [SH], concentrations. All coatings showed high [S], up to 48 at. %, and stability in aqueous solution after immersion for 24 h. Having learned that it is possible to obtain SH-terminated organic coatings from BGMs using two different CVD methods, extending this to a further BGM (acetylene / H2S) represented the second objective of this research. For this, surface- and gas-phase phenomena were studied simultaneously as a function of applied power, &lt;P&gt;, and R. The magnitude of &lt;P&gt; greatly influenced the deposition rate, r, [S] and [SH]. Coatings from BGMs were compared with ones obtained using propanethiol as a single-molecule precursor. Resulting [SH] values were found to be higher when using BGMs, rendering such coatings superior candidates for applications. Studying VUV irradiation of a BGM comprising acetylene and H2S in more detail has been the third objective of this work. A first-of-its-kind wavelength (l)-dependent photo-deposition study was carried out in order to identify a specific photon energy, hv, leading to maximum [SH]. Results showed that different hv values influence deposition kinetics and film composition, reflecting that photolytic reactions are governed by the gases' absorption coefficients, k. Coatings with the highest [SH] reported so far in the literature (~7.7 %) were obtained with a low-pressure xenon (Xe) VUV source. </description><description>L'objectif principal de cette recherche est d'examiner la synthèse de revêtements organiques stables à terminaison thiol, en utilisant des méthodes de dépôt chimique en phase vapeur (CVD pour l'anglais chemical vapor deposition). Les méthodes CVD sont avantageuses par rapport aux alternatives chimiques humides en raison de leurs temps de réaction courts, de l'absence de solvants toxiques et de résultats plus contrôlables grâce à des paramètres de procédé variables. Le CVD au plasma (PECVD) est la méthode CVD la plus utilisée pour la fonctionnalisation des surfaces. La déposition photochimique par CVD (PICVD) s'est avérée être une bonne alternative pour la fonctionnalisation de surface et implique l'exposition des surfaces et des gaz réactifs au rayonnement à basse pression. Le plus grand avantage de PICVD par rapport à son équivalent plasma est une chimie de surface plus «ciblée» due à l'utilisation de photons monoénergétiques plutôt que d'électrons «chauds».             La première étape a été d'explorer la création et la caractérisation complète de revêtements organiques à terminaison thiol à partir de mélanges gazeux binaires (plutôt que de précurseurs moléculaires) constitués d'un gaz hydrocarboné (éthylène ou butadiène) et d'un gaz fonctionnel riche en soufre (sulfure d'hydrogène). Les revêtements organiques ont été créés en utilisant un plasma radiofréquentiel (r. f.) à basse pression ou des rayons ultraviolets énergétiques (VUV pour l'anglais vacuum ultraviolet, l&lt;200 nm) avec des mélanges gazeux avec une lampe au krypton quasi-monochromatique. La variation du rapport du mélange gazeux a permis de contrôler les propriétés des films résultants, en particulier les concentrations de soufre et de thiol. Les revêtements obtenus à partir de traitements au plasma à basse pression ont montré des concentrations de thiol plus élevées, allant jusqu'à 3%. Tous les revêtements ont montré une teneur élevée en soufre, allant jusqu'à 48 at. %, ainsi que de la stabilité en solution aqueuse après immersion pendant 24 h.            Le second objectif de cette recherche était d'acquérir une meilleure compréhension des mécanismes de la chimie et de la croissance à partir d'un autre mélange gazeux binaire (acétylène et sulfure d'hydrogène) dans un système plasma. Pour cela, les phénomènes de surface et en phase gazeuse ont été étudiés simultanément en fonction de la puissance appliquée et du rapport de mélange gazeux. Les revêtements de mélange de gaz binaires ont été comparés avec des revêtements obtenus dans une décharge similaire en utilisant du propanethiol en tant que précurseur de molécule unique. Les concentrations de thiol se sont avérées plus élevées lors de l'utilisation de mélanges de gaz binaires, rendant les revêtements à terminaison thiol obtenus à partir de cette approche des candidats supérieurs pour diverses applications.            Pour compléter cette étude sur les mécanismes en jeu à l'état plasma, le troisième objectif de ce travail est l'étude des processus se produisant au cours de l'irradiation VUV d'un mélange gazeux binaire d'acétylène et de sulfure d'hydrogène. Une étude des processus de déposition en fonction de la longueur d'onde a été réalisée afin de trouver une énergie photonique spécifique conduisant à une concentration maximale en thiol. Les résultats ont montré que différentes énergies de photons influencent la cinétique de dépôt et la composition du film, ce qui montre que les réactions photolytiques sont régies par les coefficients d'absorption des gaz. Les revêtements avec les concentrations de thiol les plus élevées rapportées jusqu'ici dans la littérature (~ 7,7%) ont été obtenus par rayonnement VUV avec une source résonnante au xénon (l = 147 nm).</description><creator>Kasparek, Evelyne</creator><contributor>Pierre-Luc Girard-Lauriault (Supervisor1)</contributor><contributor>Jason Tavares (Supervisor2)</contributor><date>2019</date><subject>Chemical Engineering</subject><title>Thiol-terminated coatings using low-pressure plasma and vacuum ultraviolet radiation</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/kw52jb239.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/1831cn189</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Chemical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:w3763923b</identifier><datestamp>2020-03-21T05:00:43Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les plantes font face à des conditions de lumière extrêmes grâce à l'émission d'énergie sous forme de chaleur, un processus qui aide à maintenir l'efficacité de la photosynthèse, appelé extinction non photochimique (ENP). L'ENP est un procédé qui permet aux plantes d'émettre l'excédent de lumière sous forme de chaleur, soit une façon pour les plantes de gérer le stress lié à la lumière. Bien que la réponse au stress de lumière à haute intensité chez les plantes soit bien comprise, il existe des lacunes quant à la façon dont elles sont affectées par différentes longueurs d'onde. Ici, l'hypothèse que les plantes subissent des mécanismes différents en réponse à diverses longueurs d'onde de haute intensité est explorée aux niveaux physiologiques, protéomiques et transcriptomiques. Une expérience a été réalisée pour définir les changements du protéome sous diode électroluminescente (DEL) rouge à haute intensité. Trois zones de niveau de stress ont été formées : la zone Brûlée (zone sous le faisceau de lumière), la zone Limite (bord autour de la zone brûlée), et la zone Régulière. Le début d'une synthèse de chlorophylle a été observé après 10 jours. Les protéines exprimées dans les feuilles ont été extraites 10 jours après le traitement de lumière. Une méthode protéomique multiplex (iTRAQ) a été suivie par 2D-LC-MS / MS. Au total, 3994 protéines ont été identifiées à un taux de fausse découverte de 1% et correspondaient à des filtres de qualité supplémentaires. L'analyse de classification hiérarchique a abouti à quatre types de modèles liés à l'expression de protéines, l'un étant directement lié à l'irradiation des DEL. Un total de 37 protéines uniques ont été trouvées à l'échantillon régulier, contre 372 pour l'échantillon de la zone Limite et 1003 pour l'échantillon de la zone Brûlée. Les protéines PsbS, PsbH, PsbR et Psb28 avaient une abondance élevée dans la zone Brûlée par rapport aux autres zones foliaires. Ces protéines sont directement impliquées dans la photoinhibition via NPQ ou la biosynthèse/assemblage de PSII, et leur expression a été étudiée plus avant. Une deuxième expérience a été effectuée avec un traitement à la lumière bleue, ce qui a donné les mêmes zones de feuilles. Les paramètres de la photosynthèse, tels que l'ENP, l'efficacité photochimique du PSII (Fv/Fm) et le taux de photosynthèse (Pn), ont été mesurés sous les traitements à la lumière rouge et bleue. Les mesures ont été prises après les traitements et après une période de 10 jours après chaque traitement. Une augmentation de 3 fois de l'ENP a été détectée lors du traitement à la lumière bleue (BLT) par rapport à la RLT, mesurée après les dommages dus à la lumière. La différence entre les traitements de lumière a été étudiée au niveau des protéines. Une analyse protéomique comparative entre RLT et BLT a été réalisée pour explorer l'abondance relative des protéines clés, PsbS, PsbH, PsbR et Psb28. Bien que ces protéines aient présenté une abondance élevée dans l'échantillon Brûlé de RLT, leurs concentrations étaient faibles dans l'échantillon BLT Brûlé correspondant. Pour étudier le contrôle de la régulation des protéines clés, la quantification des transcrits psbs, psbr, psb28 et psbh a été obtenue via un RT-qPCR. Les résultats ont démontré que les transcrits de la protéine PsbS, une protéine clé dans l'une des différentes stratégies de réponse aux ENP, étaient huit fois plus abondants dans les conditions BLT. Le désaccord entre les concentrations de protéines et des niveaux d'ARNm de PsbS dans le BLT suggère un contrôle de régulation au niveau transcriptionnel. En conclusion, les résultats démontrent que les traitements par BLT induisent une réponse plus élevée d'ENP, suivie d'une régulation élevée de PsbS au niveau de l'ARNm. La réponse au traitement par RLT était plutôt impliquée par de fortes concentrations de PsbS, PsbH, Psb28 et PsbR.</description><description>The understanding of plant stress response is essential to develop crops that are capable of withstanding adverse growing conditions. Photosynthesis is the most important metabolic process in plants and is the one most impacted by abiotic stresses. Plants deal with extreme light conditions through the emission of energy in the form of heat, a process that helps to maintain photosynthesis efficiency, called non-photochemical quenching (NPQ). Although high-light stress response in plants is well understood, there is a lack of knowledge on how plants are affected by different wavelengths of light. Here, the hypothesis that plants undergo particular mechanisms in response to extreme high-intensity light under different wavelengths is explored at the physiological, protein, and mRNA levels. An experiment was performed to define the tomato (Solanum lycopersicum, Heinz H1706) leaf proteome changes under high-intensity red LED light (655 nm peak wavelength). In this study, a light-emitting diode set-up was built to create a single spot at 5,000 W m-2 irradiance with light gradients surrounding it. Three light stress level zones were formed: Burned (area under the spotted light), Limit (edge around the burned area), and Regular (area &gt;1 cm from the burned section). The most impacted zone (Burned) was photo-bleached and highly dehydrated after the treatment, suggesting the death of the tissue. The proteins expressed in the leaves were extracted 10 days after the light treatment. A multiplex labeled proteomics method (iTRAQ) was carried on by 2D-LC-MS/MS. A total of 3,994 proteins were identified at 1% false discovery rate and matched additional quality filters. Hierarchical clustering analysis resulted in four types of patterns related to the protein expression, with one being directly linked to the increased LED irradiation. A total of 37, 372 and 1,003 proteins were found unique to the Regular, Limit and Burned samples, respectively. In this dataset, the proteins PsbS, PsbH, PsbR, and Psb28 presented high abundance in the Burned zone compared to other leaf zones (Limit, Regular and control). These proteins are directly involved in photoinhibition through NPQ or in the biosynthesis/assembly of PSII and their expression was further investigated. A second experiment was performed with a blue light (470 nm peak wavelength) treatment under the same LED set up, resulting in equal leaf zones. Photosynthetic parameters: NPQ, photochemical efficiency of PSII (Fv/Fm), and net photosynthesis rate (Pn) were measured under the red and blue light treatments. The measurements were taken after the light treatments and after the 10-days period of each treatment. A 3-fold NPQ value was detected on the blue light treatment  compared to the red when measured after the light-induced damage. A comparative proteomics analysis between the red and the blue treatments was performed to explore the relative abundance of the key proteins, PsbS, PsbH, PsbR and Psb28. Although they presented high abundance in the Burned sample of the red treatment, their concentration was low in the corresponding blue Burned sample. To further explore the regulation control of the key proteins, the quantification of the psbs, psbr, psb28 and psbh transcripts was accessed through an RT-qPCR. An 8-fold transcript abundance increase of PsbS, a key protein in one of the different NPQ response strategies, was detected in the blue dataset. The low correlation between the protein and mRNA concentrations of PsbS in the blue treatment suggested a high regulation control at the mRNA level. Altogether, the results demonstrated that blue light induced a higher response of NPQ, which was continued by a strategy containing a high regulation of PsbS at the mRNA level. The red treatment response resulted in high concentrations of the PsbS, PsbH, Psb28 and PsbR proteins.</description><creator>Vieira Parrine Sant' Ana, Débora</creator><contributor>Mark Lefsrud (Supervisor)</contributor><date>2019</date><subject>Bioresource Engineering</subject><title>The impact of wavelength in plant's response to extreme light-induced stress</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/7h149r86m.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/w3763923b</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Bioresource Engineering</discipline></degree></thesis></metadata></record><resumptionToken completeListSize="47894">oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):3400</resumptionToken></ListRecords></OAI-PMH>