<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/assets/blacklight_oai_provider/oai2-b0e501cadd287c203b27cfd4f4e2d266048ec6ca2151d595f4c1495108e36b88.xsl"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd"><responseDate>2020-07-24T23:08:23Z</responseDate><request resumptionToken="oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):4725" verb="ListRecords">https://escholarship.mcgill.ca/catalog/oai</request><ListRecords><record><header><identifier>oai:escholarship.mcgill.ca:pc289m354</identifier><datestamp>2020-03-21T05:23:15Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>A non-asymptotic approach to simultaneous model and state estimation is discussed in this thesis. Specifically, given a cloud of measurement points presumed to represent a dynamical system output trajectory,a linear model is constructed to fit it best. The approach is somewhat similar to variational data assimilation in that it employs a cost functional as a measure of model fitness and computes its gradient by solution of an adjoint sensitivity problem. A kernel system model is first constructed and viewed as a linear finite dimensional subspace of a reproducing kernel Hilbert space (RKHS). The subspace is linearly parametrized by the unknown system constants whose values determine the subspace "orientation" vis à vis the cloud of measurement points. The system constants are found simultaneously with reconstructing the system state. A geometrical interpretation of this process is particularly appealing as it can be visualized as subspace re-orientation in the RKHS by way of minimizing the residual of the modelling error. The method does not employ numerical differentiation techniques. The joint model and state estimation does not require any information about boundary conditions of the presumed dynamical system nor the measurement noise characteristic. Additionally, as the optimization process searches for the best estimates of the system parameters the "empirical" statistical distribution of the modelling residual error can be tested for congruence with a priori knowledge about the measurement noise. The non-parametric Kolmogorov probability density test is used in this thesis to test such congruence. Examples confirm high accuracy of adaptive estimation, but the algorithm is computationally expensive as it employs optimization by iterative search. The fifth chapter of the thesis presents a radical re-formulation of the problem that directly employs the basis functions of the RKHS. Such formulation bypasses the evaluation of the fundamental solutions of the system model and does not require variational techniques. Numerical results pertaining to the latest problem statement will be presented in a forthcoming publication.</description><description>Cette thèse présente une approche non-asymptotique d'estimation simultanée du modèle du système et de son état. Plus précisément, un modèle linéaire est construit afin de s'ajuster à une trajectoire de sortie de système dynamique représentée par un nuage de points de mesure. L'approche est quelque peu similaire à l'assimilation variationnelle des données en ce sens qu'elle utilise un coût fonctionnel pour obtenir un ajustement de modèle et calcule son gradient par la résolution d'un problème de sensibilité adjoint. Un modèle de système à noyau est d'abord construit et considéré comme un sous-espace linéaire de dimension finie d'un espace de Hilbert à noyau de reproduction (RKHS). Le sous-espace est paramétré de manière linéaire par les constantes inconnues du système dont les valeurs déterminent l'orientation du sous-espace par rapport au nuage de points de mesure. Les paramètres du système sont trouvées simultanément avec la reconstruction de l'état du système. Une interprétation géométrique de ce processus est particulièrement attrayante car elle peut être visualisée comme une réorientation du sous-espace dans le RKHS en minimisant le résidu de l'erreur de modélisation. La méthode n'utilise pas de techniques de différentiation numérique. Le modèle conjoint et l'estimation d'état ne nécessitent aucune information sur les conditions limites du présumé système dynamique ni sur la caractéristique de bruit de mesure. De plus, pendant que le processus d'optimisation recherche les meilleures estimations des paramètres du système, la distribution statistique empirique de l'erreur résiduelle de modélisation peut être testée pour la congruence, avec la connaissance a priori du bruit de mesure. Le test de densité de probabilité non paramétrique de Kolmogorov est utilisé dans cette thèse pour tester une telle congruence. Les exemples confirment la haute précision de l'estimation adaptative, mais l'algorithme est coûteux en calcul car il utilise un processus itératif d'optimisation.Le cinquième chapitre présente une reformulation radicale du problème qui utilise directement les fonctions de base du RKHS. Une telle formulation contourne l'évaluation des solutions fondamentales du modèle de système et ne nécessite pas de techniques variationnelles. Les résultats numériques relatifs au dernier énoncé du problème seront présentés dans une prochaine publication.</description><creator>Pandey, Abhishek</creator><contributor>Hannah Michalska (Internal/Supervisor)</contributor><date>2018</date><subject>Electrical and Computer Engineering</subject><title>Deterministic variational approach to system modelling by data assimilation</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/3197xp06h.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/pc289m354</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:x346d668b</identifier><datestamp>2020-03-21T05:23:16Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The mechanism by which neutrino masses arise is still unknown, yet it is clear from neutrino oscillation experiments that they do have mass. One possible solution is through the observation of neutrinoless double beta decay, a lepton number violating process that is beyond the current Standard Model. The existence of this decay would show that the neutrino is a Majorana particle, meaning that it is its own antiparticle. This would provide a natural explanation for the smallness of the neutrino masses, as well as a measurement of the effective Majorana neutrino mass. The EXO collaboration is searching for neutrinoless double beta decay with the Xenon-136 isotope deployed in a time projection chamber. In the first part of this work, a data analysis cut has been developed and implemented to reduce the background created by Xenon-137 decay in the EXO-200 experiment, which is currently in operation. The cut is called the Xenon-137 veto and has a rejection efficiency of 21+- 4% for Phase-I data and 21 +- 5% for Phase-II data. In the second part of this work, a multiple-reflection time-of-flight mass-spectrometer has been designed and simulated for use in Barium-tagging. The Barium-tagging project seeks to allow for a background free measurement of double beta decays, by extracting and identifying the daughter Barium ions they produce. This setup will be used in future tonne-scale experiments such as nEXO, where the role of the mass-spectrometer will be to conduct systematic studies of the ion extraction technique, and provide further identification of the Ba ion. Simulations show that the device can achieve a mass resolving-power of 70000, which is sufficient for isobaric separation. Moreover it can operate in a fast turn switching mode, which makes it suitable for broad-range mass spectroscopy.</description><description>Le mécanisme par lequel la masse des neutrinos survient est encore inconnu. Pourtant, à partir des expériences d'oscillation de neutrinos, il est clair qu'ils en ont une. Une solution envisageable est l'observation de la double désintégration bêta sans émission de neutrino, un processus au-delà du modèle standard de la physique des particules actuel qui viole la conservation du nombre leptonique. L'existence de cette désintégration montrerait que le neutrino est une particule de Majorana, c'est-à-dire qu'elle est sa propre antiparticule. Cela fournirait une explication naturelle de leurs masse relativement petite, ainsi qu'une mesure de leurs masse effective. La collaboration EXO recherche la double désintégration bêta sans émission de neutrino avec l'isotope Xenon-136 déployé dans une chambre à projection temporelle. Dans la première partie de ce travail, une coupe d'analyse de données est développée et mise en oeuvre pour réduire le bruit de fond créé par la désintégration du Xenon-137 dans l'expérience EXO-200, qui est actuellement en cours. La coupe, appelée le Xenon-137 veto, a une efficacité de rejet de 21+-4% pour les données Phase-I et de 21+-5% pour les données Phase-II. Dans la deuxième partie de ce travail, un spectromètre de masse à temps de vol multi-réflecteur est conçu et simulé pour être utilisé dans le Barium-tagging. Le projet de Barium-tagging vise à permettre une mesure sans bruit de fond des doubles désintégrations bêta, en extrayant et en identifiant les ions filles du Barium qu'ils produisent. Cette configuration sera utilisée dans de futurs expériences avec une quantitée de Xenon-136 à l'échelle de tonnes, telles que nEXO, où le rôle du spectrométre de masse sera de mener des études systématiques sur la technique d'extraction d'ions et de raffiner l'identification de l'ion de Barium. Les simulations montrent que le spectromètre peut atteindre une résolution de masse de 70000, ce qui est suffisant pour la séparation d'isobares. De plus, le spectromètre peut fonctionner dans un mode de commutation à virage rapide, ce qui le rend convenable pour un vaste éventail de spectroscopie de masse.</description><creator>Murray, Kevin</creator><contributor>Thomas Brunner (Internal/Supervisor)</contributor><date>2018</date><subject>Physics</subject><title>The design and optimization of a multi-reflection time-of-flight mass-spectrometer for Barium tagging with nEXO and optimization of the Xenon-137 veto with EXO-200</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/gx41mm28d.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/x346d668b</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:gh93h1956</identifier><datestamp>2020-03-21T05:23:16Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Beaucoup d'applications en ingénierie ont besoin d'algorithmes rapides et précis pour résoudre les problèmes de complémentarité linéaire (P.C.L.) mixtes qui sont obtenus lors d'une simulation multicorps assujettie à l'unilatéralité et au frottement. L'évaluation de la précision de ces algorithmes requiert la détermination de l'erreur introduite. Cette thèse montre que les mesures d'erreur fréquemment utilisées souffrent d'incohérence d'unités physiques, dont la conséquence est que l'erreur n'a pas de signification physique. Nous proposons une mesure d'erreur qui est cohérente en unités et qui calcule les composantes de l'erreur en énergie qui sont dépendantes de la masse effective de chaque contrainte. Le compromis entre la précision et la performance de calcul est un des principaux conflits dans les simulations de dynamique multicorps en temps réel. La méthode utilisée pour résoudre les équations du mouvement est responsable pour une grande partie de ce compromis. Dans cette thèse, nous comparons quatre algorithmes de résolution du P.C.L. mixte en termes de précision et performance. Nous considérons plusieurs scénarios qui incluent le contact et le frottement tels que la prise d'un tronc d'arbre, la stabilité de modèles statiques, les modèles de boucles fermées et les longues chaînes de corps rigides. L'objectif est de déterminer les caractéristiques de précision et de performance de chaque méthode. Des contraintes redondantes peuvent apparaître en raison d'un manque d'informations sur le système physique représenté par le modèle de corps rigide. Une telle redondance empêche la détermination des forces de réaction de contraintes de façon unique. Dans cette thèse, nous présentons une étude de cas sur les problèmes de corps rigides redondants avec contact afin de déterminer les forces de réactions obtenues par l'algorithme de résolution du P.C.L de Lemke et celui de Murty. Ces méthodes choisissent toujours un sous-ensemble des contraintes pour lequel il existe une solution unique. Nous montrons que ceci est équivalent à l'approche d'élimination de contraintes redondantes couramment utilisée dans les logiciels de simulation multicorps.</description><description>Many engineering applications require fast and accurate mixed linear complementarity problem (MLCP) solution algorithms for multibody simulations subject to unilaterality and friction. Evaluation of the accuracy of MLCP solvers calls for the determination of the error introduced by the solver. In this thesis, we find that commonly used MLCP error measures suffer from unit inconsistency leading to the error lacking any physical meaning. We propose a unit-consistent error measure which computes energy error components dependent on the effective mass for each constraint. The trade-off between accuracy and computational performance is one of the central aspects of real-time multibody dynamics simulation, much of which can be attributed to the method used to solve the dynamic formulation. This thesis then compares four MLCP algorithms in terms of accuracy and computational performance. We consider several scenarios involving frictional contact such as grasping, stability of static models, closed loops, and long chains of bodies. The objective of this study is to determine the accuracy and performance properties of each solution method. In multibody simulations, redundant constraints can occur due to a lack of information about the physical system represented by the rigid body model. Such redundancy prevents the unique determination of the reaction forces for all constraints. In this thesis, we present a case study of simple redundant rigid body problems with contact in order to determine the reaction forces obtained by Lemke's or Murty's pivoting algorithm. These methods always select a subset of constraints for which there exists a unique solution. This is discovered to be equivalent to the constraint elimination approach widely used in multibody simulation software.</description><creator>Enzenhöfer, Andreas</creator><contributor>Jozsef Kovecses (Internal/Supervisor)</contributor><date>2018</date><subject>Mechanical Engineering</subject><title>Numerical solution of mixed linear complementarity problems in multibody dynamics with contact</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/2z10ws808.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/gh93h1956</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:m039k754v</identifier><datestamp>2020-03-21T05:23:17Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The spectral composition of light-emitting diodes (LEDs) has been reported to result in higher yield, prevent wilting, and reduce thermal damage to plants. The use of LEDs for postharvest storage and shelf life extension has been limited, but the potential of this technology will allow for greater applications in horticulture and the food industry. In this experiment, 'Winterbor' kale (Brassica oleracea L.) and 'Melody' spinach (Spinacia oleracea L.) plants were measured for photosynthesis response and stomata response under 14 different wavelengths of light. The data collected from the measurements were used to select two different wavelengths of LEDs and determine the proper irradiance levels for an LED irradiance storage test on spinach and kale. The hypothesis was that by selecting a wavelength that results in stomata closure and selecting an irradiance level at the light compensation of the plant, the plants can be stored successfully for a longer period, compared to conditions under fluorescent light or no light. Treatments of the blue lights (ranging from 405 nm to 470 nm), red lights (ranging from 624 nm to 661 nm), and amber light (595 nm) were effective at increasing the stomatal opening, while the green lights (ranging from 501 nm to 560 nm) resulted in reduced stomatal opening. For spinach, the light response curve resulted in 500 nm and 560 nm having a light compensation point at 65.3 and 64.7 µmol m-2 s-1, respectively. For kale, the light compensation points of 500 nm and 560 nm were at 50.8 and 44.1 µmol m-2 s-1, respectively. For the storage test experiment at room temperature, kale and spinach were stored under four different treatments: dark treatment as a control, standard white fluorescent light (at 50 µmol m-2 s-1), and 500 nm and 560 nm LED wavelengths (at 55 µmol m-2 s-1). Shelf life testing of kale resulted in the lowest moisture loss of 40% at 560 nm treatment and 41% moisture loss for spinach. The control (dark) had the highest moisture loss at 54% for kale and 52% for spinach. A visual assessment scale was monitored throughout the experiment and results showed a better visual quality in kale under 560 nm compared to the lowest visual quality under the dark treatment by day 4. For spinach, the visual quality for 560 nm treatment followed a similar pattern as fluorescent and 500 nm, resulting in a poor-quality product by day 4 and the lowest-quality product by day 5. The LED treatments improved the shelf life of spinach and kale which was the resultant of stomatal aperture closure, the photosynthetic rate near light compensation point and stability of atmospheric moisture content. </description><description>La composition spectrale de diodes électroluminescentes (LED) a été rapportée pour offrir un rendement supérieur, prévenir la flétrissure et réduire les dommages thermiques aux plantes. L'effet de l'utilisation de LED pour l'entreposage après récolte sur la prolongation de la durée de conservation a été limité, mais le potentiel de cette technologie permettra une plus grande application dans l'horticulture et l'industrie alimentaire. Dans cette expérience, le chou frisé 'Winterbor' (Brassica oleracea L.) et les épinards 'Melody' (Spinacia oleracea L.) ont été mesurés pour la photosynthèse et réponse sous 14 différentes longueurs d'onde de lumière. Les données recueillies à partir des mesures ont été utilisées pour sélectionner deux différentes longueurs d'onde de LED et déterminer le niveau d'irradiance LED pour un test d'entreposage avec des épinards et du chou frisé. L'hypothèse était qu'en sélectionnant une onde qui entraîne la fermeture des stomates et la sélection d'un niveau d'éclairement à la compensation de la lumière de la plante, les plantes peuvent être entreposées avec succès pour une période plus longue, par rapport aux conditions en lumière fluorescente ou sans lumière. Les traitements à la lumière bleue (allant de 405 nm à 470 nm), la lumière rouge (allant de 624 nm à 661 nm), et jaune (595 nm) sont efficaces pour augmenter l'ouverture des stomates, tandis que la lumière verte (allant de 501 nm à 560 nm) a permis de réduire l'ouverture des stomates. Pour les épinards, la courbe de réponse de lumière a s'est située à 500 nm et 560 nm, possédant un point de compensation de lumière à 65.3 et 64.7 µmol m-2 s-1, respectivement. Pour les choux, les points de compensation de lumière de 500 nm et 560 nm ont été de 50.8 et 44.1 µmol m-2 s-1, respectivement. Pour l'entreposage à température ambiante, le chou frisé et les épinards ont reçu quatre différents traitements : traitement sombre comme contrôle, éclairage fluorescent blanc standard (à 50 µmol m-2 s-1), et 500 nm et 560 nm (à 55 µmol m-2 s-1). La durée de conservation du chou a eu le taux de perte d'humidité de 40 % à 560 nm et de 41 % de perte d'humidité pour les épinards. À la noirceur la perte d'humidité est la plus élevée à 54 % pour le chou frisé et 52 % pour les épinards. Une évaluation visuelle a été effectuée tout au long de l'expérience et les résultats ont montré une meilleure qualité visuelle du chou sous 560 nm par rapport à la plus faible qualité visuelle sous l'obscurité après 4 jours. Pour les épinards, la qualité visuelle à 560 nm suit la même tendance que les lampes fluorescentes et à 500 nm, ce qui entraîne une mauvaise qualité de produit après 4 jours avec la plus faible qualité de produit au jour 5. Les traitements LED ont eu une incidence sur la durée de conservation et la qualité post-récolte des épinards et du chou frisé qui a été la résultante de l'ouverture des stomates, réponse photosynthétique, et la mesure des paramètres de stabilité d'humidité atmosphérique.</description><creator>Rufyikiri, Anne Sophie</creator><contributor>Mark Lefsrud (Internal/Supervisor)</contributor><contributor>Valerie Orsat (Internal/Cosupervisor2)</contributor><date>2018</date><subject>Bioresource Engineering</subject><title>The use of light emitting diodes (LEDs) for shelf life extension of spinach and kale</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/jw827d92v.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/m039k754v</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Bioresource Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:tm70mx93g</identifier><datestamp>2020-03-21T05:23:19Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Congenital Anomalies in the Kidney and Urinary Tract (CAKUT) refer to a range of phenotypes in the kidney and the urinary tract. CAKUT is present in 1 to 6 of every 1,000 live births, and is a major cause of kidney failure in children. To better understand how congenital kidney malformations arise in children, it is important to define the molecular and cellular events of two important processes in kidney development: nephric duct elongation and branching morphogenesis. The nephric duct elongates caudally and gives rise to the ureteric bud, which will undergo branching morphogenesis, thereby forming the collecting duct system of the kidney. Defects in either of these processes can lead to congenital renal malformations. Several genes have been associated with renal malformations in mice and in humans from sequencing cohorts with CAKUT. However, the variants discovered so far only account for a small subset of patients with CAKUT. Thus, it is necessary to sequence more patients for genes outside of the ones already implicated in kidney development. Previous studies by our group showed that members of the claudin family of tight junction proteins are required for nephric duct elongation and branching morphogenesis during kidney development. I hypothesize that claudin sequence variants in patients with kidney malformations will result in defects in nephric duct elongation and/or ureteric bud branching. To test this hypothesis, I first analyzed the expression patterns of claudins during nephric duct elongation in mouse and chick. Claudin-1, -3 and -4 were expressed in the chick nephric duct from HH12 (Hamburger Hamilton stage), when the nephric duct starts to form, and they continued to be expressed once the entire nephric duct was epithelialized. In the mouse embryo, Claudin-4 was expressed in the nephric duct at E10.5, as well as in the ureteric bud and trunk, and the ureter at E13.5 and E16.5. Next, I analyzed the claudin coding sequences of 96 patients with congenital renal malformations from the NIH-sponsored CKiD cohort (Chronic Kidney Disease). I identified 17 rare and novel heterozygous variants, 11 of which are predicted to be pathogenic. A rare variant in CLDN8, and a novel variant in CLDN14 were subjected to functional analysis. Retroviral overexpression of these variants in the chick embryo resulted in impaired elongation of the nephric duct, when assessing the embryos by in situ hybridization using a marker of the duct. </description><description>Les anomalies congénitales du rein et des voies urinaires (Congenital Anomalies in the Kidney and Urinary Tract, CAKUT) font référence à un spectre de malformations du rein et des voies urinaires. Les CAKUT touchent 1 à 6 nouveau-nés sur 1000 naissances, et sont une cause majeure d'insuffisance rénale chez les enfants. Pour mieux comprendre comment les malformations congénitales du rein surviennent chez les enfants, il est important de définir les événements moléculaires et cellulaires de deux processus importants dans le développement du rein: l'allongement du canal de Wolff et la morphogenèse de ramification. Le canal de Wolff s'allonge caudalement et donne naissance au bourgeon urétéral, qui va se ramifier, formant ainsi le système du canal collecteur rénal. Une anomalie dans l'un ou l'autre de ces processus peuvent entraîner des malformations congénitales du rein. Plusieurs gènes ont été associés à des malformations rénales chez la souris et chez l'homme en séquençant des cohortes atteintes de CAKUT. Cependant, les variantes génétiques découvertes jusqu'à présent ne représentent qu'un petit sous-groupe de patients avec des CAKUT. Ainsi, il est nécessaire de séquencer plus de patients pour des gènes en dehors de ceux déjà impliqués dans le développement du rein. Notre groupe de recherche a démontré dans des études antérieures que les claudines, une famille de protéines situées dans les jonctions serrées, sont nécessaires pour l'élongation du canal de Wolff et dans la morphogenèse de ramification au cours du développement du rein. Je fais l'hypothèse que les variantes dans la séquence des claudines chez les patients atteints de malformations rénales entraîneront des anomalies dans l'allongement du canal de Wolff et/ou de la ramification du bourgeon urétéral. Pour tester cette hypothèse, j'ai d'abord analysé les profils d'expression des claudines au cours de l'allongement du canal de Wolff chez la souris et le poussin. Les Claudin-1, -3 et -4 sont exprimées dans le canal de Wolff du poulet au stade HH12 (stade Hamburger Hamilton), lorsque le canal de Wolff commence à se former, et ils continuent à être exprimés suite à l'épithélialisation complète du canal de Wolff. Chez la souris, au stade E10.5 (jour 10.5 du développement embryonnaire), la Claudin-4 est exprimée dans le canal de Wolff. Elle est aussi exprimée dans le bourgeon et le tronc urétéral, et l'uretère à E13.5 et E16.5. Ensuite, j'ai analysé les séquences codantes des claudines de 96 patients atteints de malformations congénitales du rein de la cohorte CKiD parrainée par le NIH (Chronic Kidney Disease). J'ai identifié 17 rares et nouvelles variantes hétérozygotes dont 11 étaient pathogènes. Une variante rare dans CLDN8 et une nouvelle variante dans CLDN14 ont été soumises à une analyse fonctionnelle. La surexpression rétrovirale de ces variantes dans l'embryon de poulet a entraîné une altération de l'allongement du canal de Wolff observée lors de l'évaluation des embryons par hybridation in situ en utilisant un marqueur du canal de Wolff.</description><creator>Laverde Duarte, Maria</creator><contributor>Indra Gupta (Internal/Supervisor)</contributor><contributor>Aimee Ryan (Internal/Cosupervisor2)</contributor><date>2018</date><subject>Human Genetics</subject><title>Characterization of temporal claudin expression and analysis of sequence variants in early kidney development</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/h989r549g.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/tm70mx93g</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Human Genetics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:k643b346s</identifier><datestamp>2020-03-21T05:23:20Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Introduction:Colorectal cancer (CRC) is the third most common cancer worldwide. CRC liver metastases (CRCLM) is a significant clinical problem. Untreated, patients will survive for a few months only. Three CRCLM histopathological growth patterns (HGP) have been identified: 1) the desmoplastic, in which, tumors grow by neoangiogenesis with no direct contact between the tumor cells and hepatocytes 2) the pushing, in which liver cell plates are pushed aside by the growing tumor and 3) the replacement, where tumors grow by co-opting the normal hepatic vessels and the tumor cells display infiltrating pattern by replacing parenchymal cells in the liver plates. Patients with a desmoplastic type LM have a better overall survival compared to the replacement type. Two histopathological border patterns have been identified in CRC tumors, the pushing pattern where these is a clear definition of the margin, and the infiltrative type where there is absence of a clear definition of the invasive margin. Patients with a pushing pattern have a more favorable outcome compared to the infiltrative pattern. It is not known whether the border patterns of the primary CRC and their LM are related, or whether they display any similar vascular features including the microvessel density or the vascular architecture. Objectives: The purpose of the study is to determine whether the histopathological growth patterns in liver metastases can be predicted from the primary tumor by evaluating the MVD, vascular architecture and the invasive front of the CRC. Methods: 23 patients who underwent resection for primary CRC and LM were evaluated retrospectively (46 lesions). Tumor specimens were stained for the microvessel density marker: CD31, and the vascular architecture was evaluated using CD34, and Ki67. The growth patterns and type of invasive front were evaluated by H&amp;E staining. MVD was visualized using positivity of CD31 and quantified using ImageScope software. Vascular architecture was assessed based on positivity and the shape of vessels manually. Results: MVD of the CRC did not significantly correlate with the histopathological growth patterns of LM. Also, there was no difference when patients were stratified by the border patterns of CRCs. There was no difference in the vascular architecture (mature and immature vessels) between the two groups of histopathological growth patterns (desmoplastic vs replacement). Finally, analysis showed primary CRCs with a pushing border tended to form desmoplastic liver metastases, and the CRCs with an infiltrative pattern tended to form either desmoplastic or replacement liver metastases. Conclusion: There was no association between the MVD or the vascular architecture of CRC and the HGP of liver metastases. However, the invasive front of primary CRCs could be a potential prognostic factor. These results suggest that primary CRCs and their liver metastases share some histopathological features. Additional investigation is necessary to determine the biological mediators, which trigger both histopathological patterns in the primary and metastases to enhance our understanding of the metastatic process.</description><description>Introduction:Le cancer colorectal (CRC) est le troisième cancer le plus répandu dans le monde. Métastases hépatiques CRC (CRCLM) est un problème clinique important. Non traités, les patients survivront pendant quelques mois seulement. Trois modèles de croissance histopathologique CRCLM (HGP) ont été identifiés: 1) le desmoplastic, dans lequel, les tumeurs se développent par néoangiogenèse sans contact direct entre les cellules tumorales et les hépatocytes 2) la poussée, dans laquelle les plaques de cellules hépatiques sont mises de côté par la croissance la tumeur et 3) le remplacement, où les tumeurs croissent en cooptant les vaisseaux hépatiques normaux et les cellules tumorales présentent un schéma d'infiltration en remplaçant les cellules parenchymateuses dans les plaques hépatiques. Les patients avec un type desmoplastique LM ont une meilleure survie globale par rapport au type de remplacement. Deux profils histopathologiques ont été identifiés dans les tumeurs CRC, le modèle de poussée où il s'agit d'une définition claire de la marge, et le type infiltrant où il n'y a pas de définition claire de la marge invasive. Les patients avec un modèle de poussée ont un résultat plus favorable par rapport au schéma d'infiltration. On ne sait pas si les profils de bordure du CRC primaire et de leur LM sont liés ou s'ils présentent des caractéristiques vasculaires similaires, y compris la densité de microvaisseaux ou l'architecture vasculaire. Objectifs: Le but de l'étude est de déterminer si les profils de croissance histopathologiques dans les métastases hépatiques peuvent être prédits à partir de la tumeur primaire en évaluant la MVD, l'architecture vasculaire et le front invasif de la CRC. Méthodes: 23 patients ayant subi une résection pour une CRC primaire et une LM ont été évalués rétrospectivement (46 lésions). Des échantillons de tumeur ont été colorés pour le marqueur de densité de microvaisseaux: CD31, et l'architecture vasculaire a été évaluée en utilisant CD34 et Ki67. Les profils de croissance et le type de front invasif ont été évalués par coloration H&amp;E. MVD a été visualisé en utilisant la positivité de CD31 et quantifié en utilisant le logiciel ImageScope. L'architecture vasculaire a été évaluée sur la base de la positivité et de la forme des vaisseaux manuellement. Résultats: MVD de la CRC n'a pas de corrélation significative avec les modèles de croissance histopathologique de LM. De plus, il n'y avait pas de différence lorsque les patients étaient stratifiés selon les profils frontaliers des CRC. Il n'y avait aucune différence dans l'architecture vasculaire (vaisseaux matures et immatures) entre les deux groupes de modèles de croissance histopathologiques (desmoplastique vs remplacement). Enfin, l'analyse a montré que les CRC primaires avec une bordure de poussée avaient tendance à former des métastases hépatiques desmoplasiques, et les CRC avec un profil d'infiltration avaient tendance à former des métastases hépatiques desmoplasiques ou de remplacement. Conclusion: Il n'y avait aucune association entre le MVD ou l'architecture vasculaire du CRC et le HGP des métastases hépatiques. Cependant, le front invasif des CRC primaires pourrait être un facteur pronostique potentiel. Ces résultats suggèrent que les CRC primaires et leurs métastases hépatiques partagent certaines caractéristiques histopathologiques. Des recherches supplémentaires sont nécessaires pour déterminer les médiateurs biologiques, qui déclenchent à la fois les profils histopathologiques dans le primaire et les métastases pour améliorer notre compréhension du processus métastatique. </description><creator>Alshwairikh, Khaloud</creator><contributor>Peter Metrakos (Internal/Supervisor)</contributor><date>2018</date><subject>Surgery</subject><title>Can the growth pattern of colorectal cancer predict the histopathological growth pattern of the liver metastasis?</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/xs55mf286.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/k643b346s</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Surgery</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:m326m425n</identifier><datestamp>2020-03-21T05:23:21Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Background: Prenatal maternal stress (PNMS) is associated with altered hippocampal (HC) structure in animals, and HC-associated psychopathologies in humans. The objective of this study was to determine the extent to which PNMS experienced by women pregnant during the 1998 Quebec ice storm affects HC morphology (volume, surface area and shape) in their children; and to determine the extent to which the effects of maternal stress exposure differ between prenatally exposed children and a matched comparison group of early life exposed children, born 1-year before the storm.Methods: Using a longitudinal design, stress measures (objective hardship, subjective distress and cognitive appraisal) were collected from mothers following the storm, and high-resolution T1-weighted structural magnetic resonance images (MRI) were collected from the children at the age of 111⁄2. Automated neuroimaging techniques were used to derive HC morphology measures. Results: Results show that PNMS exposure, but not early life maternal stress (ELMS) exposure, lead to altered bilateral hippocampal volumes, as well as specific volumetric changes in subfields CA1, subiculum and stratum radiatum/lacunosum-moleculare. The direction of the effects depends on the aspect of the stress assessed. Overall, maternal cognitive appraisal was found to be the strongest predictor of adolescent hippocampal volumes, such that a negative appraisal of the storms consequences predicted smaller hippocampal volumes. A trend was observed for higher objective hardship and subjective distress levels predicting larger HC volumes. Hippocampal surface area and shape appear to be unaffected by either PNMS or ELMS. Discussion: These findings lend support to the idea that PNMS contributes to fetal programming and can exert long-lasting effects on children's brain structure.</description><description>Contexte: Le stress maternel prénatal (SMPN) est associé à une altération de la structure de l'hippocampe (HC) chez les animaux et à des psychopathologies associées aux HC chez les humains. L'objectif de cette étude était de déterminer dans quelle mesure les SMPN chez les femmes enceintes durant la tempête de verglas de 1998 affectent la morphologie des HC (volume, surface et forme) chez leurs enfants; et de déterminer dans quelle mesure les effets de l'exposition au stress maternel diffèrent entre les enfants exposés avant la naissance et un groupe de comparaison apparié d'enfants exposés au début de la vie, nés un an avant la tempête. Méthodes: À l'aide d'un plan longitudinal, des mesures de stress (difficultés objectives, détresse subjective et évaluation cognitive) ont été recueillies auprès des mères après la tempête, et des images par résonance magnétique structurale (IRM) à haute résolution ont été recueillies auprès des enfants âge 111⁄2. Des techniques automatisées de neuro-imagerie ont été utilisées pour dériver des mesures de morphologie de HC.Résultats: Les résultats montrent que l'exposition au SMPN, mais pas l'exposition précoce au stress maternel (EPSM), entraîne une altération des volumes bilatéraux de l'hippocampe, ainsi que des changements volumétriques spécifiques dans les sous-domaines CA1, subiculum et stratum radiatum /lacunosum-moleculare. La direction des effets dépend de l'aspect du stress évalué. Dans l'ensemble, l'évaluation cognitive maternelle s'est avérée être le prédicteur le plus fort des volumes d'hippocampe chez les adolescents, de sorte qu'une évaluation négative des conséquences des tempêtes a prédit de plus petits volumes d'hippocampe.La surface et la forme de l'hippocampe ne semblent pas affectées par le SMPN ou l' EPSM. Discussion: Ces découvertes soutiennent l'idée que le SMPN contribue à la programmation fœtale et peut exercer des effets durables sur la structure cérébrale des enfants.</description><creator>McKee, Kyle</creator><contributor>Suzanne King (Internal/Supervisor)</contributor><contributor>Megha Chakravarty (Internal/Supervisor)</contributor><date>2018</date><subject>Neuroscience</subject><title>The effects of prenatal maternal stress and early life maternal stress on adolescent hippocampal morphology: project ice storm</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/6969z3314.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/m326m425n</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Integrated Program in Neuroscience</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:6682x623x</identifier><datestamp>2020-03-21T05:23:22Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Parkinson's disease (PD) is a neurodegenerative disease characterized by the selective and progressive degeneration of dopaminergic neurons in the substantia nigra (SN). While the underlying cause of neurodegeneration is unknown, mitochondrial dysfunction has been identified as a potential cause of PD. Most PD cases are idiopathic, with familial forms accounting for less than ten percent of PD cases. Nevertheless, the identification of rare, inheritable variants of the disease has increased our understanding of PD etiology. More specifically, loss-of-function mutations in the PARK2 gene, which encodes the E3 ubiquitin (Ub)-ligase Parkin, have been linked to autosomal-recessive juvenile parkinsonism (ARJP). Parkin, along with PTEN-induced putative kinase 1 (PINK1), another PD-associated protein, mediate the removal of dysfunctional mitochondria through a specialized form of autophagy. X-ray crystallography, however, has revealed that Parkin is auto-inhibited in its basal state and requires extensive conformational rearrangement to exhibit E3 ligase activity. These structural changes can be monitored using fluorescent probes, that can be used to guide the discovery of small-molecule activators of Parkin. We have developed three fluorescence-based assays to monitor Parkin activity. The first assay detects the transfer of Ub onto Parkin as a direct consequence of Parkin activation. The second is a cell-based assay that monitors specific conformational changes in Parkin during activation using Förster resonance energy transfer (FRET). The third is a modified in vitro FRET assay that can be used to validate small-molecule activators of Parkin. Since Parkin exhibits neuroprotective activity, activators of Parkin have the potential to slow down or stop the progression of PD.</description><description>La maladie de Parkinson (MP) est une maladie neurodégénérative caractérisée par la dégénérescence progressive des neurones dopaminergiques de la substance noire. Tandis que l'agent causant la neurodégénérescence est inconnu, la dysfonction mitochondriale a été identifiée comme une cause potentielle de la MP. La plupart des cas de MP sont idiopathiques, et les formes familiales représentent moins de 10% des cas. Cependant, l'identification de variants rares et héritables de la MP ont amélioré notre compréhension de son étiologie. Plus précisément, des mutations causant une perte-de-fonction du gène PARK2, qui encode l'ubiquitin (Ub)-ligase E3 Parkin, ont été liées au parkinsonisme autosomal-récessif juvénile. Parkin agit avec la kinase putative 1 induite par PTEN (PINK1), elle aussi associée à la MP, pour contrôler l'élimination des mitochondries dysfonctionnelles via une forme spécialisée d'autophagie. La cristallographie aux rayons X a cependant révélé que Parkin est auto-inhibée dans son état basal, et requiert un réarrangement conformationnel majeur afin d'exhiber son activité ligase. Ces changements structurels peuvent être monitorés avec des sondes fluorescentes, guidant la découverte de petite molécules activatrices de Parkin. Nous avons développé trois essais basés sur la fluorescence pour monitorer l'activité de Parkin. Le premier, in vitro, peut détecter le transfert de l'Ub sur Parkin comme conséquence directe de l'activation de Parkin. Le second mesure, par transfert d'énergie par résonance Förster (FRET) in cellulo, les changements conformationnels subis par Parkin au cours de son activation. Le troisième est une version modifiée de FRET in vitro qui peut être utilisé pour valider des petites molécules activatrices de Parkin. Puisque Parkin a une activité neuroprotectrice, des activateurs de Parkin ont le potentiel de ralentir ou d'abolir la progression de la MP.</description><creator>Krahn Roldan, Andrea</creator><contributor>Edward A Fon (Internal/Supervisor)</contributor><date>2018</date><subject>Neuroscience</subject><title>Fluorescence-based screens for identifying small-molecule activators of Parkin</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/s1784p07f.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/6682x623x</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Integrated Program in Neuroscience</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:2z10ws81j</identifier><datestamp>2020-03-21T05:23:23Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Environ 10% des cancers du pancréas (CP) sont héréditaires. Cependant, peu d'entre eux sont expliqués par des gènes connus de susceptibilité au cancer. Les efforts récents pour identifier de nouveaux gènes de susceptibilité pour le CP ont été généralement infructueux, probablement à cause des limitations des méthodes utilisées quant à l'identification d'un « signal faible » dans une grande quantité de « bruit » causé par des variants non causaux. Ainsi, dans cette dissertation, j'ai teste l'hypothèse qu'un test d'association cas-contrôles par région, le Mixed-effects Score Test (MiST), va permettre d'identifier ces « signaux faibles ». En comparaison avec d'autres méthodes, MiST est une approche statistique peu biaisée qui compare la fréquence de mutation d'un gène ou d'une région entre les cas et les contrôles, tout en tenant compte des caractéristiques individuelles des variants. Dans les cas de CP, l'ADN extrait de lymphocytes en circulation ou de salive a été utilisé pour représenter l'ADN germinal. Notre série de cas consiste en 109 cas de CP à haut risque (cancer du pancréas familial ou âge au diagnostic &lt;50) avec séquençage complet d'exome et 289 cas de CP collectés prospectivement de deux centres hospitaliers canadiens (Montréal et Toronto) avec séquençage ciblé pour 710 gènes reliés au cancer. Notre série contrôle consiste en 987 cas indemnes de cancer collectés de multiples projets locaux. Tous les échantillons ont été traités sur la même pipeline et les variants étaient limités aux variants exoniques et variants d'épissage. Avant de faire ce test, nous avons utilisé une analyse en composantes principales pour exclure les valeurs aberrantes. Pour augmenter la puissance de notre étude, nous avons limité notre test d'association à 449 gènes associés à la réparation de l'ADN, pour lesquels 682 variants rares (fréquence d'allèle mineur &lt;0.5%) ont été identifiés dans 418 de ces gènes. MiST a été exécuté pour 235 gènes qui avaient &gt;10 variants rares identifiés dans tous les cas et contrôles; de ceux-ci, 42 gènes étaient significatifs avant correction pour tests multiples (p&lt;0,05), incluant les gènes connus de prédisposition au cancer BRCA1, BRCA2 et STK11. Après correction (p&lt;0,00021), seulement deux gènes sont restés significatifs : RECQL et SMG1. La méthode « drop-one » a été utilisée pour déterminer les variants candidats responsables de l'association avec le CP pour les 3 gènes connus de prédisposition héréditaire et les 2 gènes candidats. Avec cette méthode, (augmentation de valeur-p&gt;35%), nous avons pu identifier les mutations pathogènes connues dans les gènes BRCA1 et BRCA2, de même qu'une liste de variants candidats dans les 5 gènes. Comme l'association pour les gènes STK11 et RECQL était causée par des variants chez les contrôles, celles-ci ont été écartées comme un effet protecteur difficile à évaluer avec notre taille d'échantillon. En revanche,  l'association pour SMG1, notre meilleur candidat (p=3.22E-7), était causée par 15 variants identifiés chez 29 cas et 1 contrôle. Pour supporter davantage SMG1 comme gène candidat de susceptibilité au cancer, des analyses de ségrégation ont été réalisées pour deux familles pour lesquelles des échantillons étaient disponibles. Chez l'une de ces familles, nous avons observé une ségrégation du variant chez trois individus avec CP. En résumé, cette dissertation démontre la faisabilité et l'utilité d'une approche d'association génétique par région pour identifier de nouveaux gènes de susceptibilité, de même que pour prioriser les variants à investiguer pour de futures analyses fonctionnelles. En utilisant ces méthodes, nous avons identifié un nouveau gène candidat de susceptibilité au CP, SMG1, qui devra être validé davantage dans d'autres cohortes et par des analyses fonctionnelles.</description><description>Approximately 10% of pancreatic cancer (PAC) cases are hereditary in nature, however, only a fraction is explained by known susceptibility genes. Recent efforts using Next-Generation Sequencing (NGS) data to elucidate novel predisposition genes for PAC risk have been plagued with challenges due to the limitations of the methods in identifying a "faint signal" from a large amount of "noise" created by non-causal variants. Thus, in this dissertation, I have hypothesized that a region-based case-control gene association test, the Mixed-effects Score Test (MiST), will allow for the identification of these "faint signals" in NGS data sets. Compared to previous methods, MiST is a less biased statistical approach which compares mutation frequency across a gene or region for cases versus controls, while accounting for individual variant characteristics. For PAC cases, DNA extracted from circulating lymphocytes or saliva was used as a surrogate for germline DNA. Our case series consists of 109 exomes from high risk PAC cases (familial pancreatic cancer or young onset &lt;50) and 289 prospectively collected PAC cases sequenced for 710 cancer-related genes. Our control series consists of 987 non-cancer cases collected locally from multiple projects. All samples were processed on the same pipeline and variants were limited to exonic and splicing variants. Prior to analysis, we used a principal component analysis to exclude genetic outliers. To increase the power of our study, we limited our association test to 449 DNA repair genes, for which 6842 rare variants (MAF&lt;0.5%) were identified across 418 of these genes. MiST was performed for 235 genes which had &gt;10 rare variants identified across all cases and controls; of these, 42 genes were significant prior to multiple testing correction (p&lt;0.05), including the known susceptibility genes, BRCA1, BRCA2 and STK11. After correction (p&lt;0.00021), only two genes remained significant, RECQL and SMG1. The drop-one method was performed to determine candidate variants driving the association with PAC for the 3 known susceptibility genes and the 2 candidate genes. Using drop-one (increase in p-value &gt;35%), we were able to identify the known pathogenic mutations in BRCA1 and BRCA2, as well as a list of candidate variants in all 5 genes. The association for STK11 and RECQL were driven by variants in controls, thus, these were discarded as a protective effect is difficult to evaluate with our sample size. On the other hand, SMG1, our top candidate (p=3.22x10-7), was driven by 15 variants identified across 29 cases and 1 control. To further support SMG1 as a candidate susceptibility gene, segregation analyses were performed for two families, where samples were available. In one family, we observed segregation of the variant with 3 individuals with PAC. In summary, this dissertation demonstrates the feasibility and utility of using a region-based gene association study to identify novel susceptibility genes, as well as prioritizing variants for future functional analyses. Using these methods, we have identified a novel candidate PAC susceptibility gene, SMG1, which will need to be further validated in other cohorts and functional analyses.</description><creator>Wong, Cavin</creator><contributor>George Zogopoulos (Internal/Supervisor)</contributor><date>2018</date><subject>Human Genetics</subject><title>A gene association study to identify novel pancreatic cancer susceptibility genes</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/6w924f13n.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/2z10ws81j</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Human Genetics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:gf06g504r</identifier><datestamp>2020-03-21T05:23:24Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The advancements in next-generation sequencing technologies have revolutionized microbiome research by allowing culture-independent high-throughput profiling of the genetic contents of microbial communities. Nowadays, 16S rRNA based marker gene sequencing is widely used to characterize the taxonomic composition and phylogenetic diversity of complex microbial communities. However, statistical, visual and functional analysis of such data possess great challenges. In addition, many aspects of the current approaches can be improved to get a better understanding of communities. The proper analysis of the resulting large and complicated datasets remains a key bottleneck in current microbiome studies. Over the last decade, powerful computational pipelines and standard protocols have been developed to support efficient raw data processing and annotation of microbiome data. The focus has now shifted towards downstream statistical analysis and functional interpretation. To address this bottleneck, we have developed MicrobiomeAnalyst, a user-friendly web-based tool that incorporates recent progresses in statistics and interactive visualization techniques, coupled with novel knowledge bases, to facilitate comprehensive analysis of common data sets generated from microbiome studies. MicrobiomeAnalyst contains four major components, including i) a module for community diversity profiling, comparative analysis and functional prediction of 16S rRNA marker gene data; ii) a module for exploratory data analysis, functional profiling and metabolic network visualization for shotgun metagenomics or metatranscriptomics data; iii) a module to help users to interpret their taxa of interest via enrichment analysis against ~300 taxon sets manually collected from recent literature and public databases; and iv) a module to allow users to visually explore their data sets within the context of compatible public data (meta-analysis) for pattern discovery and biological insights. The tool is freely accessible at http://www.microbiomeanalyst.ca. </description><description>Les progrès dans les technologies de séquençage de nouvelle génération ont révolutionné la recherche sur le microbiôme en permettant un profilage à haut débit, indépendamment de la culture du contenu génétique des communautés microbiennes. De nos jours, le séquençage du gène marqueur basé sur l'ARNr 16S est largement utilisé pour caractériser la composition taxonomique et la diversité phylogénétique des communautés microbiennes complexes. Cependant, l'analyse statistique, visuelle et fonctionnelle de ces données présente de grands défis. En outre, de nombreux aspects des approches actuelles peuvent être améliorés pour mieux comprendre les communautés. L'analyse appropriée des données volumineuses et complexes reste un goulot d'étranglement majeur dans les études actuelles sur le microbiôme. Au cours de la dernière décennie, de puissantes méthodes computationnelles et des protocoles standardisés ont été développés pour prendre en charge un traitement et une annotation des données efficacement. Inversement, l'accent a désormais été mis sur l'analyse statistique en aval et l'interprétation fonctionnelle.Pour remédier à ce goulot d'étranglement, nous avons développé MicrobiomeAnalyst, un outil web convivial qui intègre les progrès récents dans les statistiques et les techniques de visualisation interactives, couplées avec de nouvelles bases de connaissances, pour faciliter l'analyse complète des profils taxonomiques et fonctionnels communs issus des études sur le microbiôme. MicrobiomeAnalyst comprend quatre modules majeurs, dont de i), un module pour le profilage de la diversité de la communauté, de l'analyse comparative et de la prédiction fonctionnelle des données du gène marqueur de l'ARNr 16S, de ii), un module pour l'analyse exploratoire des données, le profilage fonctionnel et la visualisation du réseau métabolique pour les données de métagénomique ou de métatranscriptomique « Shotgun », de iii), un module pour aider les utilisateurs à interpréter leurs taxons d'intérêt par l'analyse d'enrichissement contre notre base de données d'environ 300 ensembles de taxons collectés manuellement à partir de la littérature récente et de bases de données publiques, et de iv), un module pour aider les utilisateurs à explorer visuellement leurs données dans le contexte de données publiques (méta-analyse) pour la découverte de modèles et de connaissances biologiques. L'outil est librement accessible à http://www.microbiomeanalyst.ca.</description><creator>Dhariwal, Achal</creator><contributor>Jianguo Xia (Internal/Supervisor)</contributor><date>2018</date><subject>Animal Science</subject><title>Statistical, visual and functional analysis of microbiome data</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/9306t165h.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/gf06g504r</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Animal Science</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:wm117r33p</identifier><datestamp>2020-03-21T05:23:26Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Title: The role of the locus coeruleus-noradrenergic system in resilience to foot-shock stress.Background: Post-traumatic stress disorder (PTSD) is caused by exposure to extreme traumatic stress and is characterised by intrusive fear-related memories, hyper-arousal and avoidance. Most individuals exposed to traumatic stress do not develop PTSD and are considered resilient. Women are twice more likely to develop the disorder. Researchers have placed intense interest in understanding the neural basis of resilience to trauma-type stress. It is believed that the locus coeruleus-noradrenergic (LC-NA) system plays a modulatory role in resilience to trauma-type stress and may underlie gender differences in resilience. Methods: We used the learned helplessness (foot-shock) mouse model which produces two behavioural phenotypes: helpless mice that are susceptible to stress and non-helpless mice that are resilient to stress. We compared male wild-type (WT) and female WT mice and then compared them to male and female VMAT2DBHcre knock-outs (KO) which are mice depleted of noradrenergic (NA) transmission. We measured the activity of LC-NA neurones using in vivo single-unit electrophysiological recordings in WT controls, helpless and non-helpless mice. Using fluorescent retrograde tracer beads injected into the ventral tegmental area (VTA) and c-Fos immunohistochemistry, we specifically measured the neuronal activation of LC-NA neurons projecting to the VTA in WT control, helpless and non-helpless mice. Results: We found that depletion of NA transmission alleviated helpless behaviour immediately after stress exposure in males and promoted the extinction of helpless behaviour over time in females. Furthermore, in-vivo electrophysiological recordings of LC-NA neurons showed a decrease of bursting activity and frequency of LC-NA neurons in non-helpless mice compared to helpless and control mice. We showed that c-Fos expression of VTA projecting LC-NA neurons was the same between the three groups. Conclusion: These findings suggest that decreased NA transmission and more specifically an active decrease of LC-NA neuronal activity drive resilient behaviour. However, LC modulation of resilience to foot-shock stress is not mediated by LC-NA projections to the VTA. Moreover, this LC-NA modulation of resilience is sex dependent as we see a different time effect in males and females.</description><description>Titre: Le system locus coeruleus-noradrénergique dans la résilience au stress provoqué par chocs électriques.Contexte: L'état de stress post traumatique (EPST) est une pathologie consécutive à un traumatisme extrême et est caractérisé par des souvenirs intrusifs liés à la peur, l'hyperactivité et l'évitement. La majorité des individus exposés à un traumatisme ne développent pas l'ESPT et sont considérés résilients. Les femmes sont deux fois plus susceptibles de développer l'ESPT comparées aux hommes. Les chercheurs tentent d'expliquer les mécanismes neuronaux impliqués dans la résilience au stress traumatique.  Le système locus coeruleus-noradrénergique (LC-NA) pourrait jouer un rôle modulateur dans la résilience au stress traumatique et pourrait expliquer les différences hommes/femmes. Méthodes: Nous avons utilisé le modèle de souris 'd'impuissance apprise' (chocs électriques) qui produit deux phénotypes comportementaux: des souris vulnérables au stress et des souris résilientes au stress. Nous avons comparé des souris mâles et femelles wild-type (WT) à des souries mâles et femelles VMAT2DBHcre knock-out (KO) qui n'ont plus de transmission de NA centrale. Nous avons mesuré l'activité des neurones NA du LC en faisant des enregistrements électrophysiologiques in vivo chez des souris WT contrôles, susceptibles et résilientes. En utilisant des billes fluorescentes rétrogrades injectées dans l'aire tegmental ventrale (AVT) et l'immunohistochimie c-Fos, nous avons spécifiquement mesuré l'activation neuronale des neurones LC-NA projetant vers l'AVT chez ces souris.Résultats : Une déplétion de la transmission centrale de NA a atténué le comportement susceptible chez les males immédiatement après l'exposition au stress et a empêché la persistance du comportement susceptibles chez les femelles. Les enregistrements électrophysiologiques ont montré une diminution de la fréquence et des décharges en bouffées des neurones LC-NA chez les souris résilientes par rapport aux souris susceptibles et contrôles. Nous avons montré que l'expression de c-Fos dans les neurones LC-NA projetant vers l'AVT est la même pour les trois groupes. Conclusion: Ces résultats suggèrent qu'une diminution de la transmission NA et plus spécifiquement une diminution de l'activité du LC entraîne un comportement résilient. Mais cette résilience n'est pas modulée par les projections du LC vers l'AVT. De plus cette modulation est différente chez les mâles et les femelles. </description><creator>Guinaudie, Chloe</creator><contributor>Bruno Giros (Internal/Supervisor)</contributor><date>2018</date><subject>Neuroscience</subject><title>The role of the locus coeruleus-noradrenergic system in resilience to foot-shock stress</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/6969z332d.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/wm117r33p</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Integrated Program in Neuroscience</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:n583xx29f</identifier><datestamp>2020-03-21T05:23:27Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Introduction:  There is broad agreement in surgical education that we need to move away from the current time-based model of training to a proficiency-based model.   One of the challenges to this lies in the ability to have valid assessments of proficiency at various stages of training.  To this end, there has been success in developing fundamental assessments and curricula in general surgery training which include the Fundamentals of Laparoscopic Surgery (FLS), Fundamentals of Endoscopic Surgery (FES) and the Fundamental Use of Surgical Energy (FUSE).  However, there are no advanced skill assessments or curricula.  We describe our experience developing and validating an advanced laparoscopic skills (ALS) assessment and curriculum with the goal that it will become adopted in the proficiency-based model of surgical training.Methods:  We describe a series of projects which are used to build validity evidence for adoption of the ALS assessment and curriculum.  We began with a nation-wide needs assessment of stakeholders.  We then developed simulated tasks based on the survey, and began to build evidence of internal structure validity by assessing multiple levels of learners.  Finally, we developed proficiency benchmarks for the curriculum.Results:  The needs assessment targeted minimally invasive surgery (MIS) fellows, past fellows and program directors and included 186 respondents for a response rate of 64%.  The majority (73%) identified the need for an ALS curriculum and 78% identified laparoscopic suturing as the most needed portion such a curriculum.  Next, a series of laparoscopic suturing tasks was developed.  Based on our novel metrics, expert MIS surgeons out-performed surgery residents on the following tasks: needle handling, (p = 0.04) off-angle suturing, (p&lt; 0.01) back-hand suturing, (p = 0.01) confined space suturing, (p = 0.02) suturing under tension (p&lt; 0.01) and continuous suturing. (p&lt; 0.01). Next, proficiency benchmarks of time and error were set based on data from a national sample of 17 expert surgeons from 7 institutions.Conclusion:  We have begun to build validity evidence for incorporating the advanced laparoscopic skills curricula and assessment into proficiency-based surgical training.  Additional work is currently under way to improve several elements of validity including the internal structure, relationship to other variables and to determine how the curriculum and assessment can optimally be used.  With that said, this project likely represents one of the most methodologically robust curriculum development processes in the literature.</description><description>Introduction: Il existe un large consensus dans le domaine de la formation en chirurgie, selon lequel nous devons abandonner le modèle actuel de formation axé sur le temps pour adopter un modèle axé sur la compétence. L'un des défis à relever réside dans la capacité d'avoir des évaluations valides de la compétence à différentes étapes de la formation. À cette fin, on a réussi à développer des évaluations fondamentales et des programmes d'études en chirurgie générale, notamment les principes fondamentaux de la chirurgie laparoscopique, les fondements de la chirurgie endoscopique et l'utilisation fondamentale de l'énergie chirurgicale. Cependant, il n'y a pas d'évaluation avancée des compétences ni de programmes d'études. Nous décrivons notre expérience en matière de développement et de validation d'une évaluation et d'un programme d'études avancés en laparoscopie (ALS) dans le but de l'adopter dans le modèle de formation en chirurgie basé sur la compétence.Méthodes: Nous décrivons une série de projets qui sont utilisés pour construire des preuves de validité pour l'adoption de l'évaluation et du programme ALS. Nous avons commencé par une évaluation des besoins des parties prenantes à l'échelle nationale. Nous avons ensuite développé des tâches simulées basées sur l'enquête, et avons commencé à construire des preuves de la validité de la structure interne en évaluant plusieurs niveaux d'apprenants. Enfin, nous avons élaboré des repères de compétences pour le programme d'études.Résultats: L'évaluation des besoins ciblait les boursiers en chirurgie mini-invasive (SIM), les anciens boursiers et les directeurs de programme et comprenait 186 répondants pour un taux de réponse de 64%. La majorité (73%) a identifié le besoin d'un programme ALS et 78% ont identifié la suture laparoscopique comme la partie la plus nécessaire d'un tel programme. Ensuite, une série de tâches de suture laparoscopique a été développée. Sur la base de nos métriques novatrices, les chirurgiens experts MIS ont surpassé les chirurgiens résidents dans les tâches suivantes: manipulation des aiguilles, (p = 0,04) suture hors angle, (p &lt;0,01) suture en arrière, (p = 0,01) suture en espace confiné , (p = 0,02) suture sous tension (p &lt;0,01) et suture continue. (p &lt;0,01). Ensuite, des repères de compétence en termes de temps et d'erreurs ont été établis à partir des données d'un échantillon national de 17 chirurgiens experts de 7 établissements.Conclusion: Nous avons commencé à accumuler des données probantes sur l'intégration des programmes de compétences avancées en laparoscopie et de l'évaluation à la formation en chirurgie axée sur la compétence. Des travaux supplémentaires sont actuellement en cours pour améliorer plusieurs éléments de validité, y compris la structure interne, la relation avec d'autres variables et pour déterminer comment le programme et l'évaluation peuvent être utilisés de manière optimale. Cela dit, ce projet représente probablement l'un des processus de développement du curriculum les plus solides sur le plan méthodologique dans la littérature.</description><creator>Nepomnayshy, Dmitry</creator><contributor>Gerald M Fried (Internal/Supervisor)</contributor><date>2018</date><subject>Surgery</subject><title>Building validity evidence for the advanced laparoscopic suturing curriculum and assessment (ALS)</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/sf2687597.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/n583xx29f</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Surgery</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:dn39x375m</identifier><datestamp>2020-03-21T05:23:28Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les microtubules sont de longs polymères qui forment partie intégrale du cytosquelette, une structure dynamique responsable pour la coordination de la motilité cellulaire, le transport intracellulaire et la division cellulaire. En fait, la dérégulation du cytosquelette microtubulaire peut causer des problèmes développementaux et est impliqué dans certains cancers. Les microtubules peuvent être étudiés in vitro par des expériences qui ont permis d'éclaircir les bases moléculaires de leur croissance dynamique. Cependant, afin d'obtenir une compréhension plus profonde sur eux, les microtubules doivent être étudiés in vivo, dans le contexte d'une cellule complète. Contrairement à l'analyse des expériences in vitro, on ne peut pas facilement et simplement analyser les images obtenues d'expériences in vivo. Pour cette raison, en utilisant des logiciels déjà disponibles, j'ai conçu un protocole pour localiser et suivre la position des microtubules et calculer des statistiques sur la vitesse de croissance des microtubules à partir des données brutes (des vidéos de microtubules en croissance). Le protocole est flexible, facile à utiliser, efficace et presque automatique.   </description><description>Microtubules are long polymers that form an integral part of the cytoskeleton, an intracellular, dynamic scaffold responsible for coordinating cell movement, intracellular transport, and cell division.  Misregulation of the microtubule cytoskeleton can lead to various developmental problems and has been implicated in certain cancers.  Microtubules can be studied in vitro and such experiments have been highly informative as to the molecular basis of their growth dynamics.  To gain a deeper understanding, however, microtubules must be studied in vivo, in the context of the complete cell.  In contrast to in vitro experiments, there is no simple and straightforward way to analyze the data that result from in vivo experiments.  Here, using readily-available software, I have built up a microtubule tracking protocol that goes from raw data (time-lapse videos of growing microtubules) to statistical summaries of microtubule growth rates.  The protocol is flexible, straightforward, time-efficient, and mostly automatic.</description><creator>Weiner, Catherine</creator><contributor>Gary Brouhard (Internal/Supervisor)</contributor><date>2018</date><subject>Biology</subject><title>A mostly-automated «In Vivo» microtubule-tracking protocol</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/zw12z763p.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/dn39x375m</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Biology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:zp38wg14g</identifier><datestamp>2020-03-21T05:23:29Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Managers require effective methods for forecasting the impacts of introduced species, so that invasion threats can be prioritized.  An emerging tool for invasive species risk assessment is the systematic comparison of the functional response (the relationship between predation rate and prey density) of native and invasive species. This approach is based on the assumption that species that have higher resource consumption rates will be more disruptive to food webs. Here, I use functional response analysis to examine the influence of biotic interactions on per capita effects of two congeneric North American crayfishes (Orconectes limosus and O. virilis) that have extensive invasion histories throughout this continent and in Europe.  By experimental testing geographically disparate populations, I found intraspecific variation in the functional response curves and maximum feeding rates.  A second set of experiments compared O. limosus and O. virilis from their respective native and invaded ranges, and from populations that are sympatric and allopatric with one another; these experiments further revealed interspecific and intraspecific variation in functional responses. Finally, I tested the effects of perceived competitors on the functional responses of both crayfishes. The presence of conspecific and heterospecific crayfish suppressed the maximum feeding rate of O. limosus, but they had no effect on the feeding rate of O. virilis. I conclude that source population and interspecific interactions mediate per capita effects, and possibly the overall impact, of introduced crayfishes. My results caution against the use of single populations in conducting invasive species risk assessments. </description><description>Pour déterminer les priorités d'actions lors de la gestion des espèces exotique envahissantes, les gestionnaires de la faune ont un réel besoin d'outils efficaces pour prédire les effets de ces introductions. La comparaison systématique des réponses fonctionnelles (la relation entre la densité de proies disponibles et le taux de consommation de ces proies) entre les espèces indigènes et les espèces exotiques envahissantes, s'avère un outil de plus en plus commun pour évaluer les menaces et des risques associées aux espèces exotiques envahissantes. Cette approche est toutefois fondée sur la prémisse que les espèces qui consomment davantage de ressources ont un plus grand impact sur le réseau trophique de la communauté hôte. Dans cet article j'analyse les réponses fonctionnelles pour examiner l'influence des interactions biotiques sur les effets per capita de deux congénères ayant un historique d'invasion en Amérique du Nord et en Europe soit l'écrevisse à épines (Orconectes limosus) et l'écrevisse à pinces bleues (Orconectes virilis). L'étude expérimentale de populations géographiquement distinctes révèle une variation intraspécifique des courbes de réponses fonctionnelles et du taux maximal de consommation de proies. Les résultats d'une deuxième série de tests comparant O. limosus et O. virilis provenant de leur aire de répartition naturelle et leur aire d'invasion respectives et provenant de populations sympatriques et allopatriques, ont démontrés une variation interspécifique et intraspécifique des réponses fonctionnelles. Finalement, j'ai évalué l'effet de la présence de compétiteurs apparents sur les réponses fonctionnelles des deux espèces d'écrevisses. La présence d'un conspécifique ou d'une écrevisse hétérospécifique réduit le taux de consommation maximal chez O. limosus mais n'a eu aucun effet sur le taux de consommation de O. virilis. Ainsi, je conclue que l'origine de la population et les interactions interspécifiques modèrent les effets per capita et les impacts globaux des écrevisses introduites. Les résultats de cette recherche découragent l'usage de populations uniques lors de la réalisation d'évaluation des menaces et des risques associées aux espèces exotiques envahissantes.</description><creator>Grimm, Jaime</creator><contributor>Anthony Ricciardi (Internal/Supervisor)</contributor><date>2018</date><subject>Biology</subject><title>Comparative functional responses of crayfishes: variation across species and populations</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/hh63sz345.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/zp38wg14g</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Biology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:4f16c5040</identifier><datestamp>2020-03-21T05:23:35Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>In the present thesis, the transition from regular to Mach reflection in steady and unsteady flows is considered by means of numerical simulations and analytical models. The analytical model by Li and Ben-Dor for the Mach stem height in steady supersonic flow was rederived and a number of typesetting mistakes in the equations of the original Li and Ben-Dor publication were found and corrected. Numerical simulations of oblique shock wave reflection in steady supersonic flows confirmed that the transition from regular to Mach reflection (RR-MR) occurs close to the detachment point and the delay in transition beyond the detachment point depends on grid resolution. However, it was found that to observe the RR-MR transition close to the detachment point extraordinary high grid resolution is required. It is conjectured that this might be related to high reflected shock curvatures close to transition. In all simulations, either a regular reflection or a fully-developed Mach stem was observed. The Mach stem height changes abruptly at transition in contrast to recent findings for pseudosteady flows reported in the literature.   Planar shock wave reflection from a convex cylindrical arc was simulated as an example of fully unsteady shock wave reflection. In this case, it was found that there is a delay in transition as compared to the theoretical detachment point. Extrapolation of the obtained results to infinite grid resolution indicates that the delay in transition remains finite but markedly less than such delay for pseudosteady flows provided in the literature. The variation of Mach stem height upon transition is gradual and continuous. </description><description>Dans la présente thèse, le passage de l'ordinaire dans la réflexion de Mach et d'équilibre des flux instable est considéré par le biais de simulations numériques et des modèles analytiques. Le modèle analytique par Li et Ben-Dor pour la hauteur de la tige de Mach en régime supersonique est rederived et un certain nombre d'erreurs de typographie dans les équations de l'original Li et Ben-Dor publication ont été trouvés et corrigés. Simulations numériques de la réflexion de l'onde de choc oblique en écoulement supersonique a confirmé que la transition de l'ordinaire la réflexion de Mach (RR-MR) se produit près de la point du détachement et le retard en transition au-delà du point de détachement dépend de la résolution de la grille. Toutefois, il a été constaté que d'observer la transition M. RR près du point de grille haute résolution extraordinaire est nécessaire. C'est l'hypothèse que ce pourrait être lié à des courbures de choc reflète près de transition. Dans toutes les simulations, soit une réflexion régulière ou entièrement développé souches Mach a été observée. La hauteur de la tige de Mach passe brutalement au niveau de la transition en contraste avec les résultats récents pour pseudosteady s'écoule dans la littérature. La réflexion de l'onde de choc planaires à partir d'un arc cylindrique convexe a été simulé comme un exemple de réflexion de l'onde de choc instationnaire pleinement. Dans ce cas, il a été constaté qu'il y a un retard en transition par rapport à la question du détachement de théorique. L'extrapolation des résultats obtenus à résolution de grille infinie indique que le délai en transition finie mais reste nettement inférieur à un tel retard pour pseudosteady s'écoule fournis dans la littérature. La variation de la hauteur de la tige de Mach au moment de la transition est progressive et continue.</description><creator>Srikumar, Srikumar</creator><contributor>Evgeny Timofeev (Internal/Supervisor)</contributor><date>2018</date><subject>Mechanical Engineering</subject><title>Numerical study on regular-to-Mach reflection transition in steady and unsteady flows</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/z316q3876.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/4f16c5040</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:fj236456w</identifier><datestamp>2020-03-21T05:23:36Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Electrical energy demand is likely to grow at a pace more than envisaged with the pen- etration of plug-in hybrid electric vehicles(PHEV) into the grid. In other to meet this demand more expensive generation methods may be invoked. A potentially less expensive alternative is the management of demand to meet available supply through active partici- pation of consumers. In order to do so the system operators needs to establish a framework that allows control of demand to achieve economic and system reliability purposes. This framework also provides information on the ability of users to provide curtailment with respect to varying demand response(DR) signals which we consider beneficial for proficient system operation. In this work we address these aspects of demand response for our framework: demand scheduling, user selection, appliance activity detection and user characterisation. We focus on an incentive based program for residential consumers with the objective of matching a predetermined demand profile to the aggregated demand to provide peak management, reduce costs and improved system performance. In order to make these curtailments we explore the idea of finding groups of users that are reliable to provide curtailments.This is done by modelling demand response as a reinforcement learn- ing problem that decomposes the customers into clusters based on their ability to provide curtailments at time of DR signal. The tasks of activity detection and user characterisation are addressed using already existing tools for load disaggregation in the context of non- intrusive load monitoring (NILM) and adapted as an appliance detection tool. We thus redefine our objective as maintaining the utility of consumers by rescheduling activities within a scheduling time horizon while using a reinforcement learning approach that allows the aggregator to make an online decision on users reliable to provide demand management capabilities in real-time. The key components of the framework are thus addressed and demonstrated individually in this work.</description><description>La demande en énergie électrique due à l'introduction massive et distribuée de chargeurs pour véhicules électriques dans les réseaux de distribution est en constante progression. Une grande contrainte au déploiement massif de bornes de recharge est la capacité de transit limitée sur les artères de distribution, spécialement dans les réseaux à faible dimensionnement où les charges de chauffage électrique sont peu présentes. Dans le but de satisfaire cette demande, il est possible de surdimensionner les artères déjà en place à grand coût en capital. Une solution alternative, potentiellement moins coûteuse, consiste à utiliser les consommateurs pour gérer une portion de la demande en énergie électrique pour qu'elle corresponde à l'offre d'énergie et tout en satisfaisant les contraintes des réseaux. Pour y arriver, les opérateurs du réseau de distribution ont besoin d'un outil qui permet de piloter la charge. Un système de pilotage de la charge nécessite l'acquisition d'informations sur les habitudes de consommation des clients et le calcul d'indices de sensibilité de la charge en réponse à des signaux de modulation de celle-ci. Également on doit avoir un mécanisme capable de sélectionner les clients les plus susceptibles de contribuer positivement au pilotage global de la charge. Dans cette thèse, nous nous concentrons sur les fonctionnalités d'un tel système de pilotage de la demande électrique. Le système prend en charge l'agencement des effacements, la détection d'activités des consommateurs, la caractérisation et le profilage des clients. Le système a comme objectif d'agencer des profils de consommation agrégés dans le but d'améliorer la performance du réseau électrique. Un moyen que nous utilisons pour piloter la charge consiste à former des groupes de clients qui correspondent à des caractéristiques et profils spécifiques. Pour y arriver, nous proposons un outil basé sur l'apprentissage automatique par renforcement. Nous utilisons également des outils existants pour détecter les activités des clients et les classifier. Les éléments clés du système sont analysés et validés individuellement.</description><creator>Ahmed, Shamwilu</creator><contributor>François Bouffard (Internal/Supervisor)</contributor><date>2018</date><subject>Electrical and Computer Engineering</subject><title>Online framework for integration of demand response in responsive residential load management</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/1n79h6802.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/fj236456w</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:8w32r793w</identifier><datestamp>2020-03-21T05:23:37Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Il existe une hypothèse qui stipule que la matière noire est considérée comme étant un état lié de boson de jauge suivant une théorie de SU(N), surnommée boule de glu 'dark'. Ces boules de glu n'ont pas de relation directe avec le modèle standard, mais sont en effect, en mesure de se décomposer en segments égaux de particules médiatrices, phi, ayant une grande masse. Nous considérons plusieurs scénarios dans lesquels les particules phi sont liées avec hypercharge U(1), de puissantes forces SU(3), et des bosons de Higgs respectivement. De plus, nous considérons l'approche numérique afin de résoudre l'équation de Boltzmann dans Mathematica. Le progiciel proposé par Mathematica peut être manupulée afin de tester un large éventail de particules génériques de matière noire.</description><description>Dark matter is postulated to exist as a bound state of gauge bosons in a hidden SU(N) theory, dubbed 'dark glueballs'. These dark glueballs have no direct couplings to the standard model, but are instead allowed to decay via loops of heavy mediator particles, phi. We consider scenarios in which the phi particles are coupled with U(1) hypercharge, SU(3) strong forces, and Higgs bosons respectively. Additionally, a numerical approach to solving the Boltzmann equation is considered in Mathematica. This software package can be applied and modified to a wide class of generic particle dark matter candidates.</description><creator>Cyr, Bryce</creator><contributor>James M Cline (Internal/Supervisor)</contributor><date>2018</date><subject>Physics</subject><title>SU(N) Glueball Dark Matter</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/pn89d893s.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/8w32r793w</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:qv33rz895</identifier><datestamp>2020-03-21T05:23:38Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>As the need for safe water is increasing, engineers are working on finding more efficient and environment-friendly technologies for disinfection of wastewater in order to provide safe water for discharge into the environment. Chlorination, using sodium hypochlorite (NaOCl), is one of the most commonly used disinfection methods. In recent years, extensive research has been done on performic (PFA) and peracetic acid (PAA), as they have a high oxidation potential and therefore can reduce bacterial and other types of contamination in wastewater. Life Cycle Assessment (LCA) is a holistic method used in order to investigate the environmental footprint of a product or a service over its entire life cycle, i.e. from "cradle-to-grave". The objective of the thesis is to use LCA to demonstrate the advantages in using either NaOCl, PAA or PFA for wastewater disinfection, from an environmental and health perspective. USEtox and Impact 2002+ models were used in order to evaluate their impacts on freshwater toxicity, human toxicity, climate change, mineral resources and non-renewable energy depletion. Data for the study were provided by a treatment plant in Italy near Venice that has performed full-scale disinfection trials using NaOCl, PAA and PFA, and by the North West Langley treatment plant near Vancouver, which traditionally used NaOCl for chlorination and sodium bisulfite for dechlorination, but transitioned to PAA disinfection in 2014. The functional unit for the Venice treatment plant is the removal of 3 log of E. coli from the effluent at a flow of 27 MLD during two months. For the NW Langley treatment plant, the functional unit was the disinfection of the effluent to the limit of 200 MPN/100 ml fecal coliforms at the edge of the dilution zone, from April to October and at a flow of 11.5 MLD. The LCA includes chemical production, infrastructure directly related to disinfection at the treatment plant, ancillary equipment for disinfection, transportation of equipment and the disinfectants, electricity for disinfection process itself as well as the residuals from disinfection. Results from the Venice treatment plant indicate that PFA could be the best option for wastewater disinfection due to the low dose needed to achieve regulatory limits, which reduces the potential impact of chemical production as well as transport. While PFA is more toxic than PAA, the residual concentration is lower leading to similar potential impacts on toxicity. Results from the NW Langley WWTP indicate that dechlorination significantly reduces potential impacts of chlorine on freshwater toxicity, however impacts on human toxicity, climate change and resource consumption (mineral and non-renewable energy) are increased due to the added production and transport of sodium bisulfite. The best choice of disinfectant at NW Langley therefore depends on the environmental priorities of the decision makers and government regulations. For each disinfectant, production is the highest contributor to total potential impacts on climate change, human toxicity and resource depletion. The highest contributor to freshwater toxicity impacts is the chemical residual from disinfection. The choice of chemical disinfectant also depends on regulations, cost and technologies available. The DesinFix unit is only being used in Europe; at this time the US EPA and the Canadian government have not yet approved the use of PFA for wastewater disinfection. Nonetheless, with upcoming reviews in regulations, PFA could soon be available in more countries. Furthermore, a cost analysis would give more information on the economic benefits of using one disinfectant over another. </description><description>L'utilisation du chlore sous la forme d'hypochlorite de sodium est une des méthodes de désinfection des eaux usées les plus courante au niveau mondial. Depuis quelques années, on observe une augmentation de la recherche scientifique sur les acides performique et peracétique pour la désinfection. Ces composés représentent des alternatives ayant un potentiel d'oxydation élevé et pouvant donc lutter efficacement contre la présence de contaminants dans les eaux usées. L'analyse du cycle de vie (ACV) est une méthode holistique utilisée afin de déterminer l'impact sur l'environnement d'un produit ou d'un service en considérant son cycle de vie en entier. L'objectif de ce mémoire est d'utiliser l'ACV afin de déterminer les avantages à utiliser l'hypochlorite de sodium, l'acide performique ou l'acide peracétique pour la désinfection des eaux usées, du point de vue des impacts sur l'environnement et la santé. Les modèles USEtox et Impact 2002+ ont été utilisés afin d'évaluer les impacts des désinfectants sur la toxicité aquatique, la toxicité humaine, les changements climatiques, l'utilisation de ressources minérales et d'énergies non-renouvelables. Les données de l'étude proviennent de deux usines de traitement des eaux usées : la première située en Italie,  a utilisé les trois produits pour la désinfection à grande échelle, et la deuxième la North West Langley située au Canada. Cette dernière a utilisé l'hypochlorite de sodium avec déchloration pendant plusieurs années, mais utilise l'acide peracétique depuis l'été 2014.  L'unité fonctionnelle utilisée pour l'usine de Venise est l'élimination de 3 log d'E. coli de l'effluent de l'usine à un débit de 27 MLD pendant deux mois. Pour l'usine de Vancouver, l'unité fonctionnelle est la désinfection des eaux usées à la norme canadienne de 200 MPN/100 ml de coliformes fécaux à la limite de la zone de mélange, d'avril à octobre et à un débit de 11.5 MLD. L'ACV considère les étapes de production des désinfectants, des infrastructures utilisées pour la désinfection et des équipements nécessaires, ainsi que le transport d'équipements et de produits chimiques, la désinfection et les résidus restants dans l'effluent.  Les résultats obtenus pour l'usine de Venise indiquent que l'acide performique serait la meilleure option pour la désinfection des eaux usées et ce grâce à la plus petite dose de produit chimique nécessaire afin de réduire la contamination à la limite permise. L'acide performique possède une plus grande toxicité aquatique que l'acide peracétique, toutefois le résiduel est moins important : l'impact final des deux produits est donc semblable. Les résultats obtenus à l'usine de Vancouver indiquent que la déchloration avec bisulfite de sodium diminue les impacts sur la toxicité aquatique de façon significative. Toutefois, les impacts potentiels sur la toxicité humaine, les changements climatiques et l'utilisation des ressources sont plus importants. Le meilleur choix, considérant les impacts potentiels, de produit de désinfection à l'usine de Vancouver dépend donc des priorités environnementales des décideurs et des normes environnementales existantes. Pour tous les désinfectants, la production chimique est l'élément qui contribue le plus aux impacts potentiels sur les changements climatiques, la toxicité humaine et l'utilisation des ressources minérales et non-renouvelables, alors que l'impact sur la toxicité aquatique est dû en majorité au résiduel de désinfection. Le choix du désinfectant à utiliser dépend également des règlements et normes, des technologies disponibles et du coût associé. Par exemple, l'utilisation de l'acide performique n'est pas encore acceptée par les gouvernements du Canada et des États-Unis. Toutefois, avec le temps, son utilisation devrait être de plus en plus acceptée. Afin de compléter cette étude une analyse de coût permettrait de fournir plus d'informations sur les avantages et inconvénients à utiliser un désinfectant plutôt qu'un autre.</description><creator>Penkarski-Rodon, Elena</creator><contributor>Ronald Gehr (Internal/Supervisor)</contributor><date>2018</date><subject>Civil Engineering &amp; Applied Mechanics</subject><title>Life cycle assessment for wastewater disinfection using sodium hypochlorite, peracetic acid or performic acid</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/v405sc800.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/qv33rz895</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Civil Engineering and Applied Mechanics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:gt54kq50p</identifier><datestamp>2020-03-21T05:23:39Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Résumé Introduction: La régénération des défauts osseux de « taille critique » reste un défi clinique. La régénération tissulaire osseuse (BTE) apparaît comme une alternative prometteuse au greffe d'os autogène, allogénique, xénogénique ou synthétique. Un défaut de taille critique mandibulaire est caractérisé par une perte de continuité de la mandibule, et est le résultat d'une résection segmentaire. De plus, le défaut mandibulaire de taille critique peut entraîner un handicap fonctionnel dévastateur, une déformation cosmétique et des effets psychologiques. L'échafaudage idéal pour la régénéra-tion tissulaire est un échafaudage qui possède les propriétés biologiques et matérielles. Objectif: La présente étude consiste à réviser systématiquement la littérature existante afin de répondre à la question suivante: "PICO" (population, intervention, comparaison, résultat): dans les défauts mandibulaires de taille critique chez les animaux et les humains, les échafaudages en céramique sont plus efficaces comparativement aux polymères naturels / polymères synthétiques / échafau-dages natifs / échafaudages mixtes pour améliorer la régénération et la formation des os? Le but de la présente étude était (1) d'examiner systématiquement la littérature préclinique in vivo et cli-nique concernant l'ingénierie des tissus osseux pour les défauts mandibulaires de la taille critique, (2) de déterminer si les échafaudages en céramique sont cliniquement supérieurs aux autres types d'échafaudages, et (3) de comparer leur efficacité. Résultats: Échafaudages en céramique ont été trouvés pour améliorer de manière significative la régénéra-tion osseuse par rapport aux échafauds pol-ymer. Pendant ce temps, l'ajout des facteurs de crois-sance aux échafaudages de phosphate tricalcique et aux échafaudages de polymère a amélioré la régénération osseuse, mais l'ajout des facteurs de croissance aux échafaudages d'hydroxyapatite n'a pas eu le même effet. Cependant, une étude clinique chez l'homme de 34 patients a montré que le facteur de croissance seul n'est pas suffisant pour améliorer la cicatrisation osseuse. De plus, l'enrobage des scaffes métalliques avec de l'hydroxyapatite ou du bioverre a entraîné une régénération osseuse significativement meilleure que le métal non revêtu. Conclusion: La reconstruction d'un défaut de taille critique est très difficile. Cet examen systématique a éva-lué l'efficacité de différents échafaudages et de l'os autologue dans la régénération osseuse d'un défaut mandibulaire de taille critique. La plupart des études sur ce sujet sont encore en phase pré-clinique avec une seule étude réalisée chez l'humain. Nous recommandons fortement d'autres études cliniques pour évaluer l'applicabilité des résultats des études animales chez les êtres hu-mains.</description><description>Abstract Introduction: Regeneration of large, 'critical-size' bone defects remains a clinical challenge. Bone tissue engi-neering (BTE) is emerging as a promising alternative to autogenous, allogeneic, xenogeneic, and biomaterial-based bone grafting. A critical size mandibular defect is characterized by a loss of continuity in the mandible and is the result of a segmental resection performed by maxillofacial surgeries. Also, critical sized mandibular defect can result in devastating functional disability, cosmetic deformity, and psychological impairment. The ideal scaffold for tissue engineering is a scaffold made from both organic and synthetic materials. Objective: The present study systematically reviewed the existing literature in order to answer the following "PICO" (population, intervention, comparison, outcome) question: In critical sized mandibular defect of animals and humans, are ceramic scaffolds more effective when compared with natural polymers/ synthetic polymers/ native scaffolds/composite scaffolds in enhancing histomorpho-metric bone regeneration and formation? The present study thus aimed (1) to systematically re-view preclinical in vivo and clinical literature regarding bone tissue engineering for critical size mandibular defects. (2) to determine if ceramic scaffolds are clinically superior to other types of scaffolds; and (3) to compare the effectiveness of different scaffolds. Results: Ceramic scaffolds were found to significantly improve bone regeneration when compared to pol-ymer scaffolds. Meanwhile, adding the growth factors to tricalcium phosphate scaffolds and polymer scaffolds improved bone regeneration, but adding the growth factors to hydroxyapatite scaffolds did not have the same effect. However, a clinical human study of 34 patients showed that growth factor alone is not enough to improve bone healing. Additionally, coating metal scaf-fold with hydroxyapatite or bioglass resulted in significantly better bone regeneration than un-coated metal. Conclusion: The reconstruction of a critical size defect is very challenging. This systematic review evaluated the effectiveness of different tissue engineered scaffolds and autologous bone in reconstructing of critical size mandibular defects. Most of the studies on this topic were animal studies with on-ly one study performed using human participants. More studies on humans are therefore needed to evaluate the effectiveness of these scaffolds in the clinical setting.</description><creator>Almatlub, Mabrouk</creator><contributor>Simon Tran (Internal/Supervisor)</contributor><date>2018</date><subject>Dentistry</subject><title>Comparing the effectiveness of different scaffolds (ceramic scaffolds, polymers scaffolds, composite scaffolds, native scaffolds, and metal scaffolds) in regenerating critical sized mandibular bone defect</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/00000244g.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/gt54kq50p</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Faculty of Dentistry</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:bn999901h</identifier><datestamp>2020-03-21T05:23:40Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>A lot of research has been carried out using statistical methods in downscaling large-scale GCM outputs to the local site or small-scale region. Using statistical methods, empirical relationships are developed between the large-scale GCM outputs and the local site weather variables. In this methodology, it is assumed that these empirical relationships will also hold true even in the future periods of time. Environmental assessment studies for the future decade under the influence of a changing climate is the need of the hour. In order to identify the future synoptic weather types, future daily and hourly projections of the weather variables are required. Hence, this research is motivated by the lack of a comprehensive and statistically significant downscaling methodology for the various weather variables. The present study is based on the various statistical downscaling techniques developed by the researchers in the past using linear regression model because of the advantage of being less computationally intensive. In addition, an attempt has been made to develop an improved statistical downscaling methodology by combining different techniques to develop a robust method with a detailed performance assessment of the models. Linear Regression models are derived to downscale daily climate scenarios using NCEP reanalysis datasets for the predictors and weather station data available at Trudeau International Airport for the predictands during the reference period 1958-2001. The NCEP predictors are regridded to the GCM scale as the GCM outputs are to be used for deriving future climate projections. Standardization and Deseasonalization of the predictor variables are carried out followed by principal component analysis prior to their introduction into the stepwise regression model. CANESM2 is selected as the GCM model in the present work whose outputs are used as predictors in the NCEP derived regression models. A Bias Correction procedure is used to correct the systematic biases present using a quantile-quantile mapping technique on the downscaled variable using CANESM2 predictors. After downscaling the daily climate variables, hourly downscaling transfer functions are derived based on the historical relationships of the hourly values with its daily mean as well as other weather predictors where appropriate. For the future climate projections, RCP2.6, 4.5 and 8.5 are used as greenhouse gas trajectories representing the change in climate in the future decade.</description><description>Plusieurs recherches ont été menées en utilisant les méthodes statistiques de réduction d'échelle des résultats de MCG au niveau local d'un site ou d'une région à petite échelle. En usant de ces méthodes, des relations empiriques sont développées entre les données du MCG et les variables climatiques locales au niveau d'un site. Dans cette méthodologie, ces relations empiriques sont considérées invariables pour les périodes futures. Les études d'évaluation environnementales pour les prochaines décennies sous l'effet du changement climatique sont d'une grande importance. Afin d'identifier les types de temps synoptiques au futur, les projections des données journalières et horaires des variables climatiques sont nécessaires. Donc, le motif de cette recherche est le manque d'une méthode complète et statistiquement significative adoptée pour la mise en échelle des variables climatiques. Cette étude ici élaborée se base sur différentes méthodes statistiques de réduction d'échelle développées par les chercheurs en usant du modèle de régression linéaire, populaire pour nécessiter moins de calculs complexes. En plus, en combinant différentes techniques, une tentative a été faite pour développer une méthode statistique améliorée et robuste de réduction d'échelle avec une évaluation détaillée de la performance des modèles.Les modèles de régression linéaires sont dérivés pour réduire l'échelle des scénarios climatiques journaliers en utilisant l'ensemble de données de réanalyse du NCEP pour les prédicteurs et les données climatiques disponibles à la station de l'aéroport international Pierre-Elliot Trudeau pour les caractéristiques locales, durant la période de référence 1958 et 2001. Les prédicteurs de NCEP sont reportés sur une grille ayant la même échelle que le MCG, et les résultats de ce dernier seront utilisés pour dériver les projections climatiques dans le futur. Les prédicteurs sont d'abord normalisés et désaisonnalisés, pour être par la suite sujets à une analyse en composantes principales, avant d'être introduits dans le modèle de régression séquentielle. Le MCG choisi pour cette étude est le CanESM2, dont les résultats sont utilisés comme prédicteurs dans les modèles de régression dérivés de NCEP. Une méthode de correction de biais est adoptée pour corriger les biais systématiques présents en utilisant la technique de configuration quantile-quantile pour les prédicteurs variables de CanESM2 mis en échelle. Après réduire l'échelle des variables climatiques journalières, les fonctions de transfert de réduction d'échelle horaires sont dérivées en se basant sur les relations historiques des valeurs horaires, de leur moyenne journalière ainsi que d'autres prédicteurs climatiques convenables. Concernant les projections climatiques au futur, les scénarios RCP 2.6, 4.5 et 8.5 sont adoptés pour les trajectoires des gaz à effet de serre représentant le changement climatique dans les futures décennies.</description><creator>Deka, Bhargob</creator><contributor>Luc E Chouinard (Internal/Supervisor)</contributor><date>2018</date><subject>Civil Engineering &amp; Applied Mechanics</subject><title>Statistical downscaling of daily and hourly climate scenarios for the various meteorological variables at Montreal</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/qr46r3414.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/bn999901h</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Civil Engineering and Applied Mechanics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:s7526f64b</identifier><datestamp>2020-03-21T05:23:41Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The space industry has been using cyanate ester resins and autoclave processing for over 20 years to produce structures that can withstand the extreme conditions during launch and operation in space. An effort is now being made to reduce costs by implementing Out-of-Autoclave (OoA) Vacuum Bag Only (VBO) with epoxy resins. However, without knowledge and understanding of different material systems, material selection can be a very risky and costly process. The first objective of this thesis work is to develop a streamlined characterization-based methodology to select composite systems for structural space applications. Seven materials are characterized by performing thermogravimetric analysis (TGA), differential scanning calorimetry (DSC), rheology, and thermomechanical analysis (TMA). On the other hand, space structures require very tight tolerances and high levels of dimensional stability during manufacturing. Knowledge on the deformation caused by residual stresses during processing has been obtained through historical data and experience factors. This current approach fails to consider all the potential sources of residual stresses which can result in large component deviations from the original design. The second objective is to develop a workflow using process models to predict deformation due to residual stresses. This is accomplished by measuring spring-in at a small scale with L-shape laminates using the top three materials from the evaluation of the first objective. Then, the dimensional stability of a large scale demonstrator with the material that is currently used in space structures is assessed. Analytical models and an ABAQUS Finite Element simulation are created and the results are validated using experimental results. One of the current materials used in space structures ranked the highest in the evaluation, followed by a cyanate ester and an epoxy resin. In the L-shape laminates, the epoxy displayed more spring-in when compared to the cyanate esters. For the demonstrator, the simulation predicted 15% more spring-in when compared to the measured values. The results can be improved by using measurement techniques with higher accuracy to evaluate the deformation, and by performing a sensitivity analysis to evaluate the impact of different parameters in the simulation. The proposed workflow has the potential to be used in an industrial setting.</description><description>L'industrie spatiale utilise des résines d'ester de cyanate et une mise en forme en autoclave depuis plus de 20 ans pour produire des structures capables de résister aux conditions extrêmes de l'espace. Un effort est maintenant fait pour réduire les coûts avec des procédes avec bâche à vide (VBO) hors-de-l'autoclave avec des résines époxy. Cependant, sans la connaissance et la caractérisation des différents systèmes matériaux, la sélection des matériaux peut être un processus très risqué et coûteux. Le premier objectif de cette thèse est de développer une méthode simplifiée pour la selection de matériaux composites basée sur la caractérisation pour des applications spatiales structurelles. Sept matériaux sont caractérisés par l'analyse thermogravimétrique (TGA), la calorimétrie différentielle (DSC), la rhéologie, et l'analyse thermomécanique (TMA). Les structures spatiales exigent des tolérances très serrées et des niveaux élevés de stabilité dimensionnelle pendant la fabrication. La connaissance de la déformation causée lors de la mise en oeuvre par le traitement est obtenue à partir de facteurs d'expérience. Cette approche toujours employée aujourd'hui, ne tient pas compte de toutes les sources potentielles de contraintes résiduelles qui peuvent entraîner des écarts par rapport à la dimension initiale du moule. Le deuxième objectifest de développer un protocole de travail en utilisant des outils de simulation pour prédire la déformation causée par les contraintes résiduelles. Dans un premier temps, le retrait angulaire de stratifiés en L utilisant les trois matériaux selectionés par l'évaluation du premier objectif est mesurée à l'échelle du laboratoire. Ensuite, la distorsion géeométrique d'un démonstrateur fabriqué avec le matériel actuellement utilisé dans les structures spatiales est mesurée. Des modèles analytiques et des simulations par éléments finis sont créés et les résultats sont validés avec des expériences. Le matériel actuellement utilisé dans les structures spatiales a obtenu la classification le plus élevé dans l'évaluation, suivi par un ester de cyanate et une résine époxy. Pour les stratifiés en forme de L, l'époxy présentait plus de retrait angulaire par rapport aux esters de cyanate. Pour le démonstrateur, la simulation a prédit 15% plus déformation par rapport aux valeurs mesurées. Les résultats peuvent être améliorés en utilisant des techniques de mesure avec une plus grande précision pour évaluer les distorsions géométriques, et en effectuant une analyse de sensibilité pour évaluer l'impact de différents paramètres de la simulation. Le protocole de travail proposé dans cette thèse a le potentiel d'être utilisé dans un environnement industriel.</description><creator>Barroeta Robles, Julieta</creator><contributor>Pascal Hubert (Internal/Supervisor)</contributor><date>2018</date><subject>Mechanical Engineering</subject><title>A streamlined characterization-based selection of composite material systems for space structures</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/js956j313.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/s7526f64b</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:bn999902s</identifier><datestamp>2020-03-21T05:23:43Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The IceCube South Pole Neutrino Observatory consists of an array of ∼5600 photodetectors distributed within a cubic kilometre of ice ranging between 1.5 to 2.5 km in depth. This configuration allows the detection of the Cherenkov light produced by the passage of high-energy charged particles resulting from neutrino interactions. In 2013, the IceCube collaboration announced the discovery of a high-energy astrophysical neutrino flux for the first time, opening a new window for the exploration of the high-energy universe. Astrophysical neutrinos are thought to be produced from the decay of charged pions coming from hadronic interactions and are expected to be accompanied by gamma rays produced by the decay of the neutral pions. Consequently, in the same year, the VERITAS gamma-ray observatory, in Arizona, initiated its multimessenger program with the main goal of searching for a gamma-ray counterpart to the neutrino events found by IceCube. VERITAS is composed of four 12-meter telescopes that image the Cherenkov light emitted from the passage of the electromagnetic cascade in air produced by the interaction of gamma rays high in the atmosphere. This thesis describes the results from VERITAS' search for a gamma-ray counterpart to IceCube neutrino events. No gamma-ray sources were found in the regions corresponding to the arrival directions of IceCube neutrino events. On the other hand, calculations of flux upper limits constrain the gamma-ray flux to be less than ∼10% Crab. Future gamma-ray experiments with better sensitivity, such as CTA, will allow better performance for follow-up observations.</description><description>L'Observatoire de neutrinos IceCube, situé en Antarctique, consiste en un réseau de ∼5600 photodétecteurs distribués dans un volume d'un kilomètre cube de glace à une profondeur allant de 1,5 à 2,5 kilomètres. Cette configuration rend possible la détection du rayonnement Tcherenkov produit par le passage de particules chargées de hautes énergies créées résultant de l'interaction de neutrinos. En 2013, la collaboration IceCube a annoncé la découverte d'un flux de neutrinos cosmiques, ouvrant ainsi une nouvelle fenêtre sur l'astrophysique des hautes énergies. Les neutrinos astrophysiques sont produits lors de la désintégration de pions chargés provenant d'interactions hadroniques et devraient normalement être accompagnés par des rayons gamma produits par la désintégration des pions neutres. Par conséquent, la même année, l'observatoire de rayons gamma VERITAS, en Arizona, a démarré son programme d'astronomie multimessager avec comme but principal de chercher une contrepartie gamma aux événements de neutrinos découverts par IceCube. VERITAS est composé de quatre télescopes de 12 mètres de diamètre qui imagent la lumière Tcherenkov émise par le passage de la cascade électromagnétique dans l'air causée par l'interaction de rayons gamma en haute atmosphère. Ce mémoire décrit les résultats des efforts faits par VERITAS afin de trouver une contrepartie gamma aux événements neutrinos d'IceCube. Aucune source de rayons gamma n'a été trouvée dans les régions correspondant aux directions d'arrivée des événements neutrinos d'IceCube. D'un autre côté, des calculs de limites supérieures de flux contraignent les flux de rayons gamma à des valeurs inférieures à ∼10% Crab. Les futures expériences de rayons gamma, tel CTA, auront une meilleure sensibilité et seront donc plus performantes pour effectuer des observations de suivi.</description><creator>Trépanier, Samuel</creator><contributor>Kenneth J Ragan (Internal/Supervisor)</contributor><date>2018</date><subject>Physics</subject><title>Search for very-high-energy gamma-ray counterparts to IceCube neutrino events using VERITAS</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/h702q867x.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/bn999902s</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:9z903225z</identifier><datestamp>2020-03-21T05:23:44Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>We present a Deep Reinforcement Learning based approach for the task of real time machine translation. In the traditional machine translation setting, the translator system has to 'wait' till the end of the sentence before 'committing' any translation. However, real-time translators or 'interpreters' have to make a decision at every time step either to wait and gather more information about the context or translate and commit the current information. The goal of interpreters is to reduce the delay for translation without much loss in accuracy. We formulate the problem of online machine translation as a Markov Decision Process and propose a unified framework which combines reinforcement learning techniques with existing neural machine translation systems. A training scheme for learning policies on the transformed task is proposed. We empirically show that the learnt policies can be used to reduce the end to end delay in translation process without drastically dropping the quality. We also show that the policies learnt by our system outperform the monotone and the batch translation policies while maintaining a delay-accuracy trade-off.</description><description>Nous présentons une approche basée sur l'apprentissage par renforcement profond pour la tâche de traduction automatique en temps réel. Dans le cadre traditionnel de la traduction automatique, le système de traduction doit 'attendre' jusqu'à la fin de la phrase avant de 'valider' toute traduction. Cependant, les traducteurs en temps réel ou les 'interprètes' doivent décider à chaque moment s'ils doivent attendre et recueillir plus d'informations sur le contexte ou traduire et valider l'information disponible actuellement. Le but des interprètes est de réduire le délai de traduction sans perte de précision. Nous formulons le problème de traduction automatique 'simultanée' comme processus de décision markovien et proposons un cadre unifié qui joint des techniques d'apprentissage par renforcement avec des systèmes neuronaux existants de traduction automatique. Un schéma d'entraînement pour les politiques d'apprentissage sur la tâche transformée est proposé. Nous montrons empiriquement que les politiques apprises peuvent être utilisées pour réduire le retard de bout en bout dans le processus de traduction sans pour autant réduire radi- calement la qualité. Nous montrons également que les politiques apprises par notre système surpassent les politiques monotones de traduction et celles de traduction par lots tout en maintenant un compromis entre précision et retard.</description><creator>Satija, Harsh</creator><contributor>Joelle Pineau (Internal/Supervisor)</contributor><date>2018</date><subject>Computer Science</subject><title>Using deep reinforcement learning for online machine translation</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/r207tr67x.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/9z903225z</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>School of Computer Science</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:xg94hs08x</identifier><datestamp>2020-03-21T05:23:45Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>A highly glossy plane with small, smooth deformations viewed at an oblique angle with a light source near the horizon can produce specular reflections. The resulting specular reflections appear as long columns, oriented toward the viewer. Under the right conditions the specularities appear as illusory vertical columns below the surface, rather than  as highlights on the surface. This illusion can generally be seen by observing the moon over water at night, or car headlights on a wet street. If there are several light sources the illusion of columns can be strengthened. We hypothesized that the illusion is due to the orientation of the envelope of the specularities corresponding to vertical columns, creating a vanishing point in the direction of the surface normal of the ground. This work describes the conditions under which these highlights can be seen, and details three pilot experiments performed to study their effect on visual perception of slant and material properties.</description><description>Un plan geometrique tres brillant avec de petites deformations lisses vue a un angle oblique avec une source de lumiere a proximite de l'horizon peut produire des reflexions speculaires. Les reflexions speculaires resultantes apparaissent sous forme de longues colonnes, orientees vers le spectateur. Dans les bonnes conditions, les reflexions apparaissent comme des colonnes verticales illusoires sous la surface, plutot que comme des specularites sur la surface. Cette illusion peut generalement etre vue en observant la lune au-dessus de l'eau la nuit, ou les phares de voiture sur une rue mouillee. S'il y a plusieurs sources lumineuses, l'illusion des colonnes peut etre renforcee. Nous avons emis l'hypothese que l'illusion est due a l'orientation de la region occupee par les specularites correspondant aux colonnes verticales, creant un point de fuite dans le sens de la normale a la surface du sol. Ce travail decrit les conditions dans lesquelles ces specularites peuvent être vues, et detaille trois experiences pilotes effectuees pour etudier leur effet sur la perception visuelle de l'inclinaison et des proprietes des materiaux.</description><creator>Bourque, David</creator><contributor>Michael Langer (Internal/Supervisor)</contributor><date>2018</date><subject>Computer Science</subject><title>Exploring specular highlight streaks on planar surfaces</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/tm70mx94r.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/xg94hs08x</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>School of Computer Science</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:1j92g976z</identifier><datestamp>2020-03-21T05:23:46Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Dans le sillage de la décision de la Cour suprême du Canada dans Éric c Lola, le Comité consultatif sur le droit de la famille, mandaté par le gouvernement du Québec, élabore un projet de réforme majeure. Il suggère de ne pas imposer de partage de biens ou d'obligation alimentaire aux unions de fait. Néanmoins, il envisage d'élargir l'application de la prestation compensatoire aux conjoints de fait. Cette proposition est l'objet d'étude de ce mémoire. Comme la prestation compensatoire est une institution peu étudiée depuis l'adoption du patrimoine familial en droit québécois, la première partie de ce mémoire rappelle le contexte dans lequel s'inscrit la problématique à l'étude et trace l'évolution de la prestation compensatoire par une analyse des débats législatifs, des arrêts de principe et de la doctrine. La deuxième partie présente une critique du discours judiciaire dans une perspective queer. La deuxième partie se propose de démontrer que la prestation compensatoire n'est pas l'institution longtemps imaginée, c'est-à-dire égalitaire, flexible et équitable. Le discours s'est plutôt polarisé, voire catégorisé, et ce, sous quatre axes binaires : conjoint profitant / conjointe perdante, normalité de l'apport / anormalité de l'apport, ce que le juge connaît / ce que le juge ignore, la prestation compensatoire / les autres mesures du droit familial. L'étude de ces quatre binarités du discours illustre deux caractéristiques fondamentales de la prestation compensatoire, qui sont pertinentes dans le débat sur l'encadrement législatif des unions de fait. D'abord, la prestation compensatoire matérialise à travers la jurisprudence une vision genrée, hétéronormée et traditionnelle de l'identité et du rôle des conjoints dans l'union. Ensuite, elle opère en proximité intime avec le patrimoine familial, la somme globale et l'obligation légale des époux de contribuer proportionnellement à leurs facultés respectives aux charges du mariage. Ainsi, la prestation compensatoire peut difficilement exister sans les autres normes du droit familial. Le mémoire souligne les difficultés de l'emprunt d'une mesure précise au droit matrimonial et suggère la nécessité d'une étude approfondie et détaillée de cette recommandation du Comité. </description><description>In the wake of the Supreme Court of Canada's decision in Eric v Lola, the Comité consultatif sur le droit de la famille named by the Quebec government proposed a global reform. While it suggested not to subject cohabitants to any sharing of property or spousal support, it nevertheless considered extending the compensatory allowance to them. That recommendation is the object of this thesis. The compensatory allowance has barely been studied since the adoption of the family patrimony regime in Quebec civil law. After a brief description of the context underlying the Comité's recommendations, the first section surveys the evolution of the compensatory allowance by looking at parliamentary debates, leading cases and legal scholarship. The second section draws on queer theory to analyze the discourse found in case law, showing that the compensatory allowance does not exemplify a flexible institution of equality and equity. The judicial discourse is rather polarized, in what can be organized into four binary categories: spouse taking advantage / spouse being taken advantage of, normal contribution / abnormal contribution, the judicial known / the judicial unknowable, and the compensatory allowance / the remaining remedies of family law. This analysis sheds light on two defining traits of the compensatory allowance that are relevant to the debate on legislative policy respecting cohabitants. First, the compensatory allowance as elaborated by judgments express gendered, heteronormative, and traditional ideas of spousal identity and roles. Second, it operates in close relation to the family patrimony, the lump sum, and spouses' obligation to contribute towards the expenses of the marriage in proportion to their respective means. Therefore, it is difficult to conceive of the compensatory allowance's operation without its connex institutions of matrimonial law. The thesis highlights the perils of selective borrowing and points to the need for careful further study of this recommendation.</description><creator>Saint-Pierre Harvey, Laurence</creator><contributor>Robert Leckey (Internal/Supervisor)</contributor><date>2018</date><subject>Law</subject><title>Prestation compensatoire et union de fait en droit québécois : étude critique d'un discours judiciaire binaire</title><language>fre</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/x059c976s.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/1j92g976z</identifier><degree><name>Master of Laws</name><grantor>McGill University</grantor><discipline>Faculty of Law</discipline></degree></thesis></metadata></record><resumptionToken completeListSize="47894">oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):4750</resumptionToken></ListRecords></OAI-PMH>