<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/assets/blacklight_oai_provider/oai2-b0e501cadd287c203b27cfd4f4e2d266048ec6ca2151d595f4c1495108e36b88.xsl"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd"><responseDate>2020-07-25T04:18:51Z</responseDate><request resumptionToken="oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):47575" verb="ListRecords">https://escholarship.mcgill.ca/catalog/oai</request><ListRecords><record><header><identifier>oai:escholarship.mcgill.ca:hx11xk24c</identifier><datestamp>2020-07-14T18:14:30Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>La présence plus forte des travailleurs âgés sur le marché du travail reflète le vieillissement de la main-d’œuvre, mais également la participation accrue de ce groupe au marché du travail. Malgré cette plus forte participation, la situation des travailleurs âgés demeure fragile : un taux de chômage plus élevé; des difficultés de recrutement lorsqu’ils cherchent un nouvel emploi; un salaire considérablement réduit par rapport au salaire de carrière; un accès inégal à la formation. Dans cette thèse, on examine la situation des travailleurs vieillissants sur le marché du travail (45 à 49 ans, 50 à 55 ans, 56 à 60 ans et 61 ans et plus), en comparaison aux travailleurs en début de carrière (25 à 34 ans) et aux travailleurs d’âge très actif (35 à 44 ans).Dans le premier chapitre, à partir des données de l’Enquête sur la dynamique du travail et du revenu (EDTR) de Statistique Canada, on examine l’évolution (de 1993 à 2010) de la durée d’emploi et des transitions entre les trois états du marché du travail (emploi, chômage et inactivité) au Québec en comparaison de l’Ontario. On distingue la nature, volontaire et involontaire, des cessations d’emploi et leur impact à travers les groupes d’âge. Le Québec se distingue parmi les provinces comme ayant une législation de protection de l’emploi assez rigide et bénéficie de l’une des plus fortes présences syndicales. L’Ontario toutefois, est désignée comme l’une des provinces les plus flexibles en ces termes. On peut s’attendre à ce qu’une réglementation plus souple accentue les mouvements de la main-d’œuvre et qu’elle soit bénéfique en termes de créations d’emplois. Elle permettrait aux individus de se relocaliser plus facilement en emploi, ce qui pourrait favoriser les transitions et le maintien en emploi des travailleurs vieillissants davantage en Ontario qu’au Québec.Dans le deuxième chapitre, on examine la mobilité professionnelle au Canada en fonction de l’âge des travailleurs. À partir des données de l’EDTR, on mesure la mobilité professionnelle sur une période s’étalant jusqu’à six années consécutives. En particulier, on s’intéresse à la fluctuation des salaires et au redéploiement des compétences (en termes de niveau d’études requis pour exercer la profession) entre deux emplois. En observant le phénomène sur plusieurs années plutôt que sur une période de deux ans comme la majorité des études, la mobilité professionnelle apparaît plus fréquente qu’on ne le pensait et reste relativement élevée avec l’avancée en âge. On estime l’impact des épisodes de chômage et d’inactivité sur la probabilité de vivre une baisse de salaire : entre autres, l’impact du chômage sur le salaire semble plus élevé surtout à partir de 56 ans par rapport aux autres groupes d’âge.Finalement, dans le troisième chapitre, on explore plus en détails la littérature suggérant que certaines évolutions sur le marché du travail observées au cours des dernières décennies ont pu modifier le comportement d’emploi des entreprises, dans un sens qu’il serait défavorable aux travailleurs plus avancés en âge : par exemple, la diffusion massive des technologies et les restructurations internes dans les entreprises et l’adoption des pratiques innovantes. On s’interroge sur la manière dont cet environnement peut modifier la demande de travail par âge dans les entreprises en analysant la structure par âge des milieux de travail selon qu’ils utilisent ou non des nouvelles technologies, des dispositifs innovants d’organisation du travail et autres pratiques de gestion des ressources humaines. On explore ces facteurs en relation avec l’intensité de la formation dispensée dans les entreprises et l’ancienneté en emploi</description><description>The rising share of older workers in the labour market is produced by both the aging of the workforce and the increased labour force participation of those older workers. Notwithstanding their persistence in the workforce their situation suggests some disadvantage: they have a higher rate of unemployment; their job search is often challenging; there is some evidence of late career earnings declines; and they may have poorer access to training than their younger counterparts. In this thesis, I compare the labour market situation of aging (45-49, 50-55, 56-60, 61 and over) with early (25-34) and prime age (35-44) workers.In the first chapter, I use data from Statistics Canada’s Survey of Labour and Income Dynamics (SLID) to examine the evolution of employment durations between 1993 and 2010. Then, comparing Quebec and Ontario, I analyse the transitions between three labour market statuses: employment, unemployment, and inactivity. I distinguish voluntary and involuntary transitions and estimate the consequences of those transitions for the different age groups. Among provinces Quebec is distinguished by its fairly rigid employment protection legislation and its stronger unions. In contrast, Ontario’s labour market is more flexible and its union movement is weaker. One might expect this to increase both labour market mobility and employment. An implication of this would be that individuals in Ontario can more easily shift between jobs than their Quebec counterparts and this, in turn, might facilitate the continued employment of older workers.The second chapter reports analyses of occupational mobility by age in Canada. Using SLID data I examine occupational changes over six year intervals between 1993 and 2010. I focus on the changes in earnings and skill levels required by jobs held associated with switches between jobs. A first result is that observing these changes over six years rather than the two years more commonly analysed reveals higher rates of occupational mobility than usually reported, including within older age groups. I estimate the effects of episodes of unemployment or inactivity on the probability of experiencing a fall in earnings. The likelihood of an earnings decline after a period of unemployment is particularly large for those 56 and older.Finally, in the third chapter I explore in detail claims that, over the last decades, the massive diffusion of new technologies combined with organizational restructuring and innovative workplace practices have had unfavourable effects on older employees. To do this I examine the association between use of new technologies, innovative workplace organization, and human resource management practices, on the one hand, and workplace age distributions on the other. I go on to explore the associations between the various innovations and both workplace training intensity and work experience</description><creator>Gagnon, Marie-Eve</creator><contributor>Michael R Smith (Supervisor)</contributor><date>2020</date><subject>Sociology</subject><title>Mobilite de la main-d'oeuvre et gestion de la diversite des ages:  enjeux et defis lies au maintien en emploi des travailleurs vieillissants</title><language>fre</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/j96025294.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/hx11xk24c</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Sociology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:p8418s69v</identifier><datestamp>2020-07-14T18:15:04Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The whole process of fracture healing depends on a dynamic interaction of biological processes within the fracture gap. Despite the vigorous healing capacity of bone, 5–10% of fractures show either a delay in healing or a complete failure to mend effectively, resulting in bone defects. Currently, there is a variety of different approaches to accelerate the impaired bone repair process, including the 'gold standard' autologous bone graft. However, as we face a donor shortage, biomaterials seem to be the ideal candidates for human bone graft substitutes. Polymers, metals, and ceramics are biomaterials that have been used to treat bone defects, individually or mixed to make composites scaffolds. The main aim of this project was to develop a new injectable and in-situ gelling soft apatite-releasing scaffold with chitosan through a rapid purine-crosslinking reaction. The scaffolds were fabricated by mixing chitosan, adenosine or guanosine diphosphate and biominerals in an ‘all-in-one-step’ procedure. The gelling of chitosan through the crosslinker occurred in &lt; 4 seconds as measured by impedance spectroscopy. These soft gels could retain up to 4 times their weight in water. Spectroscopy analyses revealed the formation of ionic bonds between chitosan and the apatites. Morphological observations showed an interconnected, highly porous structure, with pore size ranging from 200 nm-200 μm that was maintained even with the addition of biominerals. Rheological results indicated that the viscoelastic behavior of the solutions and the elastic behavior of the sponges were maintained after the addition of ceramics up to 75% w/w ratio, and thus confirming the injectability of the soft scaffolds. Initial in vitro studies demonstrated excellent cell adhesion and morphology when pre-osteoblasts cells were cultivated on the surface of the composite scaffolds.To confirm their osteogenic potential of scaffolds in vitro and in vivo, MC3T3-E cells were encapsulated into the scaffold, and their morphology, viability, and attachment were investigated. Histological studies and colorimetric assays demonstrated that the cell-laden scaffolds significantly enhanced proliferation for up to 3-fold, doubled alkaline phosphatase activity and osterix expression, and increased calcium deposits. Then the construct was implanted in a rodded tibia fracture of a mouse model. An increase in callus formation and higher osteoid production in mice implanted were observed with the apatite-chitosan scaffold. All in all, these results indicate that the composite scaffold presented in this thesis project has a promising capacity to stimulate mineralization and promote in vivo fracture healing</description><description>L'ensemble du processus de guérison des fractures dépend d'une interaction dynamique entre les mécanismes biologiques situées au sein de la fracture. Malgré leur capacité de cicatrisation, 5 à 10% des fractures montrent soit un retard dans la cicatrisation, soit un échec total de la réparation, entraînant des défauts osseux. Il existe actuellement diverses approches pour accélérer le processus de réparation des os, y compris la greffe osseuse autologue. Cependant, face à la pénurie de donneurs, les biomatériaux semblent être les candidats idéaux pour les substituts de greffe osseuse humaine. Les polymères, les métaux et les céramiques sont des biomatériaux qui ont été souvent utilisés pour traiter les défauts osseux, individuellement ou amalgamés pour créer des matrices composites.L'objectif principal de ce projet de thèse était de développer un nouvel échafaudage gélifiant à base de chitosan formé rapidement par une réaction de réticulation de la purine, permettant d’encapsuler et de libérer par la suite de l'apatite. Les matrices ont été fabriquées en mélangeant du chitosan, de l’adénosine ou de la guanosine diphosphate et des biominéraux dans une procédure « tout-en-un ». La gélification du chitosan et de l'agent de réticulation s'effectue en moins de 4 secondes, d’après les mesures de spectroscopie d'impédance. Ces gels mous pourraient retenir jusqu'à 4 fois leur poids en eau. La spectroscopie infrarouge a démontré la formation de liaisons ioniques entre le chitosan et les apatites. Les analyses morphologiques ont dévoilé une structure hautement poreuse et interconnectée, avec une taille de pores allant de 200 nm à 200 μm, qui a été maintenue même avec l’ajout de biominéraux. La rhéologie a montré le comportement viscoélastique des solutions et le comportement élastique des matrices, et donc la capacité d'injectabilité des gels mous. Les premières études in vitro ont démontré une bonne adhésion cellulaire et une bonne morphologie lorsque des cellules pré-ostéoblastes ont été cultivées à la surface des matrices composites.Ensuite, les matrices ont été étudiées pour leur potentiel ostéogénique in vitro et in vivo. Les matrices contenant des cellules encapsulées ont été caractérisées pour la morphologie, la viabilité et l'attachement cellulaire. Des études histologiques et des analyses colorimétriques ont démontré que les matrices chargées de cellules augmentaient de manière significative, jusqu'à trois fois plus, la prolifération, doublaient l'activité de la phosphatase alcaline et l'expression d'osterix, et augmentaient les dépôts de calcium. Ensuite, la matrice a été implantée dans une fracture fixe du tibia d'un modèle murin. Une augmentation de la formation du cal osseux et une production accrue d'ostéoïde chez des souris implantées ont été observées avec l'échafaudage d'apatite. Globalement, ces résultats indiquent que la matrice composite présentée dans ce projet de thèse a une capacité prometteuse pour stimuler la minéralisation et favoriser la guérison des fractures in vivo</description><creator>Jahan, Oume Kaushar</creator><contributor>Maryam Tabrizian (Supervisor)</contributor><date>2020</date><subject>Dentistry</subject><title>Development of a novel injectable phosphate-releasing purine-crosslinked chitosan scaffold for bone tissue engineering applications</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/zs25xf012.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/p8418s69v</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Faculty of Dentistry</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:8p58pj36d</identifier><datestamp>2020-07-14T18:15:25Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Beginning in the late 1980s, North American hip-hop music grew into a dominant cultural force. Regional scenes boasted their own MCs (rappers) whose individual musical styles grew increasingly sophisticated and diverse. From approximately 2000 onward, a variety of factors have moved hip-hop music into an era of post-regionalism. Artists from disparate geographic regions routinely collaborate with one another, and the internet’s ability to facilitate virtual musical communities and genres has arguably rendered the idea of local space in hip hop less and less important. In this dissertation I first explore how rhythm and meter in flow (rapping) evolved and became more diverse during hip hop’s earlier years as a regional genre, and I then assess whether rhythm and meter in flow have become more homogeneous during this genre’s more recent, post-regional era. To achieve these goals, I developed and analyzed two song corpora drawn from Rolling Stone magazine and the Grammy Award category of Best Rap Song. I manually transcribed and analyzed 472 verses of flow from 160 songs, focusing on musical parameters such as rhyme, accent/stress, tempo, syllabic density, microtiming, and rhythm. Some of these parameters are analyzed across the 472 verses, while others—which involved more time-intensive analytical methods—are analyzed with greater scrutiny across a 249-verse subset of the corpora.Analytical results are presented in three contexts: statistical discussion, a newly developed theory of segmentation and phrasing, and a categorization scheme for flow profiles. I summarize the statistical component of the analysis by demonstrating that on the whole, flow practices of the 1990s were markedly more complex and diverse than those of the 1980s. The statistics also suggest that, rather than becoming more homogenous in the post-regional era, flow practices embody new forms of complexity and diversity in microtiming, rhythm, and syllabic density. I use the statistical norms to develop a theory of segmentation, phrasing, and meter in hip-hop music, which links the concept of phrasing to flow and the concept of meter to the beat layer (the instrumental accompaniment). This theory is contextualized in how listeners perceive phrasing and meter in hip-hop music: specifically, how phrasing and meter interact in temporal alignment and misalignment. Finally, I use observations from these statistical and phrasal analyses to propose a categorization scheme of flow profiles based on specific musical parameters of flow practices such as tempo, segmentation, or microtiming</description><description>Vers la fin des années 1980, la musique hip-hop nord-américaine était en train de devenir une force culturelle dominante. À l’époque, les scènes régionales comptaient leurs propres MCs (rappeurs) aux styles musicaux de plus en plus sophistiqués et diversifiés. Autour de l’an 2000 et au cours des années qui ont suivi, un amalgame de facteurs ont propulsé la musique hip-hop dans une ère post-régionaliste. Dans ce contexte, des artistes œuvrant dans différentes régions géographiques collaborent couramment et l’Internet promeut les communautés musicales virtuelles et les genres de manière à minimiser graduellement l’importance de la localité dans le milieu du hip-hop. Dans le cadre de cette dissertation, j’explore les manières dont le rythme et le mètre du débit (« flow ») ont évolué et sont devenus davantage variés durant les premières années du mouvement hip-hop, qui était alors caractérisé par son régionalisme. Puis, j’évalue si le rythme et le mètre se sont homogénéisés avec le temps, soit à l’ère dite post-régionaliste. Afin d’atteindre ces objectifs, j’ai élaboré et analysé deux corpus de chansons (issues de la catégorie de meilleure chanson rap de la revue Rolling Stone et des Grammy Awards). J’ai manuellement transcrit et analysé 472 couplets de rap provenant de 160 chansons — me concentrant sur divers paramètres musicaux, tels les rimes, l’accent/le stress, le tempo, la densité syllabique, les variations microtemporales et le rythme. Certains de ces paramètres sont examinés dans tous les couplets étudiés (soit 472), tandis que d’autres — exigeant une méthodologie analytique plus longue — sont analysés de manière plus détaillée dans les couplets regroupés dans une sous-catégorie de cette collection (au nombre de 249). Les résultats analytiques sont présentés par l’entremise de trois contextes : une discussion statistique, une nouvelle théorie de segmentation et de phrasé ainsi qu’un schéma pour catégoriser les profils de débit. Je résume les statistiques de cette analyse en démontrant que, dans l’ensemble, les pratiques en matière de débit des années 1990 étaient manifestement plus complexes et variées que celles des années 1980. Plus encore, selon ces statistiques, au lieu de devenir de plus en plus homogènes dans un contexte post-régionaliste, les pratiques en matière de débit présentent de nouvelles formes de complexité et de diversité en ce qui a trait aux variations microtemporales, au rythme et à la densité syllabique. À l’aide de normes statistiques, je présente comment j’ai développé une théorie de segmentation, de phrasé et de mètre pour la musique hip-hop — qui relie le concept du phrasé au débit et le concept du mètre au beat layer (accompagnement instrumental). Cette théorie est caractérisée par l’interprétation des auditeurs, soit comment ils perçoivent le phrasé et le mètre au cœur de la musique hip-hop — donc, comment ces éléments interagissent avec l’alignement ou avec le désalignement temporel. Pour conclure, par le biais d’observations de ces analyses statistiques et phrastiques, je propose une schématisation pour classifier les profils en matière de débit basé sur des paramètres musicaux propres aux pratiques de débit, soit le tempo, la segmentation ou les variations microtemporelles</description><creator>Duinker, Benjamin</creator><contributor>Nicole Biamonte (Supervisor)</contributor><date>2020</date><subject>Music</subject><title>Diversification and post-regionalism in North American hip-hop flow</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/7s75dh89t.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/8p58pj36d</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Schulich School of Music</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:hd76s469m</identifier><datestamp>2020-07-14T18:15:43Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Rationale: Previous literature has shown healthy adults are capable of quickly adapting and adjusting their gait pattern to the constraints of their environment. For instance, in a laboratory, a healthy adult can adjust and adapt their walking pattern to walk on a split belt treadmill, where each leg is driven at a different speed. The neural control underlying this process is hypothesized to require widespread use of brain networks and is under continuing investigation. Furthermore, our understanding of how neurodegeneration due to Parkinson’s disease (PD) affects this automatic control of walking and gait alterations is emerging.Objective: My doctoral work aims to identify the underlying neural mechanisms in the control of gait adaptation and adjustments to the split belt treadmill in healthy adults and adults with PD.Study 1 (Chapter 3): A systematic review of the current split belt treadmill literature was preformed to consolidate what is known about how the human central nervous system (CNS) controls adaptation to this type of symmetry perturbation. Based on the 62 studies identified, the initial gait adjustments to split belt walking is reliant on proprioceptive feedback to inform central pattern generators to modify lower limb muscle activation patterns appropriately while proprioceptive and visual feedback informs supraspinal centres for motor planning and motor output. Finally, evidence from participants with brain injury suggest that injury impedes, but does not completely take away, the ability to adjust and adapt aspects of the gait pattern to split belts. Based on this evidence, a model was hypothesized for how the human CNS controls these different aspects of the full adaptation process.Study 2 (Chapter 4): To further explore this model for split belt adaptation, a secondary cognitive task was used to assess whether split belt adaptation in young healthy adults, and the ensuing aftereffects, are altered by dual tasking. Further evidence of executive cognitive influence was found in the split belt adaptation process as participants prioritized split belt adaptation over the cognitive task. From this work, the early portion of split belt treadmill adaptation was found to be a cognitive interference period.Study 3 (Chapter 5): Using PET imaging, areas of the young healthy brain that increase and decrease in activation when continuous adjustments to the gait cycle occur were identified. Continuous gait adjustments were associated with increased activity of supplementary motor areas (SMA), posterior parietal cortex (PPC), anterior cingulate cortex and anterior lateral cerebellum, and decreased activity of posterior cingulate and medial prefrontal cortex. From these results a “fine-tuning” network for human locomotion was proposed to exist which includes brain areas for sensorimotor integration, motor planning and goal directed attention.Study 4 (Chapter 6): The final study was set out to determine whether the presence of PD would affect the areas of activation within the “fine-tuning” network proposed in young healthy adults. During continuous belt speed changes adults with PD increased supplementary and primary motor area, PPC, posterior cingulate and right and left cerebellum (lobule I-IV) compared to typical treadmill walking. In addition, adults with PD had a cluster of significantly greater activation within the premotor, supplementary and primary motor areas compared to healthy older adults. These findings are further support of the “fine tuning” network proposed in Study 3 and suggest that adults with PD are more reliant on proprioceptive feedback to adjust their gait pattern.Conclusions: From this thesis work we have been able to inform our understanding of the neural control of complex walking adjustments from one step to the next. Understanding how this model is used allows us to better understand how the CNS itself adapts to maintain locomotor performance</description><description>Fondement: La littérature démontre que les adultes en bonne santé sont capables d'adapter rapidement leur patron de marche aux contraintes environnementales. Les mécanismes neuronaux sous-jacents à ce processus nécessitent une utilisation généralisée des réseaux cérébraux qui demeurent peu compris. Toutefois, de plus en plus d’études concernant la façon dont la neurodégénération causée par la maladie Parkinson (MP) affecte ce contrôle automatique et les ajustements de la marche voient le jour. Objectif: Mon travail de doctorat vise à identifier les mécanismes neuronaux sous-jacents au contrôle de la marche automatique et à l'adaptation de la marche.Étude 1: Une revue systématique a été effectuée de la littérature actuelle au sujet du tapis roulant à double courroies pour consolider ce qui est connu sur la façon dont le système nerveux central (SNC) humain contrôle l'adaptation à ce type de perturbation de symétrie. Selon les 62 études identifiées, les ajustements initiaux à la marche avec double courroies dépendent du feedback proprioceptif qui informe les générateurs de schémas centraux de la nécessité de modifier les schémas d'activation musculaires des membres inférieurs de manière appropriée. D’autre part, les feedbacks proprioceptif et visuel informent les centres supra spinaux de la planification motrice et de la puissance motrice. Enfin, les données des participants souffrant de lésions cérébrales suggèrent que les blessures entravent, mais ne suppriment pas complètement, la capacité d'ajuster et d'adapter certains aspects de la marche sur tapis roulant à double courroies. Sur la base de ces preuves, un modèle a été proposé sur la façon dont le système nerveux central humain contrôle ces différents aspects du processus d'adaptation complet.Étude 2: Pour explorer davantage ce modèle d'adaptation aux doubles courroies, une tâche cognitive secondaire a été utilisée afin d’évaluer si l’adaptation à ce type de marche chez les jeunes adultes en santé est modifiée par une double tâche. Les résultats amènent d'autres preuves de l'influence cognitive de l'exécutif dans le processus d'adaptation aux doubles courroies. En effet, les participants semblent avoir priorisé l'adaptation à la tâche cognitive en réduisant leur précision à la tâche cognitive. La première partie de l'adaptation au tapis roulant avec double courroies est serait donc une période d'interférence cognitive.Étude 3: À l'aide de l’imagerie TEP, nous avons identifié des régions cérébrales spécifiques qui augmentent ou diminuent en activation lors d’ajustements continus du cycle de marche chez des participants jeunes et en bonne santé. Ces ajustements continus de la marche ont été associés à une activité accrue de l’aire motrice supplémentaire, du cortex pariétal postérieur, du cortex cingulaire antérieur et du cervelet latéral antérieur, et à une diminution de l'activité du cingulaire postérieur et du cortex préfrontal médial. À partir de ces résultats, un réseau de « peaufinage » a été proposé pour la locomotion humaine. Étude 4: La dernière étude visait à déterminer si la présence de la MP affecterait les régions d'activation au sein du réseau de « peaufinage » proposé chez les jeunes adultes en bonne santé. Chez les adultes atteints de la MP, l’activité de l’aire motrice supplémentaire, du cortex moteur primaire, du cortex pariétal postérieur, du cortex cingulaire postérieur et du cervelet droit et gauche (lobule I-IV), lors des changements continus de vitesse des courroies, a augmenté, ce qui n’était pas le cas lors de la marche sur tapis roulant typique. Ces résultats viennent appuyer le réseau de « peaufinage » proposé dans l'étude 3.Conclusions: À partir de cette thèse, nous avons pu éclairer la compréhension que nous avons du contrôle neuronal des ajustements de la marche complexe d'une étape à l'autre. Comprendre comment ce modèle est utilisé nous permet de mieux comprendre comment le SNC lui-même s'adapte pour maintenir la locomotion</description><creator>Hinton, Dorelle</creator><contributor>Caroline Paquette (Supervisor)</contributor><date>2020</date><subject>Kinesiology and Physical Education</subject><title>The fine-tuning network: an investigation into the neural correlates of gait cycle adjustment and adaptation to split belt treadmill locomotion</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/r781wm72n.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/hd76s469m</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Kinesiology and Physical Education</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:hm50tw942</identifier><datestamp>2020-07-14T18:16:05Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The Arctic is warming two to three times faster than the global average, however this general statement does not take into account the heterogeneous responses of different permafrost systems. This thesis illustrates some of the responses of a high Arctic polar desert system with ice-rich permafrost, focusing specifically on the initiation and dynamics of retrogressive thaw slumps (RTSs) and the impact of ice-wedge trough morphology on surficial conditions and near surface ground temperatures of a high-centred polygon system. RTSs and ice-wedge degradation are characteristic landforms produced by thermokarst (lowering of the land surface due to melting ground ice). This research takes place in the area within the vicinity of the Eureka Weather Station on Ellesmere Island, Nunavut, in the Eureka Sound Lowlands (ESL). This area (80 °N latitude) has a polar desert climate with long term (since 1947) mean annual air temperatures of -19.7 ° C. Permafrost measures over 500 m thick, is ice-rich in the upper 20 – 30 m and measures -16.5 °C at the depth of annual amplitude (15.4 m). Mean active layer thickness is 57 cm.The first study demonstrates the sensitivity of the ESL to increasing summer air temperatures by utilizing ~30 years (1989-2018) of aerial RTS observations. The number of active RTSs increased from 100 or less within a year to over 200 during the warmest summers on record. Retreat rates were assessed for 12 local RTSs in the area of the Eureka Weather Station. Mapping of RTS headwall positions between 2011 and 2018 using a differential GPS and high resolution satellite imagery provided mean annual retreat for individual RTS up to 26.7 m yr-1. My findings show that while climate initiated RTS activity, after a few years, terrain factors, such as slope, may be more important in maintaining active RTSs. Finally, this study highlights some important comparisons of high Arctic RTS dynamics to observed RTS dynamics in the low Arctic.The second study monitored three of the 12 local RTSs in the first study, on a daily basis for most of the 2017 thaw season. The purpose was to gain additional insights into RTS activity that may be obscured at annual timescales. This study explored statistical correlations between mean daily air temperature (MDAT), total daily precipitation (TDP) and thawing degree days (TDD) with mean daily retreat and mean cumulative retreat. There was a statistically significant correlation with MDAT and TDD, and mean daily and cumulative retreat for all three sites but not with TDP. TDD could explain almost all the variation (R2 &gt; 0.99) in cumulative retreat for all sites. My final study investigated how varying ice-wedge trough morphologies caused by thermokarst impacted surficial conditions and near surface ground temperatures within a high-centred polygon system. Seven ice-wedge troughs and two polygon centres were monitored for over a year (3 July, 2017 – 21 July, 2018) using thaw depths, topographic surveys, vegetation cover, soil moisture and continuous shallow (12 cm) ground temperatures. Mean annual, summer and winter ground temperatures varied by 5.1 °C, 2.5 °C and 15.2 °C, respectively, between all sites. My results show that snow redistribution by wind from polygon centres to troughs induces ground cooling in ice-wedge polygon centres that could drive new thermal contraction cracking and new ice-wedge formation. This doctoral dissertation fills a gap in high Arctic polar desert thermokarst dynamics and responses that is likely applicable to other Arctic systems. With current and future climate change occurring, research efforts should not only characterise the drivers of landscape change but also seek to understand the new system responses generated by thermokarst</description><description>L'Arctique se réchauffe deux à trois fois plus vite que la moyenne mondiale, mais cette déclaration générale ne prend pas en compte les réponses hétérogènes des différents systèmes de pergélisol. Cette thèse illustre certaines réponses d'un système de désert polaire du haut-Arctique avec un pergélisol riche en glace, en se concentrant spécifiquement sur les glissements de dégel rétrogressifs (GDR) et les coins de glaces. Les GDRs et la dégradation des coins de glaces sont produites par le thermokarst (abaissement de la surface du sol à cause de la fonte de la glace de sol). Cette recherche se déroule dans les alentours de la Station Météorologique d’Eureka sur l'île d'Ellesmere situé dans les Basses Terres de l'Eureka Sound (BTES). Cette zone a un climat désert polaire avec des températures annuelles moyennes de l'air à long terme (depuis 1947) de -19,7 ° C. Le pergélisol mesure plus de 500 m d'épaisseur, est riche en glace de sol dans les 20 à 30 m supérieurs et mesure -16,5 ° C à la profondeur de l'amplitude annuelle (15,4 m). L'épaisseur moyenne de la couche active est de 57 cm.La première étude démontre la sensibilité de l'BTES à l'augmentation des températures de l'air d'été en utilisant ~ 30 ans (1989-2018) d'observations aériennes GDRs, y compris les comptes de fréquence et la distribution de GDRs. Le nombre de GDR actifs est passé de 100 ou moins à plus de 200 pendant les étés les plus chauds jamais enregistrés (2011, 2012 et 2015). Les taux de retrait ont été évalués pour 12 GDRs locaux dans la région d'Eureka entre 2011 et 2018, et ont un recul annuel moyen jusqu'à 26,7 m an-1. Mes résultats montrent que même si l'activité GDR est déclenchée par le climat, après quelques années, les facteurs de terrain, tels que la pente, peuvent être plus importants pour maintenir les GDR actifs et découpler essentiellement l'activité GDR du climat. Enfin, cette étude met en évidence certaines comparaisons importantes de la dynamique GDR du haut-Arctique avec la dynamique GDR observée dans le bas-Arctique.La deuxième étude a suivi quotidiennement trois des 12 GDRs locaux de la première étude, pendant la saison de dégel de 2017. Le but était d'obtenir des informations supplémentaires sur l'activité GDR qui peuvent être masquées à des échelles de temps annuelles. Cette étude a exploré les corrélations statistiques entre la température quotidienne moyenne de l'air (TQMA), les précipitations quotidiennes totales (PQT) et les degrés-jours de dégel (DJD) avec la retraite quotidienne moyenne et la retraite cumulative moyenne. Il y avait une corrélation statistiquement significative avec le TQMA et le DJD, et une retraite quotidienne et cumulative moyenne pour les trois sites, mais pas avec le PQT. Le DJD a pu expliquer presque toute la variation (R2 &gt; 0,99) de la retraite cumulée pour tous les sites. Ma dernière étude a examiné la façon dont les différentes morphologies des creux provoquées par le thermokarst ont affecté les conditions de surface et les températures du sol dans un système de polygones à centre creux. Sept creux de coin de glaces et deux centres de polygones ont été surveillés du 3 juillet 2017 au 21 juillet 2018 à l'aide des mesures de profondeurs de dégel, des levés topographiques, le couvert végétal, l'humidité du sol et des températures du sol peu profondes (12 cm) mesurer continuellement. Mes résultats montrent que la redistribution de la neige par le vent des centres des polygones au creux induit un refroidissement du sol qui pourrait entraîner une nouvelle fissuration par contraction thermique et de nouvelle formation de coin de glace. Cette thèse de doctorat fournit des observations des réponses d’un système désert polaire du haut-Arctique aux changements climatiques. De plus, les efforts de recherche devraient non seulement caractériser les conducteurs du changement de paysage, mais aussi chercher à comprendre quelles sont les nouvelles réponses du système générées par ces changements</description><creator>Ward, Melissa Karine</creator><contributor>Wayne H Pollard (Supervisor)</contributor><date>2020</date><subject>Geography</subject><title>Permafrost-active layer dynamics and feedbacks with climate forcing in ice-richclimate forcing in ice-rich sediments, Fosheim Peninsula, Ellesmere Island, Nunavut</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/r781wm73x.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/hm50tw942</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Geography</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:qv33s185n</identifier><datestamp>2020-07-14T18:16:11Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The minerals industry has recently encountered significant price fluctuations leading to more business risk and unexpected overall returns on capital fund invested. This situation forces mining corporations to find new decision-making processes to improve productivity and efficiency in allocation or prioritization of business-related spending, including sustaining and working capital projects. This research aims to propose new portfolio management strategies to be used at the senior management level of global mining companies. Given that decision-making processes regarding a portfolio require risk management and diversification components, the main emphasis is on managing the trade-offs between risks and returns. Therefore, the effect of business unit performance of a project initiator, the country stability where the project will be implemented, the commodity market behavior under unexpected extreme events were reviewed and developed in the proposed portfolio optimization models. Quantification techniques were explored for portfolio optimization with operational performance, commodity market behavior, and international and country risks for extreme events. In addition, the phenomena affecting risk quantification such as reproduction of relationships between portfolio elements and reaction to unexpected cases were further embedded in the decision-making process. Risk indicators to be generated were used in the optimization process to maximize the return from a corporate portfolio while considering the risk-taking capacity of a global mining company. Ultimately, this research contributes to the development of effective and efficient portfolio management approaches, including prioritization of a weighted decision-making criterion in optimization models, such that mining stakeholders will benefit from optimal returns at an acceptable risk</description><description>L’industrie minière a connu de graves turbulences de prix ces dernières années. Ce qui oblige les entreprises minières à améliorer leurs processus de prise de décision de manière à inclure la productivité et l'efficacité de l'allocation ou de la priorisation des dépenses liées aux investissements, y compris les projets de remise à neuf et de maintien de fond de roulement. Cette recherche a pour objectif de proposer de nouvelles stratégies de gestion de portefeuille à utiliser par les cadres supérieurs des sociétés minières mondiales. Étant donné que les processus décisionnels concernant un portefeuille nécessitent des composants de gestion du risque et de diversification, l'accent a été mis sur la gestion du compromis entre risque et rendement. Les impacts de performance de l’unité d’affaire initiateur d’un projet à approuver, la stabilité du pays dans lequel le projet doit être implanter et le comportement des marches de commodité sous pression évènements extrêmes inopinés ont été revues et développés dans ce travail de recherche. En outre, les techniques d’évaluation de gestion optimale de portefeuilles liées à la performance opérationnelle, le comportement des commodités, les risques internationaux, la stabilité des pays pour des évènements extrêmes ont été explorées dans cette étude. En plus les phénomènes affectant la quantification ainsi que la relation entre les éléments de portefeuille et leurs impacts ont été inclus dans le processus de prise de décisions. Les indicateurs de risque à générer ont été utilisés dans le processus d’optimisation afin de maximiser le rendement du portefeuille de la société, ceci en tenant compte de la capacité de prise de risque d’une société minière mondiale. En fin de compte, ces travaux de recherche contribuent à la mise au point d’approches de gestion de portefeuille efficaces et efficientes, incluant la priorisation de l’importance des critères de prise de décision, dans les modelés d’optimisation de manière à ce que les parties prenantes des entreprises minières bénéficient d’un retour sur investissement optimal à un niveau de risque acceptable</description><creator>Nkuidjeu Njike, Achille</creator><contributor>Mustafa Kumral (Supervisor)</contributor><date>2020</date><subject>Mining and Materials</subject><title>Corporate portfolio management for sustainable healthy returns in the minerals industry</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/n009w671k.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/qv33s185n</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mining and Materials</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:pr76f810w</identifier><datestamp>2020-07-14T18:17:11Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Iranians were the third-largest group of admitted immigrants to Canada between 2011 and 2016 (Statistics Canada, 2016).  The current literature on Iranian immigrants in North America explores Iranians’ integration into the host society and Iranian communities (Kafili, 2013; Mostofi, 2003).  Studies that document a lack of cohesion within Iranian communities have mostly focused on the role of religious or political affiliations at the community level (Malek, 2015b).  However, an in-depth account of interpersonal interactions and a sense of belonging to the community is missing in this literature.  Moreover, research on Iranians living in North America has focused on single identities and has not considered immigrants’ intersections of identities.  Therefore, this qualitative study explored how Iranian immigrants in Québec, experienced their intersections of identities during interactions with other Iranians and the meanings immigrants made while living in or belonging to the Iranian community.  Hermeneutic phenomenology was used as a research methodology and intersectionality as the epistemological framework.  Semi-structured interviews were conducted with 12 first-generation Iranians between the ages of 32 and 44 who had permanent residency status and who lived in Québec for at least three years.  An interpretive phenomenological analysis of the results yielded several major themes: transitioning to Québec, challenges of bilingualism, career transitioning, experiences unique to women, community engagement, and interpersonal interactions.  An analysis of intersectionality highlighted the intersections of classism, sexism, ageism, racism and political affiliation in shaping interpersonal interactions and a sense of belonging to the community.  The implications for practice, policymaking, and future research are discussed.  Keywords: Iranian immigrants, intersections, community belonging, interpersonal interactions</description><description>Les Iraniens constituaient le troisième groupe en importance d'immigrants admis au Canada entre 2011 et 2016 (Statistics Canada, 2016).  La littérature actuelle sur les immigrants iraniens en Amérique du Nord explore l'intégration des Iraniens dans la société hôte et les communautés iraniennes (Kafili, 2013; Mostofi, 2003).  Les études qui documentent un manque de cohésion au sein des communautés iraniennes ont principalement porté sur le rôle des affiliations religieuses ou politiques au niveau communautaire (Malek, 2015b).  Cependant, un compte rendu détaillé des interactions interpersonnelles et d'un sentiment d'appartenance à la communauté, font défaut dans cette littérature.  De plus, les recherches sur les Iraniens vivant en Amérique du Nord se sont concentrées sur les identités uniques et n'ont pas pris en compte les intersections des identités des immigrants.  Par conséquent, cette étude qualitative a exploré la façon dont les immigrants iraniens au Québec ont vécu l'intersection de leurs identités lors d'interactions avec d'autres Iraniens et les significations créées par les immigrants lorsqu'ils vivaient ou appartenaient à la communauté iranienne.  La phénoménologie herméneutique a été utilisée comme méthodologie de recherche et l'intersectionalité comme cadre épistémologique.  Des entrevues semi-structurés ont été réalisés avec 12 Iraniens de première génération âgés de 32 à 44 ans, titulaires du statut de résident permanent et résidant au Québec depuis au moins trois ans.  Une analyse phénoménologique interprétative des résultats a permis de dégager plusieurs grands thèmes : la transition au Québec, les défis du bilinguisme, la transition de carrière, expériences uniques aux femmes, l'engagement communautaire et les interactions interpersonnelles.  Une analyse de l'intersectionnalité a mis en évidence les intersections du classisme, du sexisme, de l'âgisme, du racisme et de l'affiliation politique dans la formation d'interactions interpersonnelles et d'un sentiment d'appartenance à la communauté.  Les implications pour la pratique, l'élaboration des politiques et les recherches futures sont discutées. Mots-clés : immigrants iraniens, intersections, appartenance à la communauté, interactions interpersonnelles</description><creator>Nasrullah, Shakib</creator><contributor>Ada L Sinacore (Supervisor)</contributor><date>2020</date><subject>Educational and Counselling Psychology</subject><title>Iranian immigrants’ interactions within Iranian communities: An exploration of diversity and belonging</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/1831cq165.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/pr76f810w</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Educational and Counselling Psychology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:00000446f</identifier><datestamp>2020-07-14T18:17:24Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>La dispersion explosive d’un milieu hétérogène, spécifiquement un lit de particules saturé ou insaturé, entourant une charge de dispersion hautement explosive génère une suite d’évènements polyphasique complexe. Le nuage de particules et/ou de gouttelettes qui se développe tend à avoir une densité de distribution non uniforme et peu mener à la formation de structure ayant l’apparence de jets. Cette tendance à former des jets pendant la dispersion d’un système non saturé semble dépendre de la charge explosive ainsi que des caractéristiques des matériaux dont sont composées les particules. Les particules formées de céramiques friables ou de métaux ductiles sont plus susceptibles de former des jets, alors que les particules de dureté moyenne et d’une résistance de compression élevée le sont moins. Les mécanismes dominants qui sous-tendent la formation de ces jets ont été sujets à débats entre les chercheurs.Cette thèse présente des données expérimentales et informatiques soutenant l’hypothèse selon laquelle la consolidation de l’onde de choc, dépendant de la dureté caractéristique du matériau, à travers un lit de particules disséminées insaturé est le mécanisme qui sous-tend le phénomène des jets. Après la formation de la couche consolidée de particules, la dispersion dépend de la rupture et de la fragmentation de cette couche. Des simulations à mésoéchelle, à l’aide de l’hydrocode EDEN, montrent que pendant le compactage et l’expansion subséquente de la couche de céramique friable, les ruptures émanent de la surface interne de la couche consolidée, se propagent radialement vers l’extérieur et bifurquent, ce qui est cohérent avec la distribution bimodale des fragments observée sur les radiographies obtenues pendant des expériences de dispersion explosive d’un lit de particule de carbure de silicium. En revanche, les simulations à mésoéchelle de particules de métaux ductile dispersées par explosion présentent la fragile rupture sous pression habituellement associée avec la fragmentation d’une couche de métal, ce qui corrobore la distribution monomodale des fragments observés sur les radiographies obtenues pendant la dispersion d’un lit de particules d’aluminium.En plus du sujet principal concernant la dispersion d’un lit de particule non saturé composé d’un seul type de particule, deux problèmes additionnels, étroitement reliés sont examinés : (i) la dispersion explosive d’un mélange binaire insaturé, et (ii) la dispersion explosive de lits de particules saturés. La dispersion explosive de mélanges binaires, contenant à la fois des particules formant des jets et des particules n’en formant pas, produit un nuage de particules présentant une superposition des comportements observés pendant la dispersion individuelle des composantes. L’effet de la composition du lit de particule sur la configuration du comportement de superposition est étudié dans les mélanges binaires, en variant le ratio volumique des deux types de particules ainsi que de la masse explosive. Pour les lits de particules saturés, les effets de la cavitation hétérogène à l’interface particule-liquide sont étudiés en variant la morphologie des particules. Des estimations des vélocités de dispersion initiale des couches de particules sont aussi obtenues grâce à des enregistrements vidéo à haute vitesse des tests expérimentaux. Ces résultats sont comparés avec les modèles Gurney classique et poreux, et les modèles coïncident raisonnablement avec les résultats expérimentaux, dans les limites des marges d’erreur des vélocités expérimentales</description><description>The explosive dispersal of a heterogeneous medium, specifically a saturated or unsaturated particle bed, surrounding a high-explosive burster charge generates a complex multiphase flow. The developing cloud of particles and/or liquid droplets tend to have a non-uniform density distribution and can lead to the formation of jet-like structures. The tendency to form these jets in the dispersal of an unsaturated system has been seen to depend on the explosive loading and the material properties of the particles. Particles comprised of brittle ceramics or soft-ductile metals are more susceptible to forming jets, whereas particles with moderate hardness and high compressive strength are less prone to jet formation. The dominant mechanisms governing the generation of these jets has been a topic of debate amongst researchers.This thesis presents experimental and computational data supporting the theory that shock consolidation, dependent upon the characteristic material strengths of the medium, within a dispersed unsaturated particle bed is the governing mechanism responsible for the jetting phenomenon. After the formation of the consolidate, the dispersal events depend on the fracture and fragmentation of the particle shell. Mesoscale simulations using the EDEN hydrocode show that during the compaction and subsequent expansion of a layer of brittle ceramic particles, fractures emanate from the inner surface of the consolidate, propagate radially outward and bifurcate, consistent with the bimodal fragment size distribution observed in radiographic films obtained during experiments of the explosive dispersal of a silicon carbide particle bed. In contrast, mesoscale simulations of explosively dispersed ductile metal particles exhibit the typical shear and brittle fracture behaviour usually associated with the fragmentation of a metal casing, supporting the monomodal fragment size distribution observed in radiographic films from the dispersal of an aluminum particle bed. In addition to the main topic concerning the dispersal of an unsaturated particle bed comprised of a single particle type, two supplementary, closely-related problems are examined: (i) the explosive dispersal of unsaturated binary mixtures, and (ii) the explosive dispersal of saturated particle beds. The explosive dispersal of binary mixtures, containing both “jetting” and “non-jetting” particles, produces a particle cloud with features consistent with the superposition of the behaviour observed during the dispersal of the individual components. The effect of the particle bed configuration on the superposition behaviour is examined in the binary mixtures, by varying the volumetric ratio of the two types of particles and the charge loading. In saturated particle beds, the effects of heterogeneous cavitation at particle-liquid interfaces are examined through varying the morphology of the particles in the charge. Estimates of the initial velocities of the dispersed particle layers are also obtained from high-speed video records of the experimental tests. These results are compared with both the classical and porous Gurney models, and reasonable agreement is observed between the models and the experimental results, within the error estimates of the experimental velocities</description><creator>Marr, Bradley</creator><contributor>David Frost (Supervisor)</contributor><date>2020</date><subject>Mechanical Engineering</subject><title>The explosive dispersal of heterogeneous systems surrounding high-explosive charges</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/xg94ht989.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/00000446f</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:41687n61s</identifier><datestamp>2020-07-14T18:18:22Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>A balanced diet is very essential to stay healthy. Fresh fruits and vegetables contribute the most important part of a balanced diet. There are numerous challenges in the supply chain management to ensure that fresh produce can reach the consumer as clean, fresh, wholesome and free of undesirable contaminants as possible. Pulsed light (PL) treatment is a promising technology which can be used on fresh produce to enhance some quality enhancement. As a defense response mechanism against the harsh/unfavorable treatment, the treated fruit or vegetable produces secondary metabolites such as antioxidants and total phenolic compounds. The first objective of the study focused on PL treatment of fresh carrots (both cut and whole) for quality and shelf life enhancement. The ultraviolet light treatment (UV-C), which is known in industry for its germicidal properties, has been used for environmental air and drinking water decontamination. The second objective was focused on a comparative evaluation of PL versus UV-C light treatment on quality changes and shelf life of another vegetable, red radish.Only minimal changes in the overall appearance during the shelf life of PL treated whole carrot were observed as compared to PL treated cut carrots (Daucus carota) and total microflora in test samples also was very minimally affected demonstrating only a small reduction in surviving microflora (0.12-0.42 logarithmic cycle reduction). The study demonstrated that PL treatment did not significantly increase the overall shelf life of carrots. However, it demonstrated the significantly positive impact on quality enhancement. Peroxidase enzyme retained its  activity in both (cut and whole carrots), while the antioxidant activity showed a large increase, by several folds, along with the higher log reduction was achieved for aerobic and yeast &amp; mold count. PL treatment resulted in lowering of changes in pH for whole carrots possibly by microbial control. In the case of red radish (Raphanus sativus), the overall appearance of treated radish was found to change during the study which again showed only a small reduction in total microflora (0.12-1.07 logarithmic cycles). The skin color of untreated and treated radish turned from "shining red" to "dark black" during storage at room temperature, and the pulsed light and UV-C treatments at the highest dosage was responsible for faster rate of deterioration. However, the antioxidant activity of light treated samples increased during storage up to several folds, when compared with the untreated samples.Further research is needed to understand the adverse effects on specific commodity which could lead to the design and commercial application of this technology</description><description>Un régime alimentaire équilibré essentielle pour rester en bonne santé. Les fruits et légumes frais constituent la partie la plus importante d’un régime alimentaire équilibré. Il existe de nombreux défis dans la gestion de la chaîne d'approvisionnement pour garantir que les produits frais peuvent arriver au consommateur aussi propre, frais, sain et exempt de contaminants indésirables que possible. Le traitement à la lumière pulsée (LP) est une technologie prometteuse qui peut être utilisée sur les produits frais pour augmenter certaines améliorations de la qualité. En tant que mécanisme de réponse de défense contre le traitement dur / défavorable, il produit des métabolites secondaires tels que des antioxydants et des composés phénoliques totaux. Le traitement à la lumière ultraviolette (UV-C), qui est connu dans l'industrie pour ses propriétés germicides, a été utilisé pour la décontamination de l'air et de l'eau potable dans l'environnement. Cela a dérivé le premier objectif comme une étude du traitement PL sur les changements de qualité de carottes fraîches (coupées et entières) et l'amélioration de la durée de conservation. En outre, la recherche s'est étendue au deuxième objectif en tant qu'étude comparative du traitement de la lumière PL par rapport aux rayons UV-C sur les changements de qualité et la durée de conservation de l'ensemble du radis.Seuls des changements minimes de l'apparence globale au cours de la durée de conservation des carottes entières traitées au LP ont été observés par rapport aux carottes coupées traitées au LP (Daucus carota) et la microflore totale dans les échantillons d'essai a également été très peu affectée, démontrant seulement une petite réduction de la microflore survivante (réduction du cycle logarithmique de 0,12 à 0,42). L'étude a démontré que le traitement au LP n'augmentait pas de manière significative la durée de conservation globale des carottes. Cependant, il a démontré l'impact significativement positif sur les paramètres de qualité. Le traitement au LP a entraîné des changements de pH plus faibles pour les carottes entières. En outre, l'enzyme peroxydase n'a pas été désactivée pour les deux (carottes coupées et entières), tandis que l'activité antioxydante a montré une augmentation de plusieurs fois, ainsi qu'une réduction logarithmique plus élevée a été obtenue pour le nombre d'aérobies et de levures et moisissures.Dans le cas du radis (Raphanus sativus), l'apparence globale du radis traité a changé au cours de l'étude, mais encore la microflore totale ne montrant qu'une faible réduction (0,12-1,07 cycle logarithmique). La couleur de la peau des radis non traités et traités est passée du rouge brillant au noir foncé pendant le stockage à température ambiante, et les traitements à la lumière (LP et UV-C) à la dose plus élevée étaient responsables d'une accélération du taux de détérioration dans le cas des radis. Cependant, l'activité antioxydante des échantillons traités à la lumière a augmenté pendant le stockage jusqu'à plusieurs fois, par rapport aux échantillons non traités.Des recherches supplémentaires sont nécessaires pour comprendre les effets néfastes sur des produits spécifiques et qui pourraient conduire à la conception et à l'application commerciale de cette technologie</description><creator>Prusty, Prasant</creator><contributor>Hosahalli Ramaswamy (Supervisor)</contributor><date>2020</date><subject>Food Science and Agricultural Chemistry</subject><title>Effect of pulsed light (PL) and UV-C light treatments on the shelf-life and quality enhancement of fresh carrot and radish</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/v405sg141.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/41687n61s</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Food Science and Agricultural Chemistry</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:n296x365n</identifier><datestamp>2020-07-14T18:18:33Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Individuals from various socio-economic statuses, cultural backgrounds, and ages engage in gambling-related activities every day. Although many demonstrate a healthy style of engagement, there has been growing health concerns over the significant and detrimental short- and long-term consequences of problematic gambling, which has often been associated with a variety of indicators of maladaptive functioning including issues surrounding the poor regulation of emotions.  Although several theories of problem gambling have suggested that gambling may serve as a function to regulate one’s affect (e.g., Jacobs, 1986), most studies have investigated symptoms of emotion dysregulation (i.e., anxiety, depression, etc.), rather than exploring problem gambling from a social-psychological theoretical perspective. The present program of research applies the theory of emotion regulation (ER) to the study of problematic gambling across three separate studies with a particular interest in the specific impact of the dimensions of ER. Given the infancy of the research, Study 1 aimed to examine the current literature in the field. As such, a systematic review examining the relationship between ER and both problem gambling and gaming was conducted. A total of 20 published manuscripts were included in the review and data regarding outcome measures, sampling methods, results, and effect sizes of relationships were extracted and critically evaluated. Results indicated 90% of studies found lower ER to be associated with reports of greater video gaming or gambling disorder symptomology, with 13 studies (65%) reporting medium to large effect sizes. Following this study, the rest of the program of research focused solely on gambling behaviors. Study 2 (N = 820 gamblers; Mage= 21.14 years, SD = 2.9, 50.9% female) revealed through a linear regression that difficulties with impulse control and improved goal-directed behaviors positively contributed to problem gambling. Further, a mediation analysis demonstrated that the association often seen between problem gambling and depression was explained by deficits in the acceptance, goals, strategies, and clarity dimensions of ER. Finally, Study 3 (N = 919 gamblers; Mage= 21.16 years-old, SD = 2.90, 48.1% female) explored whether deficits in specific dimensions of ER coupled with the motivation to escape negative emotions (i.e., coping motives) increased the likelihood of problem gambling severity. A series of six moderation analyses were conducted and revealed that total models accounted for approximately 37-38% of the variance in problem gambling. Also, coping motives interacted with less difficulties in goal-directed behavior, increased lack of emotional clarity, and increased lack of emotional awareness to create a toxic mixture for problem gambling. These findings provide an in-depth analysis of the relationship between ER and problem gambling, add to our understanding of the underlying mechanisms that explain the affective consequences of problem gambling, and reveal the importance of attending to both psychological and motivational factors when implementing prevention and intervention programs</description><description>Des personnes de statuts socio-économiques, de milieux culturels, et d'âges divers s'adonnent quotidiennement à des activités liées au jeu de hasard. Bien que la majorité démontre un style d'engagement sain, on s'inquiète de plus en plus des conséquences reliées au jeu problématique, qui sont importantes et néfastes, et qui impactent la vie des gens à court et à long terme. Souvent ce comportement a été associé à divers indicateurs de fonctionnement mal adapté, y compris des questions entourant la capacité des gens à gérer leurs émotions.  Bien que plusieurs théories sur le jeu problématique aient suggéré que le jeu peut servir à réguler l’état émotionnel d'une personne (p. ex., Jacobs, 1986), la plupart des études ont porté sur les symptômes du dérèglement des émotions (c.-à-d., l'anxiété, la dépression, etc.), plutôt que d'explorer le jeu problématique d'un point de vue des théories adopté par la sociopsychologie. Le programme de recherche présenté apporte la théorie de la régulation des émotions (RÉ) à l'étude des problèmes du jeu dans le cadre de trois études distinctes, spécifiquement sur l'impact des dimensions de la RÉ. Étant donné que la recherche en est qu’à ses débuts, l'étude 1 visait à examiner la littérature existant dans le domaine. Alors, une revue systématique a été effectuée sur la relation entre la régulation des émotions et les jeux de hasard et les jeux vidéo. Au total, 20 revues publiées ont été incluses, et les données concernant les méthodes d'échantillons, les résultats et la taille des effets sur les liens ont été évaluées de façon critique. 90% des études ont indiqué qu’un niveau de RÉ plus faible était lié à des rapports de symptômes plus significatifs de jeu vidéo ou de troubles du jeu. En plus, 13 études (65%) ont conclu que l’impact était entre moyenne et grande (Cohen’s d ≥ .50). Suite à cette étude, le restant du programme de recherche s'est concentré uniquement sur les comportements de jeu. L'étude 2 (N = 820 joueurs ; Mâge = 21.14 ans, SD = 2.9, 50.9 % femmes) a démontré, par une régression linéaire, que les difficultés à contrôler les impulsions et l'amélioration des comportements axés sur des objectifs avaient un effet positif sur le jeu problématique. De plus, une analyse de médiation a démontré que l'association observée entre le jeu problématique et la dépression s'expliquait principalement par des déficits dans les dimensions du RÉ d'acceptation, des comportements axés sur des objectifs, de stratégies de régulations, et de clarté. Enfin, l'étude 3 (N = 919 joueurs ; Mâge = 21.16 ans, SD = 2.90, 48.1% femmes) visait à déterminer si les déficits dans ses dimensions spécifiques du RÉ, combiné avec la motivation d’évadé ses émotions négatifs (c.-à-d. les motifs d'adaptation), augmentaient la probabilité du jeu problématique plus sévère. Une série de six analyses de modération a été effectuée et a révélé que le total des modèles représentait environ 37 à 38% de la variance du jeu problématique. De plus, il a été démontré que les motifs d'adaptation interagissaient avec moins de difficultés avec le comportement orienté vers un but, un manque accru de clarté émotionnelle et un manque accru de conscience émotionnelle, ce qui cré un mélange toxique pour le jeu problématique. Ces résultats fournissent une analyse approfondie de la relation entre le RÉ et le jeu problématique, ajoutent à notre compréhension des mécanismes sous-adjacents qui expliquent les conséquences affectives du jeu problématique, et révèlent l'importance de s'occuper des facteurs psychologiques et motivationnels lors de la mise en œuvre de programmes de prévention et d'intervention</description><creator>Marchica, Loredana</creator><contributor>Tina Montreuil (Supervisor2)</contributor><contributor>Jeffrey L Derevensky (Supervisor1)</contributor><date>2020</date><subject>Educational and Counselling Psychology</subject><title>The impact and role of emotion regulation in problem gambling</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/bv73c5072.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/n296x365n</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Educational and Counselling Psychology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:f7623h94q</identifier><datestamp>2020-07-14T18:18:35Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>La glace de mer dans l'océan Arctique se déforme le long de lignes bien définies où, sous l'action de cisaillement et de divergence, se forment des régions d'eau libre (chenaux) et des crêtes de glace. La présence de chenaux d'eau libre et de crêtes de glace dans la banquise influence le transfert de chaleur et d'humidité entre l'océan et l'atmosphère et affecte aussi les flux de sel/eau douce dans l'océan de surface lorsque la glace se forme en hiver ou fond en été. Une représentation réaliste de ces lignes de déformations par les modèles de glace de mer est donc de mise pour reproduire le système climatique Arctique de manière fidèle. Le but de cette thèse est d'évaluer la capacité de différent modèles rhéologiques (ou rhéologies) de glace de mer à reproduire ces déformations et d'utiliser ces connaissances pour améliorer la représentation des déformations de la banquise par les modèles dynamiques de glace de mer. À cette fin, nous étudions les statistiques des champs de déformation obtenues par observations satellitaires de la banquise dans l'océan Arctique ainsi que celles obtenues avec des modèles dynamiques de glace de mer. En un premier temps, nous étudions la capacité de la rhéologie dite visco-plastique (VP) à reproduire les densités de probabilité (PDFs) des taux de déformations observés. Une meilleure correspondance avec les champs de déformation observés est obtenue avec la rhéologie VP lorsque la résistance de la glace en cisaillement est plus élevée que la résistance en compression. Notre analyse de sensibilité des PDFs simulés aux paramètres de résistance mécanique de la glace sert aussi de nouvelle méthode pour calibrer la résistance de la glace de mer utilisée par les modèles. En deuxième temps, nous étudions les statistiques de déformation des observations du RADARSAT Geophysical Processor System (RGPS) qui sont communément utilisées pour évaluer ou calibrer les modèles dynamiques de glace et nous montrons que les statistiques obtenues sont sensibles aux méthodes de prétraitement des données. Plus précisément, nous démontrons que la dépendance spatio-temporelle des lois de scaling des taux de déformation (telle qu'observée à l'aide de bouées) est présente pour les champs de déformation du RGPS seulement si les données sont préalablement discriminées selon leur erreur relative, comme pour l'analyse des données provenant des bouées. Pour cette analyse, nous développons une nouvelle formulation de l'erreur associée aux taux de déformation observés. De plus, nous tentons de détecter l'influence des contraintes thermiques (qui apparaissent lors de changements élevés de température accompagnant le passage de fronts météorologiques) sur les statistiques des champs de déformation du RGPS. Les résultats de cette analyse préliminaire ne sont pas encore concluants et des travaux supplémentaires sont proposés pour poursuivre cette étude. En troisième et dernier temps, nous comparons les statistiques de déformation pour une variété de modèles dynamiques de glace de mer, afin d'étudier la dépendance des champs de déformation simulés à la rhéologie choisie. Nous montrons que toutes les rhéologies de glace de mer étudiées sont capables de reproduire une forte localisation spatio-temporelle des déformations, une hétérogénéité (ou intermittence), ainsi qu'un caractère multi-fractal des lois de scaling des taux de déformation simulés, contrairement à ce qui était précédemment supposé dans la communauté. Cette étude permet aussi d'évaluer l'utilité des métriques de déformation actuelles pour discriminer les modèles dynamiques de glace de mer en fonction de leurs champs de déformation ainsi que d'identifier comment les champs de déformation simulés et observés doivent être comparés afin d'obtenir une interprétation physique optimale</description><description>The deformation of sea ice in the Arctic Ocean occurs along well-defined linear kinematic features, where shear and divergence form regions of open water (leads) and ice ridges. The presence of leads and ridges in the ice cover impacts ocean-atmosphere moisture and heat fluxes, as well as salt/freshwater fluxes in the surface ocean as ice forms or melts in the winter and summer, respectively. A realistic representation of sea-ice deformations by sea-ice models is therefore essential to adequately model the Arctic climate system. The goal of this thesis is to evaluate the ability of different sea-ice rheological models to simulate those deformations and use this knowledge to improve the representation of sea-ice deformation in models. To this end, we investigate both simulated and observed sea-ice deformation statistics in the Arctic Ocean. First, the ability of the viscous-plastic (VP) sea-ice rheology to reproduce the observed strain rate probability density functions (PDFs) is investigated. A better agreement with the observed sea-ice deformation fields is obtained with the VP rheology when the ratio of shear-to-compressive ice strength is increased. The presented sensitivity of simulated PDFs to changes in ice strength parameters also serves as a new basis to calibrate the geophysical strength of sea-ice used in models. Second, the observed sea-ice deformation statistics from the RADARSAT Geophysical Processor System (RGPS) that are commonly used to evaluate simulated deformation fields are shown to be highly sensitive to the pre-processing methods used to derive the deformation estimates. Specifically, we find that the space-time coupling of mean total deformation scaling exponents that is observed in buoy-derived deformation fields is absent from RGPS observations unless deformation estimates are first discriminated based on their signal-to-noise ratio, as done for buoy-derived deformation fields. To evaluate the RGPS signal-to-noise ratio, a new strain rate error formulation is developed. We also attempt to detect deformation events in the RGPS data set that are triggered by thermal stresses (resulting from large temporal changes in temperature associated with the passage of fronts) in addition to the surface wind stress. Results of this preliminary analysis are not yet conclusive, and additional work is proposed to continue this investigation. Finally, simulated sea-ice deformation statistics are evaluated for a variety of sea-ice models in order to investigate how they are affected by a chosen sea-ice rheology. We show that all analyzed sea-ice rheologies are able to reproduce a strong spatio-temporal localization, heterogeneity (or intermittency), and multi-fractality of the scaling, as opposed to what was previously assumed in the community. This study also allows us to evaluate the usefulness of the current metrics to discriminate sea-ice models based on their deformation fields and to identify how simulated and observed deformation fields should be compared for an optimal interpretation</description><creator>Bouchat, Amélie</creator><contributor>Bruno Tremblay (Supervisor)</contributor><date>2020</date><subject>Atmospheric and Oceanic Sciences</subject><title>Improving sea-ice dynamical models using observed and simulated sea-ice deformation statistics in the Arctic Ocean</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/q524jt180.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/f7623h94q</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Atmospheric and Oceanic Sciences</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:3197xr06x</identifier><datestamp>2020-07-14T18:18:54Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>This thesis details the development of a collection of methods to measure the material parameters of mechanically-created neurites. Neurites are long thin processes extending from the body of a nerve cell. To characterize the material response of a neurite to a deformation, information about both its tension and its geometry are required and the body of this thesis is divided accordingly. In the first part, we describe a platform to precisely control and manoeuverneurite-tethered polystyrene beads. We use a mechanical probe composed of a hollow micropipettewith its tip fixed to a functionalized bead to incite the formation of a neurite in a sample of rathippocampal neurons. In the second part, we measure neurite tension by optically tracking the deflection of the beaded tip as the neurite is extended. By calibrating the spring constant of the pipette we can convert this deflection to a force. We use this technique to investigate the dependence of the force-extension relationship on mechanical pull speeds. We develop statistically robust methods to categorize the results and describe the behaviour of neurites under tension. The third part of this work presents a method to measure dimensions of objects below the optical diffraction limit using diffraction analysis of out-of-focus images. We validate this method by applying it to calibration objects with correlative scanning electron microscope (SEM) measurements. We apply this procedure to obtain the diameters of neurites and investigate the dependence of geometry on mechanical pull speed. In the final part of the thesis, we develop a model of neurite growth in response to elastic deformation. We decompose applied stretch into an elastic component and a growth component and adopt an observationally-motivated growth law. We compute best-fit model parameters by fitting force-extension curves. We find a time constant for the growth law of 0.009~s$^{-1}$, similar to the diffusion rate of actin in a cell. These results characterize the kinematics of neurite growth and establish new limits on the growth rate of neurites</description><description>Cette thèse détaille le développement d’une collection de méthodes pour mesurer les paramètres matériels des neurites créés mécaniquement. Les neurites sont de longs processus minces s’étendent du corps d’une cellule nerveuse. Pour carac- tériser la réponse matérielle d’un neurite à une déformation, des informations sur sa tension et sa géométrie sont nécessaires et le corps de cette thèse est divisée en conséquence. Dans la première partie, nous décrivons une configuration pour contrôler et manœuvrer avec précision des sphères de polystyrène attachées aux neurites. Nous utilisons une micropipette avec son extrémité fixée à une sphère fonctionnalisée pour inciter la formation d’une neurite dans un échantillon de neu- rones hippocampiques de rat. Dans la deuxième partie, nous mesurons la tension des neurites en suivant optiquement la déviation de la sphère au point au fur et à mesure de l’extension de la neurite. En calibrant la constante de rigidité de la pipette, nous pouvons convertir cette déviation en une force. Nous utilisons cette technique pour étudier la dépendance de la relation force-extension sur les vitesses d’extensions mécaniques. Nous développons des méthodes statistiques pour catégoriser les résultats et décrire le comportement des neurites sous tension. La troisième partie de ce travail présente une méthode pour mesurer les dimen- sions d’objets en dessous de la limite de diffraction optique en utilisant l’analyse de diffraction d’images floues. Nous validons cette méthode en l’appliquant à des objets de tailles connues avec des mesures de microscopie électronique à balayage (MEB) corrélatifs. Nous appliquons cette procédure pour obtenir les diamètres des neurites et étudions la dépendance de la géométrie à la vitesse d’extension mécanique. Dans la dernière partie de la thèse, nous développons un modèle de croissance des neurites en réponse à la déformation élastique. Nous décomposons l’étirement appliqué en un composant élastique et un composant de croissance et adoptons une loi de croissance motivée par l’observation. Nous calculons les paramètres du modèle le mieux ajusté en ajustant les courbes force-extension. Nous trouvons une constante de temps pour la loi de croissance de 0,009 s−1, similaire au taux de diffusion de l’actine dans une cellule. Ces résultats caractérisent la cinématique de la croissance des neurites et établissent de nouvelles limites sur le taux de croissance des neurites</description><creator>Anthonisen, Madeleine</creator><contributor>Peter H Grutter (Supervisor)</contributor><date>2020</date><subject>Physics</subject><title>Tools for measuring the material parameters of neurites</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/vt150p47b.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/3197xr06x</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:5999n7583</identifier><datestamp>2020-07-14T18:19:23Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>In Computational Aeroacoustic (CAA) applications of Large-Eddy Simulations (LES), accurate control over turbulent kinetic energy (TKE) dissipation is needed to minimize aliasing and obtain accurate broad-band noise estimations. Among different LES approaches, Approximate Deconvolution Models (ADM) allow direct control of the TKE dissipation rate for the resolvable wavenumber scales. This control is obtained by applying an explicitly defined spatial filter on the computed flow fields. InADM schemes, filtered Navier-Stokes equations are used where the filtering operator is explicitly defined. Approximate deconvolution approaches are used to estimate the unfiltered quantities used in constructing the nonlinear flux terms. Given a filter operator, the energy dissipation associated with filtering can be quantified. ADM has been successfully applied on structured grids using discrete high-order filter operators. Its application on unstructured grid has been very limited due to lack of a proper filter.The objective of the present work was to extend the application of Approximate Deconvolution Models (ADM) for LES on unstructured grids by using explicit differential filters. Germano’s elliptic differential filter was successfully extended to include two free parameters. One ensured full attenuation at grid cut-off wavenumber, preventing aliasing due to LES and stabilizing the numerical scheme. The other controlled the filter cut-off wavenumber. The discretized formulation of this differential filter was developed for two- and three-dimensional elements. Dissipation and dispersion properties of the discrete differential filter were investigated in detail.A second-order classical finite element method (FEM) was used for spatial discretization of the compressible Navier-Stokes equations. Time integration was performed through the use of the standard fourth-order explicit Runge-Kutta scheme. Interpolation on high resolution grids was used to obtain the fast Fourier transform (FFT) of the flow fields on perturbed and unstructured grids.Decaying isotropic homogeneous turbulence at Reynolds number 3, 400 was modeled on both structured and unstructured grids. Results were compared to a reference direct numerical simulation (DNS) and other LES results reported in the literature. The effect of mesh anisotropy on the newly proposed differential filter performance was studied. It was observed that stable and sufficiently accurate LES results could be obtained on unstructured grids, even in the presence of highly skewed elements. Careful examination of the dissipation rate in the resolved wavenumbers suggested that grid anisotropy induces different cut-off wavenumbers in different directions resulting in higher dissipation rates than those obtained on an isotropic mesh.Taylor-Green vortex (TGV) was also studied as an excellent canonical problem for laminar to turbulence transition of a flow. LES simulations using the ADM framework were conducted in which the filter extended to three-dimensional elements was used. Investigative studies on the ADM order, the ADM under-relaxation coefficient, the degree of anisotropy in the grid, and grid resolution were performed to benchmark the filter performance in conjunction with ADM. Finally, an LES of TGV on a fully unstructured grid was performed showing the range of application of the proposed filter</description><description>Dans les applications en a´eroacoustique, les simulations num´erique par la method des grands courants de Foucault requient un contrˆole pr´ecis de la dissipation de l’´energie cin´etique turbulente afinde r´eduire l’aliasing et obtenir des estimations pr´ecises du bruit `a large bande. Parmi les diff´erentes approches LES, les mod`eles de d´econvolution approximative (MDA) permettent de contrˆoler directement le taux de dissipation de l’´energie cin´etique pour les ´echelles de turbulence `a des nombres d’ondes r´esolubles. Ce contrˆole est obtenu en appliquant un filtre spatial explicitement d´efini sur les champs d’´ecoulement calcul´es. Dans les sch´emas MDA, les ´equations de Navier-Stokes filtr´ees sont utilis´ees lorsque l’op´erateur de filtrage est explicitement d´efini. Des approches approximatives de d´econvolution sont utilis´ees pour estimer les quantit´es non filtr´ees utilis´ees dans la construction des termes de flux non lin´eaires. La dissipation d’´energie associ´ee au filtrage peut ´etre quantifi´ee. La MDA a ´et´e appliqu´e avec succ`es sur des grilles structur´ees en utilisant des op´erateurs de filtres discret d’ordre ´elev´e. Elle n’a pas, a ce jour, ´et’e appliqu´e des grilles non structur´ees a en raison de l’absence d’un filtre appropri´. L’objectif de cette ´etude ´etait d’´etendre l’application des MDA sur des grilles non structur´ees en utilisant des filtres diff´erentiels explicites. Pour le faire, le filtre diff´erentiel elliptique de Germano a d’abod ´et´e g´en´eralise. Un param´etre assure une att´enuation compl`ete au numbro d’onde de coupure de la grille, emp´echant l’aliasing et stabilisant le sch´ema num´erique. Un deuxi´eme param´etre permit de contrˆoler le nombre d’ondes de coupure du filtre. La formulation discr´ete de ce filtre diff´erentiel a ´et´e d´evelopp´ee pour des ´el´ements bidimensionnels et tridimensionnels.Pour la discr´etisation spatiale des ´equations de Navier-Stokes compressibles, on a utilis´e une m´ethode classique du deuxi`eme ordre par ´el´ements finis (FEM). L’int´egration temporelle a ´et´e r´ealis´ee `a l’aide du sch´ema Runge-Kutta explicite standard du quatri`eme ordre. L’interpolation sur des grilles `a haute r´esolution a ´et´e utilis´ee pour obtenir la transform´ee de Fourier des champs d’´ecoulement sur des grilles perturb´ees et non structur´ees. La turbulence isotrope homog`ene au nombre de Reynolds 3, 400 a ´et´e mod´elis´ee sur des grilles structur´ees et non structur´ees. Les r´esultats ont ´et´e compar´es `a une simulation num´erique directe (DNS) de r´ef´erence et `a d’autres r´esultats de l’´etude LES pr´esent´es dans la litt´erature. L’effet de l’anisotropie du maillage sur la performance du filtre diff´erentiel propos´e a ´et´e ´etudi´e. Il a ´et´e observ´e que des r´esultats de LES stables et suffisamment pr´ecis pouvaient ´etre obtenus sur des grilles non structur´ees, mˆeme en pr´esence d’´el´ements tr`es asym´etriques. Un ´examen attentif du taux de dissipation des nombres d’ondes r´esolus a sugg´er´e que l’anisotropie de la grille induit des nombres d’ondes de coupure diff´erents dans diff´erentes directions, entraˆınant des taux de dissipation plus ´elev´es que ceux obtenus sur un maillage isotrope.Le vortex de Taylor-Green (VTG) a ´egalement ´et´e ´etudi´e comme probl`eme canonique pour la transition d’´ecoulement laminaire en ´ecoulement turbulent. Des simulations LES utilisant le cadre MDA ont ´et´e r´ealis´ees dans lesquelles le filtre´etendu aux ´el´ements tridimensionnels a ´et´e utilis´e. Des ´etudes d’investigation sur l’ordre ADM, le coefficient de sous-relaxation MDA, le degr´e d’anisotropie dans la grille et la r´esolution de la grille ont ´et´e r´ealis´ees pour comparer la performance du filtre avec MDA. Enfin, un LES de VTG sur une grille enti`erement non structur´ee a´et´e r´ealis´e montrant le domaine d’application du filtre propos´e</description><creator>Najafiyazdi, Mostafa</creator><contributor>Sivakumaran Nadarajah (Supervisor2)</contributor><contributor>Luc Mongeau (Supervisor1)</contributor><date>2020</date><subject>Mechanical Engineering</subject><title>Large-Eddy simulations on unstructured grids using explicit differential filters in approximate deconvolution models</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/pc289p48m.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/5999n7583</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:t435gj37g</identifier><datestamp>2020-07-14T18:19:29Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Our legal response to transboundary pollution depends not only on the adoption of preventive measures and regulatory oversight but also on the existence of civil liability mechanisms. Victims fundamentally seek to hold polluters liable for breaching their duties or deviating from basic standards of diligence, to obtain redress for the damage that ensued and to prevent it from continuing. The process becomes difficult, however, when pollution crosses borders and several domestic regimes are involved. This is where private international law comes into play.This thesis investigates the regulatory function of private international law with respect to transboundary pollution. It uses the International Law Commission’s Principles on the Allocation of Loss in the Case of Transboundary Harm as a benchmark and assesses Canadian private international law accordingly. It suggests that states have a duty to ensure the availability of prompt and adequate compensation for all victims of transboundary pollution (local or foreign). States must implement domestic measures to facilitate claims against transboundary polluters. This includes equal access to justice and equal remedies for all victims. Private international law plays a crucial role in this context: courts must have jurisdiction to hear cross-border claims and apply a law that is favourable to compensation under choice of law rules.This thesis builds from international environmental law to identify preferable rules of jurisdiction and choice of law for transboundary pollution in the Canadian context. It also addresses the enforcement of foreign judgments against local polluters. The conclusions of this thesis have implications for all cross-border environmental litigation, including climate change litigation against greenhouse gas emitters currently unfolding in domestic courts around the world</description><description>Le traitement juridique de la pollution transfrontalière dépend non seulement de l’adoption de mesures préventives et de l’application de la règlementation, mais aussi de l’existence de mécanismes de responsabilité civile. Les victimes cherchent fondamentalement à retenir la responsabilité des pollueurs pour le manquement à leurs obligations ou leur écart par rapport aux normes élémentaires de diligence, obtenir une indemnisation pour le préjudice subi et faire cesser celui-ci pour l’avenir. Ce processus devient toutefois difficile lorsque la pollution traverse les frontières et que plusieurs régimes nationaux sont impliqués. C’est ici que le droit international privé entre en jeu. Cette thèse examine la fonction régulatrice du droit international privé à l’égard de la pollution transfrontalière. Elle prend comme point de repère les Principes sur la répartition des pertes en cas de dommage transfrontière découlant d’activités dangereuses de la Commission du droit international et évalue le droit international privé canadien en conséquence. Elle suggère que les États ont l’obligation d’assurer la disponibilité d’une indemnisation prompte et adéquate à toutes les victimes de pollution transfrontalière (locale ou étrangère). Les États doivent notamment mettre en œuvre des mesures pour faciliter les plaintes contre les pollueurs transfrontaliers. Ces mesures incluent un accès égal à la justice et aux recours de droit interne pour toutes les victimes. Le droit international privé joue un rôle crucial dans ce contexte : les tribunaux doivent posséder la compétence nécessaire pour entendre les litiges internationaux et appliquer une loi favorable à l’indemnisation en vertu des règles de conflit de lois.Cette thèse formule, à partir du droit international de l’environnement, des règles de compétence et de droit applicable adaptées au phénomène de la pollution transfrontalière dans le contexte canadien. Elle examine également la question de l’exécution des jugements rendus à l’étranger contre des pollueurs locaux. Les conclusions de cette thèse fournissent des leçons importantes pour tous les litiges environnementaux transfrontaliers, incluant les litiges contre des émetteurs de gaz à effet de serre qui se déroulent actuellement devant les tribunaux nationaux à travers le monde</description><creator>Laganière, Guillaume</creator><contributor>Genevieve Saumier (Supervisor)</contributor><date>2020</date><subject>Law</subject><title>Liability for transboundary pollution in private international law: a duty to ensure prompt and adequate compensation</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/jw827h30g.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/t435gj37g</identifier><degree><name>Doctor of Civil Law</name><grantor>McGill University</grantor><discipline>Faculty of Law</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:k0698d17g</identifier><datestamp>2020-07-14T18:20:27Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Reactive oxygen species (ROS) are critical mediators of cell signalling and are implicated acrossdifferent processes during tumorigenesis and progression. Depending on ROS levels, this can bebeneficial or harmful to the tumor. p66ShcA is an adaptor protein that is involved in mediating anoxidative stress response by promoting the production of mitochondrial reactive oxygen speciesin response to stress stimuli. In cancer, p66ShcA has been shown to have both pro and antitumorigenic functions and is expressed variably. Our work has focused on characterizing whether p66ShcA is pro or anti-tumorigenic in breast cancer during tumor outgrowth and metastasis, if this depends on redox status, and whether this contributes to cellular plasticity. A key process that increases cellular plasticity and the malignant potential of breast tumors is the epithelial-mesenchymal-transition (EMT). To model this, we looked at the role of p66ShcA in ErbB2 positive luminal breast cancer versus aggressive triple-negative breast cancer (TNBC). We outline a novel role for p66ShcA in promoting cellular plasticity by inducing an EMT in HER2 positiveluminal breast tumors through the Met RTK. p66ShcA-induced plasticity contributes to intratumoral heterogeneity, particularly in the luminal A subtype, where tumors are normally well-differentiated and express epithelial markers. We also identify p66ShcA as a biomarker of primary breast tumors possessing mesenchymal features, across molecular subtypes. Further studiesrevealed these effects appear to be independent of mitochondrial-p66ShcA. Breast cancer is the most commonly diagnosed cancer in women and metastasis to distant organs is responsible for 90% of cancer-related deaths. The metastatic cascade involves a series of steps that contribute to successful colonization. We discovered mitochondrial versus cytoplasmic pools of p66ShcA regulate different stages of the metastatic cascade in aggressive TNBC by employing expression vectors stably overexpressing wild-type p66ShcA or a nonphosphorylatable mutant(p66ShcAS36A) that cannot translocate into the mitochondria. Mitochondrial-p66ShcA is required for entry/survival in the circulation which leads to high levels of circulating tumor cells compared to p66ShcAS36A that is limited to the cytoplasm. In contrast, cytoplasmic p66ShcA was necessary for elevated migration from the primary site and increased focal adhesion turnover to facilitate colonization from the circulation. Therefore, in agreement with the literature, we show that ROScan be pro- or anti-tumorigenic both from the primary or metastatic site and depending on the molecular subtype. This work highlights p66ShcA’s pleiotropic roles in breast cancer as a promiscuous molecule in tumorigenesis and metastasis. Finally, p66ShcA has been shown to be epigenetically regulated through promoter methylation and hyperacetylation. We discovered thathigh p66ShcA expression correlates with the presence of active histone marks, including:H3K4Ac, H3K9Ac and H3K27Ac. Furthermore, the chromatin insulator, CTCF, binds to thep66ShcA promoter in breast cancer cells that express high levels of p66ShcA expression. Thesedata suggest p66ShcA may be epigenetically regulated in breast cancer</description><description>Les espèces réactives de l'oxygène (ROS) sont des médiateurs critiques de la signalisation cellulaire et sont impliquées dans différents processus au cours de la tumorigenèse et de la progression. Selon les niveaux de ROS, cela peut être bénéfique ou néfaste pour la tumeur. p66ShcA est une protéine adaptatrice impliquée dans la médiation d'une réponse au stress oxydatif. Dans le cancer, il a été démontré que p66ShcA avait à la fois des fonctions pro et anti-tumorigènes et s'exprimait de manière variable. Notre travail a principalement consisté à déterminer si p66ShcA est un pro ou anti-tumorigène dans le cancer du sein au cours de la croissance tumorale et de la métastase, si cela dépend du statut redox et si cela contribue à la plasticité cellulaire. Un processus clé qui augmente la plasticité cellulaire et le potentiel malin des tumeurs du sein est la transition épithéliale-mésenchymateuse (EMT). Pour modéliser cela, nous avons examiné le rôle de p66ShcA dans le cancer du sein luminal positif ErbB2 par rapport au cancer du sein agressif triple négatif (TNBC). Nous décrivons un nouveau rôle pour p66ShcA dans la promotion de la plasticité cellulaire en induisant un EMT dans les tumeurs mammaires lumineuses HER2 positives par le biais de la RTK Met. La plasticité induite par p66ShcA contribue à l'hétérogénéité intratumorale, en particulier dans le sous-type luminal A, où les tumeurs sont normalement bien différenciées et expriment des marqueurs épithéliaux. Nous identifions également p66ShcA en tant que biomarqueur de tumeurs primitives du sein possédant des caractéristiques mésenchymateuses, sur différents sous-types moléculaires. D'autres études ont révélé que ces effets semblent être indépendants de p66ShcA mitochondrial. Le cancer du sein est le cancer le plus souvent diagnostiqué chez les femmes et les métastases à des organes distants sont responsables de 90% des décès liés au cancer. La cascade métastatique implique une série d'étapes qui contribuent au succès de la colonisation. Par conséquent, des études complémentaires sur des modèles précliniques pertinents sont nécessaires pour surmonter les obstacles actuels en matière de traitement. Nous avons découvert des pools mitochondriaux contre cytoplasmiques de p66ShcA régulant différents stades de la cascade métastatique dans une TNBC agressif. p66ShcA mitochondriale est nécessaire à l’entrée / à la survie dans la circulation, ce qui entraîne des taux élevés de cellules tumorales en circulation par rapport aux mutants VC et p66ShcAS36A limités au cytoplasme. Au contraire, p66ShcA cytoplasmique était nécessaire pour une migration élevée à partir du site primaire et une augmentation du renouvellement de l’adhésion focale afin de faciliter la colonisation par la circulation. Par conséquent, en accord avec la littérature, les ROS peuvent être pro- ou anti-tumorigènes à la fois du site primaire ou métastatique et en fonction du sous-type moléculaire. Ce travail met en évidence les rôles pléiotropes de p66ShcA dans le cancer du sein en tant que molécule promiscuité dans la tumorigenèse et les métastases. Enfin, il a été démontré que p66ShcA était régulé de manière épigénétique par le biais d'une méthylation et d'une hyperacétylation du promoteur. Nous avons découvert qu'une expression élevée de p66ShcA est corrélée à la présence de marques d'histone actives, notamment : H3K4Ac, H3K9Ac et H3K27Ac. De plus, l'isolant de la chromatine, CTCF, se lie au promoteur p66ShcA dans les cellules du cancer du sein qui expriment des niveaux élevés d'expression de p66ShcA. Ces données suggèrent que p66ShcA pourrait être régulé épigénétiquement dans le cancer du sein</description><creator>Hudson, Jesse</creator><contributor>Giuseppina Ursini-Siegel (Supervisor)</contributor><date>2020</date><subject>Medicine</subject><title>The epigenetic regulation of p66ShcA and its role in promoting cellular plasticity and lung metastasis in breast cancer</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/9019s674n.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/k0698d17g</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Medicine</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:7h149v07k</identifier><datestamp>2020-07-14T18:20:56Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Systemic sclerosis (SSc) is a chronic and progressive autoimmune disease characterized by vasculopathy and widespread fibrosis. In addition to disfiguring skin involvement, SSc patients can suffer from extensive internal organ damage including interstitial lung disease (ILD) which is the leading cause of morbidity and mortality in this patient group. Pulmonary function tests (PFTs) are routinely used to monitor SSc-associated ILD (SSc-ILD) in clinical practice, epidemiologic studies and clinical trials for purposes of treatment initiation and follow-up. Yet, few validation studies have assessed which, if any, PFT measures are good surrogate markers for SSc-ILD onset.The first contribution of this thesis is a systematic review of the literature that determined which PFT measures have been most commonly used as outcomes for SSc-ILD in experimental and observational studies.  The systematic review also summarized the results of studies that validated PFT measures against either high-resolution computed tomography or lung biopsy results in SSc patients. Results showed that despite the current preference for the use of forced vital capacity (FVC) % predicted, available evidence does not overwhelmingly support its preferred status as a PFT surrogate marker for SSc-ILD.The second contribution of this thesis is a methodologic study which used both simulated data and real data from a large Canadian observational cohort of SSc patients to evaluate the potential of hidden Markov models (HMMs) to validate and use PFT measures for SSc-ILD ascertainment. The HMM was of interest because it can use the full PFT measurement history to model the probability of SSc-ILD occurrence while simultaneously correcting for PFT variability. Its statistical performance when using FVC was compared to that of two commonly used definitions for possible SSc-ILD onset: &lt;80% predicted FVC and ≥10% decline in FVC. The HMM had the highest specificity and lowest error rate compared to the cut-off and change in FVC algorithms.The third contribution of this thesis is the first attempt to use HMMs to validate FVC, diffusing capacity for carbon monoxide (DLCO) and total lung capacity (TLC) as surrogate markers for SSc-ILD onset in a large SSc patient cohort. Separate HMMs were evaluated using FVC, DLCO and TLC as well as for different bivariate and multivariate combinations of these PFT measures. The HMM using all three measures (FVC/DLCO/TLC) had the highest sensitivity likely making it the best PFT screening tool for SSc-ILD onset. However, all models had generally poor sensitivity for SSc-ILD. On the other hand, the joint TLC % predicted and FEV1/FVC (ratio of forced expiratory volume in one second to FVC) model had a high specificity and low error rate, followed closely by DLCO % predicted and TLC absolute. These results suggest that TLC and DLCO may be better PFT surrogate markers for SSc-ILD onset than FVC and should also be considered as main PFT outcome measures in epidemiologic studies.In summary, this thesis demonstrated that TLC and DLCO may be better PFT surrogate markers for SSc-ILD onset than FVC, thereby helping to improve the quality of evidence of future epidemiologic studies. Furthermore, these findings will inform clinical decision-making by demonstrating that serial measurements of FVC, DLCO and TLC should be used jointly to screen for SSc patients who should undergo further investigation for SSc-ILD. Nevertheless, the low sensitivities of PFT measures suggest that other avenues should be explored in the search for a suitable marker for SSc-ILD onset. Finally, from the methodologic standpoint, this thesis demonstrated that HMMs can be better suited to detect disease onset than fixed cut-offs or pre-specified changes in surrogate marker values</description><description>La sclérodermie systémique (ScS) est une maladie auto-immune chronique à pathologie progressive caractérisée par une vasculopathie et une fibrose diffuse. Les patients sclérodermiques souffrent de dommages importants aux organes internes. En particulier, la pneumopathie interstitielle (PI) est la cause principale de morbidité et de mortalité. Les explorations fonctionelles respiratoires (EFR) sont couramment utilisées pour surveiller l’apparition de la PI liée à la ScS (PI-ScS) à des fins d’initiation du traitement et de suivi. À date, peu d’études ont validé les mesures d’EFR en tant que marqueurs de substitution pour le déclenchement de la PI-ScS.La première contribution de cette thèse est une revue systématique de la littérature qui a déterminé quelles mesures d’EFR ont été utilisées les plus fréquemment comme marqueurs de substitution dans les études de PI-ScS. Cette revue systématique a également résumé les résultats d’études qui ont validé les mesures d’EFR vis-à-vis la tomodensitométrie à haute résolution ou la biopsie pulmonaire chez les patients atteints de ScS. Les résultats ont démontré que, malgré la préférence actuelle pour l’utilisation du % prédit de la capacité vitale forcée (CVF), les preuves disponibles n’appuient pas son statut comme marqueur de substitution préféré pour la PI-ScS.La deuxième contribution de cette thèse est une étude méthodologique qui a exploité à la fois des données simulées et réelles provenant d’une grande cohorte observationnelle de patients canadiens atteints de ScS. L’étude a évalué l’application d’un modèle de Markov caché (MMC) comme moyen d’établir l’apparition de la PI-ScS en utilisant des mesures d’EFR. En particulier, le MMC peut utiliser l’historique complet des mesures d’EFR pour prédire la probabilité d’occurrence de la PI-ScS tout en corrigeant pour les erreurs de mesure liées aux EFR. La performance d’un MMC utilisant des mesures de CVF a été comparé à deux définitions communes de PI-ScS : un CVF &lt;80% prédit et un déclin en CVF ≥10%. Le MMC a démontré la spécificité la plus élevée et le taux d’erreur le plus faible.La troisième contribution de cette thèse est l’utilisation d’un MMC pour valider la CVF, la capacité de diffusion du monoxyde de carbone (DLCO) et la capacité pulmonaire totale (CPT) comme marqueurs de substitution pour le déclenchement de la PI-ScS. Des MMC distincts utilisant soit la CVF, la DLCO et/ou la CPT ont été évalués. Le MMC utilisant conjointement la CVF, la DLCO et la CPT a démontré la sensibilité la plus élevée, ce qui en fait le meilleur outil de dépistage pour la PI-ScS. Par contre, tous les modèles avaient de manière générale une basse sensibilité pour la PI-ScS. En revanche, le MMC utilisant le % prédit de la CPT conjointement avec le VEMS/CVF (volume expiratoire maximal seconde divisé par la CVF) avait une spécificité élevée et un faible taux d’erreur, suivi de près par le % prédit de la DLCO et le CPT en valeur absolue. Ces résultats suggèrent que la CPT et la DLCO pourraient être de meilleurs marqueurs de substitution que la CVF pour le déclenchement de la PI-ScS. Conséquemment, ces mesures devraient être considérées comme paramètres principaux dans les études épidémiologiques sur ce sujet.En résumé, cette thèse a démontré que la CPT et la DLCO sont potentiellement de meilleurs marqueurs de substitution que la CVF pour le déclenchement de la PI-ScS et que l’observation conjointe des mesures en série de la CVF, de la DLCO et de la CPT identifie le plus grand nombre de patients pouvant bénéficier d’une enquête plus approfondie. Étant donné la faible sensibilité des mesures d’EFR, d’autres marqueurs de substitution devraient être explorées pour le déclenchement de la PI-ScS. Finalement, d’un point de vue méthodologique, cette thèse a démontré que les MMC peuvent être mieux adaptés pour détecter l’avènement d’une maladie que l’utilisation de seuils fixes ou de changements pré-spécifiés des valeurs de marqueurs de substitution</description><creator>Caron, Melissa</creator><contributor>Russell Steele (Supervisor2)</contributor><contributor>Kevin Schwartzman (Supervisor1)</contributor><date>2020</date><subject>Epidemiology and Biostatistics</subject><title>Pulmonary function tests as surrogate markers of interstitial lung disease onset in systemic sclerosis</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/9p290g06p.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/7h149v07k</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Epidemiology and Biostatistics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:zp38wh990</identifier><datestamp>2020-07-14T18:21:13Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>En février 1942, le gouvernement du Canada a exilé tous les Canadiens d’origine japonaise de la côte pacifique, incluant 76 étudiants de l’Université de la Colombie-Britannique. Alors que d’autres universités canadiennes ont admis certains Canadiens d’origine japonaise, suite aux recommandations de la British Columbia Securities Commission, le Sénat de l’Université McGill a pour sa part bannit de façon exceptionnelle tous les étudiants « d’origine raciale japonaise » en octobre 1943. S’appuyant sur des sources provenant des archives de l’Université McGill, du ministère des Affaires étrangères à Ottawa, des médias et d’entrevues avec des Canadiens d’origine japonaise, ma thèse montre la manière dont le Sénat a manipulé le règlement interne de l’Université et les conflits internes qui en ont résulté, la résistance des étudiants et du public face à cette décision, la menace d’interférence du conseil d’administration et, ultimement, la révocation de cette politique en 1945. Elle montre également que l’expulsion d’un étudiant de cette institution contraste avec la présence avérée d’au moins sept Canadiens d’origine japonaise sur le campus pendant la période de mise vigueur de cette politique. Ce déni du droit d’accès à l’éducation supérieure s’inscrit plus largement dans le processus de dépossession des Canadiens d’origine japonaise qui a cours au Canada pendant la décennie 1940</description><description>In February 1942, the Canadian government exiled all Japanese Canadians from the Pacific coast, including 76 students attending the University of British Columbia. While other Canadian universities enrolled some Japanese Canadian students recommended by the British Columbia Security Commission, McGill University’s Senate exceptionally banned all students “of Japanese racial origin” in October 1943. Using documents from McGill’s archives, the Department of External Affairs, Japanese Canadian oral histories, and press coverage, this thesis outlines exclusionist Senate members’ manipulation of rules of procedure to pass the policy, the resulting conflict at Senate, resistance from students and the public, threatened interference from the Board of Governors, and the policy’s ultimate repeal in 1945. The expulsion of one student is contrasted with the continued presence of at least seven Japanese Canadian students on campus while the policy was in effect. Denial of educational opportunity is characterized as part of the broader dispossession enacted on Japanese Canadians in the 1940s</description><creator>Elsworthy, Tess</creator><contributor>John Zucchi (Supervisor1)</contributor><contributor>Laura Madokoro (Supervisor2)</contributor><date>2020</date><subject>History and Classical Studies</subject><title>McGill University’s racial exclusion of Japanese Canadians, 1943-1945</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/9p290g07z.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/zp38wh990</identifier><degree><name>Master of Arts</name><grantor>McGill University</grantor><discipline>Department of History and Classical Studies</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:sx61dr575</identifier><datestamp>2020-07-14T18:21:31Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>In this thesis, we present the large-area graphene based ion sensitive field effect transistor (ISFET) as an attractive candidate for overcoming the challenges in the ion sensing field. We present a signal to noise ratio model for ISFETs outlining the necessity for a Nernstian limited signal, as well as minimizing noise through maximizing the active sensing area, device carrier mobility, and capacitive coupling, in order to improve sensor resolution. While these parameters are key for any ISFET, graphene is optimal because it enables large-area devices with high charge-carrier mobility using a chemical vapor deposition growth technique that is economical in comparison with traditional semiconductor growth methods.We demonstrate $\sim$ cm$^2$ graphene ISFETs for multiple ions with record sensing resolution for potentiometric sensors by saturating the physical limits of sensitivity, while exhibiting field-effect mobilities as high as 7000 cm$^2$/Vs and device capacitances at the quantum limit $\sim$ 1 $\mu$F/cm$^2$ imposed by density of states.For measuring pH, tantalum pentoxide Ta$_2$O$_5$ or aluminum oxide Al$_2$O$_3$ sensing layers were grown via atomic layer deposition (ALD). The large-area graphene is encapsulated with ultra-thin layers of parylene, a hydrophobic polymer, to protect the graphene from degradation and act as a seeding layer during ALD to ensure ultra-thin stochiometric oxide layers. We also are able to measure K$^+$, Na$^+$, NH$_4^+$, NO$_3^-$, SO$_4^{2-}$, HPO$_4^{2-}$ and Cl$^-$ using ionophore membranes as sensing layers. Due to parylene encapsulation, the devices exhibit minimal hysteresis and remarkable stability over a 5 month period with limited drift.To overcome the challenge of poor selectivity, an ISFET array is developed, where each sensor is cross calibrated for interfering ions, and a series of Nikolskii-Eisenman equations is generated to solve for the ion concentrations, even in the presence of interfering ions. With this method, the ISFETs are accurate in multi analyte solutions to within $\pm 0.01$ and $\pm 0.05 \log$ concentration for cations and anions respectively. The performance of our ISFETs is more than sufficient for many real-time monitoring applications, where they are tested in a variety of beverages, blood, and in an aquarium</description><description>Dans cette thèse, nous présentons le transistor à effet de champ sensible aux ions (ISFET) à base de graphène à grande surface comme un candidat attrayant pour surmonter les défis dans le domaine de la détection des ions. Nous présentons un modèle de rapport signal/bruit pour les ISFET soulignant la nécessité d'un signal limité par l'équation de Nernst, ainsi que la minimisation du bruit en maximisant la zone de détection active, la mobilité des porteurs de charge et le couplage capacitif, afin d'améliorer la résolution du capteur. Bien que ces paramètres soient essentiels pour tout ISFET, le graphène est optimal car il permet des dispositifs de grande surface avec une mobilité des porteurs de charge élevée en utilisant une technique de croissance par dépôt chimique en phase vapeur qui est économique par rapport aux méthodes de croissance traditionnelles des semi-conducteurs.Nous démontrons ISFETs de graphène $ \sim $ cm $^2$ pour plusieurs ions avec une résolution de détection record pour les capteurs potentiométriques en saturant les limites physiques de la sensibilité, tout en exposant des mobilités à effet de champ pouvant atteindre 7000 cm$^2$/Vs et l'appareil capacités à la limite quantique $\sim $ 1 $\mu$ F/cm$^2$ imposée par la densité des états.Pour mesurer le pH, des couches de détection d'oxyde de tantale Ta$_2$O$_5$ ou d'oxyde d'aluminium Al$_2$O$_3$ ont été déposés par dépôt de couche atomique (ALD). Le graphène de grande surface est encapsulé avec des couches ultra-minces de parylène, un polymère hydrophobe, pour protéger le graphène de la dégradation et agir comme une couche d'ensemencement pendant l'ALD pour assurer des oxydes stochiométriques ultra-minces. Nous pouvons également mesurer K$^+$, Na$^+$, NH$_4^+$, NO$_3^-$, SO$_4^{2-}$, HPO$_4^{2-}$ et Cl$^-$ en utilisant des membranes ionophores comme couches de détection. En raison de l'encapsulation du parylène, les dispositifs présentent une hystérésis minimale et une stabilité remarquable sur une période de 5 mois avec une dérive limitée.Pour surmonter le défi d'une mauvaise sélectivité, un réseau ISFET est développé, où chaque capteur est étalonné de manière croisée pour les ions interférents, et une série d'équations Nikolskii-Eisenman est générée pour résoudre les concentrations ionique, même en présence d'ions interférents. Avec cette méthode, les ISFET sont exact dans les solutions multi-analytes à une concentration de $\pm 0,01$ et $\pm 0,05 \log $ pour les cations et les anions respectivement. Les performances de nos ISFET sont plus que suffisantes pour de nombreuses applications de surveillance en temps réel, où elles sont testées dans une variété de boissons, dans du sang et dans un aquarium</description><creator>Fakih, Ibrahim</creator><contributor>Thomas Szkopek (Supervisor)</contributor><date>2020</date><subject>Electrical and Computer Engineering</subject><title>High resolution Ion sensing with large-area graphene field effect transistors at the thermodynamic and quantum limits</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/ff365955s.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/sx61dr575</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:4t64gs68s</identifier><datestamp>2020-07-14T18:23:07Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Neurodegenerative diseases are a vast grouping of conditions which invariably result in the loss of neurons. This loss is irreversible, and is one of many reasons why these diseases are notoriously difficult to treat.  However, a glimmer of hope rests with the neurotrophins and their receptors, a family of proteins highly involved in survival, differentiation, and migratory processes.  Employing trophic factors therapeutically has been a grand failure, despite a deeper understanding of their biology as well as methods for their delivery.  By and large, bulky proteins possess poor pharmacological properties, many of which are inescapable. Slow and unpredictable diffusion, instability, and the fact that they are promiscuous in the targets they interact with are but a few examples.  Small molecules offer a tantalizing solution to these issues.  The seemingly infinite molecular landscape has barely been navigated, and as we explore further, we begin to uncover novel ways to intervene therapeutically, beyond the capabilities of the endogenous ligands. For simplicity and continuity’s sake, the work will be divided into 2 overarching categories pertaining to small molecule therapeutic strategies for degenerative diseases: Anti-Neurotoxic and Pro-Survival</description><description>Les maladies neurodégénératives sont un vaste ensemble de conditions qui entraînent invariablement la perte de neurones. Cette perte irréversible est l’une des nombreuses raisons pour lesquelles il est si difficile de traiter ces maladies. On trouve cependant une lueur d’espoir auprès des neurotrophines et de leurs récepteurs, une famille de protéines fortement impliquées dans les processus de survie, de différenciation et de migration. L’usage thérapeutique des facteurs trophiques s’est avéré un grand échec, malgré une meilleure compréhension de leur biologie et des méthodes de leur délivrance. En gros, les protéines volumineuses possèdent de pauvres propriétés pharmacologiques, dont beaucoup sont impossibles à ignorer : leur diffusion lente et imprévisible, leur instabilité et leur réaction de promiscuité avec les cibles n’en sont que quelques exemples. Les petites molécules proposent une solution alléchante à ces problèmes. Nous avons à peine parcouru le paysage moléculaire qui semble infini, et, lorsque nous poussons l’exploration plus loin, nous pouvons découvrir de nouveaux modes d’interventions thérapeutiques qui dépassent le potentiel des ligands endogènes. Pour des raisons de simplicité et de continuité, le travail suivant sera divisé en deux grandes catégories de stratégies thérapeutiques des petites molécules pour les maladies dégénératives : anti-neurotoxique et neuroprotectrice</description><creator>Jmaeff, Sean</creator><contributor>Horacio Saragovi (Supervisor)</contributor><date>2020</date><subject>Pharmacology and Therapeutics</subject><title>Anti-neurotoxic and pro-survival in degenerative disease: The relevance, utility, and challenges of small molecule development</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/pz50h182d.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/4t64gs68s</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Pharmacology and Therapeutics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:0v838490b</identifier><datestamp>2020-07-14T18:23:29Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Le spin interpersonnel, aussi appelé le spin, réfère à la variabilité intra-individuelle du comportement social. De façon générale, cette variabilité est inadaptée. En effet, le spin a été lié à des traits de personnalité problématiques, des indices de faible bien-être, et à des difficultés sur le plan du fonctionnement social. À l'aide de données recueillies dans le cadre d'un projet longitudinal multi-méthodes portant sur les conséquences interpersonnelles de la variabilité intra-individuelle, j'ai examiné les processus susceptibles de sous-tendre le spin et ses effets sur le bien-être et le fonctionnement social.Un grand échantillon d'étudiants universitaires en administration a été recruté pour participer à une procédure de compte rendu d'événement (‘ECR’) pendant 20 jours. À l’aide d’une application mobile, les participants ont rapporté les aspects situationnels, comportementaux, affectifs et perceptuels de leurs interactions sociales. Avant et après cette procédure, ils ont également complété les mesures de personnalité et les mesures de bien-être. Dans le 1er manuscrit, j’utilise les données recueillies avant et pendant la procédure ECR pour appuyer l’utilisation d’une procédure basée sur une application mobile comme alternative à une procédure au stylo et papier pour mesurer le spin en démontrant la stabilité des scores de spin sur le période d'étude, et en reproduisant et en développant les corrélats de personnalité et de situation associés au spin. Dans le manuscrit 2, j’utilise les données recueillies au cours de la procédure ECR afin d’explorer les processus perceptuels qui peut sous-tendre le spin et pouvant contribuer au dysfonctionnement social associé au spin. Dans le manuscrit 3, j'utilise les données cueillies pendant et immédiatement après la procédure ECR pour proposer un mécanisme possible par lequel le spin peut avoir des effets néfastes sur le bien-être. Ces études intègrent les résultats concernant les antécédents, les difficultés sociales et l'inadaptation associées au spin, améliorant ainsi notre compréhension des processus susceptibles de sous-tendre le spin et de ses effets. Pris ensembles, ces résultats suggèrent que le spin peut être conceptualisé comme une dysrégulation comportementale qui découle en partie de fluctuations perceptuelles et affectives d’une interaction à l’autre et de la réactivité à la perception interpersonnelle d’une interaction. Ces fluctuations et cette réactivité peuvent être liées au neuroticisme et constituer une sorte de sensibilité au rejet. Je spécule que cette manière dont les individus avec un spin plus élevé se comportent et réagissent peut amener les autres à se sentir frustrés, à les repousser et à leur répondre de façon qui exacerbe leurs insécurités, maintient leur difficulté de régulation, augmente leur sentiment d’être déconnecté socialement et érode leur bien-être</description><description>Interpersonal spin, spin, refers to within-person variability in social behavior. This variability has typically been associated with potentially problematic personality characteristics and found to be maladaptive for both personal and interpersonal wellbeing. Using data collected as part of a multimethod longitudinal project examining the interpersonal consequences of intraindividual variability, I examined processes which may underlie spin and its effects on well-being and social functioning. A large sample of business students were recruited to participate in a 20-day Event-Contingent Recording procedure in which they were asked to report on the situational, behavioral, affective, and perceptual aspects of their social interactions using a smartphone application. They also completed personality measures prior to and outcome measures following this procedure. In Manuscript 1, I use data collected prior to and during the ECR procedure to provide support for the use of an app-based procedure as an alternative to a pen-and-paper procedure for measuring spin by demonstrating the stability of spin scores over the study period and by replicating and expanding on personality and situational correlates associated with spin. In Manuscript 2, I use data collected during the ECR procedure to explore the perceptual processes which may be associated with spin and which may underlie spin and its effects on social functioning. In Manuscript 3, I use data collected during and immediately after the ECR procedure to propose a possible mechanism by which spin may exert deleterious effects on well-being. These studies integrate findings regarding the antecedents, social impairments, and maladjustment associated with spin, enhancing our understanding of the processes which may underlie spin and its effects. Taken together, these findings will suggest that spin may be conceptualized as behavioral dysregulation that arises in part because of perceptual and affective fluctuations across interactions and reactivity to interpersonal perception within interactions. These fluctuations and this reactivity may be related to Neuroticism and constitute a kind of rejection sensitivity. I speculate that this manner of engaging with and responding to others may be experienced by others as off-putting and may evoke responses from others that contribute to individuals with higher spin experiencing their social needs as being unmet and that augment their insecurity, maintain their dysregulation, increase their sense of social disconnection, and erode their well-being</description><creator>Clegg, Kayleigh-Ann</creator><contributor>Debbie S Moskowitz (Supervisor)</contributor><date>2020</date><subject>Psychology</subject><title>Antecedents and consequences of interpersonal spin</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/b8515s64q.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/0v838490b</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Psychology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:rv042z64q</identifier><datestamp>2020-07-14T18:23:57Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Cette thèse examine l’application de méthodes de séparations physique sur le gisement Nechalacho situé dans les Territoires du Nord-Ouest, au Canada. La concentration est surtout sur l’utilisation des propriétés minéralogiques des gisements pour quantifier le comportement séparatoire des minéraux à travers une variété de procédés. Le but ultime est de proposer un procédé d’enrichissement du minerai pertinent pour l’industrie. Les minéraux contenant des REE de valeur sont l’allanite, le bastnäsite, le colombite (Fe), le fergusonite, la monazite, le synchysite et le zircon, et les minéraux de gangue primaire sont le quartz, le feldspath et les oxides de fer. Les résultats de l’analyse QEMSCAN indiquent que les distributions des tailles de grains et le comportement d’association des minéraux de valeur dans le gisement causent une concentration de ces minéraux dans des particules ayant une gravité spécifique (GS) haute et à des tailles de particule au-dessus de leur taille de libération. Pour prendre avantage de cette propriété, un concentrateur Knelson et un spiral sont examinés comme possibilités de méthode pour préconcentrer le minerai à des tailles de particule corses, avec le but de rejeter les minéraux de gangue à GS basse (le quartz et le feldspath) tôt dans le processus d’enrichissement. Les deux techniques ont prouvé être efficaces. Or, le spiral est recommandé d’autant plus comme procédé applicable à l’industrie dû à sa simplicité. Optimiser et appliquer un tel procédé peut avoir des effets profonds sur tous procédés postérieurs; et autant plus dans l’économie globale parce que l’énergie requise pour la comminution serait minimisée.Après un travail d’investigation de la préconcentration, un concentrateur Knelson, un séparateur multi-gravitationnel (MGS) et une table vibratoire du laboratoire Mozley ont été étudiés pour évaluer leur habileté à produire un concentré en frac de minéraux lourds (REM, zircon et oxides de fer) à des tailles de particules beaucoup plus proche de la taille de libération des minéraux de valeur. Les trois techniques sont efficaces pour valoriser le zircon et les REM. Par ailleurs, le MGS est recommandé comme procédé supérieure. Le MGS est particulièrement efficace pour le recouvrement et le concentrement du zircon. Outre, même si les REM sont effectivement valorisés, leur récupération était plutôt base. Une analyse minéralogique indique que, avec une optimisation du procédé de comminution et des conditions d’opération du MGS, des cibles de récupération satisfaisantes peuvent être atteintes. Cependant, ce fut noté que la récupération de l’allanite peut rester réduit dû à une GS relativement base comparer aux autres minéraux de valeur dans le gisement Nechalacho.Par après, les concentrés obtenus par séparation gravitationnelle provenant du MGS et de la table vibratoire du laboratoire Mozley sont procédés en utilisant un séparateur magnétique à bas intensité, pour enlever la gangue d’oxides de fer, et un séparateur magnétique mouillé à haute intensité (WHIMS), pour produire des concentrés distincts de REM et de zircon. La gangue d’oxide de fer est effectivement enlevée. Or, son association avec les REM et le zircon indique qu’une quantité minime de perte de matériel de valeur est inévitable. Le WHIMS peut produire un concentré de REM de haute qualité avec une réponse magnétique relative suivante : allanite &gt; Fergusonite &gt; colombite (Fe) &gt; monazite &gt; bastänite &gt; synchysite. Par ailleurs, une quantité significative de REM reste dans la fraction non-magnétique du zircon. Par conséquence, une étape de séparation magnétique qui peut induire une grande force magnétique sure les REM à travers des gradients de haut champ magnétique doit être examinée. Ces analyses métallurgiques couplé avec la minéralogie automatisé et d’autres méthodes de caractérisations, ont éventuellement amené vers une proposition d’un schéma d’enrichissement pour le gisement Nechalacho</description><description>This thesis examines the application of physical separations to the Nechalacho deposit in the Northwest Territories of Canada. It is specifically focused on relating the deposits mineralogical characteristics to quantify mineral separation behaviour in various processes; and ultimately proposing an industrially applicable beneficiation process for the ore. The valuable REE-bearing minerals in this ore are allanite, bastnӓsite, columbite (Fe), fergusonite, monazite, synchysite and zircon; and the primary gangue minerals are quartz, feldspars and iron oxides.Quantitative Evaluation of Minerals by Scanning Electron Microscopy (QEMSCAN) results indicate the grain size distributions and association behaviour of valuable minerals in the deposit cause them to be concentrated in high specific gravity (SG) particles, at particle sizes well above their liberation size. To take advantage of this property, a Knelson Concentrator and spiral are examined as methods to preconcentrate the ore at a relatively coarse particle size; with the goal of rejecting low SG silicate gangue minerals (quartz and feldspars) early in the beneficiation process. Both techniques are determined to be effective. However, due to its simplicity, the spiral is recommended as the more applicable process in an industrial setting. The optimization and application of such a process could have profound effects on any downstream processing, as well as in the overall economics, as it would minimize the energy required for comminution. Following the preconcentration test work, a Knelson Concentrator, Multi-Gravity Separator (MGS) and Mozley Laboratory Shaking Table are studied to assess their ability to produce a bulk heavy mineral (REM, zircon and iron oxides) concentrate, at particle sizes much closer to the liberation size of the valuable minerals. All three techniques are effective at upgrading zircon and REM. But, the MGS is recommended as the superior process. The MGS is particularly effective at recovering and concentrating zircon. Although REM are effectively upgraded, their recovery was relatively low. Mineralogical analysis indicates that with optimization of the comminution process and the MGS operating conditions, satisfactory recovery targets are likely to be achieved. However, it is noted that allanite recovery may remain depressed due to its relatively low SG compared to the other valuable minerals in the Nechalacho deposit. Gravity concentrates from the MGS and Mozley Laboratory Shaking Table are then processed using a low-intensity magnetic separator, to remove iron oxide gangue, and a wet high intensity magnetic separator (WHIMS), to produce separate REM and zircon concentrates. Iron oxide gangue is effectively removed. But, its association with REM and zircon indicates that some losses of valuable material in this step may be unavoidable. The WHIMS is capable of producing a high-grade REM concentrate; with the relative magnetic response of REM following allanite &gt; fergusonite &gt; columbite (Fe) &gt; monazite &gt; bastnӓsite &gt; synchysite. However, a significant portion of REM remain in the non-magnetic fraction with zircon. Concluding that a magnetic separation step which can induce a high magnetic force on REM through high magnetic field gradients should be examined. These metallurgical tests coupled with the use of automated mineralogy, along with other characterization techniques, led to the eventual proposal of a flowsheet to beneficiate the Nechalacho deposit</description><creator>Marion, Christopher</creator><contributor>Kristian Waters (Supervisor)</contributor><date>2020</date><subject>Mining and Materials</subject><title>A mineralogical investigation into the beneficiation of a rare-earth mineral deposit using physical separations</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/xp68km57t.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/rv042z64q</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mining and Materials</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:qn59q874v</identifier><datestamp>2020-07-14T18:24:00Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>This thesis consists of three essays in empirical asset pricing. In the first essay, I study momentum crashes in emerging equity markets. In particular, I investigate that the momentum crashes are related to volatility, unconditional of the market state. I use emerging stock markets as a laboratory because of their high volatility in both bear and bull markets. My main finding is that momentum crashes are not limited to bear markets, and in fact, one third are experienced in bull markets. These crashes do not fit into the optionality model of Daniel and Moskowitz (2016). Instead, I provide evidence that momentum crashes are linked to the market volatility.  In volatile states, the optionality payoff of momentum increases and momentum skewness decreases. Furthermore, I show that the poor performance of momentum in EMs is due to the high volatility in these markets. In the second essay, I investigate whether excessive shortselling is the primary cause for momentum crashes. My hypothesis is that the excessive shortselling of the loser stocks pushes their price below their fundamental values. When the market rebounds, the reversal in the price of the losers leads to momentum crash. I collect the data on shortselling policies across countries, and test whether momentum crashes less in markets with shortselling ban, controlling for the market state and volatility. My results show that the crashes are less severe in markets with shortselling ban, suggesting that shortselling partially explains momentum crashes.In the third essay, I study the mutual fund industry in 77 countries and examine how the fund styles are developed on the aggregate level. I apply textual analysis to the fund names in order to classify funds. I find that the 20 most frequently used words appear in over 50% of all fund names and I define 10 categories (“styles”) based on those (and related) words. These 10 categories are sufficient to classify over 85% of all funds. I find that the menu of funds are remarkably universal. My main result shows how the menu of funds offered to investors in those 77 countries converges over time to a common (“global”) menu of funds. I trace this surprisingly simple and uniform process of global menu convergence to the actions of individual fund families who follow similar growth paths. My results shed new light on the aggregate process of financial innovation and the industrial organization of the asset management industry that appears to produce the same “wholesale” menu around the world</description><description>Cette thèse est composée de trois essais sur l’évaluation des actifs. Dans le premier essai, j'étudie les « momentum crashes » sur les marchés boursiers émergents. En particulier, j’étudie le lien entre ces crashes et la volatilité, inconditionnelle de l'état du marché. J'utilise les marchés boursiers émergents comme laboratoire en raison de leur forte volatilité sur les marchés baissiers et haussiers. Ma principale conclusion est que les crashes de momentum ne se limitent pas aux marchés baissiers et, en fait, un tiers sont expérimentés dans les marchés haussiers. Ces crashes ne rentrent pas dans le modèle de l'optionnalité de Daniel et Moskowitz (2016). Au lieu, je prouve que les momentum crashes sont liées à la volatilité du marché. Dans les états volatils, le gain en option de momentum augmente et l'asymétrie de momentum diminue. De plus, je montre que la mauvaise performance de momentum des émergents est due à la forte volatilité dans ces marchés.Le deuxième essai présente la vente à découvert excessive comme la principale cause des momentum crashes. La vente à découvert excessive des actions perdantes pousse leur prix en dessous de leurs valeurs fondamentales. Lorsque le marché rebondit, le retournement du prix des actions perdantes entraîne un momentum crashe. Je collecte les données sur les politiques de vente à découvert dans les pays et vérifie si un crash de momentum arrive moins souvent sur les marchés où les ventes à découvert sont interdites, contrôlant l'état du marché et la volatilité. Mes résultats montrent que les accidents sont moins graves sur les marchés avec interdiction de vente à découvert.Le troisième essai, j'étudie l'industrie des fonds communs de placement dans 77 pays et examine comment les styles de fonds sont développés au niveau agrégé. J'applique une analyse textuelle aux noms des fonds afin de classer les fonds. Je trouve que les 20 mots les plus fréquemment utilisés apparaissent dans plus de 50% de tous les noms de fonds et je définis 10 catégories («styles») en fonction de ces mots. Ces 10 catégories suffisent pour classer plus de 85% de l'ensemble des fonds. Je trouve que le menu des fonds est remarquablement universel. Mon principal résultat montre comment le menu de fonds offert aux investisseurs dans ces 77 pays converge au fil du temps vers un menu de fonds commun («mondial»). Je trace ce processus étonnamment simple et uniforme de convergence globale des menus aux comportements des familles de fonds qui suivent des trajectoires de croissance similaires. Mes résultats aident à éclairer le processus global d'innovation financière et l'organisation industrielle de l'industrie de la gestion d'actifs qui semble produire le même menu «de gros» partout dans le monde</description><creator>Shahrad, Ali</creator><contributor>David Schumacher (Supervisor2)</contributor><contributor>Vihang R Errunza (Supervisor1)</contributor><date>2020</date><subject>Management</subject><title>Three essays in empirical asset pricing</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/n870zv93x.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/qn59q874v</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Desautels Faculty of Management</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:8g84mr806</identifier><datestamp>2020-07-14T18:24:29Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>BackgroundUnhealthy diet is the most important preventable risk factor globally for premature death and disability. Regional nutrition inequality is expected to exist across towns or even within city subdivisions due to geographic variation in socio-cultural, economic, marketing and built environment features. Effective development and evaluation of nutrition policies and community interventions rely on the availability of measures capturing geographic patterns of diets and environmental factors influencing food selection. A traditional data source for nutrition surveillance is population health surveys, which have inadequate sample size to provide stable estimates of community diets. Furthermore, surveys are unable to measure important within-store marketing of unhealthy food, such as price discounting, which may have important, local-in-time effects. Electronic Point-Of-Sales (POS, i.e. store- rather than individual shopper-level) grocery transaction records generated by product scanning have the potential to overcome these limitations with appropriate analytical approaches.  ObjectivesThe overarching goal of this thesis is to develop and evaluate measurement methods that improve nutrition surveillance. Objective 1: To estimate how neighbourhood education modifies the effect of price discounting on soda purchasing at the store level.Objective 2: To develop a small-area indicator of latent soda demand using a hierarchical Bayesian model and to evaluate the ability of the indicator to explain area-level risk of type 2 diabetes mellitus.Objective 3: To develop small-area indicators of soda, diet-soda, sugary yogurt and plain yogurt using a retail gravity model and to evaluate the ability of these indicators to explain area-level risk of type 2 diabetes mellitus.Methods and ResultsObjective 1: I modelled weekly store-level sales of soda as a function of store-level price discounting, store- and area-level confounders, and an interaction term between discounting and categorical area-level education in the area where stores were located. The estimated effect of discounting on soda sales was larger in stores in less educated neighbourhoods, most notably in pharmacies.Objective 2: I modelled the store-level proportion of soda sales out of the total sales of all beverages as a function of store- and area-level covariates using a hierarchical Bayesian model with a random effect specified by a proper Conditional Autoregressive (CAR) prior. The estimated posterior mean of the CAR prior is the proposed area-level indicator of latent soda demand and is estimable for all areas through smoothing, including in areas without stores. To evaluate the utility of the proposed indicator in small-area disease risk assessment, the indicator was added to a set of area-level covariates in a restricted spatial regression that modeled the area-level risk of Type 2 Diabetes mellitus (T2D). The inclusion of the indicator improved model fit.Objective 3:  Based on store size and road network distance from origin (census dissemination area) to destination (store), I calculated the gravity-based pairwise probability of residents in each dissemination area visiting each store in the Census Metropolitan Area (CMA) of Montreal. Store-level sales were partitioned proportional to these visit probabilities and assigned to surrounding census Dissemination Areas (DAs). The resulting area-level purchasing indicator for soda, diet-soda, yogurt and plain yogurt improved the fit of T2D risk model, fit via a modified Besag-York-Mollié CAR model containing a scaled spatial random effect to enhance the interpretability of hyperpriors along with other area-level fixed effects.  ConclusionsMy dissertation introduced and illustrated public health uses of POS data, highlighting their ability to improve nutrition surveillance, while developing and evaluating methodological approaches to address limitations of these data</description><description>ContexteLe développement et l’évaluation efficace de politiques de nutrition et d’interventions dans la communauté dépendent de la disponibilité de mesures qui reflètent les différences géographiques en alimentation et les facteurs environnementales influençant les choix de nourriture. Une source traditionnelle de données pour la surveillance de la nutrition est les enquêtes sur la santé populationnelle, qui ont un échantillon inadéquat pour fournir des estimations de la nutrition de la communauté. De plus, les enquêtes ne peuvent pas mesurer le marketing important d’alimentation malsaine à l’intérieur des magasins telle que la réduction de prix, qui peuvent avoir des effets locaux et temporels. Les dossiers électroniques de transactions de point de vente d’épicerie (plutôt qu’au niveau de client individuel) qui sont générés par la lecture de produits ont le potentiel de résoudre ces limitations avec des approches analytiques appropriées. ObjectifsObjectif 1 : L’estimation de la modification de la mesure d’effet des rabais sur les achats de boissons gazeuses, en fonction du niveau d’éducation du quartier du magasin. Objectif 2: De développer un indicateur au niveau de petit domaine de la demande latente de boissons gazeuses en utilisant la modélisation hiérarchique bayésienne et d’en évaluer son utilité en estimant le risque du diabète de type 2 au niveau régional.  Objectif 3: De développer un indicateur au niveau de petit domaine de boissons gazeuses, de boissons gazeuses diètes, de yogourt sucré et de yogourt nature en utilisant un modèle de gravité de magasins et d’en évaluer son utilité en estimant le risque du diabète de type 2 au niveau régional.  Méthodologie et RésultatsObjectif 1: J’ai modélisé les ventes agrégées de boisson gazeuses au niveau de magasins en fonction des rabais en magasin, des variables confusionnelles au niveau de magasins et de régions, et d’une interaction entre les rabais et le niveau régional d’éducation dans la région de tri d’acheminement où le magasin est situé. L’effet estimé des rabais sur les ventes de boissons gazeuses était plus grand pour les magasins dans les quartiers ayant un plus faible niveau d’éducation, surtout dans les pharmacies. Objectif 2: J’ai modélisé la proportion des ventes de boissons gazeuses en magasin parmi les ventes totales de toutes boissons en fonction de covariables aux niveaux de magasins et de régions par une modélisation hiérarchique bayésienne avec un effet aléatoire spécifié par une distribution a priori autorégressive conditionnelle (ARC). La moyenne a posteriori estimée de la distribution a priori ARC est l’indicateur proposé de la demande latente de boissons gazeuses au niveau de petit domaine et peut être estimé pour toutes régions par lissage, incluant les régions qui n’avaient pas de magasins. l’indicateur a été ajouté à un ensemble de covariables régionales dans une régression spatiale restreinte qui modélise le risque régional de diabète de type 2. L’inclusion de l’indicateur a amélioré l’ajustement du modèle.Objectif 3:  Selon la taille de magasin et la distance routière entre le point d’origine (aire de diffusion du recensement) et la destination (magasin), j’ai calculé la probabilité par paires gravitationnelle que les résidents de chaque aire de diffusion aient visité chaque magasin dans la région métropolitaine de recensement. Le ventes au niveau de magasin ont été réparties proportionnellement aux probabilités de visites et assignées aux aires de diffusions comprenant les magasins. Les indicateurs correspondants représentant les ventes de boissons gazeuses, de boissons gazeuses diètes, de yogourt sucré et de yogourt nature ont amélioré l’ajustement du modèle de risque de diabète type 2. Cet ajustement a été fait par un modèle Besag-York-Mollié. ConclusionsMa thèse introduit et démontre l’utilité pour la santé publique des données au point de vente</description><creator>Mamiya, Hiroshi</creator><contributor>David Buckeridge (Supervisor1)</contributor><contributor>Alexandra Schmidt (Supervisor2)</contributor><date>2020</date><subject>Epidemiology and Biostatistics</subject><title>Characterizing community dietary patterns using grocery point-of-sales data</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/x346d8623.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/8g84mr806</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Epidemiology and Biostatistics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:6m311t89f</identifier><datestamp>2020-07-14T18:24:45Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Le best-seller The Catcher in the Rye, écrit par Jerome David Salinger, provoque une violente polémique aux États-Unis dès sa parution en 1951. Les objections les plus vives se concentrent sur la langue crue, obscène et blasphématoire du roman. Dans le volet critique de ce mémoire, je propose d’abord une analyse détaillée des deux éléments les plus critiqués du texte, le swearing et le slang, basée sur une lecture sociocritique du roman. Je cherche ainsi à expliquer cette controverse. Je dégage ensuite des solutions de traduction capables de reconduire la dimension subversive de l’original en comparant les contextes sociolinguistiques des États-Unis et du Québec. J’explore plus particulièrement le potentiel d’une réactualisation du vernaculaire québécois employé de façon hétérogène dans le texte cible. J’étudie notamment les équivalences entre les phénomènes du swearing et du sacre/juron, ainsi que du slang et de ce que j’appelle « la langue de Brébeuf », caractérisée par l’emploi du franglais. Je précise par le fait même la visée esthétique et idéologique d’un projet de traduction que je mets en œuvre dans le volet traduction de ce mémoire. J’y propose la traduction de quatre nouvelles de J. D. Salinger, liées thématiquement et linguistiquement à Catcher : « Last Day of the Last Furlough », « A Boy in France », « This Sandwich Has No Mayonnaise » et « The Stranger »</description><description>Jerome David Salinger’s best-seller The Catcher in the Rye provoked controversy in the United States as soon as it was published in 1951. The loudest objections concerned its raw, obscene and blasphemous use of language. In the first section of this Master’s thesis, I bring forward a detailed analysis of the two most criticized elements of the novel’s language, the use of swearing and slang, based on a sociocritical reading of Catcher. Thus I attempt to explain the controversy. I then emphasize translation solutions conveying the original text’s subversive dimension by comparing the sociolinguistical contexts of the United States and Québec. I more specifically explore the potential of a reactualisation of the Québécois vernacular used in a heterogenous manner in the translation. I study the particular similarities between swearing and the use of “sacres/jurons” as well as the use of slang and what I call “la langue de Brébeuf,” characterized by the use of Frenglish. By doing so, I point out the ideological and esthetic aim of a translation project that I deploy in the second and last section of this Master’s thesis. In that section, I offer the translation of four short stories by J. D. Salinger, linguistically and thematically linked to Catcher: “Last Day of the Last Furlough,” “A Boy in France,” “This Sandwich Has No Mayonnaise” and “The Stranger.”</description><creator>Leblanc-Filion, Natacha</creator><contributor>Catherine Leclerc (Supervisor)</contributor><date>2020</date><subject>French Language and Literature</subject><title>Where Is Holden Caulfield? The Catcher in the Rye : le potentiel d’une traduction « en québécois » suivi de la traduction de quelques nouvelles de J. D. Salinger</title><language>fre</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/gq67jw48t.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/6m311t89f</identifier><degree><name>Master of Arts</name><grantor>McGill University</grantor><discipline>Department of French Language and Literature</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:dj52w903g</identifier><datestamp>2020-07-14T18:25:38Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1.0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Legionella pneumophila is one of the most important cause of waterborne disease in developed countries. It is the causative agent of Legionnaires’ disease, a severe pneumonia. L. pneumophila is a natural bacterial inhabitant of water systems where it parasitizes and grows in protozoan host species, such as amoeba and ciliates. This adaptation to grow intracellularly has also allowed it to grow within human macrophages. In this way, Legionnaires’ disease is contracted by the inhalation of aerosols contaminated with L. pneumophila, which leads to infections of the lung macrophages and subsequent pneumonia. Cooling towers, used for air conditioning and ventilation systems, are a major source of large outbreaks of Legionnaires’ disease, however the reasons for this are not well understood. Indeed, colonization of cooling towers by L. pneumophila is variable, as some cooling towers are perpetually colonized, and others are almost never colonized. As L. pneumophila is an intracellular parasite, it is believed that the microbiome of cooling towers may play a significant role in the colonization, survival, and proliferation of L. pneumophila in these systems. Consequently, our main objective was to characterize the bacterial and eukaryotic communities of cooling towers and understand their role in L. pneumophila ecology. Another objective was to isolate and characterize bacteria from cooling towers that could stimulate or inhibit L. pneumophila and to study their role in the ecology of L. pneumophila in cooling towers. In order to do this, we characterized the bacterial community of 18 different cooling towers in southern Québec, Canada, using a 16S rRNA amplicon sequencing approach. The findings revealed that the bacterial community of the towers was moulded by several physicochemical factors such as water source and application of chlorine. The continuous application of chlorine was associated with the establishment of a Pseudomonas population. The Pseudomonas counts were negatively correlated with most other taxa identified in the cooling towers, including Legionella. As a result, we concluded that continuous application of chlorine could be an effective way to reduce levels of L. pneumophila in cooling towers. As a second step, we characterized the eukaryotic community using an 18S rRNA sequencing approach and examined its interplay with the bacterial community and the Legionella community. The results revealed that cooling towers contain a diverse community of eukaryotes. Several eukaryotic and bacterial taxa formed a complex network based on co-occurrence. The network revealed that several taxa could potentially affect L. pneumophila ecology. This was demonstrated through the study of the interaction of a Brevundimonas sp. isolate and the ciliate community. These two microbial groups could promote the growth of L. pneumophila through direct and indirect mechanisms, such as nutritional supplementation or promoting host population growth. Finally, our last experiment isolated, identified, and characterized through whole genome sequencing several bacterial isolates that could inhibit L. pneumophila on plate. The analysis of the genomes of these isolates revealed a number of potential antimicrobial gene clusters that could explain the inhibition. Overall, the results suggest that the permissiveness of L. pneumophila colonization, survival, proliferation in cooling towers is dependent on the presence of positive and negative interacting species composing the microbiomes of these environments. Manipulating these microbiomes so that they are not permissive to L. pneumophila could be a potential method to control Legionnaires’ disease outbreaks</description><description>Legionella pneumophila est l’une des causes de maladie associé à l’eau dans les pays développés. Cette bactérie cause la maladie du légionnaire, une pneumonie sévère. L. pneumophila est une bactérie aquatique trouvée dans les systèmes d’eaux où elle parasite et croit dans des protozoaires, tels que les amibes ou les ciliés. La croissance intracellulaire est une adaptation qui permet à L. pneumophila de croitre aussi dans les macrophages humains. Ainsi, l’inhalation d’aérosols contaminés avec L. pneumophila est la méthode de transmission de la maladie. L’entrée de L. pneumophila dans les poumons permet à la bactérie d’infecter les macrophages et de causer la pneumonie. Les tours aeroréfrigérantes sont une source majeure d’éclosion de la maladie du légionnaire. Cependant les raisons pour ceci ne sont pas bien comprises. En effet, la colonisation des tours par L. pneumophila est variable, avec certaines tours étant perpétuellement colonisées et d’autre l’étant presque jamais. Étant donné que L. pneumophila est un parasite intracellulaire, le microbiome des tours est soupçonné comme étant un facteur majeur pour la colonisation, la survie, et la prolifération de L. pneumophila. Ainsi, nous avions pour objectif principal de caractériser la communauté bactérienne et eucaryote des tours aeroréfrigérantes et de comprendre leur rôle dans l’écologie de L. pneumophila. Nous avions aussi comme objectif d’isoler et de caractériser des bactéries qui pourraient stimuler ou inhiber la croissance de L. pneumophila et de comprendre le rôle de ces isolats dans l’écologie de L. pneumophila. Dans un premier temps, nous avons séquencé la communauté bactérienne de 18 tours au Québec, Canada, en utilisant une approche par séquençage d’amplicons du gène de l’ARNr 16S. Les résultats ont démontré que la communauté bactérienne est structurée par plusieurs facteurs physicochimiques, tels que la source d’eau et l’application du chlore. L’application en continu du chlore était associée avec l’établissement d’une population de Pseudomonas dans les tours. Cette population était corrélée négativement avec les autres taxons identifiés dans les tours, incluant avec Legionella. En conséquence, il est raisonnable de penser que l’application en continu au chlore pourrait être un moyen de réduire les niveaux de L. pneumophila. Dans un deuxième temps, nous avons aussi caractérisé la population eucaryote en utilisant une approche par séquençage d’amplicons de l’ARNr 18S et examiné son interaction avec la communauté bactérienne et la population de Légionnelles. Les résultats ont démontré que les tours d’eau contiennent une grande diversité d’organismes eucaryotes. Plusieurs taxons eucaryotes et bactériens forment un réseau basé sur des corrélations de cooccurrences. Le réseau a démontré que plusieurs taxons pouvaient affecter l’écologie de L. pneumophila. Ceci a été démontré par l’interaction d’un isolat bactérien de Brevundimonas et de la communauté de ciliés.  Ces deux groupes microbiens pouvaient stimuler la croissance de L. pneumophila par des interactions directes et indirectes, tels que par la supplémentation de nutriment ou par l’augmentation du nombre de cellules hôtes. Dans un dernier temps, nous avons isolé, identifié, et caractérisé par séquençage du génome plusieurs isolats bactériens capables d’inhiber L. pneumophila sur gélose. L’analyse des génomes des isolats a démontré un grand nombre de groupes de gènes antimicrobiens potentiels parsemant les génomes qui pouvaient expliquer l’inhibition. De façon générale, les résultats suggèrent que la permissivité de colonisation, de survie, et de prolifération de L. pneumophila dans les tours d’eau est dépendante de la présence d’espèces interagissant positivement et négativement dans les microbiomes des tours. La manipulation de ces microbiomes, pour qu’elles soient non permissive pour L. pneumophila, pourrait être une méthode potentielle pour contrôler les éclosions de maladie du légionnaire</description><creator>Paranjape, Kiran Manu</creator><contributor>Sebastien Faucher (Supervisor)</contributor><date>2020</date><subject>Natural Resource Sciences</subject><title>Studies on the importance of the microbiome of cooling towers on Legionella spp. ecology</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/jh343x25d.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/dj52w903g</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Natural Resource Sciences</discipline></degree></thesis></metadata></record><resumptionToken completeListSize="47894">oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):47600</resumptionToken></ListRecords></OAI-PMH>