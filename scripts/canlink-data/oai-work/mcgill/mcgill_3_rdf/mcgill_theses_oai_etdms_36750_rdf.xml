<?xml version="1.0" encoding="UTF-8"?><rdf:RDF xmlns:oai="http://www.openarchives.org/OAI/2.0/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:ual="http://terms.library.ualberta.ca/" xmlns:bibo="http://purl.org/ontology/bibo/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:schema="https://schema.org/" xmlns:etdms="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Acn69m818r"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Physics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>The cooling of high-magnetic-field pulsars</dcterms:title><ual:dissertant>Zhu, Weiwei</ual:dissertant><dc:abstract>Dans le chapitre 3, je présente mon travail sur la lueur résiduelle de AXP 1E2259+586. Il s'agit du premier pulsar anormal à partir duquel des pulsations similaires à celles des SGR ont été détectées. Ce pulsar a eu un sursaut énergétique majeur en 2002. Nous avons étudié la lueur résiduelle de ce sursaut au travers 10 observations faites avec le télescope XMM prises avant et après le sursaut. Nous avons trouvé que le flux du pulsar a diminué en suivant une fonction de puissance dont l'indice, −0.69±0.03, est remarquablement similaire à celui trouvé dans le rayonnement résiduel des sursauts des SGR. Nous avons aussi trouvé une corrélation entrela dureté du spectre et le flux en rayons-X, une corrélation qui est aussi observée dans d'autres AXP. Dans le chapitre 4, je présente mon travail sur la recherche de variations dans la luminosité-X de AXP 1E 1841−045. Ce pulsar est parmi les AXP qui présentent le plus fréquemment des sauts de fréquences, ou glitchs. Plusieurs théories des magnétars suggèrent une connection entre les glitchs et la variation de la luminosité-X. Malgré celà, nous n'avons pas trouvé de preuve de l'éxistence de ces variations dans la luminosité-X de 1E 1841−045 dans des observations prises entre 1993 et 2006 avec des télescopes variés. Ceci démontre l'existence de glitchs silencieux. Il est intéressant de noter qu'il existe un groupe de pulsars normaux, dont la source de luminosité est leur énergie rotationelle, qui ont un champ magnétique élevé et proche de celui des magnétars ( 1013 G). Certaines caractéristiques de ces pulsars à champ magnétique élevé sont similaires à celles des magnétars. Dans le chapitre 5, je présente la première détection en rayons-X du pulsar à champs magnétique élevé B1916+14. Nous avons trouvé que le spectre d'émission de ce pulsar est probablement thermique, avec une température de surface entre 0.08–0.23 keV. Nous n'avons pas détecté de pulsations régulières dans les données, avec une limite supérieure (1)de 0.7 sur la fraction pulsée entre 0.1 et 2 keV. Il est aussi difficile de déterminer si le spectre d'émission thermique observé est dû à un refroidissement initial ou à un courant qui réchauffe la surface du pulsar en ce moment. C'est pourquoi plus d'observations sont requises afin de prouver que le réchauffement de la surface est dû à une diminution du champ magnétique. Dans le chapitre 6, je présente mon travail effectué sur des observations faites avec le télescope Chandra du pulsar a champ magnétique élevé RPP J1718−3718. Nous avons détecté des pulsations régulières en rayons-X à un interval égal à celui de la fréquence rotationelle de ce pulsar et avec une fraction pulsée de 52%±13% entre 0.8 et 2 keV. Nous avons trouvé, en étudiant le spectre combiné de plusieurs observations,une température de corps noir de 0.19±0.02 keV. C'est une température un peu plus élevée que celle prédite par les modèles standards de refroidissement. Par contre, les modèles numériques d'atmosphère des étoiles à neutrons est en accord avec les modèles standards de refroidissement. Nous avons aussi trouvé que la luminosité bolométrique représente 0.3 de la puissance due à la perte d'énergie rotationelle, si l'on suppose une distance de 4.5 kpc. Finalement, nous avons comparé les températures de corps noirs des pulsars normaux à champ magnétique élevé avec ceux de pulsars normaux du même âge ayant un champ magnétique faible, et nous avons trouvé que les premiers avaient des températures plus élevées, comme le prédisent les modèles magnéto-thermiques ayant comme but d'unifier les pulsars normaux à champs magnétiques élevés et les magnétars.</dc:abstract><dc:abstract>Prior to ~20 years ago, only two kinds of pulsars were known: RPPs and accretion-powered pulsars. The rapid advance of X-ray astronomy in the past few decades has led to the discovery of magnetic-powered pulsars, namely "magnetars". Magnetars were first identified with the SGRs which exhibit sporadic soft gamma-ray bursts. More recently, another group of pulsars, the AXPs, characterised by their bright persistent X-ray emission that is more powerful than their spin-down luminosity, were also recognized as members of the magnetar family. Both SGRs and AXPs have very high (10¹⁴-10¹⁵ G) magnetic fields as inferred from their spin-down. Studying AXP behaviour might help us understand the physics of magnetars and their connections with normal pulsars. In Chapter 3, I present our work on the X-ray afterglow of the AXP 1E 2259+586. It is the first AXP to exhibit a SGR-like outburst. It went through a major outburst in 2002. We studied the X-ray afterglow of this outburst, using ten XMM observations taken before and after the outburst. We found that the AXP's flux decayed following a power-law of index -0.69±0.03, remarkably similar what was found from the afterglow of some SGR outbursts. We also found a strong correlation between spectral hardness and X-ray flux, as seen in other AXPs. In Chapter 4 I present our work on searching for X-ray variability from the glitching AXP 1E 1841-045. This is one of the most frequent glitchers among AXPs. Magnetar theories and observations suggest that there could be a connection between magnetar glitches and their X-ray variability. However, we found no evidence of glitch-related X-ray variability from archival X-ray data of 1E 1841-045 taken between 1993 and 2006. Our finding supports the existence of radiatively silent glitches in AXPs. Interestingly, there is also a group of RPPs that have spin-down magnetic fields close to those of the magnetars (~10¹³G). These high-magnetic-field RPPs may share some observational properties with the magnetars. In Chapter 5, I present the first X-ray detection of the high-magnetic-field RPP B1916+14. We found that the pulsar's emission is likely thermal, with a surface temperature in the range of 0.08-0.23 keV. We did not detect pulsations in the data, and set a 1σ upper limit on the pulsed fraction in the 0.1-2 keV band of ~0.7. The origin of the thermal emission is not well constrained. We cannot rule out initial cooling or return current heating for this pulsar. To look for evidence of magnetic-field-decay heating, a deeper observation is needed. In Chapter 6, I present our work on CHANDRA X-ray observations of the high-magnetic-field RPP J1718-3718. We detected X-ray pulsations at the pulsar's period with 52%±13% pulsed fraction in the 0.8-2 keV band. We found, from a merged spectrum of multiple observations, a blackbody temperature of 0.19±0.02 keV, slightly higher than predicted by standard cooling models. However, the best-fit neutron star atmosphere model is consistent with standard cooling. We also found that the pulsar's bolometric luminosity represents 0.3 of its spin-down power, assuming a distance of 4.5kpc. Finally, we compared the blackbody temperatures measured for the high-magnetic-field pulsars with those from low-magnetic field rotation-powered pulsars of the same age, and found evidence of the former being on average hotter than the latter, as predicted by magneto-thermal evolution models that attempt to unify high-magnetic-field RPPs with magnetars.</dc:abstract><ual:supervisor>Victoria Kaspi</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/w3763b98m.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/cn69m818r</ual:fedora3Handle><dc:subject>Physics - Astronomy and Astrophysics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Azw12z925p"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Epidemiology and Biostatistics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>The association between long-term exposure to traffic- related air pollution and cardiovascular mortality in Ontario, Canada</dcterms:title><ual:dissertant>Chen, Hong</ual:dissertant><dc:abstract>Introduction: Cardiovascular disease is the leading cause of mortality worldwide. There is suggestive evidence that chronic exposure to traffic-related air pollution may increase the incidence and mortality from cardiovascular disease. However, in few studies has the health effects of traffic-related air pollution been examined at the relatively lower concentrations of pollution such as observed in Canadian cities. In addition, the few studies using land use regression models to assess exposures of traffic-related air pollution at fine geographic resolutions have had modest sample sizes, so statistical power was limited. The purpose of this dissertation was to further our understanding of the aetiology of traffic-related air pollution in relation to cardiovascular outcomes through improved estimates of exposure derived through the juxtaposition of accurate information on residential addresses with improved methods of estimates of spatial concentrations of traffic-related air pollution. Methods: To achieve the research objectives of this dissertation, three studies were conducted. First, I conducted a systematic review of epidemiological evidence between 1950 and 2011 regarding the chronic health effects of ambient air pollution. Second, I developed three new methods of estimating historical exposure to traffic-related air pollution at fine geographic scales through extrapolating “current” land use regression models back in time. These three extrapolation methods entailed multiplying the predicted concentrations of NO2, a marker of traffic-related air pollution, by the ratio of past estimates of concentrations from fixed-site monitors, such that they reflected the change in the spatial structure of NO2 from measurements at fixed-site monitors. Third, I conducted a population-based cohort study to determine whether an association exists between traffic-related air pollution and cause-specific cardiovascular mortality among adults living in Ontario, Canada. This population-based cohort study used as a sampling frame the Canadian Federal income tax file and subjects living in Hamilton, Toronto, and Windsor were included in the analyses.  I made use of estimates of exposure to traffic-related air pollution using land use regression models and the back extrapolation methods. I carried out adjusted Cox regression models included known individual risk factors and selected ecological covariables and I carried out sensitivity analyses using indirect methods to adjust for smoking that was not available on individuals in the cohort.Results and Discussion: In the Ontario tax cohort study, I found that for each increase of 5 parts per billion (ppb) of NO2, the rate ratios for mortality for all cardiovascular disease and for ischemic heart disease, using different models, varied between 1.04 (95%CI: 1.00-1.09) and 1.10 (95%CI: 1.00-1.21). I found no associations between traffic pollution and cerebrovascular mortality (excess rates of mortality of 0.95 (95%CI: 0.89-1.02)). These results support the hypothesis that long-term exposure to traffic-related air pollution increases the mortality of cardiovascular disease.</dc:abstract><dc:abstract>Introduction: Les maladies cardiovasculaires représentent la principale cause de mortalité dans le monde. Il existe une preuve suggestive que l'exposition chronique à la pollution atmosphérique d'origine automobile puisse augmenter l'incidence et la mortalité attribuable aux maladies cardiovasculaires. Cependant, peu d'études ont été menées à ce jour pour évaluer les effets sanitaires de la pollution atmosphérique liée au trafic à des concentrations relativement faibles telles que celles mesurées dans les villes canadiennes. De plus, les quelques études qui se sont servi de modèles de régression de type Land-use regression pour évaluer l'exposition de la pollution atmosphérique liée au trafic à haute résolution spatiale ont été réalisées à partir d'échantillons de taille modeste, et sont conséquemment de puissance statistique limitée. Le but de cette thèse est d'approfondir notre compréhension du lien entre la pollution atmosphérique liée au trafic et les maladies cardiovasculaires, ce à l'aide de meilleures estimations de l'exposition environnementale obtenues par la juxtaposition d'information précise sur les adresses résidentielles avec de meilleures estimations des concentrations spatiales de pollution atmosphérique liée à la circulation.Méthodes: Afin d'atteindre les objectifs de recherche de cette thèse, trois études ont été menées. Tout d'abord, un examen systématique des données épidémiologiques associées aux effets chroniques de la pollution de l'air sur la santé entre 1950 et 2011 a été effectué. Deuxièmement, trois nouvelles méthodes ont été développées pour estimer rétrospectivement l'exposition à la pollution atmosphérique liée au trafic à fine résolution spatiale par extrapolation des modèles de régression ‘actuels' de type Land use regression. Ces trois méthodes d'extrapolation impliquaient la multiplication des concentrations prédites de NO2, un marqueur de la pollution atmosphérique liée au trafic, par le ratio des estimations des concentrations passées aux sites d'échantillonnage fixes, de sorte qu'elles reflètent les variations spatiales des concentrations de NO2. Dans un dernier temps, cette thèse présente une étude de cohorte menée afin de déterminer s'il existe une association entre la pollution atmosphérique liée au trafic et des causes spécifiques de mortalité cardiovasculaire chez les adultes vivant en Ontario, Canada. L'échantillon de la cohorte est tiré des bases de données fédérales d'impôt sur le revenu des particuliers. Nous avons utilisé les valeurs estimées de l'exposition à la pollution atmosphérique automobile générées par des modèles de régression de type Land-use regression et des méthodes de rétro-extrapolation. Les modèles de Cox ajustés incluent des facteurs de risque individuels connus et des covariables environnementales, alors que des analyses de sensibilité ont été réalisées avec des méthodes indirectes afin d'ajuster pour le tabagisme.Résultats et discussion: Peu d'études ont été menées pour explorer les associations entre la pollution atmosphérique liée au trafic dans les villes et la mortalité cardiovasculaire. Les résultats de cette thèse ont montré que, pour chaque augmentation de 5 parties par billiard (ppb) de NO2, les estimations du risque accru de cardiopathies ischémiques et de mortalité pour l'ensemble des maladies cardiovasculaires variaient entre 1.04 (95%CI: 1.00-1.09) and 1.10 (95%CI: 1.00-1.21). Par ailleurs, nous n'avons trouvé aucune association statistiquement significative entre la pollution liée au trafic et la mortalité cérébrovasculaire (taux de mortalité en excès de 0.95 (95%CI: 0.89-1.02). Cette thèse soutient l'hypothèse selon laquelle l'exposition à long terme à la pollution causée par le trafic accroit la mortalité cardiovasculaire, et plus particulièrement la mortalité due à une cardiopathie ischémique.</dc:abstract><ual:supervisor>Mark Goldberg</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/sj1396252.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/zw12z925p</ual:fedora3Handle><dc:subject>Health Sciences - Epidemiology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A2b88qh283"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mining and Materials</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Microstructure and texture studies on magnesium sheet alloys</dcterms:title><ual:dissertant>Masoumi, Mohsen</ual:dissertant><dc:abstract>Le AZ3, l'alliage Mg les plus courantes de feuille, est actuellement produite par laminage à chaud de l'lingot coulé DC. Alliages de magnésium forgé, en général, ont limité en raison de la formabilité hexagonale compacte structure et l'orientation préférentielle (texture). Afin d'améliorer la formabilité feuille de magnésium, une bonne compréhension de la microstructure et l'évolution de texture dans la coulée entre cylindres est nécessaire. Les objectifs de cette recherche sont d'étudier l'évolution de la microstructure et de texture dans deux cylindres en alliage fonte AZ31 feuille de Mg et de développer / modifier des compositions d'alliages avec des propriétés mécaniques améliorées (texture affaibli). Dans la première partie de l'étude, l'influence de la vitesse de refroidissement (CR) sur la structure coulée en alliage de magnésium AZ31 des a été étudiée, comme un fond pour comprendre le développement des microstructures du TRC AZ31, en utilisant des moules différents pour obtenir lente à modérée des taux de refroidissement. On a constaté que la taille des grains et secondaire espacement des bras de dendrite (DPS) diminue à mesure que les augmentations de taux de refroidissement. Par ailleurs, il a été observé que, avec une augmentation du taux de refroidissement de la fraction de particules de seconde phase augmente et les particules de seconde phase deviennent plus fins. La seconde partie a porté sur la microstructure et la texture d'étude de la distribution à double rouleau (TRC) AZ31 (% Mg-3wt. Al-1wt.% Zn) feuille. Les résultats indiquent que la TRC AZ31 présente une microstructure dendritique avec colonnaires et de grains équiaxe. Il a été noté que le montant de ces phases secondes dans l'alliage TRC est supérieure à la AZ31 conventionnellement exprimés. Recristallisation à 420 ° C conduit à une bimodale granulométrie, alors qu'une structure à grains fins est obtenue après laminage et recuit. La feuille de TRC AZ31 expositions textures basale dans le (i) que nous avons reçues, (ii) roulé et (iii) laminé recuit conditions. Cependant, le post-recuit de l'AZ31 TRC à 420 oC produit une texture relativement aléatoire qui n'a pas été observé précédemment dans le conventionnelles AZ31 feuille. La randomisation texture est attribuée à la particule stimulée par la nucléation (PSN) de nouveaux grains dans la structure de la TRC. L'évaluation préliminaire des propriétés mécaniques indique que ces traitements de recuit augmente légèrement la résistance à la traction (UTS), mais améliore significativement l'allongement. Lors de la finale d'une partie de l'étude, les microstructures et les textures des laminés et laminé / recuits Mg-1wt.% Mn et Mg-basée 1wt.% Zn-alliages à base contenant les différents niveaux de Ce et Sr ont été examinés. L'ajout Ce affine la structure du grain de coulée et roulé / recuit de Mg-1wt.% Mn (M1) en alliage. Par ailleurs, l'intensité de la texture globale pôle basal a été affaiblie par laminé ainsi que laminé / recuits Mg-Mn-Ce alliages par rapport à l'alliage M1. L'affaiblissement texture a été attribué à la solubilité solide de Cé en Mg plutôt que PSN, ou c / une altération ratio. L'ajout de Sr raffinée que la structure des grains-coulés et laminés / recuit de Mg-1wt.% Mn-Sr (MJ) et Mg-1wt.% Zn (Z1) alliage. Par ailleurs, l'intensité de la texture globale pôle basal a été affaiblie par laminé ainsi que laminé / recuits Mg-Zn-Sr (ZJ) alliages par rapport à l'alliage Z1. L'affaiblissement texture est attribué au PSN de nouveaux grains d'orientations aléatoires. </dc:abstract><dc:abstract>The AZ3, the most common Mg sheet alloy, is currently produced by hot rolling of the DC cast ingot.  Mg wrought alloys, in general have limited formability due to hexagonal close-packed structure and preferred orientation (texture). In order to improve magnesium sheet formability, a good understanding of microstructure and texture evolution in twin-roll casting is necessary. The objectives of this research are to study the microstructural and texture evolution in twin-roll cast AZ31 Mg sheet alloy and to develop/modify alloy compositions with improved mechanical properties (weakened texture). In the first part of study, the influence of cooling rate (CR) on the casting structure of AZ31 magnesium alloy has been investigated, as a background to understand microstructural development in TRC AZ31,  using different moulds to obtain slow to moderate cooling rates. It was found that grain size and secondary dendrite arm spacing (SDAS) reduces as the cooling rate increases. Moreover, it was observed that with an increase in cooling rate the fraction of second phase particles increases and the second phase particles become finer. The second part focused on the microstructure and texture study of the twin-roll cast (TRC) AZ31 (Mg-3wt.%Al-1wt.%Zn) sheet. The results indicate that TRC AZ31 exhibits a dendritic microstructure with columnar and equiaxed grains. It was noted that the amount of these second phases in the TRC alloy is greater than the conventionally cast AZ31. Recrystallization at 420 oC leads to a bimodal grain-size distribution, while a fine-grain structure is obtained after rolling and annealing. The TRC AZ31 sheet exhibits basal textures in the (i) as-received, (ii) rolled and (iii) rolled-annealed conditions. However, post-annealing of the TRC AZ31 at 420 oC produces a relatively random texture that has not been previously observed in the conventional AZ31 sheet. The texture randomization is attributed to the particle-stimulated nucleation (PSN) of new grains in the TRC structure. The preliminary evaluation of mechanical properties indicates that such annealing treatment slightly increases the ultimate tensile strength (UTS), but significantly improves elongation. In the final of part of the study, the microstructures and textures of rolled and rolled/annealed Mg-1wt.%Mn-based and Mg-1wt.%Zn-Based alloys containing different levels of Ce and Sr were examined.  The Ce addition refines the as-cast and rolled/annealed grain structure of Mg-1wt.%Mn (M1) alloy. Moreover, the overall texture intensity of basal pole was weakened for rolled as well as rolled/annealed Mg-Mn-Ce alloys compared to the M1 alloy. The texture weakening was attributed to the solid solubility of Ce in Mg rather than  PSN or c/a ratio alteration. The Sr addition refined the as-cast and rolled/annealed grain structure of  Mg-1wt.%Mn-Sr (MJ) and Mg-1wt.%Zn (Z1) alloy.  Moreover, the overall texture intensity of basal pole was weakened for rolled as well as rolled/annealed Mg-Zn-Sr (ZJ) alloys compared to the Z1 alloy. The texture weakening is attributed to the PSN of new grains with random orientations.</dc:abstract><ual:supervisor>Mihriban Ozden Pekguleryuz</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/4j03d3990.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/2b88qh283</ual:fedora3Handle><dc:subject>Engineering - Materials Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aw95054648"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Mechanical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Near-limit propagation of detonations in annular channels</dcterms:title><ual:dissertant>Jesuthasan, Anne</ual:dissertant><dc:abstract>Dans cette recherche, la propagation des détonations près de la limite dedétonation est étudiée dans des tubes annulaires. Les mélangesstoechiométriques de méthane et d'oxygène, d'acétylène et d'oxide nitreux diluésavec 50% d'argon, ainsi que d'acétylène et d'oxygène dilués avec 70% d'argonsont utilisés dans les expériences. Des mesures de célérité de la détonationainsi que des tracés des ondes de pression transverses sur feuille de suie ontété obtenus près de la limite de détonation. En normalisant l'espace annulairepar rapport à la longueur de la zone de réaction ZND ( i.e. L/ΔZND, où L estl'espace annulaire), les variations de célérité des détonations pour les différentsespaces annulaires se combinent pour former une seule courbe. Des célérités dedétonation d'environ la moitié de la valeur Chapman-Jouguet (CJ) ont étéobservées près de la limite ce qui suggère que les instabilités fournissent lemécanisme nécessaire pour entretenir la propagation malgré le déficit decélérité. Les résultats sont comparés au modèle de perte de célérité de Fay. Unaccord qualitatif est obtenu pour le mélange d'acétylène et d'oxygène dilué avec70% d'argon, mais le modèle ne parvient pas à prédire le déficit de célérité pourle mélange de méthane et d'oxygène. Les tracés sur feuille de suie indiquent quela structure de la détonation n'est pas reproductible près de la limite.Particulièrement pour les espaces annulaires minces, la structure de ladétonation est fortement affectée par la couche limite.</dc:abstract><dc:abstract>In this study, the near-limit propagation of detonations in annular channelsis investigated. Stoichiometric mixtures of methane-oxygen, acetylene-nitrousoxide diluted with 50% argon and acetylene-oxygen diluted with 70% argon areused in the experiments. Detonation velocity as well as smoked foil records ofnear-limit detonations are obtained. It is found that by normalizing the channellength scale by the ZND reaction length, ( i.e. L/ΔZND, where L is the channelgap) the velocity variations for different channels coalesces to a single curve.Thus the detonation velocity in annular channels depends on the relative rolebetween the geometry and the chemical sensitivity of the mixture. Detonationvelocities of the order of half Chapman-Jouguet (CJ) values were observed nearthe limit which tend to suggest instabilities provide the mechanism to maintainthe propagation at such low velocities. The results obtained were compared toFay's velocity deficit model. Qualitative agreement was obtained for acetyleneoxygendiluted with 70% argon mixtures, but fails to predict in methane-oxygenmixtures. Smoked foil records indicate the detonation structure is irreproduciblenear the limit. Particularly in thin channels, the detonation structure is significantlyaffected by the boundary layer.</dc:abstract><ual:supervisor>John H. S. Lee</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/5m60qw91g.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/w95054648</ual:fedora3Handle><dc:subject>Engineering - Mechanical</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Adn39x5352"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Faculty of Religious Studies</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>The narrative and discursive references to children and audience duality in The Gospel of Mark</dcterms:title><ual:dissertant>Farr, Eric</ual:dissertant><dc:abstract>The present thesis examines the rhetorical interaction of the narrative (5.21-43; 7.24-30; 9.14-29) and discursive (9.33-37; 10.13-16) instances of child language in Mark, and analyzes how and to what effect Markan child language is figured rhetorically to address distinctly the elite and non-elite tiers of the Gospel's double audience, according to Henderson's dual audience theory. It argues that the narrative child healings construct an inscribed conception of the child and the parent-child relationship that exerts a controlling influence over the reading/hearing experience of the more explicitly argumentative child discourses. This approach seeks to clarify Mark's persuasive project of advancing, on the one hand, a self-sacrificial form of community leadership addressed to proto-Christian elites, and, on the other, an intimate form of personal devotion to Christ, addressed to non-elites. In doing so, I hope to contribute to the growing discussions concerning the nature and understanding of children and childhood in the ancient world and in early Christianity, the make-up of the Markan audience, and the power dynamics and differentials of the proto-Christian community projected by the Gospel. </dc:abstract><dc:abstract>La présente thèse examine la rhétorique qui sous-tend les cas narratifs (de 5,21 à 43; de 7,24 à 30; de 9,14 à 29) et discursifs (9,33 à 37; de 10,13 à 16) dans la langue de Marc portant sur les enfants, et analyse comment et dans quelle mesure la langue Marcan relative aux enfants est présentée comme rhétorique pour s'adresser clairement aux niveaux élites et non-élites de la double audience à laquelle s'adresse l'Évangile, en se fondant sur la théorie développée par Henderson. La thèse soutient que les récits de guérisons d'enfants favorisent la construction d'une conception inscrite de l'enfant et de la relation parent-enfant, et que cette conception exerce une influence déterminante sur la lecture / l'audition des discours formellement argumentatifs. Cette approche cherche à clarifier le projet persuasif de Marc visant à promouvoir, d'une part, une forme de leadership communautaire fondé sur le sacrifice de soi qui cible les élites proto-chrétiennes, et d'une autre part, une forme intime de dévotion personnelle au Christ s'adressant aux non-élites. J'espère, de cette façon, contribuer au débat d'idées croissant sur la nature et la compréhension des enfants et de l'enfance dans le monde ancien et à l'aube du christianisme, sur la formation de l'audience Marcan, puis sur les dynamiques du pouvoir et des clivages au sein de la communauté proto-chrétienne projetée par l'Évangile.</dc:abstract><ual:supervisor>Ian H. Henderson</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/nz8064229.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/dn39x5352</ual:fedora3Handle><dc:subject>Religion - Biblical Studies</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Abz60d173p"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Mathematics and Statistics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Optimization and packings of T-joins and T-cuts</dcterms:title><ual:dissertant>Edwards, Katherine</ual:dissertant><dc:abstract>Soit G un graphe, et T un sous-ensemble de ses sommets de cardinalité pair. Nous appelons (G,T) une greffe. Définissons par T-jointure tout sous-graphe de G dans lequel les sommets de degré impair sont précisement ceux de l'ensemble T, et par T-coupure tout coupure delta(S) où S contient un nombre impair de sommets de T. Une question intéressante en optimisation combinatoire est celle de trouver les T-jointures et T-coupures optimales. Nous donnons un aperçu de divers problèmes d'optimisation auxquels ceux-ci s'appliquent, ainsi que plusieurs algorithmes pour trouver les T-jointures et T-coupures optimales.Nous considérons ensuite un problème d'empaquetage dans les greffes. C'est une observation facile que le nombre de T-jointures arête-disjointes dans le graphe G est au maximum le nombre d'arêtes dans quelconque T-coupure. Cependant on ne sait pas exactement quand ces quantités sont égales. Il a été conjecturé par Guenin que si G est planaire, que tous les T-coupures de G ont la même parité et que et le nombre d'arêtes dans chaque T-coupure est au moins k, alors G contient k T-jointures arête-disjointes. Quand k = 3 la question est équivalente au théorème des quatre couleurs, et le cas k = 4, ce qui a été conjecturé par Seymour, et k = 5 ont été prouvés par Guenin. Récemment, le cas k = 6 a été réglé par Dvorak, Kawarabayashi et Kral. Dans cette thèse, nous donnons une preuve pour le cas k = 7.</dc:abstract><dc:abstract>Let G be a graph and T an even cardinality subset of its vertices. We call (G,T) a graft. A T-join is a subgraph of G whose odd-degree vertices are precisely those in T, and a T-cut is a cut delta(S) where S contains an odd number of vertices of T. An interesting question from a combinatorial optimization perspective is that of finding optimal T-joins and T-cuts. These have applications in various places. We give an overview of several such optimization problems, as well as several algorithms for finding optimal T-joins and T-cuts from the literature.We then consider a packing problem in grafts. It is a simple observation that the number of edge-disjoint T-joins is at most the number of edges in any T-cut. However it is not known exactly when these quantities are equal. It has been conjectured by Guenin that if G is planar and all T-cuts of G have the same parity and the size of every T-cut is at least k, then G contains k edge-disjoint T-joins. The case k = 3 is equivalent to the Four Colour Theorem, and the cases k = 4, which was conjectured by Seymour, and k = 5 were proved by Guenin. Recently, the case k = 6 was settled by Dvorak, Kawarabayashi and Kral. In this thesis, we give a proof of the case k = 7.</dc:abstract><ual:supervisor>Frederick Shepherd</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/cn69m8191.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/bz60d173p</ual:fedora3Handle><dc:subject>Pure Sciences - Mathematics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Axp68km273"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Plant Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Sources of inocula of Nosema ceranae and Nosema apis in the European honeybee and their seasonal patterns in Québec</dcterms:title><ual:dissertant>Copley, Tanya</ual:dissertant><dc:abstract>La nosémose des abeilles domestiques, Apis mellifera L., est causée par deux espèces microsporidiennes: Nosema ceranae et N. apis.  Elles infectent principalement l'intestin moyen des abeilles et sont la troisième cause de perte d'abeilles au Canada.  Malgré ceci, très peu est connu de leurs épidémiologies.  Cet étude a examiné les deux espèces en utilisant un duplex de PCR quantitatif  pour examiner les variations saisonnières des espèces et divers manières de transmission (glandes et débris de ruche) en des ruches naturellement infectées entre juin 2008 et juillet 2010.    Les résultats ont démontré différentes variations saisonnières et que N. ceranae déplace N. apis.  Les deux espèces étaient présentes dans les glandes ce qui suggère que leurs sécrétions peuvent transmettre les parasites.  Elles étaient aussi présentes dans les débris et les fèces ce qui suggère que ces matières peuvent servir de vecteurs d'infections.  De plus, ces résultats proposent une méthode non invasive afin de combattre Ia nosémose des abeilles domestiques car il n'est pas nécessaire de sacrifier des abeilles afin de détecter Ia présence de parasites.</dc:abstract><dc:abstract>Noseomsis of the European honeybee, Apis mellifera L., is caused by two species of Microsporidia: Nosema ceranae and N. apis. Both species infect primarily the midgut of adult honeybees and have been reported as the third major cause of honeybee losses in Canada.  Despite their importance few studies have examined their epidemiology.  The current study monitored both species using duplex quantitative Real-Time PCR to determine the seasonal patterns, and potential transmission sources (glands and hive debris) in naturally infected hives from June 2008 to July 2010. Seasonal patterns of both Nosema species differed throughout the years with N. ceranae displacing N. apis.  Results also demonstrated the presence of both species in glands suggesting that gland secretions may transmit the parasites.  Hive debris and frass also contained spores suggesting that it may transmit the parasites and that bees need not be killed to determine if a hive is infected. </dc:abstract><ual:supervisor>Suha Jabaji</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/p2677076z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/xp68km273</ual:fedora3Handle><dc:subject>Agriculture - Animal Pathology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A6682x7943"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Integrated Studies in Education</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>The praxis of critical pedagogy and educational reform: analyzing media coverage of the U.S. charter school movement</dcterms:title><ual:dissertant>Churchill, Andrew</ual:dissertant><dc:abstract>American charter schools are entering their third decade. What began in 1991 as a single piece of legislation in a single state has grown to include legislation in 40 states and the opening of over 5,000 schools. With the passing of No Child Left Behind legislation in 2002, and the 2010 Race to the Top stimulus funding bill, charter schools have been thrust to the forefront of legislation around educational reform, specifically educational reform of urban schools with large populations of minority students where there is a long history of a significant achievement gap. This dissertation uses critical pedagogy to analyze the media coverage of the American charter school movement.The dissertation has two sections. The first focuses on theory and develops a critical pedagogical framework to discuss understandings of education and education reform. This section begins with an autobiographical exploration of ideas about education and then explores critiques of neoliberalism and mainstream theories of intelligence to challenge these ideas. The section concludes by using postformalism and critical pedagogy to develop a critical pedagogical vision for education and education reform. The second section uses this vision to analyze selected media coverage of American charter schools by detailing the coding, analysis, observations and responses to articles appearing in Time, U.S. News and World Report, Newsweek, and The New York Times referring to charter schools. The dissertation concludes with summary observations detailing how the media coverage of charter schools reinforces common sense understandings of schools, while failing to critically interrogate systemic societal issues of social injustice. It also details an important consideration for critical scholars about how past “blame the victim” discourses appear to have shifted to be focused on blaming educational failures on teachers. </dc:abstract><dc:abstract>Les écoles à charte américaines amorcent leur troisième décennie. Ce qui a débuté en 1991 comme un seul acte législatif dans un seul état s'est amplifié au point de régir 40 états et de conduire à l'ouverture de plus de 5000 écoles. Avec l'adoption de la loi « No Child Left Behind » (pas d'enfants laissés de côté) en 2002, et le projet de loi de financement pour stimuler la relance 2010 « Race to the Top » (course vers le sommet), les écoles à charte ont été propulsées à l'avant-plan de la législation en matière de réforme de l'éducation, plus précisément de la réforme de l'éducation des écoles urbaines qui comptent une grande partie d'élèves de groupes minoritaires qui accusent, depuis longtemps, un important déficit de réussite. C'est par le biais de la pédagogie critique que cette dissertation analyse la couverture médiatique du mouvement des écoles à charte américaines.Cette dissertation comprend deux sections. La première est axée sur la théorie, élaborant un cadre pédagogique critique qui va à l'encontre de la compréhension générale ayant trait à l'éducation et la réforme de l'éducation. Cette section débute par une exploration autobiographique d'idées pour ensuite explorer des critiques du néolibéralisme et des théories courantes de l'intelligence pour remettre ces idées en question. Cette section se termine en ayant recours au postformalisme et à la pédagogie critique pour élaborer une vision pédagogique cruciale pour l'éducation et la réforme de l'éducation. La seconde section reprend cette vision pour analyser la couverture médiatique choisie des écoles à charte américaines en expliquant le codage, les analyses, les observations et les réactions aux articles traitant des écoles à charte parus dans le Time, U.S. News and World Report, Newsweek et The New York Times.La conclusion de cette dissertation cerne des observations sommaires précisant comment la couverture médiatique des écoles à charte renforce de la compréhension générale des écoles alors qu'elle omet de se pencher de façon éclairée sur les problèmes sociétaux systémiques de l'injustice sociale. Cette conclusion décrit aussi en détail un élément important pour les érudits critiques, soit le fait que les discours d'autrefois sur « blâmer la victime » semblent avoir bifurqué pour rendre les professeurs responsables des échecs des étudiants.</dc:abstract><ual:supervisor>Shirley Steinberg</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/vx021k25j.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/6682x7943</ual:fedora3Handle><dc:subject>Education - General</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A8s45qd85c"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Controlling light with slab photonic crystal</dcterms:title><ual:dissertant>Khorshidahmad, Amin</ual:dissertant><dc:abstract>Cette thèse présente des concepts et plans originaux pour des applications en démultiplexage de longueur d'onde, conversion de fréquence et génération de multi-longueur d'onde, obtenue par la conception de la structure de bande et le contrôle dynamique de la dispersion de la plate-forme de cristaux photoniques (CPh) planaires.Un démultiplexeur superprisme composite, par lequel la compensation de la diffraction ainsi que la séparation angulaire des canaux réduit considérablement l'encombrement requis par les superprismes classiques, est proposé. Un modèle de conception est développé et appliqué afin d'optimiser et d'analyser les propriétés de mise à l'échelle du démultiplexeur. L'élargissement de la bande passante du superprisme classique dans une exploitation compacte hétéro-réseau stratifiée dans un schéma de réflexion est également étudiée. L'augmentation du décalage de fréquence adiabatique réalisable par le changement structurel qui est induit par la modification de l'indice de réfraction de la dalle dans une cavité imbriquée est présenté. De plus, la conversion de longueur d'onde grâce aux résonateurs imbriqués et dynamiquement reconfigurables qui sont proposés dans cette thèse est étudiée. Dans ce schéma, la modulation ultra-rapide de l'indice de réfraction, par exemple via des porteurs libres induits, transforme les photons accumulés dans la cavité d'origine en un ensemble distinct de modes propres d'un résonateur configuré dynamiquement. En conséquence, un décalage en fréquence arbitraire, déterminé par la séparation spectrale des résonances de la cavité initiale et celles du résonateur accordé, est réalisable à condition qu'un réglage soit fait rapidement. Ce système peut aussi éliminer la conversion de fréquence adiabatique qui accompagne normalement la transition entre les modes dans une cavité statique. La conception de sources peigne de fréquences optiques à spectres accordables par le contrôle dynamique de la configuration, l'adaptation de la dispersion et l'utilisation de la symétrie des profils des modes dans les résonateurs imbriqués est également proposée et démontrée numériquement.</dc:abstract><dc:abstract>This thesis presents novel designs and schemes for wavelength demultiplexing, frequency conversion and multi-wavelength generation applications, achieved by engineering the band structure and dynamic control of the dispersion in planar photonic crystal (PhC) platform.  The composite superprism demultiplexer, whereby simultaneous diffraction compensation and angular channel separation considerably reduces the footprint required in conventional superprisms, is proposed. A design model is developed and applied to optimize and analyze the scaling properties of the demultiplexer. Expanding the bandwidth of the conventional superprism in a compact stratified hetero-lattice operating in reflective scheme is also investigated. Increasing the adiabatically achievable frequency shift by the structural change induced via tuning the slab refractive index of a nested cavity is shown. Wavelength conversion using the proposed dynamically reconfigurable nested resonators is further studied. In this scheme, ultrafast modulation of the refractive index, e.g. via induced free carriers, transforms the photons stored within the original cavity into the distinct set of eigenmodes of a dynamically formed resonator. As a result, an arbitrary frequency shift, determined by the spectral separation of the resonances of the initial and the tuned cavities, is achievable provided a fast enough tuning. This scheme may also eliminate the adiabatic frequency conversion that normally co-exists with intermodal transitions in a static cavity. Optical comb sources with tunable spectrum by dynamically controlling the reconfiguration, tailoring the dispersion and utilizing the symmetry of the mode profiles in nested resonators is also proposed and numerically demonstrated.</dc:abstract><ual:supervisor>Andrew G. Kirk</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/q524js95w.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/8s45qd85c</ual:fedora3Handle><dc:subject>Engineering - Electronics and Electrical</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Atd96k6555"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Physics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Single-particle production and photon-hadron correlations in proton-proton collisions at next-to-leading order</dcterms:title><ual:dissertant>Coull, Jonathan</ual:dissertant><dc:abstract>In this thesis, we study photon production and photon-hadron correlations at next-to-leading order (NLO) in proton-proton collisions, in both the cases of isolated and non-isolated photons. To begin, fundamentals of perturbative Quantum Chromodynamics (pQCD) are reviewed, with an emphasis on describing how to compute spectra for single particle production and correlated pairs at both leading order (LO) and NLO in hadronic collisions. In particular, a discussion of infrared and collinear singularities at NLO will provide a natural introduction to the concept of factorization. These results are then specifically applied to the case of computing photon and pion single-particle cross-sections in proton-proton collisions at Relativistic Heavy-Ion Collider (RHIC) and Large Hadron Collider(LHC) energies, and compared to experimental data from the PHENIX and CMS experiments respectively. Included in this will be a short study of the theoretical systematic uncertainty generated by the dependence on factorization scales, and a discussion of the modifications needed when moving from inclusive to isolated observables. Following this, double inclusive cross-sections for production of photon-tagged hadrons are computed and compared to data from PHENIX. Once again, a full NLO treatment is given, and the effects of isolation are implemented. We will conclude with a final section on the application of photon-hadron correlations to the tomographic mapping of energy loss in heavy-ion collisions.</dc:abstract><dc:abstract>Dans cette dissertation, nous étudions la production au seconde ordre (NLO) de photons ainsi que la corrélation entre photons durs et hadrons dans les collisions proton-proton. Dans les deux cas est inclus l'effet de l'application de critères d'isolement sur les photons. Nous commençons par une synthèse des résultats fondamentaux de la chromodynamique quantique perturbative (pQCD), dans laquelle nous résumons les méthodes principales pour calculer des sections efficaces aux premier et second ordres. En particulier, nous discutons des singularités infrarouges et colinéaires qui se produisent au seconde ordre, ce qui nous amènera à introduire la théorie de la factorisation. Ces outils sont par la suite utilisés pour calculer les section efficaces pour la production de photons et de pions dans les collisions proton-proton aux énergies atteintes au Relativistic Heavy Ion Collider (RHIC)et au Grand collisionneur de hadrons (LHC). Ces résultats sont comparés aux données des expériences PHENIX et CMS respectivement. Nous étudions aussi l'incertitude théorique due aux choix des échelles de factorisation et la différence entre les sections efficaces inclusives et isolées. Finalement, nous calculons les sections efficaces pour les paires photon-hadron corrélées et comparons les résultats aux données de PHENIX, tout en incluant les effets de second ordre et d'isolement. Nous concluons avec une description qualitative de l'application des corrélations pour établir une tomographie des collisions d'ions lourds.</dc:abstract><ual:supervisor>Charles Gale</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/8k71nn200.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/td96k6555</ual:fedora3Handle><dc:subject>Physics - Elementary Particles and High Energy</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Avh53x0994"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Psychiatry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Self-criticism and personal standards dimensions of perfectionism and subjective well-being over three years: the mediating role of basic psychological needs</dcterms:title><ual:dissertant>Ma, Denise</ual:dissertant><dc:abstract>Cette étude d'adultes de la communauté (N = 164) a examiné le rôle des besoins psychologiques fondamentaux dans la relation entre l'autocritique (SC) et les standards personnelle (PS) dimensions du perfectionnisme et du bien-être subjectif au cours de trois ans. Les participants ont rempli des questionnaires dans notre laboratoire pour évaluer les dimensions du perfectionnisme, la satisfaction des besoins, et le bien-être subjectif au Temps 1, Année 2, et Année 3, respectivement. Contrairement aux standards personnelle (PS), l'autocritique (SC) était liée à une réduite de satisfaction des besoins pour le sentiment de rapport, la compétence et l'autonomie à l'Année 2; et une réduite dans la satisfaction de la vie, la vitalité et l'affect positif et un augmentation à l'affect négatif l'Année 3. Des analyses causales ont démontré que les trois besoins à l'Année 2 sont des médiateurs de la relation entre l'autocritique (SC) à Temps 1 et la diminution dans la satisfaction de vie et la vitalité à l'Année 3. Ces résultats démontrent l'importance d'essayer d'augmenter la satisfaction des besoins psychologiques fondamentaux afin d'augmenter le bien-être subjectif et de réduire la vulnérabilité à la dépression dans les individus ayant hauts niveaux d'autocritique perfectionnisme.</dc:abstract><dc:abstract>This study of community adults (N = 164) examined the role of basic psychological needs in the relation between self-criticism (SC) and personal standards (PS) dimensions of perfectionism and subjective well-being over three years. Participants completed in-lab questionnaires assessing dimensions of perfectionism, needs satisfaction, and subjective well-being at Time 1, Year 2, and Year 3, respectively. In contrast to PS, SC was related to lower satisfaction of needs for relatedness, competence, and autonomy at Year 2; and lower life satisfaction, vitality, and positive affect, and higher negative affect at Year 3. Path analyses demonstrated that all three needs at Year 2 mediated the relation between Time 1 SC and lower life satisfaction and vitality at Year 3.  These findings demonstrate the importance of trying to increase the satisfaction of the basic psychological needs in order to increase subjective well-being and reduce vulnerability to depression in individuals with higher self-critical perfectionism. </dc:abstract><ual:supervisor>Brett David Thombs</ual:supervisor><ual:supervisor>David Michael Dunkley</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/0r9677661.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/vh53x0994</ual:fedora3Handle><dc:subject>Psychology - Personality</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Asx61dr23b"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Psychology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Exploring mechanisms of typical and abnormal cognitive development: neurodevelopmental computational models of theory of mind and general intelligence</dcterms:title><ual:dissertant>Berthiaume, Vincent</ual:dissertant><dc:abstract>A useful approach to better understand the mechanisms underlying cognitive development has been that of constructive artificial neural networks (CANNs). This thesis presents several CANN models that contribute to our understanding of two typical and abnormal developmental phenomena.The first two manuscripts explore the mechanisms underlying false-belief (FB) task transitions. Typically-developing preschoolers go through two transitions on verbal FB tasks, in which they have to say where an agent will search to find (approach) or avoid (avoidance) an object that was moved from location A to location B in the agent's absence. Transition 1 occurs as children go from failure to success on the approach task, and Transition 2 occurs as children go from succeeding only at approach to succeeding also at avoidance tasks. Are these transitions due to learning about beliefs or to other factors? The first manuscript presents a model of a non-verbal FB task (which uses looking time rather than a verbal measure). The model captured the transitions observed with verbal tasks, predicting that transitions would be observed on non-verbal tasks. Results suggest that initial failure could be due to observing more true-belief (TB) than FB searches, and that Transition 1 may not be due to learning about beliefs but to overcoming default TB attributions by learning to distinguish FB from TB situations. Results also suggest Transition 2 may be due to avoidance goals being represented by more varied behaviour than approach goals.Autistic children usually fail at verbal approach FB tasks, even when they are older than the typical age of success. The second manuscript explores the impact of simulating specific autistic deficits on Transition 1. First, it is thought that social deficits in autism may be related to abnormal connectivity between the brain regions used in FB tasks. I explored this hypothesis by impairing in one group of networks the connectivity of the input unit providing the information about the agent, while in a second group of networks I impaired a start or end location input unit. Results suggest that the information from the agent node is computationally crucial to Transition 1, as only the first group had impaired performance. I next simulated the decreased autistic attention to social stimuli by replacing a random half of all network training patterns by random patterns, simulating observations of random situations. Because there is currently some doubt as to whether specific, early behavioural treatment of autism improves later deficits, I simulated different times of treatment by manipulating the duration of the attention impairment in networks. As the duration of the impairment was reduced, performance progressively improved, showing that computationally, early treatment can be beneficial for autism. In the third manuscript, I explored whether white-matter integrity (WMI) could be manipulated to simulate a range of performances on Raven's Standard Progressive Matrices (SPM), a popular test of intelligence requiring subjects to analyze a matrix to find which figure, out of a few alternatives, best fits the missing figure in the matrix. Different levels of WMI have been associated with typical, age-related cognitive improvements and decline, as well as with preterm birth. To explore the effects of different levels of WMI, I incorporated different noise proportions in the activation values of my SPM model. Best performance was obtained with no impairment, but as WMI was reduced, the model's success rate was lowered to first capture the success rate of typically-developing 9-year-olds on the SPM, and with more noise it then captured the performance of 9-year-olds born preterm. These results thus computationally support a link between WMI and typical and impaired cognitive development.In sum, these results show that CANNs are unique tools to advance our understanding of typical and abnormal mechanisms of development.</dc:abstract><dc:abstract>Une approche utile à la compréhension des mécanismes du développement cognitif sont les réseaux de neurones constructifs (RNCs). Cette thèse présente plusieurs modèles de RNCs améliorant notre compréhension de deux phénomènes typiques et anormaux du développement.Les deux premiers manuscrits explorent les mécanismes de transitions sur les tâches de fausse croyance (FC). Les enfants à croissance typique traversent deux transitions sur les tâches de FC verbales, dans lesquelles ils doivent dire où un agent cherchera pour trouver (approche) ou éviter (évitement) un objet déplacé de A à B durant son absence. La Transition 1 a lieu alors que les enfants passent de l'échec au succès des tâches d'approche, et la Transition 2 alors qu'ils passent du succès seulement aux tâches d'approche au succès des deux tâches. Ces transitions sont-elles dues à l'apprentissage des croyances ou à d'autres facteurs? Le premier manuscrit présente un modèle d'une tâche non-verbale de FC (utilisant le temps de regard au lieu d'une mesure verbale). Le modèle a reproduit les transitions observées avec les tâches verbales, prédisant des transitions avec les tâches non-verbales. Les résultats suggèrent que les transitions ne sont pas dues à l'apprentissage des croyances; la Transition 1 serait due au fait de surmonter une attribution par défaut de vraies croyances (VCs) en distinguant les situations de FCs et VCs, tandis que la Transition 2 serait due au but d'évitement étant représenté par des comportements plus variées que le but d'approche.Les enfants autistes échouent habituellement les tâches de FC verbales. Le deuxième manuscrit explore l'effet de simuler des déficits autistes sur la Transition 1.Premièrement, les déficits sociaux autistes pourraient être reliés à une connectivité anormale entre les régions du cerveau utilisées dans les tâches de FC. J'ai exploré cette hypothèse en endommageant dans un premier groupe de réseaux la connectivité de l'unité d'entrée représentant l'agent, et dans un deuxième groupe la connectivité d'une unité d'entrée représentant le départ ou l'arrivée de l'objet. Les résultats suggèrent que l'information de l'unité d'agent est critique pour la Transition 1, car seulement le premier groupe a échoué la tâche. J'ai ensuite simulé l'attention réduite aux stimuli sociaux chez l'autiste en remplaçant la moitié de toutes les situations d'entrainements par des nombres aléatoires, simulant des observations d'autres objets ou stimuli. Puisqu'il y a des doutes sur l'utilité de traitements comportementaux en jeune âge chez l'autiste, j'ai simulé différent temps de traitements en manipulant la durée de l'endommagement de l'attention. Devancer le début du traitement a eu pour effet d'améliorer progressivement la performance des réseaux, démontrant que les traitements des comportements autistes peuvent être bénéfiques. Dans le troisième manuscrit, j'ai exploré si des variations dans l'intégrité de la substance blanche (ISB) pouvaient simuler différentes performances sur les Standard Progressive Matrices (SPM) de Raven, un test d'intelligence dans lequel on doit trouver la figure qui complète le mieux une matrice de figures. Différent niveaux d'ISB ont été reliés avec l'amélioration et le déclin typique de la cognition avec l'âge, ainsi qu'avec la naissance prématurée. Afin d'explorer les effets de différents niveaux d'ISB, j'ai incorporé différents niveaux de bruits dans les activations neuronales de mon modèle des SPM. La meilleure performance a été obtenue avec le modèle non-endommagé, mais alors que l'ISB a été réduite, le taux de succès du modèle a d'abord rejoint celui d'enfants au développement typique, et ensuite celui d'enfants nés prématurément. Ces résultats supportent donc un lien entre l'ISB et le développement typique et anormal de la cognition.En somme, ces résultats démontrent que les RNCs sont des outils uniques pour améliorer notre compréhension des mécanismes typiques et anormaux du développement.</dc:abstract><ual:supervisor>Kristine Onishi</ual:supervisor><ual:supervisor>Thomas R. Shultz</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/5t34sp52j.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/sx61dr23b</ual:fedora3Handle><dc:subject>Psychology - Developmental</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A736668954"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Novel prediction and end-to-end estimation techniques for error resilient video coding</dcterms:title><ual:dissertant>Bangalore Satyan, Ramdas</ual:dissertant><dc:abstract>In recent years video communication over packet switched networks such as the Internet has become universal and very important. These networks currently provide very limited or no end-to-end Quality of Service. Transmission of compressed video over these networks is thus highly susceptible to errors and packet losses. This problem is severe and leads to a substantial deterioration of received video quality.This dissertation investigates end-to-end distortion estimation techniques and novel prediction schemes for enhancing the error resilience of video transmission over unreliable networks, such as Internet and wireless networks. The major objective is to balance the tradeoff between coding efficiency and robustness within a rate distortion framework, so that a good reproduction quality is achieved under the bit-rate budget and network constraints. The overall quality of video transmission depends on several factors including the source and channel coding schemes, channel constraints and decoder recovery schemes. The first part of this dissertation is motivated by this fact and focuses on an end-to-end approach. An efficient algorithm is derived to estimate the end-to-end distortion of compressed video at the encoder. This algorithm accounts for the effects of quantization, packet loss, temporal and spatial error propagation and error concealment. This estimate is used to choose the optimal mode for coding each macroblock in a video frame in order to minimize the end-to-end distortion.In the next part of the dissertation the concentration is on a low complexity end-to-end distortion model for applications and devices with less computational power. Specifically a low complexity frame level transmission distortion model is proposed for the latest state of the art video coding standard H.264/AVC. This model is suitable for performing error control by suitable resource allocation for real time video applications. Comparisons are made with similar methods present in literature, revealing the increased accuracy of our proposed solution. The final part of the dissertation focuses on modifying the conventional motion compensated prediction (MCP) framework of a video codec to improve the error resilience of H.264/AVC when transmitted over these networks. The entire MCP is redesigned at both the encoder and decoder and three novel prediction schemes are proposed to achieve a good tradeoff between efficiency and robustness.</dc:abstract><dc:abstract>Depuis quelques années, la transmission de vidéos sur les réseaux de commutation par paquets comme l'Internet est devenue universelle. Ces réseaux procurent une qualité de service point-à-point très limitée, sinon inexistante. La transmission de vidéos compressés sur ces réseaux est par conséquent très susceptible aux erreurs et aux pertes de paquets. Ce problème est sévère et entraîne une détérioration significative de la qualité des vidéos.Cette thèse étudie les techniques d'estimation de distortion point-à-point ainsi que de nouveaux schémas de prédiction pour améliorer la tolérance aux erreurs de vidéos transmis sur des réseaux peu fiables comme l'Internet et les réseaux sans-fil. L'objectif principal et d'obtenir un juste équilibre entre l'efficacité du codage et la robustesse à même un cadre de taux de distorsion de façon à obtenir une bonne qualité de transmission pour une série de contraintes et un budget taux-bit donnés. La qualité globale de la transmission vidéo dépend de plusieurs facteurs comme les schémas de codage de source et de codage de canal, les contraintes du canal et les techniques de correction des erreurs. La première partie de cette thèse est motivée par ce fait et insiste sur une approche point-à-point. Un algorithme efficace est présenté pour estimer la distorsion point-à-point de vidéo compressé au niveau du codeur. L'algorithme prend en compte les effets de la quantification, les pertes de paquets, la propagation spatiale et temporelle des erreurs ainsi que la dissimulation des erreurs. Cet estimé est utilisé pour choisir le mode de codage optimal pour chaque macro-bloc des trames vidéo afin de minimiser la distorsion point-à-point.La deuxième partie de cette thèse met l'emphase sur un modèle de distorsion point-à-point à faible complexité pour des appareils à puissance limitée. Plus précisément, un modèle est proposé pour le plus récent standard de codage vidéo H.264/AVC. Ce modèle peut être utilisé pour le contrôle d'erreur en allouant les ressources appropriées pour des applications vidéo en temps réel. Des comparaisons sont effectuées avec des méthodes similaires décrites dans la littérature, démontrant la précision accrue de la solution proposée. La dernière partie de la thèse présente une modification du cadre de prédiction de mouvement compensé (MCP) d'un codec vidéo afin d'améliorer la tolérance aux erreurs du standard H.264/AVC lorsque celui-ci est utilisé sur des réseaux peu fiables. Le MCP est entièrement repensé du côté codeur comme du codé décodeur. Trois nouveaux schémas de prédiction sont aussi proposés pour obtenir un bon compromis entre efficacité et robustesse.</dc:abstract><ual:supervisor>Fabrice Labeau</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/d217qv062.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/736668954</ual:fedora3Handle><dc:subject>Engineering - Electronics and Electrical</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A1v53k219r"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Chemistry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Donor-acceptor dyads for molecular rectifying devices</dcterms:title><ual:dissertant>Kondratenko, Mykola</ual:dissertant><dc:abstract>L'intérêt pour l'électronique moléculaire a commencé dans les années 70, avec la découverte d'Aviram et de Ratner.  Ils ont proposé une dyade donneur-accepteur telle que la molécule TTF–sigma–TCNQ (TTF–tetrathiafulvalene,  liaisons sigma isolées et TCNQ– tetracyanoquinodimethane) qui pourrait fonctionner comme une jonction p-n, jouant le rôle d'une diode unimoléculaire. Ce phénomène est dû à une distribution asymétrique des niveaux électroniques, ainsi qu'au très faible écart HOMO-LUMO (0.3 eV) de cette molécule.Jusqu'à présent, un grand nombre de dyades donneur-accepteur ont été étudiées comme candidates pour la synthèse de redresseurs moléculaires.  Ceux-ci incluent certaines dyades D–sigma–A avec des groupes donneur et accepteur faibles ou modérés, de nombreux dyades D–sigma–A, ainsi que des molécules ayant une forte asymétrie dans leur structure électronique.Dans cette thèse, nous présentons la conception moléculaire, la synthèse et la caractérisation d'une série de donneur-accepteurs avec de différentes combinaisons de groupes connus comme étant électroactifs (TTF-sigma-fluorene, nEDOT-3CNQ, nEDOT-NDI).  Nous décrivons les progrès que nous avons apportés au domaine complexe de redresseurs moléculaires par l'entremise du couplage de puissant donneurs et  accepteurs. Pour réussir cela, nous avons employé différentes stratégies dont: l'utilisation d'intermédiaires avec des propriétés  oxido-réductives modérées et des dérivés très réactifs pour empêcher la formation de complexes à transfert de charge ainsi que de se servir de ces mêmes  complexe pour obtenir  des liaisons covalentes.La synthèse utilisée explore deux approches qui permettent la liaison des dyades à la surface de l'électrode: (1) l'utilisation de structures amphiphiles permettant la déposition de monocouches moléculaires par la technique Langmuir-Blodgett et (2) l'utilisation de  groupes de thiol et dedisulfure permettant la liaison covalente des molécules avec des métaux. La caractérisation de ces monocouches  a été fait à l'aide de techniques spectroscopiques et électrochimiques.L'analyse de la densité, l'ordre des molécules dans les films et leur stabilité a aussi été étudié. Finalement, nous décrivons la fabrication de jonctions électrode/monocouche organique/électrode et nous discutons les résultats des mesures de transport de charges des dyades donneur-accepteur synthétises.</dc:abstract><dc:abstract>The interest in molecular electronics began in the 1970s with the work of Aviram and Ratner, who proposed that a donor-acceptor dyad, specifically TTF–sigma–TCNQ molecule (TTF–tetrathiafulvalene, sigma–nonconjugated bridge and TCNQ– tetracyanoquinodimethane), can resemble the electric properties of a p-n junction, acting as a unimolecular diode. The reason of such behaviour lies in asymmetrically distributed electronic levels, and very low HOMO-LUMO gap (0.3 eV) that was imposed for the model molecule. Up to date, numerous donor-acceptor dyads were investigated as candidates for molecular rectifiers, which included some D–sigma–A dyads with weak or moderate donor and acceptor moieties, numerous D–sigma–A and also molecules without obvious asymmetry in the electronic structure. However, neither the original TTF–sigma–TCNQ molecule nor any other molecule with similar HOMO–LUMO gap have been studied in molecular electronics applications, which was due to synthetic unavailability of such molecules.In this thesis we present molecular design, synthesis as well as characterization of series of Donor-Acceptor dyads with different combinations of well studied electroactive moieties (TTF-sigma-fluorene, nEDOT-3CNQ, nEDOT-NDI). Herein we describe our progress towards the main synthetic challenge in the field of molecular rectifiers – coupling together strong donor and strong acceptor molecules. To achieve this we employed different synthetic strategies, namely, use of intermediates with moderated redox properties and highly reactive derivatives to avoid formation of charge-transfer complexes between donor and acceptor as well as utilization of the donor-acceptor complexation which results their in covalent binding. The synthetic design includes two types of approaches allowing binding the dyad molecules to electrode surface: (1) amphiphilic structure which enables deposition of molecular monolayers via Langmuir-Blodgett technique and (2) thiol/disulfide functionality suitable for covalent grafting of molecules to metals. Characterization of such monolayers by different spectroscopic and electrochemical techniques as well as analysis of the alignment and packing of the molecules within the films as well as monolayers stability is discussed. Finally, we describe fabrication of Electrode/Organic monolayer/Electrode junctions and discuss results of the charge transport measurements of the synthesized donor-acceptor dyads.</dc:abstract><ual:supervisor>Dmytro Perepichka</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/6682x795c.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/1v53k219r</ual:fedora3Handle><dc:subject>Chemistry - Organic</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ajs956m129"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Multicore acceleration of sparse electromagnetics computations</dcterms:title><ual:dissertant>Fernández Becerra, David</ual:dissertant><dc:abstract>Les processeurs multicœurs sont devenus la tendance dominante de l'industrie pour accroître la performance des systèmes informatiques, forçant les concepteurs de systèmes électromagnétiques (EM) à reconcevoir leurs applications en utilisant des paradigmes de programmation parallèle. Cela est particulièrement vrai pour les calculs impliquant des structures de données complexes comme les calculs de matrices creuses qui surviennent souvent dans des simulations électromagnétiques (EM) avec la méthode d'analyse par éléments finis (FÉM).  Ces calculs nécessitent de manipulation de pointeurs qui rendent inutiles de nombreuses optimisations du compilateur et les bibliothèques de mémoire partagée parallèle (OpenMP, par exemple). Ce travail présente de nouvelles structures de données rares et de nouvelles techniques afin d'exploiter efficacement le parallélisme multicœur et les unités de vecteur court (dont le dernier n'a pas été exploité par des bibliothèques de matrices creuses à la fine pointe de la technologie) pour les noyaux de calcul intensif récurrents dans les simulations EM, tels que les multiplications matrice-vecteur rares (SMVM) et des algorithmes à gradient conjugué (CG). Des performances d'accélérations jusqu'à 14 fois supérieures sont démontrées pour le noyau accéléré par SMVM et jusqu'à 5,8 fois supérieures pour le noyau CG en utilisant les méthodes proposées par rapport aux approches conventionnelles pour deux architectures multicœurs différentes. Enfin, une nouvelle méthode pour résoudre la FÉM pour le traitement parallèle est présentée et une implantation optimisée est réalisée sur deux générations d'accélérateurs de GPU NVIDIA (multicœur) avec des augmentations de performances allant jusqu'à 27,53 fois par rapport aux résultats du CPU optimisé par compilateur.</dc:abstract><dc:abstract>Multicore processors have become the dominant industry trend to increase computer systems performance, driving electromagnetics (EM) practitioners to redesign their applications using parallel programming paradigms. This is especially true for computations involving complex data structures such as sparse matrix computations that often arise in EM simulations with the finite element method (FEM). These computations require pointer manipulation that render useless many compiler optimizations and parallel shared memory frameworks (e.g. OpenMP). This work presents new sparse data structures and techniques to efficiently exploit multicore parallelism and short-vector units (the last of which has not been exploited by state of the art sparse matrix libraries) for recurrent computationally intensive kernels in EM simulations, such as the sparse matrix-vector multiplication (SMVM) and the conjugate gradient (CG) algorithms. Up to 14 times performance speedups are demonstrated for the accelerated SMVM kernel and 5.8x for the CG kernel using the proposed methods over conventional approaches for two different multicore architectures. Finally, a new method to solve the FEM for parallel processing is presented and an optimized implementation is realized on two different generations of NVIDIA GPUs (manycore) accelerators with performance increases of up to 27.53 times compared to compiler optimized CPU results.</dc:abstract><ual:supervisor>Dennis Giannacopoulos</ual:supervisor><ual:supervisor>Warren Gross</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/z890rz710.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/js956m129</ual:fedora3Handle><dc:subject>Engineering - Electronics and Electrical</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A5712mb98k"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Earth and Planetary Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>The behaviour of base metals in arc-type magmatic-hydrothermal systems - insights from Merapi volcano, Indonesia</dcterms:title><ual:dissertant>Nadeau, Olivier</ual:dissertant><dc:abstract>Porphyry and high sulfidation epithermal ore-forming systems are genetically associated with calc-alkaline volcanism in subduction zones, and where erosion has not been too deep, the volcanic rocks are still commonly exposed in close proximity to the deposits. Most models for porphyry copper and high sulfidation epithermal gold systems include a shallow magmatic reservoir (the porphyry stock), an overlying hydrothermal cell, its alteration paragenesis and a stratovolcano. Some investigations also discuss the importance of underlying granitoid batholiths as feeders for porphyry stocks and their hydrothermal systems. Although it is commonly believed that the ores deposit during the waning stages of volcanism, given the time span over which these deposits form (tens of thousands to several million years) and the undeniable existence of hydrothermal systems beneath volcanoes, it is quite probable that their formation is initiated at times when volcanoes are still active. Although currently mined ore deposits are excellent places to focus research, subduction zone stratovolcanoes provide important windows on the magmatic-hydrothermal processes at play.This thesis describes an investigation of the magmatic-hydrothermal environment that resides beneath Merapi volcano, Indonesia. The research involved sampling and chemical analysis of minuscule aliquots of evolving silicate and sulfide melts trapped as inclusions at different times and in different locations in growing crystals subsequently ejected during eruptions. The research also involved sampling and analysis of fumarolic gases (and their precipitates) emitted at Merapi volcano during times of quiescence and eruptive activity, as well as compilation of published compositional data for fumarolic gases from other arc volcanoes. These gases are the surface equivalents of ore-forming magmatic-hydrothermal fluids. Finally the research involved compilation from the literature of compositional data for fluid inclusions (micron-scale droplets of magmatic volatile phases) trapped in gangue minerals in porphyry copper deposits. The focus of the research was the behaviour of copper, nickel, cobalt, zinc, lead and molybdenum in magmatic hydrothermal systems.The research reported in Chapter 1 showed that injections of sulfide melt-saturated mafic magma into shallower, more evolved and more oxidized resident magma at Merapi volcano induced exsolution of a magmatic volatile phase from the mafic magma. This hydrothermal fluid dissolved the sulfide melt and became enriched in chalcophile (notably copper) and siderophile metals. An argument is presented that the overpressure generated by the exsolution of a fluid originating in this manner triggered an explosive eruption at Merapi volcano in 2006. This is supported by the observation that the metal content, particularly of copper, was higher in the volcanic gas sampled immediately after this eruption than during periods of quiescence and that metal ratios of the gas are remarkably similar to those of sulfide melt inclusions. In Chapter 2, it is shown that the mafic magma mixed poorly with the more felsic magma, that both magmas evolved via assimilation and fractional crystallization and, most importantly, that the magmatic volatile phase transferred base metals to the more felsic magma. In Chapter 3, the fluid inclusion and volcanic gas data are used to make inferences about the evolution of porphyry ore-forming systems and link mechanisms of ore-formation to those operative during the eruptive cycles of volcanoes. Finally, the thesis integrates the findings of this study into a model that provides new insights into the formation of porphyry copper deposits below stratovolcanoes.</dc:abstract><dc:abstract>Les gîtes de types porphyriques et épithermaux sont génétiquement associés au volcanisme des zones de subduction et les roches volcaniques cogénétiques à ces gisements sont souvent encore présentes. Tous les modèles actuels de mise en place de ces gîtes définissent un réservoir magmatique peu profond, lequel est coiffé d'une cellule hydrothermale et de sa séquence complexe d'altération, ainsi que d'un stratovolcan. Certains auteurs discutent aussi de l'importance de batholites sous-jacents ayant généré le porphyre et ses fluides hydrothermaux. Quoiqu'il soit généralement accepté que ces gîtes se forment durant le déclin du volcanisme, étant donné la longévité des périodes proposées pour la formation de ceux-ci  (de dizaines de milliers à plusieurs millions d'années) et l'existence indéniable de systèmes hydrothermaux associés, il est fort probable que la formation de ces gîtes soit initiée alors que le volcanisme est encore actif. Les volcans situés en zones de subduction représentent d'importants points d'observation des processus magmatiques-hydrothermaux actuels.La présente recherche porte sur l'environnement magmatique-hydrothermal qui existe sous le volcan Mérapi, situé en Indonésie. Des échantillons de liquides silicatés et sulfurés piégés à l'intérieur de cristaux durant leur croissance à différents moments et endroits dans le magma et avant d'être éjectés hors des réservoirs magmatiques lors d'éruptions volcaniques ont été prélevés et dosés. Des gaz fumerolliens de haute température et leurs sublimats émis au volcan Mérapi durant des phases de dégazage passif et d'éruption explosive ont été échantillonnés et analysés. Des résultats similaires pour les gaz d'autres volcans, ainsi que des analyses d'inclusions fluides de systèmes hydrothermaux de porphyres cuprifères ont été compilés à partir de la littérature. Les gaz volcaniques analysés sont les équivalents superficiels des fluides magmatiques-hydrothermaux qui génèrent les gisements métallifères.Dans le premier chapitre, il a été démontré que des magmas mafiques d'origine profonde et saturés en liquide sulfuré ont été injectés dans le réservoir magmatique peu profond de Mérapi, celui-ci contenant un magma plus évolué et plus oxydé. La décompression qu'a subie le magma mafique a provoqué l'exsolution d'une phase magmatique volatile (un fluide hydrothermal) qui a dissous le liquide sulfuré et ses métaux chalcophiles et sidérophiles (notamment le cuivre). La surpression générée par l'exsolution de ce fluide hydrothermal a provoqué l'éruption explosive du volcan Mérapi de mars à août 2006. Ceci est corroboré par l'observation que certains métaux, particulièrement le cuivre, étaient enrichis dans les gaz volcaniques émis après l'explosion par rapport aux niveaux mesurés durant la phase de dégazage passif, et par le fait que les rapports des métaux dans ces gaz post-explosion étaient soudainement semblables à ceux mesurés dans les inclusions sulfurées, alors qu'ils étaient bien différents durant les phases de dégazage passif du volcan. Dans le second chapitre, je démontre que le magma plus mafique et le magma plus felsique ne se sont pas bien mélangés, que les deux magmas ont évolué via l'assimilation de roches encaissantes et la cristallisation fractionnée, et que la phase magmatique volatile qui s'est séparée du magma mafique et qui a dissous le liquide sulfuré a transféré ses métaux au magma plus felsique. Dans le troisième et dernier chapitre, les inclusions fluides et les gaz volcaniques ont été utilisés en conjonction avec les connaissances acquises et décrites dans les deux premiers chapitres afin de proposer un modèle pour l'évolution du système porphyrique et d'établir les liens qui existent entre les mécanismes de formation des gîtes porphyriques et épithermaux acides, et ceux qui opèrent durant les cycles éruptifs des volcans. Un modèle pour la formation des porphyres cuprifères sous les stratovolcans actifs des zones de subduction est finalement proposé.</dc:abstract><ual:supervisor>Anthony E. Williams-Jones</ual:supervisor><ual:supervisor>John Stix</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/dz010v026.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/5712mb98k</ual:fedora3Handle><dc:subject>Earth Sciences - Geology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A76537528h"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Neurology and Neurosurgery</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Distinct immune regulatory properties of human adult microglia and blood-derived macrophages: relevance for multiple sclerosis disease processes</dcterms:title><ual:dissertant>Durafourt, Bryce</ual:dissertant><dc:abstract>In the human central nervous system (CNS), the two major populations of myeloid cells are the blood-derived macrophages and the CNS-resident microglia. Under physiologic conditions, macrophages occupy the perivascular spaces and microglia populate the CNS parenchyma. Both cell types may be recruited to participate in neuroinflammatory responses such as in multiple sclerosis (MS) lesions, as macrophages access the parenchyma and the cell types become difficult to distinguish.  Differential responses of macrophages and microglia to environmental conditions and to therapeutic agents could be important determinants of outcome in neuroinflammatory diseases of the CNS of which multiple sclerosis is the prototype. Experiments described in this thesis were based on use of microglia derived from the adult human CNS and myeloid cells generated from peripheral blood samples. Comparisons were made with regard to responses to the novel MS therapeutic FTY720, experimental polarizing conditions, and exposure to the CNS breakdown product myelin.   As an indication of their propensity to respond to FTY720 treatment, the myeloid cell subsets were examined for their expression of sphingosine-1-phosphate (S1P) receptor mRNA. Human microglia, monocyte-derived dendritic cells (DCs) and macrophages ex vivo express relatively higher levels of sphingosine-1-phosphate (S1P) receptor 1 (S1P1) mRNA as compared to other receptor subtypes. Despite these similar receptor expression patterns, the S1P agonist FTY720 decreased extracellular-signal-regulated kinases (ERK) phosphorylation and induced myosin light chain (MLC) II phosphorylation in macrophages and DCs but not in microglia. FTY720 inhibited interleukin (IL)-12p70 production (CD40L induced) by DCs and macrophages but not microglia (poly I:C induced). IL-10 production was increased in DCs following FTY720 treatment and unaffected in the other myeloid cells. In terms of responses to polarizing conditions, macrophages can be polarized into a continuum of activation phenotypes, the extremes of which are the “pro-inflammatory” M1 and “anti-inflammatory” M2 phenotypes.  These phenotypes have been linked to functional properties including production of inflammation association molecules and phagocytic activity. The phenotypic and functional properties of microglia were compared with macrophages derived from peripheral blood monocytes in response to M1 and M2 inducing conditions. Under M1 conditions, microglia and macrophages up-regulated expression of the M1 markers CCR7 and CD80. M2 treatment of microglia induced expression of CD209 but not the additional M2 markers CD23, CD163 and CD206 expressed by M2 macrophages. M1-polarizing conditions induced production of IL-12p40 by both microglia and macrophages; microglia produced higher levels of IL-10 under M1 conditions than did macrophages. Under M2 conditions, microglia produced comparable levels of IL-10 as under M1 conditions, whereas IL-10 was induced in M2 macrophages subsequently activated with lipopolysaccharide (LPS). Myelin phagocytosis was significantly greater by microglia than macrophages under all conditions; activity was higher for M2 treated cells for both cell types. Our findings delineate distinctive properties of microglia compared to exogenous myeloid cells both under physiological conditions and in response to signals derived from an inflammatory environment in the CNS.  The results that myeloid cells exhibit differential responses to an approved MS therapeutic agent suggest that targeting the distinct myeloid populations and promoting M2 polarization, especially in the infiltrating macrophages, may be a potential therapeutic target to control inflammation in the CNS.</dc:abstract><dc:abstract>Les principales populations de cellules myeloïdes du système nerveux central (SNC) sont les macrophages, provenant de monocytes sanguins, et les microglies, résidant au sein du SNC. Les macrophages occupent normalement l'espace périvasculaire alors que les microglies peuplent le parenchyme du SNC. Le rôle de ces deux types de cellules diffère probablement dans le contexte neuro-inflammatoire caractérisant la sclérose en plaques (SP). Les macrophages accèdent alors au parenchyme et il devient difficile de les distinguer des microglies. Les expériences décrites dans la présente thèse ont été effectuées sur des microglies prélevées du SNC d'humains adultes et de cellules myéloïdes provenant du sang périphérique. Leurs réponses à différents stimuli ont été comparées. Les microglies et macrophages ont été exposés à la molécule FTY720, à des conditions expérimentales de polarisation ou à la myéline, une composante détruite dans la SP. La présence de l'acide ribonucléique codant pour les récepteurs de sphingosine-1-phosphate 1 à 5 (S1P1 à 5) a été utilisée comme indicateur du potentiel de réponse des cellules à l'agoniste FTY720. Il fut observé que les microglies, cellules dendritiques et macrophages dérivés de monocytes exprimaient le récepteur S1P1 à un niveau relativement plus élevé que les autres récepteurs (S1P2-5). En dépit de ce profil d'expression similaire, les voies de signalisation engagées par FTY720 ont différé entre les types de cellules. Les macrophages et cellules dendritiques traités avec FTY720 ont vu leur niveau de phospho-ERK baisser et leur phospho-MLC II augmenter tandis que les niveaux sont restés inchangés chez les microglies. FTY720 a mitigé la production d'interleukine (IL)-12p70 chez les cellules dendritiques et les macrophages (stimulées avec CD40L), mais pas chez les microglies (stimulées avec poly I:C). FTY720 a augmenté la production d'IL-10 chez les cellules dendritiques, mais est demeurée la même chez les autres cellules myéloïdes. Concernant les réponses à des conditions de polarisation, les macrophages peuvent être polarisés dans un spectre continu de phénotypes dont les extrêmes sont les phénotypes pro-inflammatoires « M1 » et anti-inflammatoires « M2 ». Ces phénotypes sont liés à la production de molécules inflammatoires et à l'activité phagocytaire des cellules. La fonction et le phénotype des microglies soumises aux conditions de polarisation pro-M1 et  pro-M2 ont été comparés à celles des macrophages dérivés de monocytes sanguins périphériques dans des conditions semblables. Dans des conditions pro-M1, les microglies et les macrophages augmentaient leur expression des marqueurs M1 CCR7 et CD80.  Les macrophages soumis à des conditions pro-M2 exprimaient les marqueurs C209, CD23, CD163 et CD206. Par contre, les microglies soumises à de telles conditions n'exprimaient  que CD209. Les conditions pro-M1 ont amené les microglies et les macrophages à produire IL-12p40. Cependant, les microglies produisaient davantage d'IL-10 que les macrophages dans ces mêmes conditions. Les microglies stimulées par les signaux pro-M1 et pro-M2 produisaient des niveaux comparables de IL-10 tandis que les macrophages produisaient IL-10 avec des signaux M2 plus LPS. L'activité phagocytaire était plus élevée chez les microglies que chez les macrophages, peu importe la condition polarisante. Elle était cependant augmentée chez les deux types de cellules par des conditions pro-M2. Nos résultats mettent en évidence certaines propriétés distinctes des microglies et des cellules myéloïdes sanguines dans le contexte physiologique et face aux signaux provenant d'un SNC inflammé. Les donnés démontrant un comportement unique chez chacun des types de cellules myéloïdes en réponse à un agent thérapeutique approuvé pour la SP suggèrent que de cibler les populations myéloïdes peut avoir un impact. Favoriser une polarisation M2 chez les macrophages pourrait s'avérer une stratégie visant à contrôler l'inflammation du SNC.</dc:abstract><ual:supervisor>Jack P. Antel</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/h415pf69n.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/76537528h</ual:fedora3Handle><dc:subject>Biology - Neuroscience</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ajm214t33b"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>fre</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of French Language and Literature</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Aspects et enjeux de la représentation culturelle dans la traduction du roman arabe postcolonial en français et en anglais</dcterms:title><ual:dissertant>Ettobi, Mustapha</ual:dissertant><dc:abstract>La traduction du roman arabe postcolonial en français et en anglais présente un cas intéressant où la question de la représentation culturelle peut être explorée de manière à mettre au jour ses aspects et enjeux divers et à montrer le rôle des traducteurs et des traductrices, entre autres acteurs, dans la négociation de la distance culturelle entre le romancier ou la romancière arabe (ou local-e) et le public cible, notamment occidental. Cette thèse est aussi une réflexion approfondie sur l'assimilation et la non assimilation. Elle vise à problématiser leur fonction, effet et valeur axiologique ainsi qu'à remettre en cause certains présupposés sur le fait traductif et sa dimension représentationnelle. Nous adoptons une approche historiciste plus inclusive afin d'étudier les romans et les (re-)traductions de notre corpus, de faire ressortir la complexité de la traduction littéraire et d'enrichir la réflexion sur les modalités de la transposition des traits culturels, notamment la situation de la femme, dans les versions produites.Notre étude offre également un aperçu de la traduction de la littérature arabe entre 1968 et 2004. Elle illustre plusieurs aspects de ce mouvement, y compris ses facteurs économiques, (géo-)politiques, littéraires et culturels. En outre, elle comprend une évaluation d'études déjà faites sur des aspects de cette traduction. Nous y essayons aussi de promouvoir une autre vision de la traduction et de la réception de cette production littéraire plus favorable à la création de goûts diversifiés chez le lectorat et au rapprochement des points de vue des cultures arabes et occidentales à un moment où la connaissance mutuelle est cruciale et où le débat sur le Soi et l'Autre (l'Arabe, le musulman/la musulmane) ne cesse de susciter de l'intérêt au début de ce troisième millénaire. L'éthique de cette traduction est considérée non seulement en termes de préservation ou de gommage de l'altérité, dont les effets ne sont pas nécessairement prévisibles, mais également selon les enjeux esthétiques incontournables de cette activité.</dc:abstract><dc:abstract>The translation of the postcolonial Arabic novel into English and French is an interesting case in which cultural representation can be examined in a way to shed light on its diverse aspects and repercussions as well as explore the role of the translators, among other agents, in negotiating the cultural distance between Arab novelists and the target audience, namely the Western one. This thesis looks closely at the notions of assimilation and non assimilation. It aims at reviewing their function, effect and axiological value as well as reconsidering some common assumptions about translation and its representational dimension. A historicist and more inclusive approach is adopted so as to study the novels and (re)translations selected, to highlight the complexity of literary translation and to further explore the ways in which cultural traits, especially the situation of women, are rendered in the translated versions.Moreover, this study offers a brief overview of the translation of Arabic literature into French and English from 1968 to 2004. It illustrates several aspects of the latter, including economic, (geo)political, literary and cultural factors. In addition, it presents an evaluation of relevant studies and analyses conducted on the translation of Arabic literature into these languages. An attempt is made to promote a different vision of this translation, one that is more favourable to the creation of diversified tastes for Arabic literary works and to greater convergence of the worldviews of Arab and Western cultures at a time when mutual understanding is crucial and the debate on the relations between Self and Other (the Arab and/or the Muslim) has proven increasingly relevant since the beginning of the third millennium. The ethics of the translation from Arabic into English and French is discussed not only in terms of the preservation or omission of cultural alterity, the effects of which are not necessarily predictable, but also according to the undeniable aesthetic issues raised by this activity. </dc:abstract><ual:supervisor>Michelle Laura Hartman</ual:supervisor><ual:supervisor>Gillian Lane</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/kp78gm59m.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/jm214t33b</ual:fedora3Handle><dc:subject>Literature - Middle Eastern</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A8336h576k"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Architecture</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Food paths, architecture and urban form. A case study</dcterms:title><ual:dissertant>Farah, Leila</ual:dissertant><dc:abstract>Les chercheurs en architecture ont étudié l'origine et le développement des colonies dans le Nouveau Monde et leurs travaux documentent des aspects tels que la division territoriale, l'infrastructure, la planification urbaine, les matériaux et techniques de constructions. Toutefois, les répercussions physiques que les activités journalières des colons ont eues sur l'environnement qu'ils ont occupé, particulièrement celles liées à l'alimentation, n'ont pas fait l'objet d'études approfondies. Cette thèse explore les relations entre la subsistance et l'architecture au cours du 17ième et 18ième siècles dans l'établissement de Montréal en Nouvelle France. Elle étudie comment les aliments se sont déplacés depuis les espaces de production aux lieux de consommation et de déchet. Pour cela, elle introduit le concept de trajectoires alimentaires qu'elle analyse selon les catégories suivantes: céréales, viandes, fruits et légumes. Ce travail examine les liens entre les flux alimentaires, la forme urbaine, le bâti, la structure sociale et les rythmes temporels de l'établissement humain en question.  Méthodologiquement, les nombreux inconnus liés à la période de recherche et à la nature périssable des aliments ont constitué un défi de taille et ont requis une approche interdisciplinaire. Ce travail s'appuie sur des sources iconographiques et manuscrites (plans, carnets de voyage, ordonnances, actes notariés, débris archéologiques) liés au paysage, à la forme urbaine, à l'architecture et aux artéfacts de l'époque et du site en question. Cette thèse contribue à une nouvelle interprétation de l'architecture et du développement des colonies. A travers l'analyse des trajectoires alimentaires, elle crée un nouveau cadre d'étude qui pourrait être ajusté et réutilisé pour d'autres cas circonscrits à des périodes temporelles et des espaces géographiques différents. Au delà du paradigme forme/fonction, cette étude permet une nouvelle compréhension de l'architecture qu'elle lie à des processus. </dc:abstract><dc:abstract>Architectural scholars have studied the origins, growth, and development of New World settlements and their works document aspects such as land division, infrastructure, construction materials and building techniques. Often overlooked, however, are the physical effects that everyday food-related activities had in shaping these places.This thesis explores the relationships that linked subsistence to the architecture of the cold-climate settlement of Montreal in seventeenth and eighteenth century New France. It investigates food production cycles and transformation processes, and seeks to reconstruct the spatial-temporal chains through which food made its way from field to table to wastes in a pre-industrial urban context. It does so, through introducing the concept of food paths. The correlation between food trajectories, the form of the settlement and its social structure are discussed through an investigation of areas dedicated to the production of cereals, meats and produce, their processing, consumption, storage and discharge. Methodologically, the scarcity of the relevant physical evidence, the fact that the interaction between food paths and architecture deals with several unknowns related to the timeframe, as well as the very perishable nature of food and the transient character of its trajectories pose major challenges that require an inter-disciplinary approach towards reconstructing related processes. Hence, this work seeks evidence from primary sources that include both iconographical data and manuscripts. Maps, travelersř notes, notariesř acts and archaeological data referring to landscape, urban form, architecture and artefacts also find their place in this study, in complementary roles.Among this dissertationřs original contributions is its interpretation of architecture and the development of a human settlement, by adding a new and complementary layer to morphological analysis. By examining and differentiating food paths, this work also creates a framework for the study of food and architecture that can be applied to cases across both time and space. Finally, it also contributes to understanding space beyond the paradigm of form and function and considers architecture as part of daily life processes.</dc:abstract><ual:supervisor>Vikram Bhatt</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/g158bn55d.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/8336h576k</ual:fedora3Handle><dc:subject>Communications And The Arts - Architecture</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A1j92gc36d"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Physiology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Scalable methods for modelling complex biochemical networks</dcterms:title><ual:dissertant>Ollivier, Julien</ual:dissertant><dc:abstract>Au niveau cellulaire, des réseaux complexes d'interaction biomoléculaire traitent les signaux tant environnementaux qu'endogènes dans le but de contrôler l'expression génétique ainsi que d'autres processus cellulaires. Ceci est un défi pour les chercheurs qui veulent concevoir des modèles mathématiques et calculatoires des réseaux biochimiques. Dans cette thèse, je propose des méthodes qui facilitent la gestion de cette complexité en exploitant la constatation que, tout comme d'autres systèmes biologiques, les réseaux cellulaires se caractérisent par une modularité qui transparaît à tous les niveaux d'organisation.Dans la première partie, je mets l'accent sur les propriétés modulaires des protéines et sur la façon de caractériser leur fonction, compte tenu de leur structure et de leurs propriétés allostériques. J'ai mis au point un cadre modulaire à base de règles ainsi qu'un langage formel de modélisation qui permet de décrire les calculs effectués par les protéines allostériques et qui découle de principes biophysiques. La modélisation à base de règles s'adresse conventionnellement au problème de la complexité combinatoire, où les interactions entre les protéines peuvent générer une explosion combinatoire d'états des complexes protéiques. J'examine, cependant, comment il peut s'avérer nécessaire d'utiliser un nombre combinatoire de paramètres pour décrire ces mêmes interactions. Je démontre que notre cadre à base de règles peut régler efficacement ce problème de la complexité régulatoire, et permet de décrire les protéines et les réseaux allostériques de façon unifiée, cohérente et modulaire. J'utilise le cadre développé dans trois applications. Tout d'abord, je montre que l'allostérie peut rendre l'assemblage macromoléculaire plus efficace lorsqu'une protéine qui unit deux parties distinctes d'un complexe protéique est présente en concentration excessive. Deuxièmement, je démontre qu'il est relativement simple d'analyser les interactions coopératives complexes qui surviennent lorsque des ligands compétitifs se lient à une protéine multimérique. En troisième lieu, j'analyse un nouveau modèle de la signalisation des récepteurs couplés aux protéines G qui explique leur sélectivité fonctionnelle tout en limitant le nombre des paramètres utilisés. Globalement, je montre que ce cadre basé sur des règles, qui est implémenté dans le logiciel ‘Allosteric Network Compiler', peut faciliter la modélisation et l'analyse d'interactions allostériques complexes.Si les réseaux cellulaires sont modulaires, il en résulte que des sous-systèmes peuvent être étudiés séparément, à la condition que les entrées et les perturbations externes du système puissent être modélisées adéquatement. Cependant, ces réseaux sont soumis à l'influence du bruit intrinsèque, qui est endogène au système, mais également au bruit extrinsèque, venant des entrées bruyantes. De plus, de nombreuses entrées peuvent être dynamiques. Cela motive, dans la deuxième partie de ce travail, le développement d'algorithmes efficients de simulation stochastique pour les réseaux biochimiques qui peuvent tenir compte de paramètres biochimiques dynamiques. En me fondant sur la méthode maintenant célèbre de Gillespie, d'appellation ‘First Reaction Method', et sur celle de Gibson et Bruck, la ‘Next Reaction Method', j'ai développé deux nouveaux algorithmes qui permettent des entrées dynamiques de forme fonctionnelle arbitraire tout en s'échelonnant bien sur les systèmes qui comportent de nombreuses réactions biochimiques. J'analyse leurs propriétés d'échelonnement et je constate que, pour certaines applications, la ‘First Reaction Method' modifiée s'échelonne mieux que la ‘Next Reaction Method' modifiée.La troisième et dernière partie cette thèse est la présentation d'un nouvel outil informatique, Facile, qui simplifie la création, la mise à jour et la simulation de modèles de réseaux biochimiques.</dc:abstract><dc:abstract>In cells, complex networks of interacting biomolecules process both environmental and endogenous signals to control gene expression and other cellular processes. This poses a challenge to researchers who attempt to develop mathematical and computational models of biochemical networks that reflect this complexity. In this thesis, I propose methods that help manage complexity by exploiting the finding that, as for other biological systems, cellular networks are characterized by a modularity that appears at all levels of organization.The first part of this work focuses on the modular properties of proteins and how their function can be characterized through their structure and allosteric properties. I develop a modular rule-based framework and formal modelling language that describes the computations performed by allosteric proteins and that is rooted in biophysical principles. Rule-based modelling conventionally addresses the problem of combinatorial complexity, whereby protein interactions can generate a combinatorial explosion of protein complex states. However, I explore how these same interactions can potentially require a combinatorial number of parameters to describe them. I demonstrate that my rule-based framework effectively addresses this problem of regulatory complexity, and describes allosteric proteins and networks in a unified, consistent, and modular fashion. I use the framework in three applications. First, I show that allostery can make macromolecular assembly more efficacious when a protein that joins two separable parts of a complex is present in excessively high concentrations. Second, I demonstrate that I can straightforwardly analyze the complex cooperative interactions that arise when competitive ligands bind to a multimeric protein. Third, I analyze a new model of G protein-coupled receptor signalling and demonstrate that it explains the functional selectivity of these receptors while being parsimonious in the number of parameters used. Overall, I find that my rule-based modelling framework, implemented as the Allosteric Network Compiler software tool, can ease of modelling and analysis of complex allosteric interactions.If cellular networks are modular, this implies that small sub-systems can be studied in isolation, provided that external inputs and perturbations to the system can be modelled appropriately. However, cellular networks are subject to both intrinsic noise, which is endogenous to the system, but also extrinsic noise, arising from noisy inputs. Furthermore, many inputs may be dynamic, whether due to experimental protocols or perhaps reflecting the cyclic process of cell division. This motivates my development, in the second part of this work, of efficient stochastic simulation algorithms for biochemical networks that can accommodate time-varying biochemical parameters. Starting from Gillespie's well-known First Reaction Method and Gibson and Bruck's Next Reaction Method, I develop two new algorithms that allow time-varying inputs of arbitrary functional form while scaling well to systems comprising many biochemical reactions. I analyze their scaling properties and find that a modified First Reaction Method may scale better than a modified Next Reaction Method in some applications.The third and last part of this thesis introduces a new software tool, Facile, that eases the creation, update and simulation of biochemical network models. Models created through a simple and intuitive textual language are automatically converted into a form usable by downstream tools, for example ordinary differential equations for simulation by Matlab. Also, Facile conveniently accommodates mathematical and time-varying expressions in rate laws.</dc:abstract><ual:supervisor>Peter Swain</ual:supervisor><ual:supervisor>Leon Glass</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/ff365924s.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/1j92gc36d</ual:fedora3Handle><dc:subject>Biology - Physiology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3At435gj04x"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Chemistry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Stereoselective formation of all carbon quaternary centers: synthesis of alpha,alpha-disubstituted beta-amino carbonyl compounds via the Mannich reaction and total synthesis of (-)-puraquinonic acid</dcterms:title><ual:dissertant>Tiong, Erica</ual:dissertant><dc:abstract>Un auxiliaire chiral de type lactame thioglycolate bicyclique permettant la formation de carbones asymétriques quaternaires via une alkylation d'énolates fut développé précédemment par notre groupe de recherche. Cette méthode est remarquable puisqu'elle permet la génération stéréocontrôlée d'énolates d'amide alpha, alpha-disubstitués, sans égard à la différence stérique entre les substituants alpha Démontrant également une excellente discrimination faciale lors de l'approche de l'électrophile, ce protocole est maintenant devenu une méthode générale et pratique pour la préparation énantiosélective de carbones quaternaires par alkylation. Cette thèse rapporte l'application de cette méthode à la formation stéréosélective de composés beta-amino carbonyles alpha, alpha-disubstitués via la réaction de Mannich, ainsi que son utilisation dans la synthèse totale de l'acide (-)-puraquinonique.Les énolates alpha, alpha-disubstitués de lithium, générés de façon stéréosélective à partir de lactames thioglycolates bicycliques alpha, alpha-disubstituées, réagissent avec des imines benzènesulfoniques pour créer des acides beta-aminés avec grande efficacité et diastéréosélectivité. La réaction est générale pour une panoplie d'imines aromatiques, incluant celles comportant des substituants pauvres en électrons, riches en électrons, hétéroaromatiques, ainsi que des imines alpha, beta-insaturées. Les substituants alphades énolates d'amides peuvent être des groupements méthyle, éthyle, propyle benzyle et allyle. L'addition emploie un état de transition de type Zimmerman-Traxler où la géométrie de l'énolate contrôle le ratio d'addition anti/syn. Des méthodes de déprotection du groupe amino libérant des acides beta-aminés et des alcools sont décrites.Une courte synthèse de l'acide (-)-puraquinonique a été réalisée en utilisant l'auxiliaire chiral de type lactame thioglycolate bicyclique pour créer l'unique centre quaternaire présent tôt dans la séquence. Par la suite, un processus tandem fermeture de cycle par métathèse - cycloaddition de Diels-Alder produit un dihydroindene constituant le sytème bicyclique de l'acide puraquinonique. La quinone centrale est formée via une séquence comprenant un réarrangement de Curtius  et une oxydation par le sel de Fremy. Lorsque la synthèse est complétée, l'auxiliaire est enlevé par une hydrolyse acide, révélant la fonctionnalité acide carboxylique nécessaire. La synthèse est complétée avec un rendement de 21 % à partir d'une lactame commercialement disponible, et ce, en 14 étapes. Cette synthèse représente donc une amélioration de 24 étapes par rapport à la synthèse asymétrique précédente.</dc:abstract><dc:abstract>A bicyclic thioglycolate lactam chiral auxiliary was previously developed in our group for the asymmetric formation of quaternary carbon stereocenters via enolate alkylation. This method is notable for the stereocontrolled generation of alpha, alpha-disubstituted amide enolates, without reliance on the steric differences of the alpha-substituents. Coupled with excellent facial discrimination in electrophilic approach, this led to a general and practical method for enantioselective preparation of quaternary carbon centers via alkylation reactions. This thesis describes the extension of this methodology to the stereoselective formation of alpha, alpha-disubstituted beta-amino carbonyl compounds via the Mannich reaction and also the application of the alkylation method to the total synthesis of (-)-puraquinonic acid.alpha,alpha-Disubstituted lithium enolates, stereoselectively generated from alpha,alpha-disubstituted bicyclic thioglycolate lactams, undergo Mannich addition to benzenesulfonyl imines to form beta-amino acid derivatives with high yield and diastereoselectivity. The reaction is general for a number of aromatic imines, including those with electron rich and electron poor substituents, heteroaromatic, and alpha, beta-unsaturated imines. alpha-Substituents on the amide enolate can be varied to include methyl, ethyl, propyl, benzyl, and allyl groups. The addition occurs via a closed Zimmerman-Traxler transition state with anti/syn relationships controlled by enolate geometry. Methods for N-deprotection and removal of the auxiliary to afford beta-amino acids and alcohols are described. A concise synthesis of (-)-puraquinonic acid is accomplished using the bicyclic lactam chiral auxiliary to set the lone quaternary center at an early stage.  A tandem ring-closing cross metathesis process followed by Diels-Alder cycloaddition generates a dihydroindene, which makes up the bicyclic system of puraquinonic acid. The central quinone is formed by a Curtius rearrangement/Fremy's salt oxidation sequence.  Upon completion, the auxiliary is removed via acidic hydrolysis to give the required carboxylic acid functionality. The synthesis is completed in 14 steps from commercially available lactam in 21% overall yield, and represents a 24 step improvement over the previous asymmetric synthesis. </dc:abstract><ual:supervisor>James L. Gleason</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/pk02cf87k.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/t435gj04x</ual:fedora3Handle><dc:subject>Chemistry - Organic</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aqr46r513c"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Epidemiology and Biostatistics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Factors effecting adenoma detection during screening colonoscopy</dcterms:title><ual:dissertant>Al-Madi, Majid</ual:dissertant><dc:abstract>Contexte. Le taux de détection d'adénome (TDA) a été associé avec l'incidence intervalle subséquente du cancer colorectal (CRC) chez les patients subissant une coloscopie de dépistage. Objectif. Cette étude visait à identifier les facteurs affectant la détection d'adénome au cours d'une coloscopie de dépistage.Méthodes Une étude rétrospective transversale a été menée chez les patients ayant subi une coloscopie de dépistage entre le 1er Juin et 25 août 2009 au Centre universitaire de Santé McGill. Les variables ont été extraites à partir de deux bases de données électroniques Endoworks (pour les rapports de coloscopie) et OACIS (rapports de pathologie pour les polypes enlevés). Une analyse multivariable de régression logistique a été effectuée en utilisant le logiciel R. Résultats. 430 coloscopies effectuées successivement rencontrèrent les critères d'admissibilité et ont été incluses. En analyse univariable, une probabilité de détection d' adénomes accrue a été notée chez les patients de sexe masculin, plus àgés, ayant eu une ablation de polypes antécédente, s'il y avait eu photo-documentation du caecum, et avec la présence d'un nombre de polypes plus élevés. La probabilité de détecter un adénome était affaiblie chez les patients à risque moyen de CCR, si la coloscopie était effectuée par un chirurgien, et avec un nombre croissant d'endoscopies et coloscopies complétées avant la coloscopie le même jour, ainsi qu'en augmentant la durée de temps passé ce jour-là dans l'unité d'endoscopie. En analyse multivariable, une augmentation de la probabilité de détection d'adénome a été associée avec l' augentation de l'âge du patient (en années) (OR=1,04 (IC 95% (1,02 à 1,07)), un nombre accru de polypes détectés (OR = 3,71 ( 95% IC, 2,70 à 5,10), tandis qu'une plus faible probabilité de détection d'adénome était associée avec une augmentation du temps (en heures) passé depuis le début de la session endoscopie jusqu'à la coloscopie de dépistage donnée (OR 0,51 (IC 95%: 0,31 à 0,79). Conclusions. En plus des caractéristiques de patients reconnus, la fatigue de l'endoscopiste, telle que reflétée par le temps écoulé depuis le début de la session d'endoscopie est associée avec une diminution significative du taux de détection d' adénomes. Ce facteur important doit donc être pris en compte dans la planification de la liste d'endoscopie dans un contexte de dépistage. D'autres recherches sont nécessaires pour évaluer les facteurs qui permettent d'optimiser la détection des adénomes et la performance de la coloscopie comme outil de dépistage pour le CCR.</dc:abstract><dc:abstract>Background. Adenoma detection rate (ADR) has been associated with the incidence of interval colorectal cancer (CRC) in patients undergoing screening colonoscopy. Objective. This study aimed to identify factors that effect adenoma detection during screening colonoscopy.  Methods. A retrospective cross sectional study was conducted of patients who underwent screening colonoscopy between June 1st and August 25th 2009 at the McGill University Health Center.  Variables were abstracted from two electronic databases: Endoworks (for colonoscopy reports) and OACIS (for pathology reports for polyps removed). Multivariable logistic regression analysis was performed using the software R to determine the association between patient, colonoscopy, endoscopist related variables, and adenoma detection.Results. 430 sequentially performed colonoscopies met eligibility criteria and were included. In univariable analysis, higher likelihood of detecting adenomas was associated with male patients, increasing patient age, prior polyp removal, photo-documentation of the cecum, and increasing number of polyps detected; a lower likelihood of detecting adenomas was associated with average risk for CRC, colonoscopy performed by surgeon, increasing number of endoscopies and colonoscopies before the index colonoscopy, and increasing duration of time in the endoscopy unit. In multivariable analysis, increased likelihood of adenoma detection was associated with increasing patient age (in years) OR 1.04 (95%CI, 1.02 to 1.07), the more polyps detected the higher the odds of detecting an adenoma (OR 3.71 (95%CI, 2.70 to 5.10), while lower likelihood for detecting adenoma was increased time (in hours) from the beginning of the endoscopy session till the index colonoscopy (OR 0.51 (95%CI, 0.31 to 0.79). Conclusions.  In addition to patient characteristics, operator fatigue, as evidenced by a decrease in adenoma detection as time progresses from the start of the endoscopy session, is an important factor that should be considered in endoscopy scheduling. Further research is required to evaluate factors that would optimize the adenoma detection and performance of colonoscopy as a screening tool for CRC.</dc:abstract><ual:supervisor>Alan N. G. Barkun</ual:supervisor><ual:supervisor>Maida Sewitch</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/s1784q584.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/qr46r513c</ual:fedora3Handle><dc:subject>Health Sciences - Epidemiology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Atm70n043v"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Computer Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>A Bayesian Framework for Online Parameter Learning in POMDPs</dcterms:title><ual:dissertant>Atrash, Amin</ual:dissertant><dc:abstract>Decision-making under uncertainty has become critical as autonomous and semi-autonomous agents become more ubiquitious in our society. These agents must deal with uncertainty and ambiguity from the environment and still perform desired tasks robustly.  Partially observable Markov decision processes (POMDPs) provide a principled mathematical framework for modelling agents operating in such an environment.  These models are able to capture the uncertainty from noisy sensors, inaccurate actuators, and perform decision-making in light of the agent's incomplete knowledge of the world. POMDPs have been applied successfully in domains ranging from robotics to dialogue management to medical systems. Extensive research has been conducted on methods for optimizing policies for POMDPs.  However, these methods typically assume a model of the environment is known. This thesis presents a Bayesian reinforcement learning framework for learning POMDP parameters during execution.  This framework takes advantage of agents which work alongside an operator who can provide optimal policy information to help direct the learning.  By using Bayesian reinforcement learning, the agent can perform learning concurrently with execution, incorporate incoming data immediately, and take advantage of prior knowledge of the world.  By using such a framework, an agent is able to adapt its policy to that of the operator.  This framework is validated on data collected from the interaction manager of an autonomous wheelchair.  The interaction manager acts as an intelligent interface between the user and the robot, allowing the user to issue high-level commands through natural interface such as speech.  This interaction manager is controlled using a POMDP and acts as a rich scenario for learning in which the agent must adjust to the needs of the user over time.</dc:abstract><dc:abstract>Comme le nombre d'agents autonomes et semi-autonomes dansnotre société ne cesse de croître, les prises de décisions sous incertitude constituent désormais un problème critique.  Malgré l'incertitude et l'ambiguité inhérentes à leurs environnements, ces agents doivent demeurer robustes dans l'exécution de leurs tâches. Les processus de décision markoviens partiellement observables (POMDP) offrent un cadre mathématique permettant la modélisation des agents et de leurs environnements. Ces modèles sont capables de capturer l'incertitude due aux perturbations dans les capteurs ainsi qu'aux actionneurs imprécis.  Ils permettent conséquemment une prise de décision tenant compte des connaissances imparfaites des agents. À ce jour, les POMDP ont été utilisés avec succès dans un éventail de domaines, allant de la robotique à la gestion de dialogue, en passant par la médecine.  Plusieurs travaux de recherche se sont penchés sur des méthodes visant à optimiser les POMDP. Cependant, ces méthodes requièrent habituellement un modèle environnemental préalablement connu. Dans ce mémoire, une méthode bayésienne d'apprentissage par renforcement est présentée, avec laquelle il est possible d'apprendre les paramètres du modèle POMDP pendant l'éxécution. Cette méthode tire avantage d'une coopération avec un opérateur capable de guider l'apprentissage en divulguant certaines données optimales. Avec l'aide du renforcement bayésien, l'agent peut apprendre pendant l'éxécution, incorporer immédiatement les données nouvelles et profiter des connaissances précédentes, pour finalement pouvoir adapter sa politique de décision à celle de l'opérateur.  La méthodologie décrite est validée à l'aide de données produites par le gestionnaire d'interactions d'une chaise roulante autonome. Ce gestionnaire prend la forme d'une interface intelligente entre le robot et l'usager, permettant à celui-ci de stipuler des commandes de haut niveau de façon naturelle, par exemple en parlant à voix haute. Les fonctions du gestionnaire sont accomplies à l'aide d'un POMDP et constituent un scénario d'apprentissage idéal, dans lequel l'agent doit s'ajuster progressivement aux besoins de l'usager.  </dc:abstract><ual:supervisor>Joelle Pineau</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/ht24wp40h.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/tm70n043v</ual:fedora3Handle><dc:subject>Applied Sciences - Computer Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Adj52w8772"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Microbiology and Immunology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>Positive and negative regulation of the interferon antiviral response</dcterms:title><ual:dissertant>Paz, Suzanne</ual:dissertant><dc:abstract>The innate immune system senses pathogen invasion through the recognition of distinct molecular patterns collectively known as PAMPs (Pathogen-Associated Molecular Patterns). The mitochondrial molecule MAVS a central adaptor linking sensing of incoming viruses by the cytoplasmic RNA helicases RIG-I/MDA5 to downstream activation of the classical IKK complex and IKK-related kinases IKKepsilon and TBK1, resulting in the activation of transcription factors NF-kappaB and IRF3/IRF7 respectively. NF-kappaB and IRFs co-operate in the induction of type I interferons, but also play specific roles in the induction of pro-inflammatory and antiviral genes, respectively. Strict regulation of the RIG-I pathway must exist to mount an effective immune response while protecting the host from excessive inflammation and tissue damage. The work presented here is a compilation of three studies that highlight both positive and negative aspects of RIG-I signaling that contributed to a better understanding of the molecular events surrounding MAVS, TRAF3, and IKKepsilon. In the first study, presented in Chapter II, we linked RIG-I signaling to the phosphorylation events mediated by TBK1 and IKKepsilon and established the minimal serine consensus motif targeted by the kinases TBK1 and IKKepsilon in the transcription factors IRF3 and IRF7. This study confirmed the role of TBK1 and IKKepsilon as positive regulators of the RIG-I pathway. In the study presented in Chapter III, we followed up on a precious finding that IKKepsilon, but not TBK1, was recruited to the mitochondria. Here, we mapped the interaction of MAVS and IKKepsilon to the C-terminal region of MAVS, and more specifically to Lysine 500. We showed that Lysine 500 is a K63-linked polyubiquitin acceptor site; thus demonstrating that MAVS is ubiquitinated following virus infection. Mutation of Lysine 500 lead to an upregulation of downstream genes involved in the inflammatory and antiviral responses; demonstrating for the first time a novel role for IKKepsilon in the negative regulation of the RIG-I pathway. Chapter IV demonstrated that recruitment of IKKepsilon to ubiquitinated Lysine 500 lead to a disruption of TRAF3-MAVS interaction; which was shown to be mediated by a novel and functional TRAF3 binding site in the C-terminus of MAVS. </dc:abstract><dc:abstract>La réponse immune innée est déclenchée suite à la détection d'une variété considérable de pathogènes. Ces derniers sont reconnus comme étrangers à l'hôte à travers des motifs spécifiques conservés communément appelés PAMPs (Pathogen-Associated Molecular Patterns). La protéine mitochondriale MAVS, suite à la reconnaissance d'ARNs viraux intra cytoplasmiques par les ARN hélicases RIG-I et MDA5,  permet les activations cruciales du complexe classique des kinases IKK et des kinases IKKepsilon et TBK1 qui sont responsables in fine de l'activation des facteurs de transcription NF-kappaB et IRF3/IRF7. NF-kappaB et IRFs, coopèrent pour l'induction des interférons de type 1 et stimulent respectivement l'expression spécifique de gènes pro inflammatoires et antiviraux. Une régulation stricte de ces signalisations est essentielle pour la mise en place d'une réponse immune efficace tout en protégeant l'hôte d'une inflammation excessive. Le travail présenté ici dans cette thèse est une compilation de trois études qui, d'une part mettent en avant les propriétés régulatrices positives et négatives  de la voie de signalisation RIG-I, d'autre part contribuent à une meilleure compréhension des mécanismes moléculaires impliquant MAVS, TRAF3 et IKKepsilon. Dans la première étude présentée dans le Chapitre II, nous avons approfondi nos précédentes études et avons établie une séquence spécifique minimal de phosphorylations des IRF3 et IRF7 par les kinases TBK1 et IKKepsilon et donc confirmé le rôle des kinases dans la régulation positive de la signalisation par RIG-I. Dans la seconde étude du Chapitre III, nous avons approfondi l'observation antérieure démontrant que IKKepsilon et non TBK1 est recruté à la mitochondrie suite à une infection virale. Ici, nous avons démontrés que IKKepsilon était recrutée par MAVS via la région C-terminale et plus spécifiquement à la Lysine 500. De plus, nous avons caractérisé la Lysine 500 de MAVS comme étant capable de lier une chaîne de polyubiquitine K63 et donc démontré que MAVS est ubiquitiné suite à une infection virale. Enfin, la mutation de la Lysine 500 aboutit à une stimulation des voies pro inflammatoires et antiviraux; ainsi, pour la première fois nous avons démontré un nouveau rôle pour IKKepsilon dans la régulation de la voie de signalisation dépendante de RIG-I. Chapitre IV met en évidence que le recrutement d'IKKepsilon  modifiait l'interaction de TRAF3 à MAVS à un motif C-terminal nouvellement identifié dans cette étude et démontré essentiel pour l'induction de la voie de signalisation.</dc:abstract><ual:supervisor>John Hiscott</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/jd4731438.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/dj52w8772</ual:fedora3Handle><dc:subject>Biology - Molecular</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A44558j26f"><ual:graduationDate>2011</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Psychiatry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><dcterms:title>The developmental origins of a helplessness endophenotype in children</dcterms:title><ual:dissertant>O'Donnell, Katherine</ual:dissertant><dc:abstract>Research has demonstrated the link between helplessness cognitions and the development of anxiety and depression.  However, there is a paucity of research on the origins of such thinking styles in children.  There is mounting evidence for an interactive influence between the polymorphism within the promoter region of the serotonin transporter gene (5-HTTLPR) and early life adversity on the risk for anxiety, depression and their intermediate phenotypes.  We hypothesized that children with one copy of the S allele of the 5-HTTLPR gene and an insecure attachment would manifest increased helplessness and stress reactivity when faced with a challenge.  The data are drawn from a sub-sample of mother-child dyads from the MAVAN study. The pairs completed the laboratory measure of attachment security and buccal cheek swabs taken when the child was 18 and 36 months, respectively.  At 60 months, the child performed a Response to Challenge Puzzle (RCP) task. The task was designed to assess the child's response to "failure", as some puzzles were impossible to solve. The child's self-report, behavioral response and average heart rate were assessed during the RCP task.  Multivariate analyses revealed significant effects of attachment and 5-HTTLPR on multiple dimensions of helplessness.  The results suggest a potential pathway between child genotype, environment and risk for anxiety and depression.  </dc:abstract><dc:abstract>La litérature scientifique endosse un lien entre les cognitions qui accompagnent un sentiment de résignation et le développement de l'anxiété et la dépression. Toutefois, il existe peu d'études sur les origines de telles cognitions chez les enfants. De plus en plus, les recherches au sein de ce champ d'investigation suggèrent que l'intéraction entre le gène transporteur de sérotonine, qui comporte une région promotrice dont le siège est un polymorphisme fonctionnel (5-HTTLPR), et l'adversité vécue durant l'enfance sont associés avec un risque accru pour l'anxiété, la dépression et leurs phénotypes intermédiaires. Dans la présente recherche, nous avons émis l'hypothèse que les enfants portant une copie de l'allèle court du gène 5-HTTLPR et ayant un attachement insécure manifesteront davantage de résignation et de réactivité au stress lorsque confrontés par un défi, comparés aux enfants ne possédant aucune de ces caractéristiques. Les données proviennent d'un sous-échantillon de dyades mère-enfant issues du projet MAVAN. Chaque paire a complété la mesure d'attachement en laboratoire à 18 mois. Des échantillons d'ADN furent receuillis par l'entremise d'une serpillère de joue (prélèvement buccal) à 36 mois. À 60 mois, l'enfant exécuta un Casse-Tête Impossible (CI). Cette tâche, qui comporte des casse-têtes insolubles, fut conçue dans le but d'évaluer la réaction de l'enfant à "l'échec". L'auto-évaluation, les réponses comportementales ainsi que la fréquence cardiaque de l'enfant furent mesurés lors du CI. Des analyses multivariées démontrèrent des effets significatifs de l'attachement et du 5-HTTLPR pour diverses composantes de la resignation. Les résultats suggèrent une avenue potentielle entre le génotype de l'enfant, l'environnement et le risque pour l'anxiété et la dépression.</dc:abstract><ual:supervisor>Michael Meaney</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/w95054673.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/44558j26f</ual:fedora3Handle><dc:subject>Psychology - Developmental</dc:subject></rdf:Description></rdf:RDF>