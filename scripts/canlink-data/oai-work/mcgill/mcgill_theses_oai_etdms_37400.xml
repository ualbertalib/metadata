<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/assets/blacklight_oai_provider/oai2-b0e501cadd287c203b27cfd4f4e2d266048ec6ca2151d595f4c1495108e36b88.xsl"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd"><responseDate>2020-07-25T03:31:39Z</responseDate><request resumptionToken="oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):37400" verb="ListRecords">https://escholarship.mcgill.ca/catalog/oai</request><ListRecords><record><header><identifier>oai:escholarship.mcgill.ca:hd76s4647</identifier><datestamp>2020-03-23T20:16:45Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Il y a une variabilité considérable dans l’apprentissage fructueux des langues à l’âge adulte. Les différences individuelles de l’aptitude naturelle d’apprendre les langues peuvent contribuer à cette variation et en partie peuvent manifester d’une préformation des caractéristiques de la structure cérébrale ou des ‘biomarqueurs’.Dans cette étude, nous avons utilisé Voxel-Based Morphometry (VBM) pour explorer si a priori les différences du volume de matière grise (GMV) a correlé avec le succès de 18 adultes (composés d’un groupe de 10 anglais monolingues et un groupe de 8 mandarin-anglais bilinguals) qui ont appris le français au cours de 12 semaines de formation intensive. Aussi nous avons exploré des changements de GMV après la formation associée avec la neuroplasticité. L’objet principal de nos mesures comportementales c’était de quantifier l’amélioration de l’articulation française plus objectivement que dans les études précédentes, utilisant les tâches de production de la parole, y compris la répétition orale, la lecture, et le discours libre, et des analyses acoustiques precises des changements du Voice Onset Time (VOT) et de formants des voyelles, qui peuvent indexer la parole autochone.Nos résultats principaux ont montré: (1) une amélioration d’articulation en français dans les deux groupes, y compris dans les mesures acoustiques du VOT et /i/ production voyelle; (2) pas d’effets neuroplastiques; et (3) une préformation de biomarqueurs GMV pour l’amélioration d’articulation française (par l’acoustique, le taux d’articulation et les mesures de répétition orale) dans les régions cérébrales liées à  l’apprentissage motrice implicite (le cervelet bilatéral et les noyaux gris centraux de la côté gauche) et le traitement phonologique (le caudé gauche, le lobe droit pariétal inférieur (IBL), et le cervelet droit). Des biomarqueurs uniques, peut-être liés aux exigences cognitives des tâches spécifiques, réquerant ou la lecture ou la répétition orale ont emergé aussi.Ces conclusions aident à élucider la base structural neurale de l’aptitude linguistique et clarifient les sources de variabilité pour la réussite de l’apprentissage articulatoire à l’âge adulte. Une proportion de cette variabilité est à cause d’une structure cérébrale a priori dans les régions sous-jacentes des capacités motrices, phonologiques, et perceptuelles</description><description>There is considerable individual variation in successful language learning in adulthood. Individual differences in natural ‘aptitude’ for language learning may contribute to this variation and partly manifest in pre-training brain structure characteristics or ‘biomarkers’. In this study, we used anatomical Magnetic Resonance Imaging and Voxel-Based Morphometry to explore whether a priori differences in grey matter volume (GMV) correlated with how successfully 18 adults (consisting of a group of 10 English-L1 speakers and a group of 8 Mandarin-L1, English-L2 speakers) learned French over 12 weeks of intensive training. We also explored changes in GMV after training associated with neuroplasticity. The focus of our behavioral measures was to quantify improvement in French articulation more objectively than in previous studies. We used speech production tasks including oral repetition, reading, and free speech, and precise acoustic analyses of changes in Voice Onset Time (VOT) and vowel formants, which can index speech nativeness. Our main results showed: (1) improvement in French articulation in both L1 groups, including in the acoustic measures of VOT and /i/ vowel production; (2) no neuroplasticity effects; and (3) pre-training GMV biomarkers for French articulation (indexed by improvement on acoustic measures, articulation rate, and oral repetition) in brain regions linked to implicit motor learning (the bilateral cerebellum and the left basal ganglia) and phonological processing (the left caudate, the right inferior parietal lobe, and the right cerebellum). Unique biomarkers likely linked to the cognitive demands of specific tasks requiring reading and oral repetition also emerged. These findings help to elucidate the structural neural basis of language aptitude and clarify sources of variability for articulatory learning success in adulthood. Some of this variability is due to a priori brain structure in regions underlying motor, phonological, and perceptual abilities</description><creator>Pearse, Rebecca</creator><contributor>Shari Baum (Supervisor2)</contributor><contributor>Denise Klein-Broomberg (Supervisor1)</contributor><date>2020</date><subject>Neuroscience</subject><title>Exploring individual differences in grey matter relating to language learning in adulthood</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/5x21tk885.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/hd76s4647</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Integrated Program in Neuroscience</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:mk61rn55v</identifier><datestamp>2020-03-23T20:16:58Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Over the last decade, Alberta, Canada has been the site of several highly publicized events focused on individuals labelled as “sovereign citizens”. From the occupation of homes and traplines claimed as sovereign territories (CBC 2013a, 2013b) to larger than life court cases with wide public audiences (Meads v. Meads 2012 ABQB 571), sovereign citizens capture the Albertan imagination with their unusual beliefs and practices. Asserting discourses of freedom, liberty, and self-governance, these individuals surface at the junctures of everyday life, challenging state authority through obscure and obtuse interpretations of law. Whether they appear through hegemonic state classificatory schemes used by the media, in stories narrated by Albertans trying to understand the movement, or in legal courts, sovereign citizens are entangled with the worlds they seek to reject. A doubling—the sovereign and the citizen—thus emerges and remerges as individuals challenge the state and its assumed duties, laws, and expectations, all the while they move, transgress, or realign themselves in relationship to the state.Drawing on four months of ethnographic fieldwork between May to September 2018, this thesis thus explores the relationship between sovereign citizens as an elusive and slippery category and Albertans, asking: how do sovereign citizens and Albertan citizens narrate their own (and others’) challenges to the Albertan state</description><description>Au cours de la dernière décennie, l'Alberta, situé au Canada, a été le théâtre de plusieurs événements très médiatisés axés sur des individus qualifiés de «citoyens souverains». De l'occupation de maisons et de terrains de piégeage revendiqués comme territoires souverains (CBC 2013a, 2013b) à des procès plus vastes que jamais, impliquant un large public (Meads v. Meads 2012 ABQB 571), les citoyens souverains captivent l'imagination des Albertains avec leurs croyances et leurs pratiques inhabituelles. Affirmant des discours sur la liberté et l'autonomie gouvernementale, ces individus font surface au tournant de la vie quotidienne, défiant l'autorité de l'État par le biais d'interprétations obscures et obtuses du droit. Qu'ils apparaissent à travers des schémas de classification étatiques hégémoniques utilisés par les médias, dans des récits racontés par des Albertains cherchant à comprendre le mouvement, ou devant des tribunaux, les citoyens souverains sont empêtrés dans les mondes qu'ils cherchent à rejeter. Un dédoublement - le souverain et le citoyen - émerge et resurgit alors que les individus défient l’État et ses devoirs, lois et attentes assumés, tout en se déplaçant, en transgressant ou en se réalignant par rapport à l’État. S'appuyant sur quatre mois de travail ethnographique sur le terrain entre mai et septembre 2018, cette thèse explore donc la relation entre les citoyens souverains en tant que catégorie insaisissable et glissante et les Albertains, demandant: comment les citoyens souverains et les citoyens albertains racontent-ils leurs propres défis (et ceux des autres) à l'état albertain</description><creator>McCuaig, Vanessa</creator><contributor>Diana Allan (Supervisor)</contributor><date>2020</date><subject>Anthropology</subject><title>The uncanny doubling of sovereign and citizen: anti-state narrativity in Alberta</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/k3569858r.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/mk61rn55v</identifier><degree><name>Master of Arts</name><grantor>McGill University</grantor><discipline>Department of Anthropology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:tx31qp261</identifier><datestamp>2020-03-23T20:17:12Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>With the development of electronic technologies and their emerging use in the fields of biomedicine and bio-interfaces, there is a need for greener alternative materials that can be sustainable for the environment and non-toxic.  The use of self-healing conductive protein materials in bio-electronics could revolutionize the field as they exhibit several advantages over conventional semiconductors; they are biodegradable, biocompatible, flexible and can be genetically engineered. Here, we aim at characterizing and studying the material’s properties of engineered curli protein nanofibers produced by E. coli bacteria.  These fibers were engineered to contain a high density of tryptophan residues, an aromatic amino acid, to mimic the conductivity mechanism observed in the naturally-conductive bacteria, Geobacter sulfurreducens.  To confirm π-orbital overlap for electron delocalization, the effect of pH and the beta-sheet folded conformation of the proteins; fluorescence, Raman spectroscopy, and circular dichroism measurements were performed. Next, electrical characterization was performed to study the effect of factors on conductance such as thickness and drying of the film. It was observed that charge transport is both a surface and bulk phenomenon as a direct relationship between conductance and thickness was established. Moreover,  humidity was found to play an important role in maintaining the protein configuration for the charges to flow. Furthermore, the desirable self-healing nature of the protein fibers was also examined via microscopy and electrical measurements, for applications as flexible and stretchable materials. Successful electrical healing with more than 95% recovery of conductivity for Trp1 mutant thin film was observed with the use of water, suggesting that they can potentially be used as self-healing humidity sensors</description><description>Avec le développement de technologies électroniques et leur utilisation émergente en biomédecine et en bio-interfaces, il y a un besoin pour des matériaux alternatifs verts soutenables pour l’environnement et non-toxiques. L’utilisation de matériaux à partir de protéines conductives et auto-régénératrices pourra révolutionner le domaine puisque ces protéines démontrent plusieurs avantages comparés aux semiconducteurs conventionnels; elles sont biodégradables, biocompatibles, souples et peuvent être modifiées génétiquement. Dans ce document, nous cherchons à caractériser et étudier les propriétés de nanofibres de protéines curli génétiquement modifiées produites par la bactérie E. coli. Ces fibres ont été modifiées génétiquement pour exprimer une plus haute densité de résidus de tryptophane, un acide aminé, pour imiter le mécanisme de conductivité observé chez la bactérie naturellement conductive, Geobacter sulfurreducens. Afin de confirmer le recouvrement de liaisons π qui crée une délocalisation d’électrons, l’effet du pH et la conformation de feuillets bêta de la protéine pliée, la fluorescence, la spectroscopie de Raman et le dichroïsme circulaire ont été exécutés. Ensuite, la caractérisation électronique a été réalisée afin d’étudier l’effet de facteurs sur la conductance tel l’épaisseur et le taux d’humidité du film. Le transport de charge a été observé à la surface et à l’intérieur du matériel, puisqu’une relation directe entre l’épaisseur du film et la conductance a été établie. De plus, l’humidité a été démontrée de jouer un rôle dans la conservation de la structure de la protéine pour permettre le flux de charges. La qualité auto-régénératrice des fibres de protéines a aussi été examinée par microscopie et par quantification électrique, pour être utilisées en tant que matériaux souples et flexibles. La conductivité électrique a été rétablie avec plus de 95% d’efficacité pour un film à partir du mutant Trp1 à l’aide d’une goutte d’eau, ce qui suggère l’application potentielle de ces films comme capteurs d’humidité auto-réparateurs</description><creator>Wasim, Saadia</creator><contributor>Noémie-Manuelle Dorval Courchesne (Supervisor)</contributor><date>2020</date><subject>Chemical Engineering</subject><title>Characterization of conductive self-healing protein materials</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/qz20sz32w.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/tx31qp261</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Chemical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:q524jt17q</identifier><datestamp>2020-03-23T20:17:28Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Introduction: There is a lack of data in the Democratic Republic of Congo on hospital capacity, surgical, anesthesia and obstetrics providers, surgical volume and met surgical needs. Without such data, a National Surgical Anesthesia and Obstetrical Plan is not possible.Objectives: To assess the surgical capacity of all health sectors in the North Kivu province of the DRC and correlate it to the surgical burden in the province. To estimate the number of surgical interventions performed yearly and the rate of surgical interventions performed per 100,000 population. .Methods: We conducted a cross-sectional survey of all hospitals performing major surgery in the North Kivu province in DRC from January to December 2017. Using the WHO -PGSSC hospital assessment tool and operating room registries, we determined hospital characteristics, surgical capacity score, surgical outputs, and rates, types of surgeries, met and unmet needs. Findings: Forty-three facilities were found performing surgery in the province, including 39 zone general hospitals and 4 regional hospitals. Faith-based/NGO hospitals comprised 56.8% of facilities. There were, per 100, 000 population, 78.4 hospital beds, 1.2 operating rooms, 0.4 surgical providers (including general surgeons, orthopedic surgeons and obstetricians), 2.2 general doctors performing surgery, 0.8 nurse-anesthetist, and no physician anesthetists. The surgical rate per 100.000 population was 49 for children aged ≤ 18 years and 481 for adults. By specialty, 64% of all procedures were gynecological/obstetrical, 18.9% general surgical, 14.8 % trauma-related, and 2.3% eye and dental-related. These characteristics varied by rural/urban setting as well as by safe compared to unsafe areas. No reliable mortality data were available. The average surgical capacity score was 39 (SD 14.5). Between 1/2 and 1/3 of patients could not receive surgical care despite presenting to a hospital in a timely fashion.Conclusions: Access and delivery of essential surgery are lower in the observed DRC study compared to other African settings. Investment in basic health-care facilities and surgical workforce and training is urgently needed. Faith-based/ non-public hospitals make a valuable contribution to surgical provision. The capacity of hospitals to provide care is low</description><description>Introduction : Il manque de données sur la République Démocratique du Congo concernant la capacité chirurgicale des hôpitaux, des chirurgiens, des anesthésiologistes et des obstétriciens, le taux de chirurgies et les besoins chirurgicaux. Sans ces données, il est difficile de mettre en place un plan national pour améliorer ces services.Objectifs :  Évaluer la capacité chirurgicale de tous les secteurs de la santé dans la province du Nord-Kivu en République Démocratique du Congo et la corréler avec le fardeau chirurgical dans la province. Évaluer le nombre d'interventions chirurgicales réalisées dans un an et le taux d’interventions chirurgicales pour 100 000 habitants dans tous les hôpitauxMéthodes : Nous avons mené une enquête transversale auprès de tous les hôpitaux pratiquant des interventions chirurgicales majeures dans la province du Nord-Kivu en RDC de Janvier à Décembre 2017. L'outil du Programme de Chirurgie Globale et du Changement Social crée par l’Université Harvard et adopté par l’Organisation Mondiale de la Santé pour l’évaluation des hôpitaux et les registres des salles d'opération ont permis de déterminer le score de capacité chirurgicale, caractéristiques des hôpitaux, les taux de chirurgie, les types de chirurgies, les besoins satisfaits et non satisfaits. Résultats : Quarante-trois hôpitaux ont été évalués dont 39 hôpitaux de référence de Zones de Santé, et 4 hôpitaux provinciaux. 56,8% étaient des hôpitaux confessionnels / des ONG. Il y avait pour 100 000 habitants 78,4 lits, 1,2 salle d’opération, 0,4 prestataires chirurgicaux pour (y compris les chirurgiens généralistes, les chirurgiens orthopédiques et les obstétriciens), 2, 2 médecins généralistes pratiquant la chirurgie, 0,8 infirmiers-anesthésistes et aucun médecin anesthésiste. Le taux de chirurgies pour 100 000 habitants était de 49 pour les enfants de 18 ans et moins et de 481 pour les adultes. Par spécialité, 64% des interventions étaient gynécologiques / obstétriques, 18,9% en chirurgie générale, 14,8% en traumatologie et 2,3% en ophtalmologie. Ces caractéristiques varient selon le milieu rural / urbain ainsi que selon les zones sûres par rapport aux zones non sécurisées. Aucune donnée fiable sur la mortalité n'était disponible.Le score moyen de capacité chirurgicale modifiée était de 39 (DS : 14.5). Entre 1/2 et 1/3 de patients ne recevront pas de soins chirurgicaux même s'il est arrivé à l'hôpital à tempsConclusions : Selon l'étude réalisée en RDC, l'accès aux interventions chirurgicales essentielles, la main-d'œuvre nécessaire à ces interventions et les prestations correspondantes sont moins importants que dans d’autres pays africains, et il est urgent d'investir dans les établissements de soins de santé de base, le personnel chirurgical et la formation. Les hôpitaux confessionnels / non-publiques apportent une contribution précieuse à la fourniture de services chirurgicaux. La capacité des hôpitaux à fournir des soins chirurgicaux est faible</description><creator>Malemo, Luc</creator><contributor>Jean-Martin Laberge (Supervisor2)</contributor><contributor>Dan Poenaru (Supervisor1)</contributor><date>2020</date><subject>Surgery</subject><title>Estimation of met surgical needs at hospitals in the North Kivu Province of the Democratic Republic of Congo</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/sx61dr54b.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/q524jt17q</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Surgery</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:3484zn218</identifier><datestamp>2020-03-23T20:17:48Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Walking and powered two-wheelers (PTWs) play a fundamental role in the urban mobility of many Latin American (LA) cities. Despite their importance, the road safety of these transportation modes, referred to as vulnerable road users, is still a major concern. In the Americas, pedestrians and PTWs represent almost half of the road fatalities as they account for 22% and 20% of the fatalities respectively. Moreover, the percentage of PTWs users in traffic fatalities has been rapidly increasing in Latin American countries - for instance, in Bogota, this increased from 18% in 2013 to 28% in 2017. To deal with the road safety problem, many developed countries have established programs and methodologies for the countermeasure implementation (e.g. “vision zero”). However, such programs are often absent in LA countries. Among many other factors, this is attributed to the lack of economic resources, safety-related data and tools for safety diagnosis and evaluation. Road crash data frequently does not exist or lacks quality. Crash data often suffers from inconsistencies, misclassification, inaccurate geographical locations and inaccurate identification of crash contributory factors. In addition, it can take years to gather sufficient information for diagnosis and countermeasure evaluation, making it difficult to determine failure mechanisms that lead to crashes and contributing risk factors. In this context, there is a need for data and methodologies for safety analysis and countermeasure evaluation that do not rely on crashes.   To address the issues with observed crash data, alternative approaches have emerged in the last few years. This includes the surrogate-road-safety approach which includes conflict techniques or measures of proximity and speed measures derived from video trajectory data. This alternative approach is a low cost and time efficient way to get data for safety diagnosis and evaluation of countermeasures. Despite the recent developments in surrogate safety measures and tools, very few studies exist in the literature regarding the investigation of vulnerable road users (VRUs) road safety in developing countries using alternative surrogate safety methods. Very little is documented about the contributing factors related to dangerous vehicle-VRUs interactions and the low-cost road-design treatments that can help reduce VRUs injuries in LA context. As walking and PTWs are essential modes of transportation in LA countries, PTW face a great risk of injury and are involved in a disproportionate number of vehicle collisions in urban areas. The general objective of this thesis is to introduce the use of surrogate measures of safety and methodologies for VRUs in the context of LA using video analysis techniques. More specifically, the first part of this research has two objectives. First, this thesis proposes and implements a proactive surrogate safety methodology using video trajectory data in the context of developing countries to identify risk-contributing factors to vehicle-pedestrian interactions at distinct types of junctions and to evaluate the effectiveness of temporary low-cost countermeasures using Cochabamba and Bogota as case studies. In the second half, extensive video trajectory data was obtained, automatically processed and used to analyze the effect of speed, conflicts and driver behavior, including maneuvering and traffic violations, on PTWs safety and to identify the factors contributing to PTWs risk in the LA context. Hundreds of hours of video data were automatically processed using a specialized computer vision software to generate speed and road-user trajectory data to investigate the risk factors</description><description>La mobilité urbaine en Amérique Latine se base fondamentalement sur la marche à pieds et les deux roues motorisés DRM. Mise à part l’importance des deux derniers, la sécurité routière de ces moyens de transport représente une vulnérabilité assez remarquable par leurs utilisateurs. Cela s’impose en tant que problématique majeure dans les pays concernés. Le taux d’accident que provoque la marche à pieds ou les DRM représente à peu près la moitié du taux d’accidents sur le continent Latino-Américain selon des statistiques établit en 2015 par l’organisation mondiale de la santé qui déclare que 22% des accidents routières émanent des piétons tandis que 20% font partie des DRM. Le pourcentage des accidents commises par les DRM augmente de fur et à mesure de sorte qu’il est arrivé de 18% en 2013 jusqu’à 28% en 2017 en Bogota. Pour résoudre les problèmes de sécurité routière, plusieurs pays développés ont établi des programmes et des méthodologies de mise en œuvre des contre-mesures ex : vision zéro. Malgré cela ces programmes sont quasi inexistants dans les pays latino-américains. Parmi plusieurs facteurs le manque des capacités économiques représente l’élément essentiel de l’absence de routes sécurisées, suite à l’absence des données nécessaires ainsi que les moyens de sécurisation diagnostique et d’évaluation. Ainsi que les données existantes sont souvent de mauvaise qualité. Elles subissent encore de l’incohérence, du mal classement, de mauvaises localisations géographiques, des mauvaises identifications des accidents, des facteurs contributifs. En plus ça pourra prendre des années pour réunir suffisamment d’information pour analyser et contre mesurer une évaluation ce qui rend difficile la détermination des échecs des mécanismes qui mènent à des crises et qui créent des risques. Des approches alternatives sont apparues durant les dernières années en ayant comme exemple le substitut de la sécurité routière. Cela consiste des conflits techniques ou des mesures de proximités et de vitesse issus des données vidéo. Cette approche alternative représente un avantage de diminution de coût ainsi que l’accélération du temps d’obtention des données pour diagnostiquer les mesures de sécurité ainsi que l’évaluation des contre-mesures. Malgré le développement des mesures et des outils de sécurité, peu d’études existent dans la littérature qui s’intéresse à la vulnérabilité des utilisateurs des routes. Il n’existe pas beaucoup d’études d’enquêtes sur les utilisateurs vulnérables des routes UVR. Peu de documentation sur les facteurs liés au danger dû aux interactions des UVR et l’amélioration de la qualité des routes qui peut diminuer les accidentes menés par les UVR comme c’est le cas dans le modèle latino-américain LA. Alors que comme les marches à pieds et les DRM sont des moyens de transport essentiel aux pays LA. Les DRM affrontent des risques majeurs d’accident sur le niveau humain toute en provoquant des accidents de véhicules dans les zones urbaines. L'objectif général de cette thèse est d'introduire l'utilisation de SMS et de méthodologies pour les UVR dans le contexte de LA utilisant des techniques d'analyse vidéo. La première partie de cette recherche propose une méthodologie proactive de sécurité routière de substitution qui utilise un outil d'analyse vidéo automatisé dans le contexte des pays en voie de développement afin d'identifier les facteurs contribuant au risque pour les interactions véhicule-piéton à différents types de jonctions et d'évaluer l'efficacité de contre-mesures temporaires à faible coût en utilisant Cochabamba et Bogota comme études de cas. Au cours de la seconde partie, de nombreuses données vidéo ont été utilisées pour analyser l’effet de la vitesse, des conflits et du comportement du conducteur, y compris les manœuvres et infractions au code de la route, sur la sécurité des deux-roues motorisés et identifier les facteurs contribuant au risque dans le contexte latino-américain</description><creator>Elagaty, Mohamed</creator><contributor>Luis Miranda-Moreno (Supervisor)</contributor><date>2020</date><subject>Civil Engineering and Applied Mechanics</subject><title>A surrogate video-based safety approach for vulnerable road users in the Latin American context</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/05741x150.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/3484zn218</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Civil Engineering and Applied Mechanics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:st74cw09g</identifier><datestamp>2020-03-23T20:18:00Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Souvent mal résolues par les modèles climatiques conventionnels, les fractures dans la banquise sont néanmoins des composantes importantes de l'océan Arctique. Nous utilisons un modèle non-hydrostatique à haute résolution (dx = dz = 1 m) pour étudier des processus à petite échelle produient par des flux surfaciques de flottabilité et discontinuités dans le stress glace-océan sous des fractures actives. Nous tentons d'imiter des observations de flux de chaleur anormalement forts et de remontée de la pycnocline prises durant le projet SHEBA en mars 1998.  McPhee et al. (2005) ont interprété celles-ci comme une conséquence directe du rotationnel positif dans le stress de surface dû à la glace. Nous montrons que nos résultats entrent en contradiction avec cette interprétation puisque la remontée des eaux produite par un tel stress de surface ne provoque ni des flux de chaleur à la base de la couche de mélange ni de remontée de la pycnocline de plus de 3 m. Cependant, des flux autour de 20 et 100 Wm-2 sont mesurés avec des forçages de flottabilité et de stress au rotationnel négatif, respectivement. Nous associons ceux-ci avec des flux de chaleur vers la glace estimés d'environ 0.7 Wm-2 et présentons une analyse de l'instabilité de la ciculation dans le but de comprendre l'asymétrie entre descente et remontée des eaux</description><description>Sea ice leads are important components of the Arctic ocean but are still poorly resolved in relatively low resolution global climate models. This study is motivated by an event of anomalously high vertical turbulent heat fluxes (400 Wm-2) and pycnocline upwelling (14 m) that happened during the SHEBA project in March 1998 (McPhee et al., 2005). They interpreted these as a consequence of the positive curl in the surface ice-ocean stress and associated Ekman pumping. We use a high-resolution (dx = dz = 1 m) non-hydrostatic model to look at small-scales processes induced by surface buoyancy fluxes and by discontinuities in the ice-ocean stress underneath active leads. We find that upwelling from positive surface stress curl neither induces heat fluxes at the mixed layer base nor displaces the pycnocline more than 3 m upwards. On the other hand, fluxes averaged at the base of the mixed layer under the lead reach around 58 and 16 Wm-2 with downwelling and buoyancy forcings, respectively. As such, the response of the ocean is very different under a positive stress curl compared to a negative one. Compared to the buoyancy-only forcing, a combination with the upwelling forcing yields similar heat fluxes (22 Wm-2), while a combination with the downwelling forcing is much stronger (111 Wm-2). We relate the mixed layer fluxes with estimated heat fluxes to the ice of approximately 0.7 Wm-2 (averaged in a region twice as wide as the lead) and present a stability analysis to understand the asymmetry between upwelling and downwelling. This contradicts the interpretation from McPhee et al. (2005)</description><creator>Bourgault, Pascal</creator><contributor>David N Straub (Supervisor2)</contributor><contributor>Bruno Tremblay (Supervisor1)</contributor><date>2020</date><subject>Atmospheric and Oceanic Sciences</subject><title>Vertical heat fluxes under sea ice leads in a high resolution idealized model</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/2z10ww02h.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/st74cw09g</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Atmospheric and Oceanic Sciences</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:1v53k243t</identifier><datestamp>2020-03-23T20:18:13Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Privacy is a major concern for information gathering and data publishing across a wide range of online applications. Various privacy-preserving algorithms and models have been extensively studied for relational data, network graph data, trajectory data, and transactional data, etc. Among them, text data is the most prevalent unstructured data on the Internet, but the studies on sanitizing textual data are still preliminary. Most privacy protection studies for textual data focus on removing explicit sensitive identifiers. However, personal writing style, a strong indicator of authorship, is often neglected. Most works focus only on removing or replacing explicit sensitive phrases or personal identifiers in the text. Text data, such as anonymous peer review comments or product reviews, carries an implicit personal trait: writing style. Modern stylometric techniques can identify the actual author of a given anonymous text snippet from 10,000 candidates. However, only a few works are proposed for writing style anonymization, and the ones that satisfy privacy requirements only treat text as numeric vectors that are difficult for the recipients to interpret.To tackle this problem we propose two novel text generation models for authorship anonymization. Combined with a semantic embedding reward loss function and the exponential mechanism, our proposed auto-encoder can generate differentially-private texts that have a close semantic and similar grammatical structure to the original text while removing personal traits of the writing style.  It does not require any conditioned labels or paralleled text data during training. Another model uses a generative adversarial network with back-translation loss function; the model is able to hide the authorship by imitating the writing style of a reference dataset. We evaluate the performance of the proposed models on the real-life peer reviews dataset and the Yelp review dataset. The result suggests that our models outperform the state-of-the-art on semantic preservation, authorship obfuscation, and stylometric transformation</description><description>La confidentialité est une préoccupation cardinal pour l’ensemble d’informations et la publication de données sur une grande gamme d’applications en ligne. Divers algorithmes et modèles préservant la confidentialité ont été largement étudiés pour les données relationnelles, les données de graphe de réseau, les données de trajectoire, les données transactionnelles, etc. Parmi eux, les données textuelles sont les données non structurées les plus répandues sur Internet, mais les études sur la désinfection des données textuelles sont encore préliminaires. La plupart des études de protection de la confidentialité des données textuelles se concentrent sur la suppression des identifiants sensibles et explicites. Cependant, le style d'écriture personnel, comme un indicateur fort d'auteur, est souvent négligé. La majorité des travaux se concentrent uniquement sur la suppression ou le remplacement de phrases sensibles ou d'identifiants personnels explicites dans le texte. Les données textuelles, telles que les commentaires anonymes par les pairs ou les critiques de produits, comportent un trait personnel implicite: le style d'écriture. Les techniques stylométriques modernes permettent d'identifier l'auteur réel d'un extrait de texte anonyme parmi 10 000 candidats. Pourtant, seules quelques œuvres sont proposées pour l'anonymisation du style d'écriture, et celles qui répondent aux exigences de confidentialité ne traitent le texte que comme des vecteurs numériques difficiles à interpréter par les destinataires. Pour résoudre ce problème, nous proposons deux nouveaux modèles de génération de texte pour l'anonymisation de la paternité. Combiné à une fonction de perte de récompense incorporée sémantique et au mécanisme exponentiel, notre encodeur automatique proposé peut générer des phrases différentiellement-privées qui possèdent une structure sémantique et grammaticale proche du texte original, tout en supprimant les traits personnels du style d’écriture. Il n’exige pas d’étiquettes conditionnées ni de données textuelles en parallèle pendant la formation. Un autre modèle utilise un réseau contradictoire génératif avec une fonction de perte de traduction arrière; le modèle peut masquer la paternité en imitant le style d'écriture d'un jeu de données de référence. Nous évaluons la performance des modèles proposés sur l'ensemble de données de la critique réelle et sur celle de Yelp. Le résultat suggère que nos modèles surpassent l’état de la technique en matière de conservation sémantique, d’obscurcissement de l’auteur et de transformation stylométrique</description><creator>Bo, Haohan</creator><contributor>Benjamin Fung (Supervisor)</contributor><date>2020</date><subject>Computer Science</subject><title>Authorship anonymization: Differentially-private text generation and writing style transfer</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/j3860c25j.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/1v53k243t</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>School of Computer Science</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:6t053m74k</identifier><datestamp>2020-03-23T20:18:29Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Nous passons en revues les développements récents dans la théorie de la gravité quantique dans un espace-temps à deux (2) et trois (3) dimensions.Nous considérons d’abord la théorie gravitationnelle de Jackiw-Teitelboim (JT), un modèle de gravité pure à deux dimensions. Inspirée par la théorie des matrices aléatoires (TMA), elle présente une nouvelle perspective sur une expansion perturbative dans la version quantique de la théorie. La TMA saisit les propriétés universelles de la dynamique à temps très tardive d’une énorme classe de systèmes quantiques chaotiques. Il semble que la théorie JT est en parfait accord avec la TMA, saisissant plus d’information que les propriétés universelles. Nous adoptons une approche lagrangienne à la quantification de la théorie JT. Nous constatons que nous devons effectuer des intégrales de chemins sur des géométries 2D de genre arbitraire. En intégrant d’abord le champ de dilaton, nous sommes restreints à des surfaces à courbures négatives constantes. Nous établissons alors un lien avec des relations de récursivité topologique d’Eynard et d’Orantin qui permettent de calculer des intégrales de chemins pour des surfaces à genres arbitraires en les coupant en paires de culottes. La série asymptotique des observables de la théorie JT correspond alors à une série d’expansion en genre de la TMA.Des travaux récents ont également dérivé une description de graviton de frontière pour la théorie gravitationnelle pure de AdS3. La théorie pure en trois dimensions a longtemps été considérée triviale puisqu’il n’y a pas de de gravitons à degrés de liberté locaux. Néanmoins, des effets globaux rendent la théorie non-triviale. Nous utiliserons des méthodes d’intégration de chemin standard afin de se rapprocher de la fonction de partition gravitationnelle autour d’une géométrie de fond semi-classique. L’action d’un graviton de frontière avec un dilaton obtenu par cette méthode décrit des excitations globales autour d’un espace-temps AdS3 de fond. Elle sert d’une généralisation à deux dimensions de la théorie Schwarzian à une dimension, qui elle décrit les fluctuations à la bordure de la théorie JT</description><description>We discuss recent developments in theories of quantum gravity in two and three spacetime dimensions.We begin by considering the Jackiw-Teitelboim (JT) theory, a model of pure 2d gravity, providing a new perspective on a perturbative expansion in the quantum version of this theory, inspired by random matrix theory. Random matrix theory (RMT) captures universal properties of the very late-time dynamics of an extremely wide class of chaotic quantum systems. JT gravity goes farther by agreeing precisely with a random matrix ensemble, beyond universal quantities. We take a Lagrangian approach to quantization of JT theory. We find that we must perform path integrals over 2d geometries of arbitrary genus. Integrating first over the dilaton field restricts to constant negative curvature surfaces. A connection is made with the topological recursion relations of Eynard and Orantin, which allow the computation of such path integrals for arbitrary-genus surfaces by cutting them apart into pairs of pants. The resulting asymptotic series for JT observables matches onto the genus expansion familiar from random matrix theory.Recent work has also appeared deriving a boundary graviton description of pure AdS3 gravity. Pure gravity was thought for some time to be trivial in three spacetime dimensions since there are no local graviton degrees of freedom. Nevertheless global effects render the theory nontrivial. We will use standard path integral methods to approximate the gravity partition function around a semiclassical background geometry. The resulting boundary graviton scalar field action describes global excitations around background AdS3 spacetime. It serves as a 2d generalization of the 1d Schwarzian theory, which itself describes boundary fluctuations of the JT theory</description><creator>Hodel, Matthew</creator><contributor>Alexander Maloney (Supervisor1)</contributor><contributor>Sarah Harrison (Supervisor2)</contributor><date>2020</date><subject>Physics</subject><title>Progress in low-dimensional anti-de Sitter gravity</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/v405sg116.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/6t053m74k</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:bk128g37t</identifier><datestamp>2020-03-23T20:18:49Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>When the Universe was around 10 billion years old, it became dominated by dark energy andbegan to accelerate in its expansion. This stage in the expansion history of the Universe iscrucial for distinguishing dark energy models. The Canadian Hydrogen Intensity MappingExperiment (CHIME) is a radio telescope designed to measure the expansion during this periodby mapping the large-scale distribution of neutral hydrogen gas. CHIME will directly detect thehydrogen 21 cm emission redshifted to frequencies between 400 and 800 MHz. Astrophysicalforegrounds are several orders of magnitude brighter than the 21 cm cosmological signal.There is an ongoing effort to understand the instrument to the level of precision required forforeground removal. Measurements of the foregrounds are useful ancillary data products in and of themselves. Todate, the best measurement of the full sky in this frequency regime is the 408 MHz map byHaslam et al. from a survey that was conducted in the 70's. As a drift-scan interferometer,CHIME is uniquely capable of producing maps of the full radio sky on a daily basis. Thesedetailed maps provide valuable astrophysical data at unexplored frequencies. In this work, I willdescribe m-mode formalism, a non-traditional mapmaking strategy in which we take advantage of the instrument's drift-scan design. To validate the method, I show the result of applying it to simulated data. I will present the first CHIME all-sky map at 702 MHz. I discuss the sources of map artifacts and describe the steps that are being taken to mitigate them in forthcoming maps</description><description>Lorsque l’Univers était âgé de 10 milliards d’années après le Big Bang, l’énergie sombre est devenue la cause principale de son expansion, et en a provoqué l’accélération. Cette époque dans l’histoire de l’Univers est cruciale pour distinguer les différents modèles d’énergie sombre. L’expérience canadienne de cartographie de l’intensité de l’hydrogène (Canadian Hydrogen Intensity Mapping Experiment, CHIME) est un radiotélescope à interféromètre conçu pour mesurer l’expansion en cartographiant la distribution distante de l’hydrogène gazeux. CHIME détectera l’émission radio à 21 cm de l’hydrogène intergalactique. Certains fonds astrophysiques peuvent être beaucoup plus lumineux que le signal cosmologique de 21 cm. Des travaux sont en cours pour comprendre l’instrument avec la précision requise pour la soustraction de ces radiosources. Les mesures de ces fonds astrophysiques sont également utiles en elles-mêmes. Présentement, la meilleure mesure du ciel complet entre 400 et 800 MHz consiste en une carte à 408 MHz créée par Haslam et al. CHIME est un instrument unique car il est capable de cartographier quotidiennement l’entièreté du ciel. Ces cartes détaillées fournissent de précieuses données astrophysiques sur une bande de fréquences jusqu’ici inexplorées. Dans ce travail, je décrirai le ``m-mode formalism”, une stratégie de cartographie non-traditionnelle tirant profit de la conception unique de l’instrument. Pour démontrer la validité de cette méthode, je produirai une carte en utilisant des données simulées. Je présenterai ainsi la première carte du ciel entier générée par CHIME à 702 MHz. J’élaborerai sur les sources d’artefacts cartographiques et décrirai les mesures permettant de les atténuer pour les prochaines versions de ces cartes</description><creator>Boubel, Paula</creator><contributor>Matthew Adam Dobbs (Supervisor)</contributor><date>2020</date><subject>Physics</subject><title>Mapping the radio sky with CHIME using spherical harmonic imaging</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/pv63g450w.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/bk128g37t</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:x920g208s</identifier><datestamp>2020-03-23T20:19:02Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The fault-valve model is a conceptual model widely accepted to describe how overpressured fluids can promote seismic failure by lowering effective stresses. Faults and shear zones exposed at the Triangle and Lamaque mines at Val-d’Or, Quebec show evidence of mixed deformation and fault-valve behavior within a compressive setting. Quartz-tourmaline veins hosted within shear zones can be grouped into a subhorizontal set known as “flats” and a steeply dipping set known as “steeps.” The orientations of these veins show an Andersonian paleostress field orientation with a horizontal σ1 and vertical σ3. The magnitudes of the fluid pressure cycling and stress cycling that occurred within the system are constrained and presented in this study. Tourmaline fibers within the veins are oriented parallel to the shear direction, and their tensile strengths were measured via tensile tests and the peak strength value (44 MPa) is used here as a proxy for the shear strength of the structures. This shear stress corresponds with differential stress values of 35 MPa for a fault dipping 56˚ and 105 MPa for a fault dipping 70˚. A deformation temperature of 308˚ C was determined via tourmaline intersector thermometry. The lithostatic pressure was estimated from geothermal and pressure gradients to be between 258 to 307 MPa. The liquid-vapor equilibrium pressure was calculated to be PE = 9 to 10 MPa. The estimated minimum shear stress drop is 44 MPa and the estimated pressure drop is 222-267 MPa. Our results imply a very high pore fluid factor (λ≈0.9) would have been necessary for these structures</description><description>Le modèle fault-valve est un modèle conceptuel largement accepté pour décrire comment les fluides en état de surpression peuvent produire une rupture sismique en réduisant les contraintes effectives. Les mines de Triangle et Lamaque à Val-d’Or, Québec, montrent des signes de déformation mixte et de comportement de fault-valve dans une configuration de compression. Les veines de quartz-tourmaline se situant dans les zones de cisaillement peuvent être regroupées dans un ensemble subhorizontal sous le nom de « plats » et un ensemble fortement incliné sous le nom de « raides ». Les orientations de ces veines montrent une orientation de champ de paléocontrainte d’Anderson avec un σ1 horizontal et un σ3vertical. Les magnitudes des cycles de pression de fluide et de contrainte qui ont lieu dans le système sont contraintes et présentées dans cette étude. Les fibres de tourmalines situées dans les veines sont orientées parallèlement à la direction de cisaillement et leurs résistances à la traction ont été mesurées grâce à des tests de traction. La valeur maximale de résistance (44 MPa) est utilisée comme un indicateur de la valeur de résistance au cisaillement des structures. Cette contrainte de cisaillement correspond à une valeur de contrainte différentielle de 35 MPa pour une faille inclinée à 56° and de 105 MPa pour une faille inclinée à 70°. Une température de déformation a été évaluée à 308°C en utilisant la thermométrie intersectorielle de la tourmaline. La pression lithostatique a été estimée à partir de gradients géothermique et de pression entre 258 et 306 MPa. La pression d’équilibre liquide-vapeur a été calculé comme étant PE = 9 à 10 MPa. La chute de contrainte cisaillante minimale estimée est de 44 MPa et la chute de pression estimée est de 222-267 MPa. Nos résultats impliquent qu'un facteur de fluide dans les pores très élevé (λ≈0,9) aurait été nécessaire pour ces structures</description><creator>Ogasa, Nicholas</creator><contributor>Vincent Johan van Hinsberg (Supervisor2)</contributor><contributor>James Kirkpatrick (Supervisor1)</contributor><date>2020</date><subject>Earth and Planetary Sciences</subject><title>Constraining the fault-valve model at Val-d’Or, Quebec</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/3n204355h.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/x920g208s</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Earth and Planetary Sciences</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:bz60d204g</identifier><datestamp>2020-03-23T20:19:19Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les composés fibreux ordonnés biologiques sont des matériaux multifonctionnels trouvés dans les plantes (cellulose), cuticules d’insectes (chitine) et dans les ossements et cornée chez les mammifères (collagène). Malgré les différentes réactions biochimiques des précurseurs des composés fibreux présentes dans la nature, la rigidité, la chiralité et le rapport élevé entre la longueur et le diamètre des fibres produisent des architectures semblables à celles des cristaux liquides chiraux. Un parcours biomimétique menant à des composés fibreux ordonnés synthétiques multifonctionnels doit être basé sur l'auto-assemblage de cristaux liquides tel qu'observé dans la nature. Les travaux passés portent sur les fonctions mécano-optiques des structures composites fibreuses en trois dimensions. Cette thèse se penche plutôt sur la théorie et la simulation des nanostructures de surfaces de précurseurs de cristaux liquides chiraux, avec regard sur les possibles fonctions optiques, tribologiques et mécaniques. Elle cherche à révéler et caractériser les mécanismes fondamentaux qui contrôlent l'émergence de nano-rides lors de l'assemblage de cristaux liquides chiraux à surface libre, dans le but de reproduire les surfaces à rides à échelles multiples se trouvant dans la nature, tel que les fleurs de lotus, les exocuticules de coccinelle, les pétales de tulipe ainsi que la peau de requin, utilisant des techniques d'auto-assemblage à température ambiante avec solvants à base d'eau. Les résultats présentés dans cette thèse montrent de quelles façons l'auto-assemblage de surface en matière molle, chiralité et ancrage créer des rides simples et multiples ainsi que des champs de contraintes sur les surfaces actives</description><description>Biological plywoods are multifunctional fibrous composites materials found in plants (cellulosic), insect cuticles (chitin) and bones and cornea tissues in mammals (collagen). Despite the different precursors' biochemistries of nature's fibrous composites, the rigidity, chirality and large length-to-diameter aspect ratio of the fibres produces architectures analogous of those of chiral liquid crystals. A biomimetic pathway to multifunctional synthetic plywoods must then be based on nature's green manufacturing based on liquid crystal self-assembly. Fruitful past work focused on mechano-optical functionalities arising from the 3-D bulk plywood structures. In this thesis, we focus on theory and simulation of surface nanostructuring of chiral liquid crystalline precursors for emerging potential optical, tribological, and mechanical functionalities. The goal and scope of this thesis is to reveal and characterize the fundamental mechanisms that control the emergence of nano-wrinkling as chiral liquid crystals assemble at free surface with the ultimate goal of reproducing nature's multiscale wrinkles surfaces such as lotus leaves, beetle exocuticles, tulip petals, and sharkskin using self-assembly at ambient temperatures and water-based solvents. Taken together with the new results presented in this thesis show how surface self-assembly in soft matter, chirality, and anchoring create simple and multiple wrinkling with active surface stress fields, of interest to biomimetics of nature's functional surfaces</description><creator>Wang, Ziheng</creator><contributor>Alejandro D Rey (Supervisor1)</contributor><contributor>Phillip Servio (Supervisor2)</contributor><date>2020</date><subject>Chemical Engineering</subject><title>The geometry and mechanics of wrinkling patterns in biological plywoods and cholesteric liquid crystals</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/z603r2973.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/bz60d204g</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Chemical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:wd376204j</identifier><datestamp>2020-03-23T20:19:33Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Ces dernières années, un certain nombre de technologies nano-activées ont été développées pour le traitement d’eau. Parmi celles-ci, les hydrogels (éponges) d’oxyde de graphène (OG) sont parmi les plus importantes. Ces macrostructures poreuses à base de graphène combinent les propriétés uniques du nanomatériau - une surface superficielle exceptionnelle, une chimie de surface amphiphile et une résistance mécanique élevée – avec une aise de manipulation d'un matériau macroscopique. Bien que la capacité d'adsorption de ces adsorbants pour une multitude de composés organiques et inorganiques ait été déterminée dans des conditions de laboratoire contrôlées, l'effet des co-contaminants organiques et de la chimie de l'eau sur l'adsorption des polluants cibles est souvent omis. De plus, les éponges OG n'ont que rarement été comparées au standard actuel, le charbon actif granulaire (CAG).Cette thèse évalue une éponge OG en conditions pertinentes pour l'environnement. En appliquant une simple méthode CLHP-UV/vis pour la quantification du bleu de méthylène (BM) dans des eaux complexes, cette thèse démontre ensuite qu’une éponge d’OG réduit (OGr) et de cellulose nanocristalline (CNC) surpasse la capacité du CAG à éliminer le colorant modèle de solutions à contaminant unique ainsi que d’eaux complexes contenant des matières organiques naturelles. De plus, le taux d'adsorption initial de BM est plus élevé pour l'éponge OGr-CNC que pour le CAG. La différence est attribuée aux chimies de surface et aux morphologies des adsorbants. Enfin, l’adsorption sur l’éponge OGr-CNC n'est que faiblement affectée par le pH et la force ionique de la solution, ce qui démontre la polyvalence du nouvel adsorbant</description><description>In recent years, a number of nano-enabled technologies have been developed for water treatment applications. Among the most prominent are graphene oxide (GO) hydrogels or sponges. These porous graphene-based macrostructures combine the unique properties of the nanomaterial, namely an exceptional surface area, amphiphilic surface chemistry, and high mechanical strength, with the easy handling of a macroscopic bulk material. While the adsorption capacity of these adsorbents for a myriad of organic and inorganic compounds has been determined under controlled laboratory conditions, the effect of organic co-contaminants and water chemistry on the adsorption of target pollutants has often been omitted. Furthermore, GO sponges have only rarely been compared to the current industrial standard granular activated carbon (GAC).This thesis evaluates a GO sponge under environmentally relevant conditions. By applying a simple HPLC-UV-vis method for the quantification of methylene blue (MB) in complex waters, we demonstrate that a reduced GO (rGO)-cellulose nanocrystal (CNC) sponge outperforms GAC in the removal of the model dye from single contaminant solutions and complex waters containing natural organic matter. Furthermore, the initial adsorption rate for MB is higher on the rGO-CNC sponge than on GAC. The difference in adsorptive performance is attributed to the surface chemistries and morphologies of the adsorbents. Lastly, adsorption on the rGO-CNC sponge is only mildly affected by the pH and the ionic strength of the contaminant solution, which demonstrates the versatility of the novel adsorbent</description><creator>Allgayer, Raphaela</creator><contributor>Nathalie Tufenkji (Supervisor)</contributor><date>2020</date><subject>Chemical Engineering</subject><title>Influence of water chemistry on methylene blue adsorption by hybrid graphene oxide-cellulose nanocrystal sponges</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/j3860c26t.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/wd376204j</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Chemical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:gx41mp27j</identifier><datestamp>2020-03-23T20:19:48Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les matériaux bi-dimensionels et certains types de surfaces font l’objet d’intenses recherches, puisque leur dimentionalité réduite met en valeur des effets quantiques uniques. Les propriétés expérimentales “topologiques” de ces systèmes peuvent être modifiées et mesurées grâce aux interactions électromagnétiques. Le fourchette de fréquences THz est bien adaptée à cette tâche, puisque plusieurs échelles énergétiques appropriées s’y trouvent et que de large champs électromagnétiques de basse fréquence peuvent y être appliqués. Différentes polarisations optiques peuvent être utilisées pour préparer et mesurer des états quantiques spécifiques, puisque la physique de ces systèmes est souvent liée aux spins des transporteurs de charge.En combinant les avantages de ces deux approches, un instrument de mesure unique, conçu pour cette thèse, est capable de pompe THz, sonde polarimétrique optique ultrarapide, ou vice-versa. La polarisation optique est modulée par une cellule Pockels, permettant de changer rapidement entre les différentes orientations de polarisation. Utilisant ces aptitudes, l’instrument peut sonder des interactions sous-cycles avec un pulse THz moteur, ou mesurer les dynamiques des transporteurs de charge de manière sélective du spin.Cette these élabore un domaine d’expériences maintenant possible, la conception et la construction de l’instrument lui-même, la calibration et les tests de sensibilité confirmant la résolution rendue possible par le matériel d’acquisition, en plus d’expériences futures et une étude de cas dans un matériau optique actif bien connu (ZnTe). Cet instrument de mesure performe de manière similaire aux spectromètres THz conventionnels, et démontre que la sensibilité necessaire à la mesure dynamique de dichroïsme circulare, de biréfringence, et de rotation de Faraday. Cette performance, combinée à la pléthore de matériaux candidats et de physique à observer, représente la création d’un nouvel outil à être utilisé pour une meilleure compréhension de la physique des matériaux</description><description>Currently, two-dimensional materials and some material surfaces are under intense study due to quantum effects brought upon by their low dimensionality. Experimentally, the “topological” properties of these systems can be modified and probed by electromagnetic interactions. The terahertz (THz) frequency range is well-suited for this task, with many material energy scales in this regime and large low-frequency electric and magnetic fields able to be applied. As the physics in these systems are often linked to the carrier spin and orbital degrees of freedom, different polarizations of light can also be used to both prepare and probe specific states.By combining the advantages of these two approaches, the instrument built for this thesis is uniquely capable of THz-pump, ultrafast optical polarimetry-probe measurements, or vice versa. The optical polarization is modulated by a Pockels cell, allowing for rapid switching between different polarization orientations and subsequent lock-in detection. Using these capabilities, the instrument can probe sub-cycle interactions with a driving THz pulse or measure spin-selective carrier dynamics.This thesis will elaborate on the range of experiments now possible, the design and build process of the instrument itself, calibration and sensitivity tests confirming the resolution enabled by the acquisition hardware, as well as future experiments and a case study in a well known optically active material (ZnTe). It is found that this instrument performs similar to traditional THz spectrometers, and demonstrates the necessary sensitivities to measure small time-resolved circular dichroism, birefringence, and Faraday rotation signals. This performance, combined with the wide array of candidates for materials and physics to observe, represents the creation of a new tool to be used for greater understanding in material physics</description><creator>Dringoli, Benjamin</creator><contributor>David Cooke (Supervisor)</contributor><date>2020</date><subject>Physics</subject><title>Design and demonstration of an ultrafast terahertz spectrometer with femtosecond polarization-resolved detection capabilities</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/d791sm63x.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/gx41mp27j</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:p8418s61n</identifier><datestamp>2020-03-23T20:20:12Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Classical two derivative gravity of type IIB String theory is insufficient to satisfy theno-go theorems in order to have a four dimensional spacetime with positive cosmologicalconstant such as De Sitter. But on the other hand, quantum corrections couldallow for de Sitter solutions provided certain constraints are satisfied. But in thetime independent background it is found that in order to maintain such constrainan infinite numbers of time-independent corrections are needed. As they have norelative suppression it causes a breakdown in the effective field theory description.Therefore in this study we look for more general time dependent solutions, whereboth the internal space as well as the background fluxes are all time-dependentwith full De Sitter isometry in four dimensional spacetime. We analyse the both theperturbative and non perturbative quantum corrections in such background anddetermined their corresponding type IIA string coupling gs scaling. Surprisinglywe find out that time dependency allow a finite number of quantum terms at anygiven order in gs thus allowing an EFT description. We also show how the no-gotheorems and the swampland criteria are avoided in time dependent background.Newton’s constant can be kept both time dependent or independent dependingupon the ansatz. But the former has a late time singularity which is not present inthe later case. We try to present convincing arguments to justify the presence of alate time de Sitter vacuum with time independent Newton’s constant to be presentin the IIB string landscape and not in the swampland</description><description>La gravité classique à deux dérivées de type IIB de la théoriedes cordes est insuffisante pour satisfaire les théorèmes non-passables, afin de disposerd’un espace-temps à quatre dimensions avec une constante cosmologiquepositive telle que De Sitter. Par contre, les corrections quantiques pourraient permettredes solutions de Sitter à condition de respecter certaines contraintes. Dans lecontexte indépendant du temps, il est évident que pour maintenir une telle contrainte,un nombre infini de corrections indépendantes du temps est nécessaire.Comme ils n’ont pas de suppression relative, cela entraîne une rupture de la descriptionde la théorie du champ effectif. Par conséquent, dans cette étude, nouscherchons des solutions plus générales dépendantes du temps, où l’espace interne,ainsi que les flux de fond, dépendent du temps avec une isométrie De Sitter dansun espace-temps à quatre dimensions. Nous analysons les corrections quantiquesperturbative et non-perturbative dans un tel arrière-plan et déterminons la miseà l’échelle de leur couplage de chaînes de type IIA correspondant. De manièresurprenante, nous découvrons que la dépendance temporelle permet un nombrefini de termes quantiques pour tout ordre donné, permettant ainsi une descriptionde l’EFT. Nous montrons également comment les théorèmes d’interdiction et lescritères de swampland sont évités dans un contexte dépendant du temps. La constantede Newton peut être dépendante du temps ou indépendante en fonction del’Ansatz. Mais le premier a une singularité tardive qui n’est pas présente dans ledernier cas. Nous essayons de présenter des arguments convaincants pour justifierla présence d’un vide de Sitter à temps tardif avec une constante de Newtonindépendante du temps, présent dans le paysage des cordes IIB et non dans lesswampland</description><creator>Faruk, Mir Mehedi</creator><contributor>Keshav Dasgupta (Supervisor2)</contributor><contributor>James M Cline (Supervisor1)</contributor><date>2020</date><subject>Physics</subject><title>De Sitter solution in the string landscape</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/bg257k059.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/p8418s61n</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:4t64gs64p</identifier><datestamp>2020-03-23T20:20:35Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>La documentation des langages de programmation et de leurs interfaces de programmation (API, de l’anglais Application Programming Interface) existe sous plusieurs formes, qu’il s’agisse de références officielles, d’articles de blogs crées par les utilisateurs, ou d’autres supports textuels et visuels. Des recherches antérieures ont suggéré que les développeurs apprenant une nouvelle API passent souvent d’un type de documentation à l’autre, avec une tendance à alterner entre les références officielles et la documentation de type tutoriel. De plus, la création de documentation est un processus exigeant beaucoup d’efforts qui mène souvent à une répétition de l’information entre différents types de documentation, ce qui crée le risque d’insérer de l’information contradictoire. Cette thèse explore la relation entre la documentation instructive et la documentation de référence des API de trois bibliothèques de domaines différents, les expressions régulières, les connections URL et les entrées-sorties du système de fichiers, dans deux langages de programmation, Java et Python. Notre étude a révélé qu’environ la moitié des phrases de la documentation instructive étudiée décrivent des renseignements relatifs à l’API que l’on s’attendrait à retrouver dans la documentation de référence. Nous étudions également l’étendue de la réutilisation de l’information entre les types de documentation, en nous concentrant sur les phrases de la documentation instructive qui sont des copies exactes, manipulées ou remplaçables par rapport à celles de la documentation de référence. Nous obtenons quatre modèles de réutilisation de l’information basés sur nos observations et découvrons un total de 38 réalisations de ces modèles dans la documentation instructive étudiée. Nous proposons des techniques aidant l’automatisation de chaque modèle de réutilisation pour réduire les efforts liés à la création de documentation, informer la conception de la documentation et promouvoir la cohérence de l’information. Nous évaluons l’impact de l’automatisation de ces modèles de réutilisation sur la documentation actuelle et déterminons que 15 des 38 réalisations de ces modèles n’entraîneront pas de perte d’information, les autres étant modifiées à différents niveaux. Cette thèse est une première étape vers la compréhension de la nature de la réutilisation de l’information à travers différents types de documentation. Les travaux futurs peuvent utiliser nos observations pour améliorer les artefacts documentaires, et automatiser le processus de création de documentation instructive</description><description>Documentation of software programming languages and their APIs exist in many forms, whether as official reference documentation, user-created blog posts or other textual and visual mediums. Prior research has suggested that developers often switch between different types of documentation while learning a new API, with a tendency to alternate between reference and  tutorial-like documentation.  Further, documentation creation is an effort-intensive process that often leads to repeated information across different documentation types, generating a risk of information inconsistency. This thesis explores the relationship between instructional and API reference documentation of three libraries on the topics: regular expressions, URL connectivity and file input/output in two programming languages, Java and Python. Our investigation discovers that about half the sentences in the instructional documentations studied describe API-related information, such as syntax, behaviour, usage  and performance of the API, that is expected to be found in the reference documentation. We also study the extent of information reuse across the documentation types, focusing on sentences in instructional documentation that are exact, manipulated and replaceable matches of those in reference documentation. We elicit four information reuse patterns based on our observations and discover a total of 38 instances of these patterns in the studied instructional documentations. We propose techniques to assist automation of each reuse pattern to reduce documentation creation efforts, inform documentation design and promote information consistency. We assess the impact of automation of these reuse patterns on the current documentation and determine that 15 instances of these patterns will not result in any information loss, the remaining affected with varying levels of modification. This work is a first step towards understanding the nature of information reuse across different documentation types. Future work can use our observations to improve documentation artifacts and automate the instructional documentation creation process</description><creator>Arya, Deeksha</creator><contributor>Jin Guo (Supervisor1)</contributor><contributor>Martin Robillard (Supervisor2)</contributor><date>2020</date><subject>Computer Science</subject><title>Exploring the correspondence between types of documentation for Application Programming Interfaces</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/7d278z82w.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/4t64gs64p</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>School of Computer Science</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:cf95jg79q</identifier><datestamp>2020-03-23T20:20:52Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>During high-energy radiotherapy, secondary neutrons are generated as an unwanted byproduct. It is imperative to accurately determine the neutron spectrum in order to assess the secondary cancer risks posed to the patients by these neutrons. Instruments like the Nested Neutron Spectrometer (NNS) can be used to measure the neutron fluence spectra but the active neutron detector in the spectrometer may be inaccurate when the neutron fluence rate is high due to pulse pileup. Wedeveloped a passive NNS with gold foils that may be operated in high fluence rate environments without suffering from pulse pileup.The passive NNS was used to measure the neutron fluence spectrum at 100 cm from isocentre, along the treatment couch, that was generated by a Varian TrueBeam linac operated at 15 MV. The specific saturation activities of the irradiated gold foils were determined post-irradiation using a high purity germanium detector.To process the measured activities, the response functions of the passive NNS were required. The response functions were generated by first modelling the passive NNS, including moderator shells and gold foils, within the Geant4 v10.4 simulation environment. Simulated irradiations of monoenergetic neutrons were then performed to determine the number of (n,g) neutron capture reactions per unit neutron fluence as a function of neutron energy (i.e. detector response). Theresponse functions generated showed that the detector response shifts to higher energies as the neutron moderator thickness increases, as expected.Using the response functions, the data were iteratively unfolded (i.e. deconvoluted) using the Maximum-Likelihood Expectation-Maximization (MLEM) algorithm to obtain the neutron fluence-rate spectrum at the location of the measurement. The spectrum had two prominent peaks: (i) a dominant peak at the fast neutron energy and (ii) a small peak at the thermal energies. This spectrum was also compared with the spectrum generated with a conventional NNS</description><description>Au cours de la radiothérapie à haute énergie, les neutrons secondaires sont générés en tant que sous-produit indésirable. Il est impératif de déterminer avec précision le spectre des neutrons afin d’évaluer les risques de cancer secondaires que ces neutrons présentent pour les patients. Des instruments tels que le Nested Neutron Spectrometer (NNS) peuvent être utilisés pour mesurer les spectres de la fluence neutronique, mais le détecteur de neutrons actifs dans le spectromètre peut être imprécis lorsque le taux de fluence neutronique est élevé en raison de la superposition d’impulsions. Nous avons développé un NNS passif avec des feuilles d’or pouvant être utilisé dans des environnements à taux de fluence élevé sans souffrir d’empilement.Le NNS passif a été utilisé pour mesurer le spectre de la fluence neutronique à 100 cm de l’isocentre, le long du lit de traitement, généré par un linac Varian TrueBeam fonctionnant à 15 MV. Les activités de saturation spécifiques des feuilles d’or irradiées ont été déterminées après irradiation à l’aide d’un détecteur au germanium de haute pureté. Pour traiter les activités mesurées, les fonctions de réponse du système NNS passif étaient nécessaires. Les fonctions de réponse ont été générées en modélisant d’abord le système NNS passif, y compris les coques de modérateur et les feuilles d’or, dans l’environnement de simulation Geant4 v10.4. Des irradiations simulées de neutrons monoénergétiques ont ensuite été effectuées pour déterminer le nombre de réactions de capture de neutrons (n, g) par unité de fluence neutronique enfonction de l’énergie des neutrons (c’est-à-dire la réponse du détecteur). Les fonctions de réponse générées ont montré que la réponse du détecteur passe à des énergies plus élevées à mesure que l’épaisseur du modérateur de neutrons augmente, comme prévu.En utilisant les fonctions de réponse, les données ont été dépliées de manière itérative (c’està-dire déconvolué) avec l’algorithme MLEM (Maximum-Likelihood Expectation-Maximization ) pour obtenir le spectre du taux de fluence neutronique à l’emplacement de la mesure. Le spectre présentait deux pics importants : (i) un pic dominant pour l’énergie des neutrons rapides et (ii) un petit pic pour les énergies thermiques. Ce spectre a également été comparé au spectre généré avecun NNS conventionnel</description><creator>Mathew, Felix</creator><contributor>John Kildea (Supervisor)</contributor><date>2020</date><subject>Medical Physics Unit</subject><title>Measurement of neutron fluence spectra using a passive nested neutron spectrometer with gold foils</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/nk322k122.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/cf95jg79q</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Medical Physics Unit</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:sf2689662</identifier><datestamp>2020-03-23T20:21:11Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Pour réussir en sciences informatiques, les étudiants doivent apprendre à traduire la réalité en langage informatique, algorithmes et autres pratiques et conventions informatiques. Samantha Breslin desgine ces procédés et pratiques le "rendu technique," un terme qui décrit comment ces procédés et pratiques informatiques façonnent aussi bien la réalité physique que conceptuelle. Ces rendus techniques générent des 'mondes' virtuels qui permettent aux étuidants d'apprehender leur environment en termes de problèmes technologiquement solvables. Toutefois, ceci n'est possible qu'à condition d'omettre les réalités sociales, culturelles, politiques, économiques et historiques de la réalité. Et a compter du moment ou les étudiants s'approprient le concept de "rendu technique" il leur devient tout de suite très difficile d'apprehender toute la complexité de la réalité.Cette thèse est une autoethnographie de certaines de mes expériences pour penser les limites construites par les procédés de "rendu technique." Mon récit s'articule autour d'un atelier/intervention politique que j'ai organisé avec/pour des collègues et amis dans l'industrie de la technologie pour amorcer une conversation sur les défis sociaux et politiques que nous avons a relever en tant que technologues. L'atelier a été une étape dans mon parcours d'élève en informatique, depourvu d'esprit critique, à un programmeur et éducateur en programmation devenant de plus en plus sceptique au dogme de la Silicon Valley, et enfin en tant qu'étudiant diplômé souhaitant s'engager et contribuer a la tâche difficile et politiquement intéressée de défaire les hiérarchies sociales et les oppressions produites par des rapports de colonialisme.Ces descriptions denses et détaillées du chemin que j'ai parcouru, et de mes invitations faites aux autres technologues à apprendre et contribuer, auront je l'espére le potentiel d'inspirer académiciens et autres éducateurs/activistes intéressés par l'enseignement du changement social</description><description>In order to succeed in a computer science (CS) education, CS students must learn how to translate reality into code, algorithms and other computational knowledges and practices. Samantha Breslin refers to these processes and practices as "rendering technical," a term which also encompasses how computational processes and practices shape reality in material and conceptual ways. Technical renderings create computational 'worlds' that enable computer scientists and CS students to frame their world in terms of problems and solutions that are computationally solvable. However, this is possible only by reducing or omitting the social, cultural, political, economic and historical aspects of reality. And once CS students learn how to render technical it becomes very difficult to think in ways that honour the full complexity of reality.This thesis is an autoethnography of some of my experiences in learning to think around the boundaries constructed by processes of rendering technical. My narrative revolves around a workshop/political intervention I organized and facilitated with/for friends and colleagues in tech to begin a conversation about the social and political challenges facing us as technologists. The workshop was a step in my journey from an uncritical CS student, to a computer programmer and computer programming educator becoming increasingly skeptical of Silicon Valley dogma, to a graduate student wishing to commit and contribute to the difficult and non-innocent work of undoing the social hierarchies and the interlocking oppressions produced by the relations of settler colonialism.Thick descriptions of the path I have walked, and of my experiences trying to invite other technologists to learn from and contribute to my learning, can offer insights for academic and activist educators interested in education for social change</description><creator>Halmaghi, Horatiu</creator><contributor>Elizabeth Patitsas (Supervisor)</contributor><date>2020</date><subject>Education</subject><title>Learning computer science was hard. Unlearning computer science is harder.</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/pc289p41p.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/sf2689662</identifier><degree><name>Master of Arts</name><grantor>McGill University</grantor><discipline>Department of Integrated Studies in Education</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:5712mc29c</identifier><datestamp>2020-03-23T20:21:33Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The Belle II experiment represents an opportunity to study the properties of B meson decays, and explore the nature of CP violation through e+e− collisions. Upgrading from the original Belle experiment, Belle II is designed to operate at an instantaneous luminosity which is increased by a factor of 40 to 8×10^35 cm^(−2) s^(−1). To accommodate this increase in luminosity, a significant upgrade was required for many of Belle’s sub-detector systems. Colliding-beam commissioning runs during 2018 (“Phase 2”) and early Belle II data taking runs (“Phase 3”) offered many op- portunities to study the inner components and material within the detector. Using the detector’s precise vertexing capabilities and software tools, an analysis of the material makeup and geometry was performed using γ → e+e− photon conversions, as well as other interactions. Within the detector, many gamma rays originating from the beam collision underwent pair productions within the inner detector region as they interacted with the nuclei of material atoms, transferring their energy into electron/positron pairs. These e+e− pairs then traversed the detector, and were de- tected by tracking sub-detectors such as the Pixel Vertex Detector (PXD), Silicon Vertex Detector (SVD) and Central Drift Chamber (CDC). The path of these par- ticle pairs were then reconstructed to determine the point of conversion. When put together with the millions of other vertices, this offered a very visual and unique way to explore the material and geometry of the inner Belle II sub-detectors</description><description>L’exp ́erience Belle II repr ́esente une opportunit ́e d’ ́etudier les propri ́et ́es des d ́esint ́egrations de m ́esons B et d’explorer la nature de la violation de CP par le biais de collisions  ́electron-positron. Mise `a niveau de l’exp ́erience Belle d’origine, Belle II est conc ̧ue pour fonctionner a` une luminosit ́e instantan ́ee augment ́ee d’eu facteur de 40 a` 8×1035 cm−2s−1. Pour faire face `a cette augmentation de luminosit ́e, des am ́eliorations importantes  ́etaient n ́ecessaires pour de nombreux syst`emes de sous-d ́etecteurs Belle. Les op ́erations de mise en service de faisceaux en collision en 2018 (“Phase 2”) et les premi`eres op ́erations de prise de donn ́ees Belle II (“Phase 3”) ont offert de nombreuses opportunit ́es pour  ́etudier les composants internes et le mat ́eriau dans le d ́etecteur. En utilisant les capacit ́es de vertexing et les outils logiciels pr ́ecis du d ́etecteur, une analyse de la constitution du mat ́eriau et de la g ́eom ́etrie a  ́et ́e r ́ealis ́ee a` l’aide de conversions photoniques γ → e+e−, en plus d’autres interactions. Au sein du d ́etecteur, de nombreux rayons gamma provenant de la collision de faisceaux ont men ́e a` des productions de paires dans la r ́egion du d ́etecteur interne lorsqu’ils interagissaient avec les noyaux d’atomes du mat ́eriel, transf ́erant leur  ́energie en paires e+ e− . Ces paires e+ e− ont ensuite travers ́e le d ́etecteur et ont  ́et ́e d ́etect ́ees par des sous-d ́etecteurs de suivi tels que le d ́etecteur de vertex de pixel (PXD), le d ́etecteur de vertex de silicium (SVD) et la chambre de d ́erive centrale (CDC). Le trajet de ces paires de particules a ensuite  ́et ́e reconstruit pour d ́eterminer le point de conversion. Mis en place encorrelation avec des millionsivd’autres sommets, cela offrait un moyen tr`es visuel et unique d’explorer le mat ́eriau et la g ́eom ́etrie des sous-d ́etecteurs internes de Belle II</description><creator>MacGibbon, Angus</creator><contributor>Steven Robertson (Supervisor)</contributor><date>2020</date><subject>Physics</subject><title>A study of vertex reconstruction performance and spatial distribution in the Belle II detector</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/70795d198.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/5712mc29c</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:8623j335s</identifier><datestamp>2020-03-23T20:21:42Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Divers recherches ont fait des suppositions théoriques sur les raisons pour lesquelles les états ratifient les traités sur les droits la personne. Bien que certains recherches empiriques qualitatifs et quantitatifs de ces théories se soient ensuivis, la recherche qui suit ont souvent des carences. Les études des cas qualitatifs sont souvent difficiles à généraliser à l'ensemble des candidats à la ratification, alors que la vaste analyse des travaux empiriques ne tient souvent pas compte des effets d'acculturation et souffre par conséquent d'un biais variable omis qui mine la confiance dans leurs résultats. Dans cette recherche, j'utilise des méthodes qualitatives et quantitatives pour remettre en question les hypothèses existantes de la littérature, montrer comment les effets d'acculturation peuvent fonctionner et montrer que l'acculturation doit être modélisée de manière appropriée par les spécialistes lors de l'estimation des probabilités de ratification des états relatives aux traités relatifs aux droits de la personne. En commençant par une étude de cas du premier instrument de défense des droits de la personne de l'époque après-guerre, la Convention sur le Génocide, j'utilise de recherche juridique existante pour démontrer les pressions considérables (et partiellement auto-imposées) subies par le Royaume-Uni en raison de sa position déviante à l'égard du traité. Je passe ensuite à une vaste analyse de l'historique des événements de la majorité des principaux instruments des Nations Unies relatifs aux droits des personnes de la période après-guerre (ICCPR, ICESCR, CERD, CEDAW, CRC, CAT, CPED),  en utilisant un ensemble de variables de retard spatial avec des covariables compatibles avec d’autres theories de la littérature afin de déterminer si l'acculturation influe sur la probabilité de ratification par les états, ainsi que de déterminer si les autres théories de la ratification sont robustes à l'ajout de variables qui tiennent compte des processus d'acculturation. Je trouve que la quantité et la qualité de la pression sociale, ainsi que sa durée, semblent influer de manière constante sur la probabilité de la ratification d'un traité d'État relatif aux droits de la personne</description><description>Various scholars have made theoretical suppositions as to why states ratify human rights treaties. While some qualitative and quantitative empirical testing of these theories has ensued, there are deficiencies in the current research. The qualitative case studies are often difficult to generalize to the body of potential ratifiers as a whole, while the large n analysis of empirical work often fail to appropriately account for acculturative effects and accordingly suffer from omitted variable bias which undermines confidence in their results. In this research, I use both qualitative and quantitative methods to challenge the existing assumptions of the literature, demonstrate how acculturative effects may function, and show that acculturation must be appropriately modelled by scholars when estimating state ratification probabilities concerning human rights treaties. Beginning with a case study of the earliest human rights instrument of the post-war era, the Genocide Convention, I use existing legal scholarship to demonstrate the substantial (and partially self-imposed) pressure the United Kingdom faced due to its deviant stance on the treaty. I then turn to a large N event history analysis of the majority of the major UN human rights instruments of the post-war era (ICCPR, ICESCR, CERD, CEDAW, CRC, CAT, CPED), employing a set of spatial lag variables in addition to a number of covariates consistent with other theorization in the literature to assess whether acculturation influences state ratification probability, as well as assess whether the other theories of ratification are robust to the addition of variables that account for acculturative processes. I find that both the quantity and quality of social pressure, as well as its duration seems to consistently influence the probability of state human rights treaty ratification</description><creator>Smith, Henry</creator><contributor>Vincent Pouliot (Supervisor)</contributor><date>2020</date><subject>Political Science</subject><title>It's not what you know, it's who you know: acculturation theory and human rights treaty ratification, an event history analysis</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/k930c261j.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/8623j335s</identifier><degree><name>Master of Arts</name><grantor>McGill University</grantor><discipline>Department of Political Science</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:s7526h715</identifier><datestamp>2020-03-23T20:22:05Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Quadrotors have become the most common and highly popularized small aerial vehicles among robotics researchers, consumers and commercial users. Traditionally, these platforms have been designed with each of the four propellers to spin in the designated direction (two clockwise and two counter-clockwise) to produce unidirectional thrust—this allows the vehicle to oppose gravity when in its nominal hover orientation. In this thesis, we present a quadrotor which is capable of bidirectional thrust actuation: it is generated by reversing the direction of the motors and hence propeller spins. This configuration is motivated by the desire to increase the vehicle’s agility, as well as to imbue it with functionalities not available to standard unidirectional thrust platforms. We present the dynamics model of a quadrotor with bidirectional thrust, with adaptations made to the standard quadrotor model for this type of actuation. To this end, the transient thrust characteristics of a bidirectionally actuated symmetric propeller are experimentally determined, highlighting the maximum rate of change and dead-zone specific to bidirectional thrust. We then demonstrate that a previously developed universal UAV controller can be directly applied to a quadrotor with this type of thrust. Two methods are presented to address the physical constraints of the motor dynamics: optimal control allocation, and model predictive control. We apply these methods to two aggressive maneuvers in simulation and demonstrate improved performance when compared to a direct control allocation method. Experimental testing is also conducted with an outdoor platform for a half flip maneuver, validating the optimal allocation strategy, and demonstrating inverted hover with a bidirectional thrust quadrotor</description><description>Les quadrirotors sont aujourd'hui les véhicules aériens autonomes (UAV) les plus utilisés dans la recherche en robotique, chez les consommateurs, et chez les utilisateurs commerciaux. Traditionnellement, ces plateformes ont été conçues avec chaque moteur pouvant tourner dans une direction donnée (deux dans le sens horaire et deux dans le sens anti-horaire) pour produire une force unidirectionnelle - cela permet au véhicule d'opposer la gravité durant son orientation de vol nominal. Dans cette thèse nous présentons un quadrirotor dont les moteurs et les hélices peuvent tourner en sens inverse afin de générer un actionnement bidirectionnel efficace. Cette configuration est inspirée par le désir d'augmenter l'agilité du véhicule, et d'ajouter des fonctionnalités qui ne sont pas offertes par une plateforme conventionnelle. Nous adaptons le modèle dynamique standard d'un quadrirotor au cas des moteurs bidirectionnels. À cette fin, les caractéristiques transitoires sont déterminées expérimentalement, soulignant le taux de variation maximal et la zone morte spécifiques à la propulsion bidirectionnelle. Ensuite, nous démontrons qu'un contrôleur UAV universel développé antérieurement peut être directement appliqué à un quadrirotor avec ce type de propulsion. Deux méthodes sont présentées pour prendre en compte les contraintes physiques des dynamiques du moteur: l'attribution optimale de contrôle, et le contrôle prédictif. Nous appliquons ces méthodes à deux manœuvres agressives en simulation et démontrons une amélioration de performance, comparé à la méthode d'attribution de contrôle direct. Des évaluations expérimentales sont aussi menées en extérieur avec une plateforme sur une manœuvre de demi-tour, ce qui valide la stratégie d'allocation optimale, et démontre un vol stationnaire inversé avec le quadrirotor</description><creator>Jothiraj, Walter</creator><contributor>Meyer Nahon (Supervisor1)</contributor><contributor>Inna Sharf (Supervisor2)</contributor><date>2020</date><subject>Mechanical Engineering</subject><title>Control of a bidirectional quadrotor for aggressive maneuvers</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/mp48sj19h.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/s7526h715</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:p2677097q</identifier><datestamp>2020-03-23T20:22:14Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Hydrated electron dosimetry consist of measuring absorbed radiation dose to water by monitoring the concentration of hydrated electrons – short-lived radicals produced by the radiolysis of water – using rapid absorption spectrophotometry. The technique has been used in high dose-per-pulse (∼10-100 of Gy per pulse) radiation chemistry research for the past 60 years, but its application in low dose-per-pulse conditions was never investigated due to important technical limitations. The aim of the work presented in this thesis was to assess the suitability of hydrated electron dosimetry in the very low dose-per-pulse regime, that is below 1 cGy per pulse, for possible applications of the technique in radiotherapy dosimetry. A fiber-coupled spectrophotometry prototype was developed for a proof of concept under irradiation of both photon (10 MV FFF, 6 MV FFF, 6 MV) and electron (6 MeV) beams with a Varian TrueBeam medical linear accelerator. Multiple iterations of the prototype were tested, with modifications regarding the design of the absorption cavity (from 3.3 L down to 60 mL) and the chemical composition of the solution (addition of NaOH, removal of dissolved O2). The measured radiation doses ranged from (0.6±0.1) mGy to (1.8±0.2) mGy, and latest results were in agreement with the radiation doses delivered. Optical absorbance appeared to increase linearly with dose delivered, which was suggested, but never reported before in the very low dose-per pulse regime. The half-life of the hydrated electron was also evaluated to ∼ 24μs in the solution. The results presented in this thesis suggest the suitability of hydrated electron dosimetry in the very low dose-per-pulse regime. Thereby, it also suggests the feasibility of employing this water-based technique for dosimetry in radiotherapy. Future work will focus on miniaturizing this technology into a fiber-coupled microcavity for the development of an in vivo dosimeter, for evaluation of the dose delivered to patients during the course of radiation treatments</description><description>La dosimétrie à électrons hydratés consiste à mesurer la dose de rayonnement absorbée dans un volume d’eau en évaluant sa concentration en électrons hydratés – radicaux à courte durée de vie produits via la radiolyse de l’eau – par spectrophotométrie rapide en absorption optique. Depuis plus de 60 ans, cette technique est employée à des fins de recherche en radio-chimie, à l’aide de sources de rayonnement à forte dose-par-impulsion (∼10-100 Gy par impulsion). Cela dit, la technique n’a pas encore été étudiée à l’aide de sources à faible dose-par-impulsion à ce jour, en raison d’importantes limitations techniques. L’objectif du travail présenté dans ce mémoire consiste à évaluer l’adaptabilité de la dosimétrie à électrons hydratés en régime de dose-par-impulsion très faible, soit inférieur à 1 cGy par impulsion, pour de possibles applications en radiothérapie. Pour ce faire, un prototype expérimental de spectrophotométrie couplé par fibre optique a été développé pour la validation du principe, sous irradiation de faisceaux de photons (10 MV FFF, 6 MV FFF, 6 MV) et d’électrons (6 MeV) à l’aide d’un accélérateur linéaire médical TrueBeam de Varian. Plusieurs itérations du prototype se sont succédées, en modifiant notamment le design de la cavité d’absorption (de 3.3 L à 60 mL) et la composition chimique de la solution (ajout de NaOH, élimination de O2 dissous). Les doses de rayonnement mesurées avec le prototype s’échelonnent de (0.6 ± 0.1) mGy à (1.8±0.2) mGy, et les plus récents résultats concordent avec les doses administrées. De plus, une linéarité a été observée entre l’absorbance optique de la solution et la dose administrée. Cette relation a été suggérée, mais jamais observée en régime de dose-par-impulsion très faible auparavant. La demi-vie de l’électron hydraté a également été évaluée à ∼ 24μs dans la solution. Les résultats présentés dans ce mémoire suggèrent l’adaptabilité de la dosimétrie à électrons hydratés en régime de dose-par-impulsion très faible. Ils laissent aussi entrevoir la possibilité d’utiliser cette technique à des fins dosimétriques en radiothérapie. Les travaux futurs porteront sur la miniaturisation de cette technologie dans une microcavité optique, pour le développement d’un dosimètre in vivo en radiothérapie</description><creator>Mégrourèche, Julien</creator><contributor>Shirin Abbasi Nejad Enger (Supervisor1)</contributor><contributor>Lilian Childress (Supervisor2)</contributor><date>2020</date><subject>Medical Physics Unit</subject><title>Development of a hydrated electron dosimeter for radiotherapy applications: a proof of concept</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/sq87bz72s.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/p2677097q</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Medical Physics Unit</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:r781wm67s</identifier><datestamp>2020-03-23T20:22:26Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Society recognises the impact of greenhouse gases on the climate, which has encouraged operators and the government to combine forces to develop more sustainable waste management practices. To enforce these essential changes, waste management laws have been reformed and improved methods for waste handling are urgently required. The Government of Quebec announced the ban of  incineration and landfilling of municipal organic waste by 2022, which directly affects the wastewater treatment industry. Activated sludge treatment of wastewater produces waste activated sludge (WAS), which is treated and dewatered to produce carbon and nutrient-rich biosolids. Many wastewater treatment plants (WWTPs) are opting for anaerobic digestion (AD) of WAS to recover energy, reduce biosolids management costs and disinfect biosolids. However, in cold-climate countries, approximately a third of the energy produced during digestion is used to heat the digesters to an operation temperature of 35 °C, as temperatures of wastewater can be lower than 10 °C for large portions of the year.  The goal of this thesis was to reproduce at pilot-scale using a different source of WAS, experiments conducted previously in our laboratory that evidenced the capacity of ozone pretreatment to enhance suspended solids destruction and methane production during anaerobic digestion. The project to which this thesis contributes intends to develop a process operating at either 20 or 35 °C. However, this thesis reports on a one-year pilot-scale digester experiment performed only at 35 °C, and a series of lab-scale complementary experiments performed at both temperatures.In the pilot-scale experiment performed with Victoriaville WAS, ozone treatment of the recirculating digestate solubilized chemical oxygen demand (COD) and improved the destruction of volatile suspended solids (VSS), however enhanced methane production rates were not observed. To further explore this result and assess the benefits and limitations of ozone pretreatment of WAS on AD with adequate replication, several 500-mL serum-bottle semi-continuous digester experiments were conducted. They confirmed that ozone pretreatment had a significant positive impact on VSS destruction and methane production, as they increased linearly with the dose of ozone transferred to the WAS. However, comparison of Victoriaville and La Prairie WAS suggested that the La Prairie WAS reacted more readily to treatment than Victoriaville, which resulted in higher process enhancement. Therefore, the origin of the sludge has a significant impact on the efficiency of the pretreatment. Finally, the effect of the ozone pretreatment and AD on pathogen inactivation was studied to evaluate the sanitation potential of the pretreatment. The results from this last experiment revealed that ozone inactivated approximately 1 log of E. coli, but the surviving population was more resistant to inactivation during anaerobic digestion than the original population</description><description>Aujourd’hui, la société comprend mieux que jamais l’impact des gaz à effet de serre sur le climat, ce qui amène la population et le gouvernement à s’unir pour révolutionner l’utilisation et la disposition des ressources. Concrètement, cette transformation prend forme avec des lois votées sur la gestion des déchets. Dès 2022, le Gouvernement du Québec interdira l’incinération et l’enfouissement des déchets organiques municipaux, ce qui impactera directement l’industrie du traitement des eaux usées. Le procédé de traitement des eaux usées dit « à boues activées » génère comme coproduits, des boues activées qui sont traitées et asséchées pour produire des biosolides riches en carbone et en nutriments. Plusieurs usines de traitement des eaux usées (UTEU) choisissent la digestion anaérobie (DA) des boues activées pour récupérer l’énergie tout en réduisant les coûts associés à la gestion des biosolides et la concentration en pathogène. Une problématique subsiste toutefois dans les pays nordiques où la température des eaux usées peut descendre sous 10 °C pour une grande partie de l’année. Dans de telles conditions, il a été estimé que le tiers de la production d’énergie générée au cours de la digestion est nécessaire pour maintenir la température d’opération à 35 °C. L’objectif du projet dont traite cette thèse est de reproduire, à l’échelle pilote et avec des boues activées d’origines et de nature différentes, des expériences qui ont été conduites dans notre laboratoire mettant en évidence la capacité du prétraitement à l’ozone d’augmenter la destruction de solides suspendus et la production de méthane durant la digestion anaérobie. Initialement, le but était de développer un processus qui fonctionne à 20 °C et à 35 °C. Cette thèse traite toutefois de digesteurs anaérobies à l’échelle pilote ayant été opérés pendant une année à 35 °C seulement, et d’une série d’expériences complémentaires à l’échelle de laboratoire opérant à 20 °C et à 35 °C. Dans l’expérience à l’échelle pilote faite avec les boues activées de Victoriaville, le traitement à l’ozone du digestat en recirculation a augmenté la concentration de la demande chimique en oxygène (DCO) dans la fraction soluble du digestat et a augmenté le taux de destruction des matières volatiles en suspension (MVES). Toutefois, le traitement n’a pas amélioré la production de méthane. Pour comprendre ce résultat et estimer les bénéfices et les limitations du pré-traitement à l’ozone des boues activées sur la DA, plusieurs expériences de digestion anaérobie en mode semi-continu dans des flacons de 500 ml ont été conduites. La taille et les bas coûts d’opération des bouteilles ont permis de maximiser la quantité de réplicas et de paramètres pouvant être testés. Les expériences en bouteilles ont confirmé que le pré-traitement à l’ozone a un impact positif significatif sur la destruction des MVES et sur la production de méthane, deux variables qui ont une relation linéaire positive avec les doses d’ozone transférées.  Toutefois, une comparaison entre les boues activées de Victoriaville et de La Prairie suggère que les boues activées de cette dernière réagissent plus positivement au traitement.  Par conséquent, l’origine des boues aurait un impact significatif sur l’efficacité du pré-traitement. Finalement, l’effet du pré-traitement à l’ozone et de la digestion anaérobie sur l’inactivation des pathogènes a été étudié pour évaluer le potentiel d’assainissement du pré-traitement. Les résultats de cette dernière expérience ont révélés que lorsque l’ozone est conjugué avec la DA, environ 1 log de E. coli de plus est inactivé. Toutefois, la population ayant survécu à l’ozonation était plus résistante à l’inactivation durant la DA que la population originale</description><creator>Tremblay, Alexandre</creator><contributor>Dominic Frigon (Supervisor1)</contributor><contributor>Jean-François Lemay (Supervisor2)</contributor><date>2020</date><subject>Civil Engineering and Applied Mechanics</subject><title>Novel ozonation treatment to enhance anaerobic digestion of WAS: scale-up challenges and sanitary potential</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/wd376205t.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/r781wm67s</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Civil Engineering and Applied Mechanics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:ff365950d</identifier><datestamp>2020-03-23T20:22:37Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>In recent years, management strategies for diverticulitis have evolved in response to increasingevidence to support more conservative treatment approaches. Due to this shift in increased nonoperativemanagement, recurrent diverticulitis has become a significant problem. Though not allpatients will develop a recurrence, up to 35% may experience at least one more episode of thedisease. There is uncertainty and inconsistency regarding the predictors of recurrentdiverticulitis, which has led to challenges in clinical decision making. Several retrospectiveanalyses reported on the predictors of recurrence of acute diverticulitis but only one studyreported on the impact of family history on the risk of recurrence. To address this gap, weinvestigated the role of family history on the risk of diverticulitis recurrence. We observed astrong impact of family history on the risk of diverticulitis recurrence. Furthermore, we foundthat the number of relatives has a specific role. Finally, we observed a significant impact offamily history on the risk of a complicated recurrence. These findings can guide future decisionmaking when considering risk factors for recurrence and the role of elective colectomy</description><description>Ces dernières années, les stratégies de gestion de la diverticulite ont évolué grâce aux donnéesplus probantes en faveur de traitements plus conservatrices. En raison de ce changement dans lagestion non-opératoire accrue, la diverticulite récurrente est devenue un problème important.Bien que tous les patients ne développent pas de recurrence, jusqu'à 35% d’entre eux peuventsubir au moins un autre épisode de la maladie. Les prédictions de la diverticulite récurrente sontpeu fiables et inconsistents, ce qui a rendu la prise de décision clinique difficile. Plusieursanalyses rétrospectives ont rapporté certains facteurs prédicteurs de la réapparition de ladiverticulite aiguë, mais une seule étude s’est intéressé à l'impact des antécédents familiaux surle risque de récurrence. Pour combler cette lacune, nous avons étudié le rôle des antécédentsfamiliaux sur le risque de recurrence de la diverticulite. Nous avons observé un fort impact desantécédents familiaux sur le risque de recurrence de diverticulite. De plus, le nombre Membresde la famille joue un rôle spécifique. Enfin, nous avons observé un impact significatif desantécédents familiaux sur le risque de récurrence compliquée. Ces résultats peuvent guider lesdécisions futures lors de la prise en compte des facteurs de risque de recurrence et du rôle de lacolectomie élective</description><creator>Almalki, Turki</creator><contributor>Marylise Boutros (Supervisor)</contributor><date>2020</date><subject>Surgery</subject><title>Family history as predictor of recurrent diverticulitis after an episode of uncomplicated diverticulitis</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/t435gj323.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/ff365950d</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Surgery</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:gt54ks52n</identifier><datestamp>2020-03-23T20:22:59Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>A two-step non-asymptotic approach for parameter and state estimation in ReproducingKernel Hilbert Space (RKHS) is presented in this thesis. It begins with the understandingand derivation of double sided kernel representation for a fourth order linear system andproceeds into discussing and developing methods for state and parameter estimation fromsingle noisy realizations of the system output on a time interval [a; b]. Once the parametersare estimated the output is reconstructed by projection onto the span of fundamentalsolutions and this in turn is used to reconstruct the time derivatives of the system output</description><description>Une approche non asymptotique en deux étapes pour l’estimation de paramètres et d’étatsbasée sur la reproduction du noyau de Hilbert (RKHS) est présentée dans cette thèse. Ilcommence par la compréhension et la dérivation de la représentation du noyau à doubleface pour un système linéaire de quatrième ordre, puis discute et développe des méthodesd’estimation d’états et de paramètres à partir de réalisations bruitées de la sortie systèmesur un intervalle de temps [a; b]. Avec les paramètres estimés, la sortie est reconstituée parprojection dans la portée linéaire de solutions fondamentales, ce qui permet ensuite dereconstruire les dérivées temporelles de la sortie du système</description><creator>John, Anju</creator><contributor>Hannah Michalska (Supervisor)</contributor><date>2020</date><subject>Electrical and Computer Engineering</subject><title>Estimation for SISO LTI systems using differential invariance</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/47429f910.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/gt54ks52n</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:8910jz75m</identifier><datestamp>2020-03-23T20:23:19Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Nuclear reactions induced during high-energy radiation therapy produce secondary neutrons that, due to their carcinogenic potential, constitute an important risk for the development of iatrogenic cancer. Experimental and epidemiological findings indicate a marked energy dependence of neutron relative biological effectiveness (RBE) for carcinogenesis, but little is known about its physical basis. While the exact mechanism of radiation carcinogenesis is yet to be fully elucidated, numerical microdosimetry can be used to predict the biological consequences of a given irradiation based on its microscopic pattern of energy depositions. Building on recent work, this thesis studies the physics underlying neutron RBE by using the microdosimetric quantity dose-mean lineal energy (yD) as a proxy.A simulation pipeline was constructed to explicitly calculate the yD of radiation fields that consists of (i) the open source Monte Carlo (MC) toolkit Geant4, (ii) its radiobiological extension Geant4-DNA, and (iii) a weighted track-sampling algorithm. This approach was used to evaluate the yD of mono-energetic neutrons with initial kinetic energies between 1 eV and 10 MeV at multiple depths in a tissue-equivalent phantom approximately the size of a human adult torso. Spherical sampling volumes with diameters between 2 nm and 1000 nm were considered. To obtain a measure of RBE, the neutron yD values were divided by those of 250 keV x-rays that were calculated in the same way. Qualitative agreement was found with published radiation protection factors and simulation data, allowing for the dependencies of neutron RBE on depth and energy to be discussed in the context of the neutron interaction cross sections and secondary particle distributions in human tissue</description><description>Les réactions nucléaires induites par la radiothérapie à haute énergie produisent des neutrons secondaires qui, en raison de leur potentiel cancérogène, constituent un risque important pour le développement du cancer iatrogène. Les résultats expérimentaux et épidémiologiques indiquent une importante dépendance énergétique de l'efficacité biologique relative (EBR) des neutrons pour la cancérogenèse, mais on en connait toutefois très peu sur ses fondements physiques. Bien que le mécanisme exact de la cancérogenèse par rayonnement n’ait pas encore été complètement élucidé, la microdosimétrie numérique peut être utilisée pour prédire les conséquences biologiques d’une irradiation donnée sur la base de son modèle microscopique de dépôts d’énergie. En s'appuyant sur des travaux récents, cette thèse étudie la physique sous-jacente à l'EBR des neutrons en utilisant la dose-énergie moyenne linéaire (yD), une quantité microdosimétrique, comme proxy.Un structure de simulation a été développée pour calculer explicitement la yD des champs de rayonnement. Elle est constituée de (i) la boîte à outils en libre accès Monte Carlo (MC) Geant4, (ii) de son extension radiobiologique Geant4-DNA, et (iii) d’un algorithme d’échantillonnage pondéré. Cette approche a été utilisée pour évaluer la \yD{} de neutrons mono-énergétiques, dont l’énergie cinétique initiale était comprise entre 1 eV et 10 MeV, à plusieurs profondeurs dans un fantôme de composition équivalente aux tissus biologiques et de taille comparable à torse humain adulte. Des volumes d'échantillonnage de forme sphérique ont été considérés, avec un diamètre compris entre 2 nm et 1000 nm. Pour obtenir une mesure de l'EBR, les valeurs de yD de neutrons ont été divisées par celles de rayons X de 250 keV, calculées de la même manière. Un accord qualitatif a été établi entre les facteurs de radioprotection publiés et les données de simulation. Cela a permis un discussion sur les dépendances de l'EBR sur la profondeur et l'énergie sont discutées dans le contexte des sections efficaces d'interaction neutronique et de la distribution des particules secondaires dans les tissus humains</description><creator>Lund, Christopher</creator><contributor>John Kildea (Supervisor)</contributor><date>2020</date><subject>Medical Physics Unit</subject><title>A microdosimetric analysis of the interactions of mono-energetic neutrons with human tissue</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/x346d8577.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/8910jz75m</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Medical Physics Unit</discipline></degree></thesis></metadata></record><resumptionToken completeListSize="47894">oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):37425</resumptionToken></ListRecords></OAI-PMH>