<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/assets/blacklight_oai_provider/oai2-b0e501cadd287c203b27cfd4f4e2d266048ec6ca2151d595f4c1495108e36b88.xsl"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd"><responseDate>2020-07-25T01:03:33Z</responseDate><request resumptionToken="oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):15950" verb="ListRecords">https://escholarship.mcgill.ca/catalog/oai</request><ListRecords><record><header><identifier>oai:escholarship.mcgill.ca:7m01bq13r</identifier><datestamp>2020-03-21T19:55:46Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Age Related Macular Degeneration (AMD) is the most common cause of vision loss among the elderly in developed countries.  It occurs primarily in individuals over the age of 50.  Currently, 1.75 million people in the US suffer from the advanced form of AMD.  AMD is characterised as an abnormality of retinal pigment epithelium (RPE) and/or choroid leading to photoreceptor degeneration of central retina (macula).  There are two forms of AMD: Dry AMD (characterized by the build-up of drusen between the choroid and RPE layer resulting in RPE and photoreceptor cell death) and Wet AMD (characterized by abnormal blood vessel growth from the choroid into the retinal pigment epithelium). Current pharmacotherapies in AMD include anti-angiogenics (anti-VEGF) such as Macugen, Avastin, and Lucentis.  Thus current research is focusing on trying to form combination therapies (such as anti-VEGF and other agents) to achieve better clinical efficacy.  We will be investigating the compound 3,4 dihydroxyphenyl ethanol (DPE), which is a polyphenol present in virgin olive oil known to have antioxidant, anti-angiogenic, anti-inflammatory, and antithrombotic properties.  Previous studies have investigated DPE and its ability to prevent cardiovascular diseases and treat different types of cancer.  We believe that DPE can reduce angiogenic signalling in the macula.  Our objective is to assess the potential utility of DPE as a therapeutic agent in combination with anti-VEGF drugs.  ARPE-19 cells were treated with 0.25 mg/ml bevacizumab to study the effects of bevacizumab on the secretion of pro-angiogenic cytokines.  The cells were then treated with 100M DPE for 24 hours in culture in both normoxic and CoCl₂-simulated hypoxic conditions. RPE cells were also treated with the combination of DPE and bevacizumab in order to determine the effectiveness of the combination therapy on RPE cells.  Media was then harvested after 24 hours for sandwich ELISA-based angiogenesis arrays.  The secretion of the following 10 pro-angiogenic cytokines was measured: Angiogenin, ANG-2, EGF, bFGF, HB-EGF, PDGF-BB, Leptin, PlGF, HGF, and VEGF-A.  The secretion of three (Angiogenin, ANG-2, and EGF) was increased following treatment with bevacizumab, however only Angiogenin was significant.  Angiogenin and VEGF-A were secreted under normoxia, and significantly increased under CoCl₂-simulated hypoxia, whereas ANG-2, HB-EGF, and PlGF were increased under hypoxia.  Following treatment with DPE, levels of Angiogenin and VEGF-A were significantly reduced under normoxia, whereas secretion of all 5 secreted cytokines were significantly decreased under hypoxia.  The combination of DPE and bevacizumab significantly reduced the secretion of Angiogenin under both normoxic and hypoxic conditions compared to bevacizumab alone.  Considering the implications of angiogenesis in AMD, these studies could provide the framework for future studies to further investigate a potential therapeutic role for DPE.  DPE may reduce the secretion of pro-angiogenic cytokines, such as Angiogenin, that are up-regulated following treatment with bevacizumab as a possible compensatory mechanism.  Therefore, the combination of DPE and bevacizumab may represent a valuable therapeutic option for the wet form of AMD.</description><description>La dégénérescence maculaire liée à l'âge (DMLA) est la cause la plus fréquente de la perte de la vision chez les personnes âgées dans les pays développés. Il survient principalement chez les personnes âgées de plus de 50 ans. Actuellement, 1,75 millions de personnes aux États-Unis souffrent de la forme avancée de la DMLA. La DMLA se caractérise par une anomalie de l'épithélium pigmentaire rétinien (EPR) et / ou de la choroïde, conduisant à la dégénérescence des photorécepteurs de la rétine centrale (macula). Il existe deux formes de DMLA: la DMLA de type sèche (caractérisée par l'accumulation de petites taches blanches sous la rétine (drusen) entre la choroïde et l'EPR résultant en la mort des cellules photoréceptrices) et la DMLA de type humide (caractérisée par une croissance anormale de néovaisseaux choroïdiens (NVC) sous l'épithélium pigmentaire de la rétine). Les pharmacothérapies actuelles pour traiter la DMLA comprennent les anti-angiogéniques (anti-VEGF), tels que Macugen, Avastin et Lucentis. Ainsi la recherche actuelle se concentre à essayer de former des combinaisons thérapeutiques (tels que des agents anti-VEGF et d'autres) pour parvenir à une meilleure efficacité clinique. Nous examinerons le 3,4 dihydroxyphenyl ethanol (DPE), qui est un polyphénol présent dans l'huile d'olive vierge connu pour avoir des propriétés antioxydantes, anti-angiogéniques, anti-inflammatoire, et des propriétés antithrombotiques. Des études antérieures sur le DPE ont démontré sa capacité à prévenir les maladies cardio-vasculaires et traiter les différents types de cancer. Nous croyons que le DPE peut réduire la signalisation angiogénique dans la macula. Notre objectif est d'évaluer la potentielle utilité de DPE comme agent thérapeutique en combinaison avec des médicaments anti-VEGF. Les cellules ARPE-19 ont été traitées avec 0,25 mg/ml de bevacizumab pour étudier les effets du bevacizumab sur la sécrétion de cytokines pro-angiogéniques. Ces cellules ont ensuite été traitées avec 100µM de DPE en culture pendant 24 heures à la fois dans des conditions normoxiques et des conditions simulé hypoxiques (CoCl₂). Les cellules de l'EPR ont également été traitées avec la combinaison de DPE et de bevacizumab en vue de déterminer l'efficacité de la thérapie avec cette combinaison sur les cellules de l'EPR. Le milieu de culture a ensuite été récolté après 24 heures pour proceder au sandwich ELISA pour tester l'angiogenèse. La sécrétion des 10 suivants cytokines pro-angiogéniques a été mesurée: Angiogenin, ANG-2, EGF, bFGF, HB-EGF, PDGF-BB, Leptin, PlGF, HGF, and VEGF-A. La sécrétion de trois d'entre eux (Angiogenin, ANG-2, et EGF) a été augmentée à la suite du traitement par bevacizumab, mais seulement celle de l'Angiogenin a été significative. L'Angiogenin et le VEGF-A ont été sécrétés sous normoxie, et ont considérablement augmenté en vertu de l'hypoxie simulée par CoCl₂, alors que le ANG-2, le HB-EGF et le PlGF ont été augmentée en vertu de l'hypoxie. Après le traitement avec DPE, les niveaux de Angiogenin et le VEGF-A ont été considérablement réduits en normoxie, tandis que la sécrétion de toutes les 5 cytokines sécrétées a été significativement diminuée sous l'hypoxie. La combinaison de la DPE et du bevacizumab a considérablement réduit la sécrétion de l'Angiogenin dans des conditions à la fois normoxiques et hypoxiques en comparaison avec le bevacizumab utilisé seul. Considérant les implications de l'angiogenèse dans la DMLA, ces études pourraient servir de base pour de futures études pour pousuivre les recherches sur le rôle thérapeutique potentiel du DPE. Le DPE peut réduire la sécrétion de cytokines pro-angiogéniques, comme celle de l'Angiogenin, qui augmentent après un traitement par le bevacizumab comme un possible mécanisme compensatoire. Par conséquent, la combinaison du DPE et du bevacizumab peut représenter une option thérapeutique valable pour la forme humide de la DMLA.</description><creator>Granner, Tamara</creator><contributor>Miguel Noel Burnier (Internal/Supervisor)</contributor><date>2013</date><subject>Health Sciences - Pathology</subject><title>Investigation of anti-angiogenic effects of 3,4 dihydroxyphenyl ethanol in macular degeneration</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/5712mb430.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/7m01bq13r</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Pathology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:df65vc15t</identifier><datestamp>2020-03-21T19:55:47Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Le design de réseaux robustes (RND) est celui qui applique le concept de robustesse, issu de l'optimisation avec incertitude, au domaine de la conception de réseaux. Les principales motivations derrière cette application découlent de demandes provenant des réseaux de télécommunication. La prémisse principale est que les demandes à travers les réseaux sont variables ou imprévisibles. Toutefois, nous savons que ces demandes proviennent d'un ensemble prédéfini appelé univers de demandes. De plus, des contraintes pratiques du design de réseaux requiert que le routage des demandes soit inconscient, ou fixé d'avance, et qu'il ne dépende pas d'une instanciation particulière de l'univers de demandes. Des contraintes additionnelles, connues sous le nom de modèle de routage, s'appliquent souvent à la structure du routage. Les routages par chemins les plus courts (SP) et par moyeu unique (HUB) ont reçu une attention importante, tant au niveau théorique que pratique. Dans cette thèse, nous introduisons un nouveau modèle de routage appelé routage hiérarchique par moyeux (HH), qui est une généralisation de HUB. Nous étudions les propriétés théoriques de RND restreint à HH (RNDHH). Plus particulièrement, nous démontrons son caractère APX-difficile et fournissons un algorithme O(log n)-approché. Par la suite, nous montrons comment RNDHH devient facilement soluble lorsque restreint à un univers de demandes particulier, basé sur des demandes qui peuvent être routées sur un arbre donné. Nous comparons également le coût des solutions optimales lorsque RND utilise HH ainsi que d'autres modèles de routage inconscients importants. Finalement, nous exploitons HH dans une étude pratique sur un nouvel univers de demandes, appelé modèle par tuyaux restreints, qui est un mélange de deux univers de demandes largement utilisés soit le modèle par tuyaux et le modèle par conduits. Nous utilisons le modèle par tuyaux restreints pour caractériser quel univers de demandes favorise un routage similaire à SP contrairement à un routage HH. Pour ce faire, nous développons un algorithme heuristique pour RNDHH et évaluons notre approche par rapport à SP à l'aide de réseaux d'opérateur ainsi que plusieurs types de demandes du modèle par tuyaux restreints, ceux-ci ayant été paramétrés par leur similitude à un modèle par tuyaux ou un modèle par conduits. Cette étude révèle les conditions à travers lesquelles le routage par multiples moyeux, c'est-à-dire HH, surpasse celui par HUB et SP.</description><description>Robust network design (RND) applies the concept of robustness from optimization with uncertainty to the area of network design. Primary motivations stem from applications in telecommunication networks. The main presupposition is that demands across the networks are variable or unpredictable. They originate from a predefined demand set, called a demand universe. Moreover, practical impediments of network design enforce the routing of the demands to be oblivious, or fixed in advance, and to not depend on a particular instantiation from the demand universe. Additional restrictions, referred to as a routing model, are often enforced on the routing's structure. Shortest paths (SP) and hub (HUB) routing models have received particular attention, both on the theoretical and practical level. In this work, we introduce a new routing model, called the hierarchical hub routing model (HH), as a generalization to HUB. We study the theoretical properties of RND restricted to HH (RNDHH). Namely, we show its APX-hardness and provide a O(log n)-approximation algorithm. We then show how RNDHH is tractable when the problem is constrained to a particular demand universe based on demands routable on a tree. We also compare the costs of optimal solutions to RND using HH and other important oblivious routing models. Finally, we leverage HH in a practical study of a new demand universe called the capped hose model, which is a blend of the hose and the pipe model, two widely used demand universes. We use the capped hose model to shed light on which demand universes favour more a SP-like as opposed to a HH-like routing. To do so, we develop a heuristic algorithm for RNDHH, and benchmark our approach against SP using representative carrier networks and a variety of capped hose demands, parametrized by their similitude to a hose or pipe model. This study reveals conditions under which multi-hub routings, that is HH, gives improvements over single-hub and shortest path routings.</description><creator>Fréchette, Alexandre</creator><contributor>Frederick Shepherd (Internal/Supervisor)</contributor><date>2013</date><subject>Pure Sciences - Mathematics </subject><title>Hub routing for the robust network design problem</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/v405sf039.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/df65vc15t</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Mathematics and Statistics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:pv63g3705</identifier><datestamp>2020-03-21T19:55:47Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Of interest are two knowledge translation [27] research projects conducted by and with the ITPCRG (Information Technology Primary Care Research Group) during the period 2010-2012, as well as their underlying statistical analyses. For physicians, continuing medical education (CME) is a critical activity that helps them acquire new knowledge and keep their practice up to date. In Canada, popular CME programs are structured around the reading of short synopses or summaries of important clinical research on e-mail. After reading one synopsis, the physician completes a short reective exercise, using the Information Assessment Method (IAM). IAMis a brief questionnaire that asks physicians to reect on the following: -Therelevance of the information? -The impact of the information e.g. did you learn something new? -If they intend to use the information for a specic patient? -Whether they expect to see health benets for that patient as aresult? This type of CME is very popular. Since September 2006, about4,500 members of the Canadian Medical Association have submitted more than one million IAM questionnaires linked to e-mailed synopses. Previous work suggests the response format of the IAM questionnaire can impact the willingness of physicians to participate, and that information use for a specic patient might be linked to certain factors measurable by IAM. Therefore, the objectives were to improve CME programs that use the IAM questionnaire by determining which response formats optimize physician participation and their reective learning, and explore the determinants of information use. These were accomplished by implementing a survival analysis framework, as well as mixed logistic regression models.</description><description>Ce memoire porte sur deux projets de mise en pratique des connaissances menes par et avec le ITPCRG (Information Technology Primary Care Research Group) de 2010 a 2012, ainsi que l'analyse statistique qui s'en est issue. La formation medicale continue est une activite essentielle qui aide l'acquisition de nouvelles connaissances et la mise a jour des pratiques pour les medecins. Au Canada, des programmes populaires utilisentla lecture de courts synopsis ou de sommaires de recherches cliniques importantes transmis par courriel. Apres la lecture du synopsis, le medecin complete un bref exercice de reexion en utilisant le Information Assessment Method (IAM). IAM est un petit questionnaire qui demande aux medecins de reechir aux sujets qui suivent: -La pertinence de l'information? -L'impacte de cette information ex : avez-vous appris quelque chose?-L'intention d'utiliser cette information pour un patient specique? -Anticipent-ils observer des beneces de sante pour ce patient grâce a cetteinformation? Ce type de formation continue medicale est tres populaire. Depuis septembre 2006, pres de 4500 membres de l'Association medicale canadienne ont soumis plus d'un million de questionnaires IAM relies auxsynopsis recus par courriel. Les recherches precedentes suggerent que leformat de reponse des questionnaires IAM peut inuencer la participationdes medecins et que l'utilisation de l'information pour un patient specique peut être liee a certains facteurs mesurables par IAM. Les mêmes recherches indiquent que certains formats peuvent stimuler des reponses plus reechies. Aucune recherche n'a etudie l'eet de ce genre de formation continue surla sante de patients speciques. Les objectifs etaient donc d'ameliorer les programmes d'education continue medicale qui utilisent les questionnaires IAM en determinant les formats de reponse qui optimisent la participation des medecins ainsi que l'apprentissage reectif, et d'explorer les facteurs relies a l'utilisation de l'information. Ceux-ci ont ete accomplis en executant une analyse de la survie, ainsi que des modeles de regression logistique mixtes.</description><creator>Moscovici, Jonathan</creator><contributor>Alain Charles Vandal (Internal/Supervisor)</contributor><contributor>Roland Grad (Internal/Cosupervisor2)</contributor><date>2013</date><subject>Pure Sciences - Statistics </subject><title>Statistical applications in knowledge translation research implemented through the information assessment method</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/wm117s48h.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/pv63g3705</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Mathematics and Statistics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:b8515r86j</identifier><datestamp>2020-03-21T19:55:48Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The Ni-base superalloy, Waspaloy, was linear friction welded (LFWed) under various processing conditions. Specifically, axial shortening, in which all linear friction welding (LFW) parameters such as amplitude, frequency, pressure, and time are included, was varied from 0.7 to 4.6 mm. In-situ temperature measurements during welding were carried out by placing thermocouples at different locations from the weld interface. Mechanical properties of the weldments in the as-welded and post-weld heat treated (PWHTed) conditions were evaluated using tensile testing, integrated with the optical image correlation Aramis® system, and microhardness measurements. To correlate the process parameters and microstructural changes that affect the mechanical properties, microstructure evolution across the weld in the as-welded and PWHTed conditions was investigated using optical microscopy, electron backscatter diffraction (EBSD), scanning electron microscopy (SEM), energy-dispersive spectroscopy (EDS), transmission electron microscopy (TEM), and micro computed tomography imaging. Tensile testing indicated that there is a critical axial shortening value (2 mm) below which LFWed and PWHTed Waspaloy exhibited weak integrity. At and above this critical shortening, the yield strength and ultimate tensile stress values were more or less the same as those of the parent material. However, total elongation continued to increase with axial shortening even above the critical value due to decrease in the width of the thermomechanically affected zone (TMAZ). The sample with the highest axial shortening (4.9 mm), exhibited an elongation equal to 91 % of the parent material elongation. Weak integrity at axial shortening below 2 mm was mainly due to lack of bonding and/or presence of oxides at the weld interface. Microstructure examination using EBSD mapping revealed that dynamic recrystallization (DRX) occurred in a region about 1 mm wide on either side of the weld interface. In the as-welded condition, SEM and TEM studies indicated that progressive dissolution of γ' precipitates took place from the parent material to the weld interface, where almost no γ' precipitates were observed. The γ' dissolution significantly influenced the hardness profile measured across the TMAZ. The applied post-weld heat treatment (PWHT) restored the hardness in the weld region.The data recorded by thermocouples indicated that during LFW the temperature in the weld area reached up to 50 C below the melting point of the bulk alloy (1330 C). This temperature is well above the liquation temperature of the low melting point components in the alloy (1245 C). The possible occurrence of liquation and consequent microcracking were investigated in this study. It was shown that the high pressure applied during the oscillation and forge phases of the LFW process and the resulting grain refinement contributed in preventing liquation and microcracking in the weldments.Finally, hot compression tests were conducted on Waspaloy in the 1060 to 1140 C temperature range and 0.001 to 1 s-1 strain rate range up to a strain of 0.83 to study the high temperature deformation behavior of the alloy. Flow softening and microstructure investigation indicated that DRX occurred during deformation. For the investigated conditions, the activation energy for hot deformation of Waspaloy was determined to be 462 kJ/mol. The equations relating the dynamic recrystallized (DRXed) grain size to temperature and strain rate were developed from the hot compression experiments. The developed equations were then used to predict the grain size and strain rate in LFWed Waspaloy. The prediction results were validated against experimental findings and data reported in the literature. It was found that the developed equations can reliably predict the grain size of LFWed Waspaloy. Moreover, the predicted strain rate (1520 s-1) was in agreement with the finite element modeling (FEM) data reported in the literature.</description><description>Le superalliage base Nickel, Waspaloy, est soudé par friction linéaire sous diverses conditions. Précisément, la réduction axiale, dans laquelle tous les paramètres du soudage par friction linéaire (SFL) tels que l'amplitude, la fréquence, la pression et le temps sont inclus, varie de 0.7 à 4.6 mm. Les mesures de température durant le soudage ont été effectuées en plaçant des thermocouples à différents endroits à partir de l'interface soudée. Les propriétés mécaniques des soudures dans les conditions « tel que soudé » et « traité thermiquement après soudure » ont été évaluées par des essais de traction et par mesures de dureté. L'évolution microstructurale à travers la soudure dans les conditions précitées a été étudiée par microscopie optique, EBSD, SEM, TEM et par imagerie tomographique.Les essais de traction ont indiqué qu'il existe une valeur critique de réduction axiale (2 mm) en dessous de laquelle la soudure n'est pas totale. A ce niveau critique de réduction et au delà, les valeurs de limite élastique et de résistance en traction sont plus ou moins les mêmes que celles obtenues dans le matériau de base. Cependant, l'allongement total continue d'augmenter avec la réduction axiale même au delà de la valeur critique à cause de la diminution dans la largeur de la zone affectée thermo-mécaniquement (ZATM). L'échantillon avec la plus grande réduction axiale (4.9 mm) présente un allongement équivalent à 91% de l'allongement du matériau de base. La faible qualité de soudure pour la réduction axiale en-dessous de 2 mm est essentiellement due au manque de lien et/ou à la présence d'oxydes à l'interface de la soudure. L'examen de la microstructure par EBSD a révélé que la recristallisation dynamique (DRX) se produisait dans une région d'environ 1 mm de large de chaque côté de l'interface. Dans la condition « tel que soudé », les études par SEM et TEM ont indiquées que la dissolution progressive des précipités γ' avait lieu du matériau de base vers l'interface de la soudure. La dissolution des γ' influence grandement le profil de dureté mesuré à travers la ZATM. Le traitement thermique après soudure appliqué restaure la dureté dans la zone soudée.Les données enregistrées par les thermocouples ont indiqué que durant le procédé SFL, la température dans la zone soudé atteignait jusqu'à 50 C en dessous du point de fusion du matériau massif (1330 C). Cette température est bien au-dessus de la température de liquéfaction des constituants à faible point de fusion de l'alliage (1245 C). La possibilité de liquéfaction and la conséquente microfissuration a été examinée. Il a été démontré que la haute pression appliqué durant le procédé SFL ainsi que la réduction de taille de grains en résultant contribuent à limiter la liquéfaction et la microfissuration dans les soudures.Finalement, des tests de compression à chaud ont été conduit dans la gamme de température 1060-1140°C et dans la gamme de vitesse de déformation 0.001-1 s-1 pour étudier le comportement en déformation à chaud de cet alliage. Les courbes contrainte-déformation et l'étude de la microstructure ont révélé que la DRX se produisait pendant la déformation. Dans les conditions testées, l'énergie d'activation de déformation serait 462 kJ/mol. Les équations reliant la taille des grains recristallisés dynamiquement à la température et à la vitesse de déformation ont été développées à partir d'expériences de compression à chaud. Les équations développées permettent de prédire la taille de grain et la vitesse de déformation dans le Waspaloy soudé par friction linéaire. Les résultats prédits ont été validés avec les données expérimentales obtenues et celles provenant de la littérature. Il a été démontré que les équations développées peuvent prédire de manière fiable la taille de grain du Waspaloy soudé par friction linéaire. De plus, la vitesse de déformation prédite (1520 s-1) est en accord avec les données de modélisation par éléments finis rapportés dans la littérature.</description><creator>Chamanfar, Ahmad</creator><contributor>Mohammad Jahazi (Supervisor)</contributor><date>2013</date><subject>Engineering - Metallurgy</subject><title>Evolution of microstructure and mechanical properties in linear friction welded waspaloy</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/1g05fg366.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/b8515r86j</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mining and Materials</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:3197xq23c</identifier><datestamp>2020-03-21T19:55:49Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>De nombreuses applications des réseaux de capteurs sans fil peuvent être formulées comme des cas particuliers du problème de consensus moyenné distribué. Ce problème nécessite d'atteindre un état du réseaux où tous les capteurs ont la même valeur; à savoir la moyenne des valeurs initiales. Arriver à un tel consensus peut représenter un défi dans certains scénarios pratiques si la topologie du réseau change au cours du temps, par exemple à cause de la mobilité des capteurs ou un manque de fiabilité des liens sans fil. Dans ces cas particuliers, les algorithmes de colportage sont des alternatives attrayantes étant donné qu'ils n'ont pas besoin de routes spécifiques: ils dépendent d'échanges asynchrones effectués entre les capteurs. Cependant, la communication supplémentaire ajoutée par le colportage est élevée pour les topologies utilisées pour modeler les réseaux de capteurs sans fil. Nous proposons ici de nouveaux algorithmes de colportage qui arrivent à un consensus en utilisant moins de transmissions sans fil que le colportage aléatoire. Nous proposons tout d'abord un algorithme de colportage avec écoute du voisinage. Cet algorithme profite de la nature des transmissions sans fils pour épier les échanges dans le voisinage. Ainsi, quand un noeud se réveille pour une mise à jour de l'algorithme de colportage, au lieu de choisir un noeud voisin en manière aléatoire, il choisit le voisin qui a la valeur la plus éloignée de la sienne. Nous prouvons que ces mises à jour sont garanties de converger plus rapidement que le colportage aléatoire et que l'économie en terme de communication peut être exprimée en fonction du nombre maximum de voisins dans le réseau. Nous étudions ensuite le problème d'arriver à un consensus sur un vecteur de grande dimension. Le consensus sur les composantes d'un vecteur peut être réalisé avec l'utilisation du colportage en parallèle pour chaque composante. Cependant cette façon peut être peu économique dans le cas où seulement quelques composantes sont significatives. Cette thèse présente deux algorithmes, dénommés colportage sélectif seuil et top-m, qui visent à arriver à un consensus seulement sur les composantes significatives du vecteur considéré. Les deux algorithmes se focalisent sur l'utilisation des ressources de communication à chaque mise à jour en échangant seulement les composantes significatives du vecteur local. Nous prouvons que de telles mises à jour identifient avec succès les composantes significatives du vecteur. Utilisant ces algorithmes, nous proposons de nouvelles méthodes pour la compression décentralisée et les filtres à particules distribués dans les réseaux de capteurs. Nos expériences numériques démontrent que des économies de communication sont réalisées sur les méthodes existantes. Les algorithmes proposés dans cette thèse sont des alternatives appropriées au colporatage aléatoire parce qu'ils n'ont pas besoin de l'information additionnelle qui doit être transmise au-delà du voisinage proche. Pris dans leur ensemble, nos résultats indiquent qu'il est possible de diminuer l'excès de communication du colportage aléatoire tout en gardant ses propriétés bénéfiques.</description><description>Many applications of wireless sensor networks can be formulated as instances of the distributed average consensus problem. This problem involves reaching a network state where each node has the same value---the average of the initial values. Reaching a consensus can be challenging in practical scenarios where the network topology varies in time due to node mobility or unreliable wireless communication links. Randomized gossip algorithms are attractive methods for such scenarios because they do not require specialized routes; they rely on asynchronous updates between random pairs of nodes. However, the communication overhead of gossip is high on topologies that are generally used for modeling wireless sensor networks. Here we propose novel gossip algorithms that reach the consensus with fewer wireless transmissions compared to randomized gossip.We first propose greedy gossip with eavesdropping. This algorithm takes advantage of the broadcast nature of wireless transmissions such that nodes eavesdrop on the updates in their neighborhood. Consequently, when a node wakes up for gossip update, instead of choosing a neighbor randomly, it chooses the neighbor which has the most different value than its own. We prove that greedy updates in this fashion are guaranteed to converge faster than randomized gossip and the communication savings can be expressed as a function of the maximum number of neighbors in the network.Then we move on to studying the problem of reaching consensus on a high-dimensional vector. Although consensus on the entries of a vector can be achieved by running gossip in parallel for each entry, this can be wasteful when only few entries of the vector are significant. This thesis presents threshold and top-m selective gossip algorithms which aim to reach a consensus only on the significant entries of the consensus vector. Both algorithms focus communication resources at each update on exchanging only the significant entries of the local vectors. We prove that such myopic updates identify the significant entries of the consensus vector successfully. Using these algorithms, we propose novel approaches to decentralized compression and distributed particle filtering in wireless sensor networks. Numerical experiments demonstrate communication savings over existing methods.The methods proposed in this thesis are appropriate alternatives to randomized gossip because they do not require additional information to be transmitted beyond local neighborhoods. Taken together our results indicate that it is possible to decrease the communication overhead of randomized gossip while preserving its attractive properties.</description><creator>Üstebay, Deniz</creator><contributor>Michael Rabbat (Supervisor)</contributor><date>2013</date><subject>Engineering - Electronics and Electrical</subject><title>Efficient distributed consensus in wireless sensor networks</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/kp78gk983.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/3197xq23c</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:gh93h311k</identifier><datestamp>2020-03-21T19:55:50Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les flammes réalisées à l'intérieur de canaux tubulaires chauffés sont étudiées expérimentalement et numériquement pour comprendre les effets du transfert de chaleur entre la flamme et la paroi. Tout d'abord, le processus de combustion est étudié dans une configuration de brûleur à flamme stabilisée, avec un profil de température imposé le long du tube. Ceci permet d'isoler et de comprendre le rôle de la perte de chaleur de la flamme vers la paroi, sans recirculation de la chaleur. Il est alors observé que les flammes sont influencées parla compétition existant entre l'énergie requise pour le préchauffage des réactifs, la chaleur dégagée par la combustion, et la perte de chaleur. Pour modéliser ces flammes, une extension à la formulation volumétrique et unidimensionnelle, typiquement utilisée, est proposée. Cette nouvelle formulation inclue la chimie détaillée, le transport moyenné du mélange, et un sous-modèle de transfert de chaleur interfaciale. Le sous-modèle de transfert de chaleur interfaciale utilise une source de chaleur non-linéaire pour tenir compte de la combustion, et saisit l'effet de transfert de chaleur interfaciale à l'intérieur de la zone de réaction. La quantité de perte de chaleur dans la zone de réaction se trouve à être sensible au dégagement de chaleur non-linéaire. Le dégagement de chaleur lié aux réactions chimiques agit comme une discontinuité thermique locale, entraînant des gradients de température importants et une perte de chaleur. Ceci est absent des formulations volumétriques typiques et des corrélations de transfert de chaleur standard, qui ne tiennent pas compte des réactions chimiques, et traitent l'écoulement comme thermiquement établi. Le modèle est ensuite comparé à des expériences. Dans les expériences, des flammes axisymétriques à brûlage fort, de méthane et d'air, sont stabilisées le long du profil de température de la paroi. Les flammes sont plates pour les tubes de petites dimensions. Le modèle nouveau est en accord avec les résultats expérimentaux et donne des prédictions améliorées pour la position de la flamme, par rapport à l'approche volumétrique standard. Les profils de température et d'espèces chimiques sont également comparés à ceux obtenus à partir d'une formulation détaillée multidimensionnelle, qui hypothétiquement prédit la structure réelle de la flamme. Encore une fois, le nouveau modèle volumétrique montre une amélioration significative par rapport à la formulation standard. Les écarts entre le nouveau modèle et le modèle détaillé sont également étudiés afin de déterminer la nature des effets multidimensionnels non-considéré.</description><description>Flames in heated tubular channels, with radii on the order of the flame thickness, are investigated experimentally and numerically to understand the various effects of flame / wall interfacial heat transfer. First, combustion is studied in a burner-stabilized configuration, with an imposed temperature profile along the tube wall, to isolate and understand the role of flame / wall heat loss, without heat recirculation. The flames are found to be influenced by competition between energy required to preheat the reactants, heat released by combustion, and heat lost to the wall. To model such flames, an extension to the standard 1--D, volumetric formulation is proposed which uses detailed chemistry, mixture-averaged transport, and an interfacial heat transfer sub-model. The interfacial heat transfer sub-model uses a non-linear, radially-varying heat source to account for combustion and captures enhanced interfacial heat transfer inside the reaction zone. The degree of heat loss in the reaction zone is found to be sensitive to non-linear heat release. Heat release, from chemical reactions, acts as a local thermal discontinuity resulting in steep temperature gradients and high heat loss.  This is absent in present volumetric formulations and in standard interfacial heat transfer correlations; which do not account for chemical reactions and treat the flow as thermally fully-developed. The model is, then, validated with experiments. In the experiments, strongly burning, axisymmetric methane / air flames, stabilized inside the wall temperature profile, are found to be "flat" for sufficiently small tube dimensions. The extended model is also found to be in agreement with experimental results and gives improved quantitative predictions for flame stabilization position, compared to the standard volumetric approach. Temperature and species profiles are also compared to those obtained from a detailed multi-dimensional formulation; which is assumed to predict the actual structure of the flame. Again, the extended volumetric model shows significant improvement compared to the standard formulation. Deviations between the extended model and the detailed model are also investigated to determine the nature of the unconsidered multi-dimensional effects. Finally, propagation and extinction in a participating channel is modeled to understand the combined effects of flame / wall heat transfer and heat recirculation on burning rate. These phenomena are deemed to be the leading-order effects for this case. The interfacial heat transfer sub-model is reformulated to use a non-linear heat source, for combustion, and radial convection, for flow redirection. The model is evaluated for stoichiometric flames over a range of channel inlet flow velocities and confirms the existence of regimes for fast and slow flame propagation, which have non-monotonic variation for burning rate. Peak heat loss is also found to coincide with peak heat release, rather than the maximum temperature location. The numerical model is, once again, found to give improved quantitative predictions over other approaches which neglect the effects of heat release, without the additional computational cost of multi-dimensional, detailed simulations.</description><creator>Watson, Graeme</creator><contributor>Jeffrey Bergthorson (Supervisor)</contributor><date>2013</date><subject>Engineering - Mechanical </subject><title>The influence of interfacial heat transfer on stable flame propagation in small channels</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/gf06g613x.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/gh93h311k</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:n583xz65h</identifier><datestamp>2020-03-21T19:55:50Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Le présent mémoire porte sur deux moments concrets dans la carrière professionnelle du chirurgien et l'anatomiste Charles Nicholas Jenty (?-au moins 1777) dont la biographie comprend de longs séjours et en Angleterre et en Espagne. Bien que la vie professionnelle de Jenty a été étudié dans le contexte des illustrations qui font partie de ses atlas anatomiques renommés, sa biographie et l'étendue de ses activités scientifiques se distinguent par des lacunes notables. Le mémoire se concentre sur l'adhésion de Jenty dans la Society for the Encouragement of Arts, Manufactures and Commerce, et la réalisation de ses expériences chimiques en 1761 lorsqu'il se trouvait à Londres. Le mémoire présente pour la première fois en langue anglaise l'analyse de son traité de chirurgie publié en 1766 en langue espagnole au moment où il a débuté sa carrière en Espagne. De nouvelles informations biographiques sont également présentées dans le cadre d'une étude préliminaire qui mènera éventuellement à une étude plus approfondie. </description><description>This thesis addresses two specific moments in the professional career of the French surgeon and anatomist Charles Nicholas Jenty (?-at least 1777) whose biography includes long residencies in both England and Spain.  While generally being studied in the context of the illustrations included in his anatomical atlases Jenty's biography and the extent of his scientific activities are marked by notable gaps.    This thesis focuses on Jenty's membership in The Society for the Encouragement of Arts, Manufactures and Commerce and the chemical experiments he performed in London in 1761. It introduces for the first time in English an analysis of his surgical treaty published in 1766 in Spanish during his initial career in Spain.  Finally, new biographical information is provided as a preliminary study for further investigation.   </description><creator>Calabro, Cosimo</creator><contributor>Faith Wallis (Internal/Supervisor)</contributor><date>2013</date><subject>History - History of Science</subject><title>Cosmopolitan anatomy and surgery in the age of the enlightenment: two poles in the career of Charles Nicholas Jenty</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/gf06g6146.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/n583xz65h</identifier><degree><name>Master of Arts</name><grantor>McGill University</grantor><discipline>Department of History and Classical Studies</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:5d86p388v</identifier><datestamp>2020-03-21T19:55:51Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les mécanismes des éruptions volcaniques ont été étudié pendant plusieurs dizaines d'années, mais il y a encore plusieurs inconnus dut au fait que nous ne pouvons pas observer ces processus in-situ.  Pour ce projet de recherche nous avons utilisé l'imagerie microtomographique sur des échantillons de vitres volcaniques, riches en volatiles, avec des simulations numériques afin de voir les effets de la distribution des épaisseurs des murs sur la force relative du foam magmatique durant son évolution. La microtomographie a été utilisé pour obtenir des images 2-dimensionnelles pour reconstructions 3-dimensionnelles d'un foam magmatique pendant les 18 premières secondes de son évolution temporel.  Les échantillons sont chauffés dans un four à base de laser jusqu'à 1200 C.  Nous pouvons prendre des images de résolution près de 5.96 um.  Nous avons obtenu des valeurs pour la distribution des épaisseurs des murs et à l'aide de simulations numériques nous avons trouvé que la valeur de force par fibre ne changent quasiment pas dans les deux échantillions. Nous pensons que le peu de chagements dans la force par fibre ainsi que les propriétés de connectivités indiquent que, si un foam peu survivre aux premiers moments chaotiques, il est possible que cela suffit pour diminuer les chances d'éruptions volcaniques dévastatrices.</description><description>The mechanisms of volcanic eruptions have been studied for many decades, but there are still many unknowns due to our inability to observe the process in-situ. We have used microtomographic imaging of synthetic, volatile-rich volcanic melt foams with numerical simulations in order to investigate the effects that bubble wall thickness distributions have on the critical force per fibre (analogous to strength) of magmatic foams.Synchrotron X-ray microtomography was used to image experimentally produced, hydrous melts as they vesiculated, expanded and ultimately failed. The microtomographic imaging was conducted as the melt was heated to 1200 C in order to image the sample at a resolution of 5.96 um from the beginning of bubble expansion to 18 s of bubble growth. These experiments led to bubble wall thickness data over the lifespan of the foam. These data were then used as the input for computer simulations based on the fibre bundle model (FBM) where the squared widths of the bubble walls become the squared widths (and therefore strengths) of the fibres. The critical force per fibre of all experimental data sets does not change significantly with time. The relatively constant strength together with the insignificant change in the bubble wall thickness distributions with time imply that these magmatic foams have constant strength (the water-poor sample) or are not weakening with time, and if given enough growth time without fragmentation the constant strength could decrease potential volcanic hazards.</description><creator>O'Shaughnessy, Cedrick</creator><contributor>Don Baker (Internal/Supervisor)</contributor><date>2013</date><subject>Earth Sciences - Geochemistry</subject><title>The failure of silicate foam caused by bubble expansion</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/vd66w330z.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/5d86p388v</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Earth and Planetary Sciences</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:tx31qn337</identifier><datestamp>2020-03-21T19:55:52Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les calculs liés à de nombreux problèmes scientifiques et techniques demandent qu'on consacre beaucoup de temps à la résolution de grands systèmes linéaires creux. Améliorer la performance de ces résolveurs sur l'architecture paralléle moderne permet aux scientifiques de simuler de grands modèles précis et de manipuler une quantité massive de données dans des délais raisonnables. Les méthodes sous-espaces Krylov (KSM) sont des techniques itératives utilisées pour résoudre de grands systèmes creux. Les noyaux principaux qui demandent beaucoup de temps dans les KSMs sont la multiplication matrice-vecteur creuse (SpMV), les opérations sur les vecteurs (produits scalaires et sommes vectorielles) et la manipulation de préconditionneur. Ce travail présente les techniques et les algorithmes pour accélérer certains de ces noyaux sur une génération récente d'architecture parallèle appelée processeurs multicoeurs. La performance des optimisations proposées est testée sur des processeurs graphiques (GPU) et comparée aux travaux antérieurs.Le noyau SpMV est accéléré sur les processeurs graphiques et des accélérations jusqu'à 3.3 fois plus rapides sont atteintes par rapport aux implémentations de l'algorithme des processeurs graphiques précédents. Le gradient conjugué du résolveur itératif est accéléré sur des cartes graphiques NVIDIA et une accélération 12.9 fois plus rapide est réalisée par rapport à l'implémentation optimisée du noyau sur des processeurs multicœurs. Le préconditionneur approximatif inverse creux est accéléré sur les processeurs graphiques et utilisé pour améliorer le taux de convergence du résolveur itératif BiCGStab. Le préconditionneur est généré sur un NVIDIA GTX480 pour la même durée nécessaire à 16 processeurs AMD Opteron 252 pour générer le même préconditionneur.La communication de données entre les niveaux d'une hiérarchie de mémoire et des processeurs est longue et coûteuse en KSMs. Les résolveurs sans communication (communication-avoiding ou CA) de Krylov n'utilisent qu'un nombre k d'étapes d'une méthode de sous-espace de Krylov (KSM) pour un coût de communication équivalent comme une étape qui permet de réduire les frais généraux des communications dans les KSMs standards. Le noyau des pouvoirs de matrice dans les résolveurs de Krylov sans communication est accéléré sur les processeurs graphiques NVIDIA et des accélérations jusqu'à 5.7 plus rapides sont atteintes pour les problèmes testés par rapport à l'implémentation standard de k des noyaux SpMV.</description><description>Computations related to many scientific and engineering problems spend most of their time in solving large, sparse linear systems. Improving the performance of these solvers on modern parallel architecture enables scientists to simulate large accurate models and manipulate massive amounts of data in reasonable time frames. Krylov subspace methods (KSM) are iterative techniques used to solve large sparse systems. The main time consuming kernels in KSMs are sparse matrix vector multiplication (SpMV), vector operations (dot products and vector sums) and preconditioner manipulation. This work presents techniques and algorithms to accelerate some of these kernels on a recent generation of parallel architecture called manycore processors. The performance of the proposed optimizations are tested on graphic processing units (GPUs) and compared to previous work. The SpMV kernel is accelerated on GPUs and speedups of up to 3.3 times are achieved compared to previous GPU implementations of the algorithm. The conjugate gradient iterative solver is accelerated on NVIDIA graphic cards and a 12.9 fold speedup is achieved compared to optimized implementation of the kernel on multicore CPUs. The sparse approximate inverse preconditioner is accelerated on GPUs and used to enhance the convergence rate of the BiCGStab iterative solver. The preconditioner is generated on NVIDIA GTX480 in the same time as it takes 16 AMD 252 Opteron processors to generate the same preconditioner.Communicating data between levels of a memory hierarchy and processors is time consuming and costly in KSMs. Communication-avoiding (CA) Krylov solvers take k steps of a KSM for the same communication cost as one step to reduce the communication overhead in standard KSMs. The matrix powers kernel in communication-avoiding Krylov solvers is accelerated on NVIDIA GPUs and speedups of up to 5.7 are achieved for the tested problems compared to the standard implementation of k SpMV kernels.  </description><creator>Mehri Dehnavi, Maryam</creator><contributor>Dennis Giannacopoulos (Supervisor)</contributor><date>2013</date><subject>Engineering - Electronics and Electrical</subject><title>Krylov subspace techniques on graphic processing units</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/c534fs31k.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/tx31qn337</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:tb09j890b</identifier><datestamp>2020-03-21T19:55:53Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les conclusions d'évaluation scolaire récentes démontrent que les préoccupations pédagogiques regardant les garçons sont pertinents et à propos. Les garçons lâchent l'école à des taux plus élevés, atteignent des notes inférieures, et ils semblent perdre leur avantage historique en science et en math relatif aux femmes, ce qui à créé des affirmations ardentes en ce qui concerne les garçons, la masculinité et la scolarité. Cette dissertation tente de tempérer ces affirmations en développant un dialogue entre la perspective de la masculinité d'après les conceptions sociales, la perspective déterministe biologique et les conclusions de la psychologie évolutionniste. La dissertation commence en examinant la genèse historique du débat concernant l'éducation des garçons en vue de comprendre la terminologie technique qui s'est manifestée pour discuter des garçons et la masculinité et les pièges théoriques qui se présentent en adressant les préoccupations scolaires des garçons. Ensuite, une comparaison critique des perspectives courantes est entamée dans le but de progresser envers le développement d'un model de Conception Social plus (SC+) de la masculinité qui est principalement un model de conception social mais qui prend en compte les tendances des différences de sexe tel que démontré par la recherche en psychologie évolutionniste. De cette nouvelle perspective théorique, les considérations pertinentes dans le débat sur l'éducation des garçons sont réévaluées, y compris la (sous)performance des garçons, le désir de voir davantage d'enseignants masculins dans les écoles et la violence des hommes, incluant des changements subtils mais importants dans la perspective suggérée pour de futures recherches et analyses.</description><description>Recent educational assessment findings demonstrate that educational concerns about boys are relevant and timely. Males are dropping out of school at higher rates, achieving lower grades, and appear to be losing historical advantages in math and science relative to females. These statistics have led to some fervent assertions being made regarding boys, masculinity, and education.  Those assertions are tempered by developing a dialogue between social constructionist perspectives of masculinity, so-called biological determinist perspectives, and evolutionary psychology findings. First, a review of the historical generation of the debate about boys' education is conducted in order to understand both the technical terminology that has evolved to discuss boys and masculinity as well as the theoretical pitfalls in the turn to address boys' educational concerns. Next, a critical comparison of current perspectives is taken up in order to move towards developing a Social Constructionist Plus (SC+) theory of masculinity that is primarily social constructionist in emphasis but also accounts for sex difference trends demonstrated by evolutionary psychology research. Then, from this new theoretical perspective, relevant considerations in the debate about boys' education are re-evaluated, including males' educational (under)performance, the desire for more male teachers in schools, and males' violence. There are subtle but important shifts in perspective suggested for future research and theorizing. Lastly, the importance of a SC+ perspective is discussed relative to the future of boys' educational discussions and equality more generally, providing significant avenues for further research and analysis.</description><creator>Roemmele, David</creator><contributor>Ratna Ghosh (Supervisor1)</contributor><contributor>Michael Hoechsmann (Supervisor2)</contributor><date>2013</date><subject>Education - General</subject><title>Peach fuzz: boys, masculinity and education</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/8k71nm79g.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/tb09j890b</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Integrated Studies in Education</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:h415pf19f</identifier><datestamp>2020-03-21T19:55:54Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Sur les continents, l'évolution diurne de la couche limite convective (CLC) est entrainée par l'évolution des forçages de surface. Les modélisations récentes ont démontré que les modèles ne peuvent pas capturer l'évolution observée en raison des difficultés pour représenter les processus turbulents dans les nuages cumulus de la CLC et peu profonde. Le but de cette thèse est de faire progresser notre compréhension de la structure de la couche limite claire et nuageuse sur les terres en fournissant des données d'observation à long terme et en utilisant un ensemble de données. Plus précisément, elle met l'accent sur la documentation de la structure turbulente et les propriétés de la CLC et des cumulus de beau temps (CBT) à l'aide d'observation acquise au site de recherches « Southern Great Plains » (SGP) du programme « Atmospheric Radiation Measurement » (ARM) du Département américain de l'énergie. Premièrement, les mesures de vitesse Doppler des insectes, qui occupent les 2 km au bas de la couche limite pendant les mois d'été, sont utilisés pour cartographier la composante de vitesse verticale dans la CLC. Les observations portent sur quatre périodes estivales (2004 - 08) et sont classées en conditions de couche limite claire et nuageuse. Un procédé d'échantillonnage conditionnel est appliqué aux données d'origine de vitesse Doppler pour extraire les structures cohérentes de vitesse verticale et pour examiner  la dimension du panache et sa contribution au transport turbulent total. Les profils de la variance et de l'asymétrie de la vitesse verticale et du flux de masse sont estimés pour étudier l'évolution diurne de la CLC au cours de ces conditions. Les propriétés des nuages CBT d'été sont analysées en utilisant le long jeu de données d'observations (14 années) à partir du sol de MMCR (Radar de nuages à longueur d'onde millimétrique) au laboratoire SGP de ARM afin de documenter les propriétés macroscopiques et dynamiques des nuages CBT. Les vitesses Doppler sont traitées pour des seuils inférieurs de réflectivité qui contiennent de petites gouttelettes de nuages ayant des vitesses terminales négligeables; ainsi les vitesses Doppler sont utilisées comme des traceurs de mouvements de l'air. Un algorithme de logique floue a été développé pour éliminer les échos radar d'insectes dans la couche limite qui entravent notre capacité d'élaborer des statistiques représentatives des nuages. L'ensemble de données raffineés est utilisé pour documenter l'évolution diurne compose des statistiques de vitesse verticale des nuages, les paramètres de surface et des profils des fractions de courant ascendant et descendant, la vitesse des courants ascendant et descendants, et les flux de masse ascendant. Les statistiques sur les propriétés géométriques des nuages  telles que, l'épaisseur des nuages, la longueur de corde des nuages, l'espacement des nuages et les rapports d'aspect sont calculés sur l'échelle nuageuse. Enfin, quelques uns des aspects des courants ascendants dans  les paramétrisations existantes du flux de masse sont testées en utilisant les observations récentes du lidar Doppler déployé depuis l'expérience « Midlatitude Continental Convective Cloud Experiment » (MC3E). Les caractéristiques des courants ascendants de deux schéma existants de flux de masse sont évalués avec les observations de vitesse verticale du lidar Doppler pour tester son applicabilité sur les terres. Les observations de vitesse verticale dans la couche sous les nuages sont analysées pour différentes conditions nuageuses, en décomposant séparément les régions claires et nuageuses, et pour différentes fractions de nuages pour étudier le rôle des nuages sur la structure sous-nuageuse.</description><description>Over land, the daytime evolution of the convective boundary layer (CBL) is driven by the strong surface forcing evolution. Recent modeling studies have demonstrated that models cannot capture the observed evolution because of difficulty to represent turbulent processes in CBL and shallow cumulus clouds. The purpose of this thesis is to advance our understanding of the structure of clear and cloudy boundary layer over land by providing observational evidence using long-term dataset. Specifically, it focuses on documenting the turbulent structure and properties of CBL and FWC using observations at the U.S. Department of Energy Atmospheric Radiation Measurement Program (ARM) Southern Great Plains (SGP) Climate Research Facility. First, Doppler velocity measurements from insects occupying the lowest 2 km of the boundary layer during summer months are used to map the vertical velocity component in the CBL. The observations cover four summer periods (2004–08) and are classified into cloudy and clear boundary layer conditions. A conditional sampling method is applied to the original Doppler velocity data set to extract coherent vertical velocity structures and to examine plume dimension and its contribution to the total turbulent transport. Profiles of vertical velocity variance, skewness, and mass flux are estimated to study the daytime evolution of the convective boundary layer during these conditions. The properties of summer time FWC clouds are analyzed using the long data record (14–year) of ground-based MMCR (Millimeter Wavelength Cloud Radar) observations at the ARM facility at the SGP site to document the macroscopic and dynamical properties of FWC clouds. Doppler velocities are processed for lower reflectivity thresholds that contain small cloud droplets having insignificant terminal velocities; thus Doppler velocities used as tracers of air motion. A fuzzy-logic based algorithmis developed to eliminate insect radar echoes in the boundary layer that hinder our ability to develop representative cloud statistics. The refined data set is used to document composite daytime evolution of cloud vertical velocity statistics, surface parameters and profiles of updraft and downdraft fractions, updraft and downdraft velocity, and updraft mass fluxes. Statistics on the cloud geometrical properties such as, cloud thickness, cloud chord length, cloud spacing and aspect ratios are calculated on the cloud scale. Lastly, some of the updraft aspects in existing mass-flux parameterizations are tested using the recent observations from Doppler lidar deployed since Midlatitude Continental Convective Cloud Experiment (MC3E). The updraft aspects from two existing mass-flux schemes are evaluated with the Doppler lidar vertical velocity observations to test its applicability over land. The vertical velocity observations in the subcloud layer are analyzed for various cloud conditions by separately decomposing into clear and cloudy regions, and for various cloud fractions to investigate the role of clouds on the subcloud structure.</description><creator>Chandra, Arunchandra</creator><contributor>Pavlos Kollias (Supervisor)</contributor><date>2013</date><subject>Earth Sciences - Atmospheric Sciences</subject><title>The turbulent structure of the clear and cloud-topped convective boundary layer over land</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/sj139564j.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/h415pf19f</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Atmospheric and Oceanic Sciences</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:vm40xw529</identifier><datestamp>2020-03-21T19:55:55Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>La fréquence des tempêtes extrêmes est un facteur critique dans la conception et gestion d'un grand nombre de projets de ressources en eau. Dans la pratique courante, l'estimation des pluies extrêmes est réalisée en se basant sur l'analyse de fréquence statistique des données de précipitations maximales. L'objectif de cette analyse de la fréquence est donc d'estimer le montant maximal de précipitations qui tombent à un moment donné pendant une durée déterminée, ainsi que la période de retour. Les résultats de l'analyse de la fréquence des précipitations sont souvent résumés par les relations Intensité-Durée-Fréquence (IDF) pour un site donné. Toutefois, les méthodes traditionnelles dans le développement des relations IDF ont deux limites majeures. Tout d'abord, ces méthodes n'ont pas été en mesure de tenir compte des caratéristiques des précipitations extrêmes sur des différentes échelles de temps. Deuxièmement, ces méthodes traditionnelles ne tiennent pas compte des impacts potentiels de la variabilité climatique et du changement climatique. Par conséquent, l'objectif principal de cette présente étude est de proposer une méthode d'estimation des précipitations extrêmes améliorée qui pourrait surmonter ces limitations. La méthode proposée a été basée sur l'échelle d'invariance de distribution GEV et la procédure de réduction d'échelle statistique pour construire des relations IDF dans le contexte du changement climatique. La méthode des moments non-centraux a été utilisée pour l'estimation des trois paramètres de la GEV. Les résultats obtenus par une application numérique des données de Précipitations Maximales Annuelles (PMA) à partir d'un réseau de 14 stations pluviométriques en Corée de Sud ont démontré la faisabilité et la précision de la méthode proposée. La série de PMA observée a particulièrement affiché une propriété d'échelle simple. En outre, les liens entre les variables climatiques globaux donnés par les deux Modèles Climatiques Globaux (MCGs) (un en provenance d'Environnement Canada et l'autre du Centre Hadley du Royaume-Uni) et les caractéristiques des précipitations locaux extrêmes ont été établis avec succès pour prédire les changements qui résultent des relations IDF selon des différents scénarios climatiques - A2, A1B, et B2. Il a été constaté que des relations IDF pour les périodes futures (les années 2020, 2050, et 2080) ont démontré des tendances qui augmentent ou diminuent dépendemment des MCG utilisés et du scénario climatique à l'étude. </description><description>The occurrence of extreme storms is a critical consideration in the design and management of a large number of water-resource projects. In current engineering practice, the estimation of extreme rainfalls is accomplished based on statistical frequency analysis of maximum precipitation data. The objective of this frequency analysis is hence to estimate the maximum amount of precipitation falling at a given point for a specified duration and return period. Results of precipitation frequency analysis are often summarized by "intensity-duration-frequency" (IDF) relationships for a given site. However, traditional methods in the development of IDF relations have two major limitations. Firstly, these existing methods were not able to account for the extreme rainfall characteristics over different time scales. Secondly, these traditional methods cannot take into account the potential impacts of climate variability and climate change. Therefore, the main objective of the present study is to propose an improved method for extreme rainfall estimation that could overcome these limitations. The proposed method was based on the scale-invariance GEV distribution and the statistical downscaling procedure to construct the IDF relations in the context of climate change. The Non-Central Moment method was used for the estimation of the three parameters of the GEV. Results of a numerical application using Annual Maximum Precipitation (AMP) data from a network of 14 rain-gauge stations in South Korea has indicated the feasibility and accuracy of the suggested method. In particular, the observed AMP series displayed a simple scaling behaviour. In addition, the linkages between global climate variables given by two Global Climate Models (GCMs) (one from Environment Canada and one from the UK Hadley Centre) and the local extreme rainfall characteristics have been successfully established for predicting the resulting changes of the IDF relations under different climate change scenarios A2, A1B, and B2. It was found that the IDF relations for future periods (2020's, 2050's, and 2080's) showed increasing or decreasing trends depending on the GCM used and the climate scenario considered. </description><creator>Lee, Min Young</creator><contributor>Van-Thanh-Van Nguyen (Internal/Supervisor)</contributor><date>2013</date><subject>Engineering - Civil</subject><title>Statistical modeling of extreme rainfall processes in the context of climate change</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/zc77st591.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/vm40xw529</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Civil Engineering and Applied Mechanics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:js956k55w</identifier><datestamp>2020-03-21T19:55:56Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Abu Nasr Qursawi is a seminal figure in the history of the Muslim communities of the Russian Empire. Controversial in his own time, as well as posthumously, he came to be seen as an inspiration for Jadidism, the modernist Muslim reform movement in the Russian Empire in the early 20th century. During his lifetime, he was intimately involved in the most important debates within these Muslim communities regarding issues central to their religious and social self-conception as Muslims. This dissertation explores Qursawi's writings and thought in light of his socio-historical circumstances and within the context of the classical Islamic scholarly tradition, and addresses his connection with Jadidism. The dissertation is divided into two sections. The first presents the events of Qursawi's life within the broader history of the Muslims of the Russian Empire between 1750 and 1850. This section shows the ways in which these Muslim communities, particularly the scholarly class ('ulama'), were affected as a result of their incorporation into the governing structures of the Russian imperial state. It attempts to fit Qursawi into this specific historical context. The second section features an analysis of Qursawi's thought based on an in-depth study of his entire extant oeuvre. This analysis focuses on the two most pressing issues of the religious discourse among these communities: debates on the theological issue of God's attributes (sifat) and the permissibility and purpose of independent legal interpretation (ijtihad). Qursawi's respective positions on these two issues reflect his view regarding religious belief, the Islamic theological tradition (kalam) and Islamic legal institutions and their role within the Islamic social order. I analyze Qursawi's stance on each in light of the discursive tradition of pre-modern Islamic scholarship and show the ways in which he adheres to, or departs from, that tradition. I conclude that although Qursawi's thought is firmly grounded within that tradition, and he is not the radical modernizer he is often depicted as in secondary scholarship, he does indeed depart from tradition in a number of significant ways. This was the result of the social marginalization of Islamic institutions under the Russian state. Qursawi's stance on legal issues is more unconventional than his stance on theological issues due to the fact that legal institutions were impacted to a greater degree by imperial rule. Furthermore, while his socio-religious Weltanschauung bears virtually no relation to the self-conscious modernism of Jadidism, it is evident how some of Qursawi's views, particularly those regarding education and religious knowledge, could serve as a basis for modernist reforms as espoused by Jadids.</description><description>Abu Nasr Qursawi est une figure majeur dans l'histoire des communautés musulmanes de l'Empire russe. Il était une personne controversée en son temps et aussi après sa mort, et il est devenu considéré comme une source d'inspiration pour le jadidisme, le mouvement de la réforme musulmane moderne dans l'Empire russe au début du 20e siècle. Au cours de sa vie, il a participé aux débats les plus importants concernant les questions essentielles religieux et sociaux au sein de ces communautés musulmanes. Cette thèse explore ses ouvrages compte tenu des circonstances socio-historiques et dans le contexte de la tradition scolaire pré-moderne de l'Islam, et elle s'adresse à la connexion de Qursawi au jadidisme. La thèse se divise en deux parties. La première présente les événements de la vie de Qursawi dans l'histoire des musulmans de l'Empire russe entre 1750 et 1850. Cette partie s'adresse aux manières dont ces communautés musulmanes, en particulier la classe savante ('ulama'), ont été affectées en conséquence de leur incorporation aux structures de gouvernance de l'état impérial russe. Elle tente de mettre Qursawi dans ce contexte historique spécifique. La deuxième partie inclut une analyse de la pensée de Qursawi en regarde de l'étude en détail de son oeuvre existant entière. Cette analyse se concentre sur les deux questions les plus urgents du discours religieux de ces communautés : débats sur la question théologique des attributs divins (sifat) et la permissibilité et le but de l'interprétation juridique indépendante (ijtihad). Les positions de Qursawi sur ces deux questions reflétent son point de vue concernant la tradition théologique islamique (kalam) et les institutions juridiques musulmanes et leur rôle au sein de l'ordre social islamique. J'analyse la position de Qursawi sur chacun compte tenu de la tradition discursive de l'érudition islamique de la période pré-moderne et montre les manières dont il se conforme à, ou s'éloigne de, cette tradition. Je conclus que, bien que la pensée de Qursawi est solidement ancrée dans cette tradition et il n'est pas le modernisateur radical comme il est souvent dépeint par la littérature secondaire, en fait il s'éloigne de la tradition de plusieurs manières significatives. Ce fut à cause de la marginalisation sociale des institutions islamiques sous l'état russe. La position juridique de Qursawi est plus non-conventionelle que sa position sur questions théologiques en raison du fait que la domination impériale a eu un plus grand impact sur les institutions juridiques. De plus, alors que sa perspective socio-religieuse n'a aucun rapport avec le modernisme explicite du jadidisme, il est clair que les jadids pourraient baser leurs réformes modernistes sur certains de ses positions, en particulier celles qui concerne l'enseignement et le savoir religieux.</description><creator>Spannaus, Nathan</creator><contributor>Robert Wisnovsky (Supervisor)</contributor><date>2013</date><subject>Religion - Religion General</subject><title>Islamic thought and revivalism in the Russian Empire: an intellectual Biography of Abū Nasr Qūrsāwī (1776-1812)</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/r207ts68d.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/js956k55w</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Institute of Islamic Studies</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:c821gp498</identifier><datestamp>2020-03-21T19:55:56Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>This thesis examines the production and mobilization of Inuit knowledge in and beyond the Canadian Arctic in the context of climate change.  Drawing on multi-scale, ethnographic research, it focuses in particular on the role of institutions, such as government departments and community-based organizations, in mediating between different understandings of change.  Inuit knowledge is increasingly transmitted through land-based programs and supported by grant funding from outside the community.  I argue that adaptation to climate change is therefore as much about understanding how to work within political and institutional frameworks as it is about responding to changes in the local environment. I examine how Inuit knowledge, represented in material forms – such as reports, maps, and films – as well as through the work of spokespersons, is mobilized in scientific conferences, bureaucratic office environments, and multilateral governance meetings like the United Nations Framework Convention on Climate Change (UNFCCC).  I argue that spokespersons play a particularly significant role in rendering Inuit knowledge palatable for qallunaat (non-Inuit) audiences by selecting particular aspects to emphasize over others, and by translating Inuit observations into the language of science.  A variety of factors constrain the mobility of Inuit knowledge and limit its potential to shape territorial, national, and global decision-making about climate change.  These include bureaucratic commitments to written documents and hierarchical organization, as well as public narratives that emphasize the vulnerability of Inuit to climate change based on a simplistic conception of Inuit identity as traditional and rooted on the land.  Inuit, however, engage with the changing Arctic in a variety of ways, including exploring the potential of oil and gas development in increasingly ice-free waters.  Ultimately, I suggest that Inuit knowledge reflects the ability to endure in the face of change—most recently, anthropogenic climate change.  Drawing on the comfort and stabilizing force of tradition, it is also informed by and shapes political relations in spaces far from the Arctic tundra.</description><description>Cette thèse examine la production et la mobilisation des connaissances inuit dans l'Arctique canadien et ailleurs, dans le contexte des changements climatiques. S'appuyant sur une recherche ethnographique menée à plusieurs échelles, elle s'intéresse particulièrement au rôle de différentes institutions qui servent d'intermédiaire entre différentes perspectives sur ces changements, incluant des protocoles éthiques, des ministères et des organisations communautaires. Les connaissances inuit, comprises traditionnellement comme étant liées au territoire, sont de plus en plus supportées par des programmes institutionnels et des subventions venant de l'extérieur de la communauté. Je mets de l'avant que l'adaptation aux changements climatiques dépend donc autant de la compréhension du fonctionnement de la politique et du cadre institutionnel que des réponses aux changements développées dans l'environnement local. J'examine comment les connaissances inuit, représentées sous forme matérielle (comme des rapports, des cartes ou des films) et par des porte-parole sont mobilisées dans des conférences scientifiques, des environnements bureaucratisés et des réunions de gouvernance multilatérale, comme celles de la Convention-cadre des Nations Unies sur les Changements Climatiques (UNFCCC). Je soutiens que les porte-parole jouent un rôle particulièrement significatif en rendant les connaissances inuit acceptables pour les qallunaat (non-Inuit) en mettant l'accent sur certains aspects et non sur d'autres et en traduisant les observations inuit dans un langage scientifique.Plusieurs facteurs contraignent la mobilité des connaissances inuit et limitent son potentiel à influencer la prise de décision aux niveaux territorial, national et global au sujet des changements climatiques. Ces facteurs incluent les engagements bureaucratiques envers des documents écrits et des organisations hiérarchiques, de même que des récits publics qui insistent sur la vulnérabilité des Inuits aux changements climatiques basée sur une conception simpliste de l'identité inuit qui serait traditionnelle et enracinée sur le territoire. Les Inuit, toutefois, interagissent avec l'Arctique qui se transforme de diverses manières, incluant des explorations pétrolières dans les eaux ouvertes qui augmentent de plus en plus rapidement. Ultimement, je suggère que les connaissances inuit reflètent la capacité de perdurer malgré ces changements, c'est-à-dire les changements plus récents qui sont anthropogéniques et touchent le climat. S'inspirant du confort et de la force stabilisatrice de la tradition, ces connaissances interagissent avec des relations politiques établies loin de la toundra arctique. </description><creator>Johnson, Noor Jehan</creator><contributor>Ronald Niezen (Supervisor)</contributor><date>2013</date><subject>Anthropology - Cultural</subject><title>Mobilizing Inuit knowledge: representation and institutional mediation in the era of global climate change</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/mg74qq73b.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/c821gp498</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Anthropology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:m900nz23m</identifier><datestamp>2020-03-21T19:55:57Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Recent developments in cutting edge fabrication techniques have led to research focused oncombining multiple optical and electronic functionalities, and to the integration of variousactive and passive components onto a single die. Integrated optical devices using waveguidesare being developed to replace traditional free space propagating devices utilizinglenses and mirrors. Silicon based photonic integrated circuits (PIC) are being investigatedfor intra-board and board-to-board optical interconnection in high speed computing, andas transceivers in high speed optical communications. One of the major aspects in PICresearch is the miniaturization of passive devices performing various functions such as opticalfiltering, multiplexing, demultiplexing, wavelength routing, and optical isolation. Inthis thesis, we study and demonstrate the feasibility of using sidewalled gratings for opticalfiltering, wherein optical feedback is obtained by the interaction of the guided mode withperiodic index modulations realized vertically on the waveguide walls.A novel design based on a horizontal evanescent coupling scheme in sidewalled gratingresonators for realizing low reflection filters that reduces the need for isolators or circulatorsis presented and demonstrated. We utilize the narrowband filter response of aquarter wave shifted cavity to obtain transmission filters with 110 GHz bandwidth. A coupledcavity scheme wherein multiple identical cavities are coupled back-to-back to obtaintop-hat like filter response (with 50 dB/nm roll-off) is also demonstrated. Multichannelwavelength filters designed using sampled grating distributed Bragg reflectors (SG-DBR)is introduced, and a novel dual apodization method for realizing comb resonances withhigh side mode suppression ratio (SMSR) is discussed in detail. The apodized comb filtersare shown to exhibit channel spacing, bandwidth, and SMSRs within prescribed standardlimits. We also present through numerical simulations, the feasibility of employing sidewalledgratings as wavelength selective cavities in hybrid III–V silicon lasers, and in tunableevanescent lasers. Results on dimensional engineering of the cavity to obtain the requiredlasing wavelength and confinement factors are presented. All the devices being presentedin this thesis were fabricated using an inductively coupled plasma based dry etch processon a silicon-on-insulator (SOI) wafer, where the pattern transfer was done using e-beamlithography (EBL).</description><description>Les développements récents dans les techniques de fabrication de pointe ont conduit à des recherches axées sur la combinaison de multiples fonctionnalités optiques et électroniques et à l'intégration de différents composants actifs et passifs sur une matrice unique. Des dispositifs optiques intégrés à l'aide de guides d'ondes sont en cours de développement pour remplacer les traditionnels dispositifs de propagation en espace libre qui utilisent des lentilles et des miroirs. Des circuits photoniques intégrés (PIC) à base de silicium sont à l'étude pour les connexions optiques à l'intérieur d'un circuit ou entre des circuits pour le traitement de donnée à haute vitesse et comme émetteurs-récepteurs dans les communications optiques à haut débit. L'un des aspects majeurs de la recherche sur les PIC est la miniaturisation des dispositifs passifs qui exercent des fonctions diverses telles que le filtrage optique, le multiplexage, le démultiplexage, le routage des longueurs d'onde et l'isolement optique. Dans cette thèse, nous étudions et démontrons la faisabilité d'utiliser des réseaux sur les parois d'un guide d'ondes pour le filtrage optique. Dans ces dispositifs, la rétroaction optique est obtenue par l'interaction du mode de propagation avec des modulations périodiques de l'index de réfraction réalisées à la verticale sur les parois du guide d'ondes.Une conception originale reposant sur un système de couplage évanescent horizontaldans les résonateurs à réseaux est présentée et démontrée pour la réalisation de filtres à faible réflexion qui réduit la nécessité d'utiliser des isolateurs ou des circulateurs. Nous utilisons la réponse à bande étroite d'une cavité quart d'onde décalée afin d'obtenir des filtres de transmission de 110 GHz de bande passante. Un système dans lequel plusieurs cavités identiques sont couplées en série pour obtenir un filtre avec une réponse chapeau haut de forme (avec 50 dB/nm d'affaiblissement) est également démontré. Des filtres de longueurs d'onde à multiple canaux conçus à l'aide de réflecteurs de Bragg à multiples réseaux (SGDBR) sont présentés et une nouvelle méthode d'apodisation double pour la réalisation de filtre en peigne avec une grande suppression des modes secondaires (SMSR) est discutée en détails. Les filtres en peigne apodisés présentés démontrent des espacements de canaux, des bandes passantes et des SMSRs dans les limites standard. Nous présentons également, pardes simulations numériques, la possibilité d'employer des réseaux sur les parois d'un guide d'ondes comme cavités sélectives de longueurs d'onde pour les lasers hybrides III–V sur silicium et les lasers évanescents accordables. Les résultats sur la détermination des dimensions de la cavité pour obtenir la longueur d'onde d'émission et les facteurs de confinement requis sont présentés. Tous les dispositifs présentés dans cette thèse ont été fabriqués en utilisant un procédé de gravure sèche à plasma couplé par induction sur un substrat de silicium sur isolant (SOI), où le transfert de motif a été fait en utilisant la lithographie par faisceau d'électrons (EBL).</description><creator>Veerasubramanian, Venkatakrishnan</creator><contributor>Andrew G Kirk (Supervisor)</contributor><date>2013</date><subject>Engineering - Electronics and Electrical</subject><title>Applications of sidewalled grating resonators</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/765374513.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/m900nz23m</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:cz30px379</identifier><datestamp>2020-03-21T19:55:58Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>In this thesis, we have developed a parallel GPU accelerated code for carrying out transport calculations within the Non-Equilibrium Green's Function (NEGF) framework using the Tight-Binding (TB) model. We also discuss the theoretical, modelling, and computational issues that arise in this implementation. We demonstrate that a heterogenous implementation with CPUs and GPUs is superior to single processor, multiple processor, and massively parallel CPU-only implementations. The GPU-Matlab Interface (GMI) developed in this work for use in our NEGF-TB code is not application specific and can be used by researchers in any field without previous knowledge of GPU programming or multi-threaded programming. We also demonstrate that GMI competes very well with commercial packages.Finally, we apply our heterogenous NEGF-TB code to the study of electronic transport properties of Si nanowires and nanobeams. We investigate the effect of several kinds of structural defects on the conductance of such devices and demonstrate that our method can handle systems of over 200,000 atoms in a reasonable time scale while using just 1-4 GPUs.</description><description>Dans cette thèse, nous présentons un logiciel qui effectue des calculs de transport quantique en utilisant conjointement la théorie des fonctions de Green hors équilibre (non equilibrium Green function, NEGF) et le modèle des liens étroits (tight-binding model, TB). Notre logiciel tire avantage du parallélisme inhérent aux algorithmes utilisés en plus d'être accéléré grâce à l'utilisation de processeurs graphiques (GPU). Nous abordons également les problèmes théoriques, géométriques et numériques qui se posent lors de l'implémentation du code NEGF-TB. Nous démontrons ensuite qu'une implémentation hétérogène utilisant des CPU et des GPU est supérieure aux implémentations à processeur unique, à celles à processeurs multiples, et même aux implémentations massivement parallèles n'utilisant que des CPU. Le GPU-Matlab Interface (GMI) présenté dans cette thèse fut développé pour des fins de calculs de transport quantique NEGF-TB. Néanmoins, les capacités de GMI ne se limitent pas à l'utilisation que nous en faisons ici et GMI peut être utilisé par des chercheurs de tous les domaines n'ayant pas de connaissances préalables de la programmation GPU ou de la programmation "multi-thread". Nous démontrons également que GMI compétitionne avantageusement avec des logiciels commerciaux similaires.Enfin, nous utilisons notre logiciel NEGF-TB pour étudier certaines propriétés de transport électronique de nanofils de Si et de Nanobeams. Nous examinons l'effet de plusieurs sortes de lacunes sur la conductance de ces structures et démontrons que notre méthode peut étudier des systèmes de plus de 200 000 atomes en un temps raisonnable en utilisant de un à quatre GPU sur un seul poste de travail.</description><creator>Harb, Mohammed</creator><contributor>Hong Guo (Internal/Supervisor)</contributor><date>2013</date><subject>Physics - Solid State</subject><title>Quantum transport modeling with GPUs.</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/1r66j453m.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/cz30px379</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:kd17cx815</identifier><datestamp>2020-03-21T19:55:59Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Ralph Adams Cram (1863–1942) was among the rare Gothicists who practiced both Gothic architecture and literature. He designed several Gothic-Revival churches and campuses across North America, and he wrote a book of Gothic ghost stories in 1895, calling the collection Black Spirits and White. Traditionally, scholars have assumed that the discourses of modern, Anglo-Gothic architecture and literature parted company after the 1830s. Scholars have based that assumption on two interrelated arguments. First, the Victorian Gothic novel evolved beyond the distinctly medieval; whereas, Victorian Gothic architects became rigorously attentive to structural and cultural principles of the Middle Ages. Second, and more importantly, even though architecture has been thematic for Gothic literature, scholars of the genre have concentrated on the domesticity of haunted houses. This has not been as problematic for scholars of Georgian Gothic architecture, where Gothic details plastered over domestic architecture; Victorian Gothic architects, however, expressed their principles most effectively through church building. The modern Gothic church, as the true house of God, is supposed to have exorcized any confusion with the domestic architecture of man, providing sanctuary from the haunting conditions of a secular, urban-industrial, modern world.Ralph Adams Cram complicates that assumption. In the darkest moments of his despair, Cram designed churches that were not resurrected Gothic beauties, but spectral remnants of a murdered past beyond his powers to avenge. His Gothic literature expressed that impotent horror, addressing several houses that modernity, having murdered the medieval past, haunted. So did the new St. Mary's Anglican Church of Walkerville, Ontario. Using the hauntological strategies of Jacques Derrida, this project deconstructs the Walkerville church to solicit the withered horror of a spectral hand haunting the Anglican house of God. Cram designed the Walkerville church for Edward Chandler Walker, de facto king of Walkerville, who was secretly dying of syphilis. Cram encrypted Edward's illness in the Walkerville church through the withered limb of a biblical leper. Edward's withered "hand" was then visualized through the spreading fingers of the letter "k," its grammatological mark silently concealed and revealed in the Gothick moniker that its structural, spatial, social, and semiotic languages declare to the modern world. Ultimately, the Walkerville church calls for a Grail Knight's arrival, one whose holy hand can end the suffering of the Fisher King, Edward Walker—and, by extension, a knight who might end the dark night of decadent modernity. Yet will the Grail Knight ever arrive? </description><description>Ralph Adams Cram (1863–1942) a été un des rares adeptes du gothique à s'adonner à la fois à l'architecture et à la littérature. Surtout connu comme concepteur de plusieurs églises et campus universitaires en Amérique du Nord, il a aussi publié en 1895 un recueil de contes gothiques qu'il intitula Black Spirits and White. Il est pourtant généralement convenu, qu'après 1830, l'architecture néo-gothique et le roman gothique ont suivi des routes divergentes, opinion fondée sur deux arguments interdépendants: 1- les romanciers gothiques de l'époque victorienne ont généralement cessé de cadrer leur récit dans un contexte historique strictement médiéval alors qu'au contraire les architectes néo-gothiques de la même période se sont attachés à faire revivre le moyen âge le plus scrupuleusement possible; 2- quand les romanciers gothiques victoriens mettent en scène un cadre architectural, il se concentre généralement sur l'espace domestique, telle la maison hantée, alors que chez les architectes, ce sera l'église qui sera l'objet principal de la passion gothique. Envisagée comme la « maison » de Dieu, l'église était conçue en opposition au monde domestique, offrant ainsi un refuge contre les hantises d'un monde séculier, urbain et industriel.Le cas de Ralph Adams Cram remet en question cette idée d'une étanchéité entre littérature et architecture gothique après 1830. À l'instar de ses contes gothiques où il met en scène de vieilles maisons assaillies par une modernité destructrice du passé, Cram conçoit ses églises non pas comme une résurrection mais comme le retour spectral d'un passé à jamais disparu et qu'il n'a pas le pouvoir de faire revivre. C'est le cas, en particulier, de l'église anglicane de Ste. Marie de Walkerville en Ontario construite entre 1902 et 1904 sur les dessins de Cram. Ayant recours aux strategies hantologiques élaborées par le philosophe français Jacques Derrida, la thèse tente une déconstruction de l'église anglicane de Walkerville en faisant ressortir l'horreur de ce spectre qui hante la maison de Dieu telle que conçue par Cram. L'église de Walkerville était une commande de Edward Chandler Walker, puissant chef d'entreprise qui contrôlait comme un monarque la ville de Walkerville. Cet homme de pouvoir était atteint d'une maladie honteuse et fatale: la syphilis. Le programme iconographique de l'église de Walkerville encrypte cette maladie dégénérative sous la figure biblique d'un lépreux au membre atrophié apparaissant dans un des vitraux du bas-côté. C'est cette figure qui permet d'initier une analyse « déconstructive », la « main » rognée du lépreux étant lu comme les doigts écartés de la lettre « k », marque grammatologique dissimulée dans le terme anglais « gothic » mais révélée dans sa forme archaïque « gothick ». La thèse démontre comment, de par sa configuration structurale, spatiale, sociale et iconographique, l'église St-Mary de Walkerville propose une sémiotique de l'abjection face au monde moderne. Elle prépare ainsi l'arrivée du Chevalier du Saint-Graal, dont seule la main sainte peut mettre fin aux souffrances du Roi Pêcheur, Edward Walker, et, par extension, terminer la nuit sombre de notre modernité décadente. Mais le Chevalier du Saint-Graal arrivera-t-il jamais? </description><creator>Macdonell, Cameron</creator><contributor>Martin Bressani (Supervisor)</contributor><date>2013</date><subject>Communications And The Arts - Architecture</subject><title>Haunted by the gothic: deconstructing the new St. Mary's Anglican church, Walkerville, Ontario</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/9z903357f.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/kd17cx815</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>School of Architecture</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:mw22v925x</identifier><datestamp>2020-03-21T19:56:00Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Dans cette thèse, nous étudions le transport quantique dans l'isolant topologique (TI) Bi2Se3 à partir d'un modèle d'échelle atomique. Un TI est un matériau ayant une structure de bande de type isolant bien qu'on y retrouve des états hélicodaux en surface. Ces états hélicoı̈daux ont une relation de dispersion linéaire, dite dispersion de Dirac, qui traverse la bande interdite du cristal. Ces électrons voyageant selon les relations de Dirac sont contraints à se mouvoir perpendiculairement à leur spin. La structure électronique particulière de l'isolant topologique Bi2Se3 est due à une forte interaction spin-orbite et est protégée par une symétrie par renversement du temps. Cette thse comporte deux grands segments. Dans un premier temps, nous présentons une synthèse de la théorie générale des isolants topologiques. Nous présentons ensuite les résultats de nossimulation de transport quantique dans le matériau Bi2Se3.  Dans notre résumé de la théorie des TI, nous présentons une revue de littérature et décrivons conceptuellement, dans la mesure du possible, le comportement des TI de sorte à rendre notre texte intelligible au non-expert. La théorie des TI est expliquée à artir de phénomènes classiques et quantiques connus tels que l'effet Hall, l'interaction spin-orbite, le courant de spin, l'effet Hall de spin, etc. Le concept de la phase de Berry est ensuite introduit pour faire le pont avec la classiﬁcation traditionnelle des TI, laquelle se base sur les invariants topologiques de Z2. Le tout est présenté avec la théorie des bandes en ﬁligrane.  Dans le second segment de cette thése, nous étudions les propriétés physiques du Bi2Se3 à partir de simulations numériques. Après une brève discussion de certains éléments pertinents empruntés de la théorie du transport quantique et du modèle des liens étroits d'échelle atomique, nous présentons les résultats d'une simulation dans laquelle des électrons voyagent à travers un film de Bi2Se3 ayant une dépression en son milieu. Un tel défaut provoquerait une forte diffusion des porteurs de charge dans un conducteur standard. Dans le cas qui nous concerne, la diffusion des états hélicoı̈daux est endiguée par la contrainte qui force ces états à voyager perpendiculairement à leur spin. Néanmoins, de larges dépressions dans le ﬁlm peuvent provoquer le mélange des états hélicoı̈daux de surface et des états localisés à l'intérieur du cristal, ce qui affecte le transport des porteurs de charge.</description><description>In this thesis we investigate quantum transport properties of topological insulator (TI) Bi2 Se3 from atomistic point of view. TI is a material having an energy gap in its bulk but supporting gapless helical states on its boundary. The helical states have Dirac-like linear energy dispersion continuously crossing the bulk band gap with a spin texture in which the electron spin is locked perpendicular to the electron momentum. The peculiar electronic structure of TI material Bi2 Se3 is due to a strong spin-orbit interaction and is protected by the time reversal symmetry. The thesis consists of two main parts. The first reviews the theory of TI and the second presents our atomistic calculations of electron transport in the Bi2 Se3 material.  In the theoretical review of the physics of TI, I follow the literature and attempt to present it in a reasonably accessible manner. The theory of TI is explained in terms of well known physical phenomena including classical and quantum Hall effects, spin-orbit coupling, spin current, and spin-Hall effect. The concept of Berry's phase is then introduced to link with the formal conventionalclassification of TI by the topological Z2 invariants. The entire discussion is within the well known Bloch band theory. In the second part of this thesis, numerical studies of transport properties of Bi2 Se3 are presented. After a brief discussion of the relevant quantum transport theory and the tight binding atomistic model, we present our calculated quantum transport results of Bi2 Se3 films having a trench in the middle. Such a large defect, if on normal conductors, would cause significant back scattering of the carriers. Here, by topological protection of the helical states, back scattering is forbidden due to the spin-momentum locking. Nevertheless, large trenches in the film may cause the helical states on the surface to mix inside the trench, thereby affecting the transmission.</description><creator>Nemytov, Vadim</creator><contributor>Hong Guo (Internal/Supervisor)</contributor><date>2013</date><subject>Physics - Solid State</subject><title>Topological insulators: theory and electronic transport calculations</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/dv13zx83t.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/mw22v925x</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:d504rp82q</identifier><datestamp>2020-03-21T19:56:01Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>This dissertation examines the problems that religious and moral diversity raise in public bioethics, both in the historical development of the field and in our contemporary situation, and is an attempt to develop the foundations of a bioethical methodology that is able to adequately address the issues of pluralism without losing sight of the fact that bioethics emerged out of the need for shared moral guidelines and rigorous ethical analysis of novel medical technologies. It has been my intention to contribute new insights into the processes of bioethical inquiry, deliberation and policy formation through the development of a dialogical method of public ethics that is able to quest for consensus while simultaneously maintaining a respect for, and making possible the accommodation of, incommensurable moral and ontological differences amongst religious traditions and philosophical systems. The aim is to implement modes of deliberation that can adequately cope with the reality of pluralism and to help produce bioethical policies suited for our multicultural and religiously diverse society. </description><description>Cette thèse examine les problèmes que la diversité religieuse et morale soulèvent dans la bioéthique publique, à la fois dans le développement historique de la discipline et dans la situation contemporaine; de plus, elle constitue une tentative pour développer les fondements d'une méthodologie bioéthique qui est en mesure d'aborder les enjeux du pluralisme sans perdre de vue le fait que la bioéthique a émergé du besoin de lignes directrices morales partagées et d'une analyse éthique rigoureuse des nouvelles technologies médicales. Il a été mon intention de jeter un nouveau regard sur les processus d'enquête et de délibération bioéthiques et d'élaboration de politiques bioéthiques par le développement d'une méthode dialogique d'éthique publique qui puisse permettre simultanément la recherche du consensus ainsi que le respect et l'accommodement des différences morales et ontologiques incommensurables entre traditions religieuses et systèmes philosophiques différents. L'objectif est d'implémenter des modes de délibération qui puissent faire face à la réalité du pluralisme et d'aider à développer des politiques bioéthiques adaptées à notre société multiculturelle et multiconfessionnelle. </description><creator>Durante, Christopher</creator><contributor>Gaelle Fiasse (Supervisor1)</contributor><contributor>Daniel M Cere (Supervisor2)</contributor><date>2013</date><subject>Religion - Philosophy of </subject><title>Public bioethics &amp; the reality of religious pluralism: coping with moral diversity in bioethical methodology</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/sq87bx89r.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/d504rp82q</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Faculty of Religious Studies</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:vh53x038d</identifier><datestamp>2020-03-21T19:56:01Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Modern information systems, consisting of an application server tier and a database tier, offer several consistency guarantees for accessing data where strong consistency is traded for better performance or higher availability. However, it is often not clear how an application is affected when it runs under a low level of consistency. In fact, current application designers have basically no tools that would help them to get a feeling of which and how many inconsistencies actually occur during run-time of their particular application. In this thesis, we present new approaches to detect and quantify consistency anomalies for arbitrary multi-tier or cloud applications accessing various types of data stores in transactional or non-transactional contexts. We do not require any knowledge on the business logic of the studied application nor on its selected consistency guarantees. Our detection approaches can be off-line or on-line and for each detected anomaly, we identify exactly the requests and data items involved. Furthermore, we classify the detected anomalies into patterns showing the business methods involved as well as their occurrence frequency. Our approaches can help designers to either choose consistency guarantees where the anomalies do not occur or to change the application design to avoid the anomalies. Furthermore, we provide an option in which future anomalies are dynamically prevented should a certain threshold of anomalies occur. To test the effectiveness of our approaches, we have conducted a set of experiments analyzing the occurrence of anomalies in the benchmarks RUBiS and SPECj Enterprise 2010 under the multi-tier platform JavaEE and the benchmarks JMeter andYahoo! YCSB under the cloud platforms Google App Engine and Cassandra, respectively.</description><description>Les systèmes d'information modernes, consistant d'un niveau de serveur d'applications et d'un niveau de base de données, offrent plusieurs guaranties de consistance pour accéder à des données où la consistance est compromise pour une meilleure performance ou pour une haute disponibilité. Cependant, il n'est souvent pas claire comment une application est affectée lorsqu'elle s'exécute sous un bas niveau de consistance. En effet, les concepteurs d'applications actuelles ne disposent pas d'outils qui leur permettent d'avoir un de idée sur quelles inconsistances ainsi que sur leur nombre d'occurence during l'execution de leur application particulière. Dans cette thèse, nous présentons de nouvelles approches pour détecter et quantifier les anomalies de consistance pour des applications arbitraires multi-niveaux ou cloud accédant différent types de bases de données sous des contextes transactionels ou non-transactionels. Nous ne nécessitons aucune connaissance sur la logique de l'application étudiée ni sur ses guaranties de consistance. Notre approche de détection peut êetre hors ligne ou en ligne et pour chaque anomalie détectée, nous identifions exactement les requêetes et les éléments de données impliquées. En outre, nous classons les anomalies détectées en des patrons d'anomalies montrant les méthodes impliquées ainsi que leur fréquence d'occurrence. Notre approche peut aider les concepteurs à choisir soit des garanties de consistance sous lesquelles les anomalies ne se produisent pas ou modifier la conception de leur applications afin d'éviter les anomalies. En outre, nous fournissons une option sous laquelle les futures anomalies sont dynamiquement évitées si un certain seuil d'anomalies est dépassé. Pour tester l'efficacité de nos approches, nous avons mené une série d'expériences analysant l'occurrence d'anomalies pour les bancs d'essai RUBiS et SPECj Enterprise 2010 sous la plate-forme multi-niveaux JavaEE ainsi que pour les bancs d'essai JMeter etYahoo! YCSB respectivement sous les plates-formes cloud Google App Engine et Cassandra.</description><creator>Zellag, Kamal</creator><contributor>Bettina Kemme (Supervisor)</contributor><date>2013</date><subject>Applied Sciences - Computer Science</subject><title>On the detection and prevention of consistency anomalies in multi-tier and cloud platforms</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/cc08hk15z.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/vh53x038d</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>School of Computer Science</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:1v53k166x</identifier><datestamp>2020-03-21T19:56:02Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Proliferation of plug-in electric vehicles (PEVs) with poor regulation might impose additional stress on distribution grids and cause severe local grid problems. To alleviate potential negative impacts due to PEV loadings and to avoid costs from installing new generation capacity and postponing network reinforcements, coordinated schemes need to be applied. This thesis focuses on a multidisciplinary study on smart management of PEVs with the aid of advanced information and communication technologies (ICTs). A co-simulation environment is developed in collaboration with the Optical Zeitgeist Laboratory. A coordination scheme is then proposed, with the goals of achieving peak shaving and valley filling and maximum utilization of local renewable energy sources without compromising on vehicle owners' driving needs. The scheme is optimized in terms of different objectives with respect to either charging or discharging periods and multiple charging locations. The study is based on real-world transportation data and a residential network model mapped with a fiber-wireless communications infrastructure. By performing co-simulation, the proposed scheme is examined from the aspect of power engineering, including power demand, losses, and nodal voltage magnitudes obtained from load flow analysis, and simultaneously from the communications perspective such as required channel bandwidth and delay. The scheme is tested in comparison to a benchmark coordinated charging scheme, and its efficacy is also verified in a sensitivity analysis with a higher PEV penetration level.</description><description>La prolifération de véhicules électriques (VEs) sans régulation pourrait causer des problèmes au niveau des réseaux de distribution et causer des problèmes majeurs localement dans le réseau. Pour éviter les impacts dûs à la charge des VEs et éviter les coûts d'installation de mise à jour du réseau, il est possible d'éviter ces problèmes en opérant des mécanismes de coordination. Cette thèse porte sur l'étude multidisciplinaire de la gestion intelligente (dite smart) de VEs en exploitant les technologies de l'information et de la communication (TICs). Un co-simulateur est développé en collaboration avec Optical Zeitgeist Laboratory. Un mécanisme de coordination est ensuite proposé ayant pour but de diminuer le pic d'utilisation du réseau et en augmentant l'utilisation lors des périodes de faible pointe en utilisant les énergies renouvelables sans compromettre les besoins des propriétaires de VEs. Le mécanisme optimise selon plusieurs objectifs en planifiant des périodes de charge et décharge en tenant compte de l'emplacement des VEs. Les co-simulations effectuées prennent en compte des données statistiques réelles des comportements de transport et le réseau de distribution est contrôlé en utilisant un réseau de télécommunications de prochaine génération optique et sans-fil (FiWi). Le mécanisme de coordination tient compte de plusieurs aspects du génie électrique, tel que la demande de puissance, les pertes, le voltage ainsi que plusieurs métriques du réseau de communication FiWi. Le mécanisme est testé et comparé à un mécanisme existant proposé dans la littérature. De plus, le mécanisme  proposé est analysé en variant le nombre de VEs dans une population donnée. </description><creator>Xu, Da Qian</creator><contributor>Geza Joos (Internal/Supervisor)</contributor><date>2013</date><subject>Engineering - Electronics and Electrical</subject><title>Coordination of plug-in electric vehicles over a communications infrastructure</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/4q77fw18b.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/1v53k166x</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Electrical and Computer Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:d217qt473</identifier><datestamp>2020-03-21T19:56:03Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Bien que les méthodes mécaniques de transfert de gènes présentent de nombreux avantages, elles sont généralement peu efficaces en raison d'un manque de compréhension des principes sous-jacents. Nous avons donc développé une nouvelle technique simple de transfert de gènes, spatialement restreint, dans le but d'élucider certains des mécanismes sous-jacents de plusieurs méthodes de transfert des gènes mécaniques. Nous démontrons qu'il est possible d'utiliser un appareil simple construit à partir d'un capillaire rectiligne ancré à un tube de polymère, laissant pénétrer un jet de gaz inerte pour faciliter la transfection localisée d'ADN plasmidique à la fois dans un modèle in vitro et in vivo. Nous démontrons aussi qu'il existe un équilibre délicat entre la transfection et la mort cellulaire. Dans une étude antérieure, Leduc (2011) a utilisé cinq différents diamètres internes de tubes capillaires et trois gaz inertes différents pour traiter des cellules HeLa, en espérant pouvoir mieux comprendre la relation entre l'efficacité du transfert d'ADN (sur la surface couverte par le jet de gaz) et les propriétés des différents gaz utilisés (densité et viscosité). Nous avons étendu le travail de Leduc (2011) en confirmant que la gamme de pressions dynamiques qui conduit à la transfection transitoire est mince (environ 300 Pa), à la fois dans les cellules adhérentes HeLa et HAAEC cultivées en monocouche. Suite à ces constatations, nous avons photographié: le phénomène de transfection à l'aide d'une solution de dextran 10 kDa pour évaluer l'efficacité de la transfection (qui a culminé à 450 Pa à 73 ± 5% d'efficacité), la mort cellulaire et le décapage de cellules avec l'aide d'un microscope confocal et une solution de bleu de Trypan dans les cellules HeLa. Nous avons découvert qu'à partir de 300 Pa de pression dynamique le décapage et la perméabilisation permanente (mort cellulaire) devenaient substantielles. Ces résultats ont été reproduits dans la lignée HAAEC. Nous avons ensuite confirmé que les pores transitoires créés permettaient le passage de molécules de dextrans jusqu'à 40 kDa de taille par microscopie confocale. En se basant sur la découverte de Leduc (2011), selon laquelle les pores se referment en moins de 5 secondes après le traitement, nous avons été en mesure de fixer chimiquement une lignée de cellules adhérentes assez rapidement pour pouvoir les photographier par microscopie électronique à balayage (MEB). Une nouvelle technique de MEB a été développée pour photographier la densité et le diamètre des pores, sur des spécimens soumis à différentes pressions dynamiques, en utilisant des cellules HeLa et HAAEC. La technique de transfection a été également utilisée avec succès in vivo sur la membrane chorio-allantoïde (CAM) d'embryons de poulets âgés de sept jours. Pour conclure, la perméabilisation temporaire de cellules adhérentes et la CAM peut être facilitée grâce à un traitement simple de jet de gaz inerte.</description><description>Though mechanical means of gene transfection have many advantages, they are generally of low efficiency due to a lack of understanding of the underlying principles. We used a novel spatially restricted transfection technique in an attempt to gain significant insight into the underlying mechanisms of mechanical transfection methods. We demonstrate that a jet of inert gas flowing out of a straight capillary tube can facilitate local transfection of plasmid DNA both in vitro and in vivo. We show that there is a fine balance between transfection and cell death. Previous to this work, Leduc (2011) had used five different capillary tube inner diameters and three different gases to treat HeLa cells to understand the relationship between the transfection efficiency to the area covered by the gas jet, flow rate and properties of the different gases used (density and viscosity). He found temporary permeabilization to be specific to the path treated by the jet and that the efficiency was best correlated to the dynamic pressure of the jet. We extended the work to show that the range of dynamic pressures that led to transfection was small, in both HeLa and HAAEC adherent monolayer of cells (approximately 300 Pa). Following his findings, we imaged the transfection phenomenon using a 10 kDa dextran solution to evaluate the transfection efficiency (which peaked at 450 Pa at 73 ± 5%), along with stripping and cell death through Trypan-blue staining and confocal microscopy. We found a similar window of permeabilization, above which permanent permeabilization (cell death) and stripping of the cells occurred.  These findings were reproduced in the HAAEC line. We then confirmed that the transient pores created allowed the passage of dextrans up to 40 kDa through confocal microscopy. Using Leduc's (2011) finding that the pores reclose in less than 5 seconds after treatment, we were able to fix an adherent cell line rapidly enough to image the pores. An SEM technique was developed to image the density and diameter of the pores, based on different dynamic pressures, created in HeLa and HAAEC. The transfection technique was also successfully used in vivo in the chorioallantoic membrane (CAM) of 7-day old chick embryos. Hence, temporary permeabilization of adherent cells and the CAM can be achieved by treating the biological tissue with an inert jet of gas.</description><creator>Chouinard-Pelletier, Guillaume</creator><contributor>Richard L Leask (Supervisor2)</contributor><contributor>Elizabeth Jones (Supervisor1)</contributor><date>2013</date><subject>Engineering - Chemical</subject><title>Spatially restricted gene transfection using an inert gas and straight capillary tubes</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/xs55mg46b.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/d217qt473</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Chemical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:cz30px38k</identifier><datestamp>2020-03-21T19:56:04Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les pathologies cardiovasculaires, en particulier les maladies des valves cardiaques, restent toujours les causes prédominantes de mortalité, à un taux de 4% dans les pays développés et à 42% dans les pays en développement. La valve aortique et les artères coronariennes sont l'emphase de nombreux articles récents. Cela est surtout attribuable à l'occurrence élevée de ses maladies dans ces régions et les conséquences critiques qui suivent. Avec certaines pathologies cardiovasculaires, comme la sclérose aortique, les microstructures de la racine et des feuillets aortiques peuvent être modifié avec des contraintes résultantes des changements de l'épaisseur ou de l'élasticité du tissu, ou des deux. Ces pathologies sont reliées à l'altération du débit sanguin, ce qui peut être mortel.Des études numériques ont assisté considérablement à la compréhension des biomécaniques de la fonctionnalité et des pathologies des feuillets, leur effet sur les tissus cardiaques et l'hémodynamie locale. Par contre, ces investigations ont plutôt analysé la structure des valves que les interactions entre le sang et le tissu cardiaque. Ce facteur simple mais sophistiqué est critique pour suffisamment étudier la réponse du système face aux conditions physiologiques. Par ailleurs, à cause de la complexité inhérente de l'analyse d'interaction fluide-structure des feuillets aortiques, il y a un manque évident d'une représentation globale de la région des valves aortiques, ce qui pourrait avancer la connaissance du comportement global de cette structure sous les conditions physiologiques. L'objectif primaire de cette thèse est d'expliquer ce phénomène inconnu dans lequel une pathologie régionale conduit à des variations globales structurelles et hémodynamiques au niveau de la région aortique ainsi que sur les artères coronaires. Ces dernières ont été ajoutées dans le modèle global pour explorer la possibilité d'une interrelation entre ces structures et la valve aortique. Par conséquent, cette thèse est concernée par trois aspects en particulier : la modélisation physiologique de la valve cardiaque, l'investigation des variables hémodynamiques des coronaires par rapport aux pathologies valvulaire, et l'impact possible d'une sténose coronarienne sur la dynamique valvulaire. Ce modèle peut aider à explorer et confirmer le comportement de base de l'interaction entre la valve aortique et le débit coronarien. En plus, au point de vue de la pratique clinique, notre modèle a le potentiel d'être un outil diagnostique ; considérant que les chirurgiens cardiaques et les cardiologues interventionnels pourraient profiter des données additionnelles fournies par ce modèle pour mieux planifier le moment idéal pour l'intervention chirurgicale dans la région de la valve aortique pathologique.</description><description>Cardiovascular pathologies specifically valvular heart diseases remain the biggest cause of deaths worldwide with a mortality rate of 4% in industrialized countries and up to 42% in developing countries. Aortic valve and coronary arteries have in particular been the focus of many studies during the recent years. This is due to the prevalence of pathologies in these regions and the subsequent critical consequences. In certain pathological conditions such as aortic sclerosis, the micro-structure of the aortic root and the aortic valve leaflets are altered in response to stress resulting in changes in tissue thickness, stiffness or both. Such pathologies are thought to affect coronary blood flow which could be life threatening. Numerical studies have greatly assisted in understanding the biomechanics of the aortic valve, its function as well as the impact of pathologies on cardiac tissue mechanics and local hemodynamic. The interaction between the blood and the cardiac tissue is critical in properly studying the response of the system to its physiological conditions. However, due to the inherent complexity of fluid-structure interaction modeling of aortic leaflets, there is a clear lack of a global representation of the aortic valve region which would aid in understanding the overall behaviour of this structure in pathological conditions. Recently, there have been clinical investigations that have observed simultaneous structural and hemodynamic variations in the aortic valve and coronary arteries due to regional pathologies. The main objective of this work is to elucidate this observed and yet unexplained phenomenon where, a regional pathology could lead to global variations in the structure and hemodynamic of the aortic valve region as well as in the coronary arteries. Therefore this thesis concentrates on three aspects: physiological heart valve modeling, investigating coronary hemodynamic variables in presence of valvular pathologies, and the possible impact of coronary stenosis on valvular dynamics. This model can aid in explaining the underlying behaviour that leads to the observed inter-relation between the aortic valve and coronary flow. Moreover, within the clinical practice our model has the potential to serve as a possible diagnostic tool; as the cardiac surgeons and interventional cardiologists can benefit from the additional input provided by this model for choosing the time of surgical intervention in the diseased aortic valve region.</description><creator>Nobari, Soroush</creator><contributor>Raymond Cartier (Supervisor2)</contributor><contributor>Rosaire Mongrain (Supervisor1)</contributor><date>2013</date><subject>Engineering - Biomedical</subject><title>Fluid structure interaction and hemodynamic analysis of the aortic valve</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/kk91fq461.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/cz30px38k</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Biomedical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:4q77fw19m</identifier><datestamp>2020-03-21T19:56:05Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Cette étude examine les différences entre un groupe de femmes qui font de l'exercice dans un cadre de motivation intrinsèque en comparaison avec un de motivation extrinsèque, et ce, en matière : (i) de valeurs de la perception de l'effort (PE), du pourcentage de la fréquence cardiaque maximale (% de FC max) et du compte total de l'activité physique au cours d'une séance d'exercice; (ii) de la persistance à l'exercice immédiatement après la manipulation expérimentale; (iii) du changement positif ou négatif de l'humeur au cours d'une séance d'exercice; et (iv) des effets sur la motivation postexpérimentale (c.-à-d. la compétence perçue dans l'exécution de la tâche, l'intérêt, l'effort et la pression/tension ressentie au cours de l'exécution de la tâche). Des jeunes femmes inactives de poids santé (N = 42; Mâge = 21.59 + 3.31 ans MIMC = 21.59 + 2.11 kg/m2) ont été assignées, au hasard, à faire de l'exercice sur un tapis roulant à côté d'une chercheuse complice qui, selon le groupe expérimental, exprimait des énoncés verbaux soit de type intrinsèque ou de type extrinsèque. La durée de l'exercice, la fréquence cardiaque et l'effort physique ont été enregistrés. Les participantes ont aussi rempli un questionnaire d'auto-évaluation à propos de leur humeur avant et après l'activité physique et de leur motivation après l'exercice physique. Les participantes associées au groupe de motivation intrinsèque de la chercheuse complice ont déclaré, après 8 minutes d'exercice, de plus grandes valeurs de perception de l'effort, elles ont exécuté l'exercice à un plus grand pourcentage de leur fréquence cardiaque maximale, ont enregistré un plus grand nombre de comptes d'activité physique et un plus grand pourcentage des participantes a fait l'exercice sur une plus longue période de temps en comparaison à celles du groupe extrinsèque. Dans les deux groupes, une augmentation de la perception de la vigueur a été déclarée après l'exercice. De plus, les participantes du groupe de motivation intrinsèque ont perçu qu'elles ont fait de plus grands efforts que celles du groupe de motivation extrinsèque. Dans l'ensemble, les résultats montrent que la motivation dans le cadre de l'exercice peut être « contagieuse » grâce aux  énoncés verbaux et que le fait de pratiquer de l'exercice avec un compagnon ou une compagne qui est intrinsèquement motivée peut apporter des résultats avantageux sur le comportement à l'exercice d'une personne. </description><description>This experimental study examined differences between a group of women who exercised within an intrinsically-motivating group or extrinsically motivating group on: (i) ratings of perceived exertion (RPE), percentage of maximal heart rate (% of HR max) and total physical activity (PA) counts during an exercise session; (ii) exercise persistence immediately following the experimental manipulation; (iii) change in positive and negative mood during an exercise session; and (iv) post-experimental motivation outcomes (e.g., perceived competence in executing the task, interest, effort and pressure/tension experienced in performing the task). Young inactive healthy weight females (N = 42; Mage = 21.59 + 3.31 years; MBMI = 21.59 + 2.11 kg/m2) were randomly assigned to exercise on a treadmill alongside a confederate who was providing them with either intrinsic or extrinsic verbal cues, depending on the experimental group. Exercise duration, HR and exertion were recorded. Participants also completed a self-report questionnaire assessing mood pre and post-PA and post-PA motivation. Participants in the intrinsic motivation confederate group reported significantly higher RPE values after 8 minutes of exercise, exercised at a higher % of their HR max, had a higher PA count, and a greater percentage exercised for a longer duration when compared to participants in the extrinsic motivation group. An increased perception of vigor was reported after the exercise, regardless of group. In addition, participants in the intrinsic motivation group perceived that they exerted more effort than those in the extrinsic motivation group. Overall these findings suggest that exercise motivation can be "contagious" through verbal cues and that exercising with a partner who is intrinsically motivated can have beneficial outcomes on one's own exercise behaviours.</description><creator>Scarapicchia, Tanya</creator><contributor>Catherine Sabiston (Internal/Supervisor)</contributor><date>2013</date><subject>Education - Health</subject><title>The motivational effects of social contagion on exercise participation in young women</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/9593tz814.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/4q77fw19m</identifier><degree><name>Master of Arts</name><grantor>McGill University</grantor><discipline>Department of Kinesiology and Physical Education</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:bz60d1137</identifier><datestamp>2020-03-21T19:56:06Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Background: Fixed dose combination (FDC) anti-tuberculosis (TB) therapy is currently recommended to facilitate patient adherence, and prevent inadvertent or intentional mono-therapy. We have conducted a systematic review to evaluate the risk of treatment failure or relapse and acquired drug resistance, as primary outcomes, and the proportion of bacterial conversion after two months of treatment, adverse events, adherence, and treatment satisfaction, as secondary outcomes, associated with treatment of active TB using FDC or separate-drug formulations. Methods: We searched four electronic databases for randomized controlled trials and cohort studies published in any language since 1980.  Results from trials that directly compared FDC to separate-drug formulations were pooled.  Results from other studies were reported separately.  Results: We identified 2450 potentially eligible articles from which 15 trials that directly compared FDC and separate-drug formulations as well as four additional relevant studies were included.  In the 15 randomized trials there were no differences in acquired drug resistance, bacterial conversion after two months of treatment, or adverse drug reactions with FDC or separate-drug formulations. There was a trend toward higher risk of the combined outcome of treatment failure or disease relapse with FDC (pooled risk ratio, 1.28 [95% CI: 0.99, 1.65]). Based on individual study results, only one of two trials that assessed treatment satisfaction, and none of five that assessed patient adherence favored FDC's. Conclusion: The results of this systematic review do not support the current recommendation for the use of FDC formulations for treatment of active TB.</description><description>Contexte : La combinaison à dose fixe est la forme de thérapie contre la tuberculose recommandée pour favoriser l'adhésion du patient et prévenir la mono thérapie intentionnelle ou accidentelle.  Nous avons procédé à un examen systématique visant à évaluer le risque d'échec ou de rechute, de résistance aux médicaments, à titre de résultats primaires; et la proportion de conversion bactérienne après deux mois de traitement, d'effets secondaires, d'adhésion et de satisfaction reliés au traitement de la TB active, à titre de résultats secondaires,  en comparant la combinaison à dose fixe et la multithérapie. Méthodes : Nous avons consulté quatre bases de données pour des essais contrôlés randomisés et des études de cohortes publiés en toute langue depuis 1980.  Les résultats d'essais comparant directement la combinaison à dose fixe et la multithérapie furent regroupés. Les résultats d'autres études furent rapportés séparément. Résultats : Nous avons identifié 2450 articles potentiellement éligibles.  Quinze essais comparant directement la combinaison à dose fixe à la multithérapie, ainsi que quatre autres études pertinentes sont inclus. Les 15 essais randomisés n'ont soulevé aucune différence  en rapport à la pharmaco-résistance, à la conversion bactérienne après deux mois de traitement, ou aux effets secondaires entre la combinaison à dose fixe et la multithérapie. On note toutefois une tendance de risque plus élevé avec la combinaison à dose fixe lorsqu'on combine les résultats 'échec du traitement et rechute' (ratio de risque combiné, 1.28 (95% CI :0.99, 1.65).  Selon les résultats individuels d'études, seul un des deux essais randomisés évaluant la satisfaction du traitement et aucun des cinq études évaluant l'observance ont favorisé la combinaison à dose fixe.Conclusion : Les résultats de cet examen  systématique d'appuient pas la recommandation actuelle qui favorise l'utilisation de combinaisons à dose fixe pour le traitement de la TB active.</description><creator>Albanna, Amr</creator><contributor>Richard Ian Menzies (Internal/Supervisor)</contributor><date>2013</date><subject>Health Sciences - Epidemiology</subject><title>Does fixed dose combination anti-tuberculosis therapy improve treatment outcomes? A systematic review and meta-analysis</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/7m01bq141.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/bz60d1137</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Epidemiology and Biostatistics</discipline></degree></thesis></metadata></record><resumptionToken completeListSize="47894">oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):15975</resumptionToken></ListRecords></OAI-PMH>