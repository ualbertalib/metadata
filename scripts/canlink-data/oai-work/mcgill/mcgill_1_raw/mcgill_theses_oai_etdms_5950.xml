<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/assets/blacklight_oai_provider/oai2-b0e501cadd287c203b27cfd4f4e2d266048ec6ca2151d595f4c1495108e36b88.xsl"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd"><responseDate>2020-07-24T23:13:26Z</responseDate><request resumptionToken="oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):5950" verb="ListRecords">https://escholarship.mcgill.ca/catalog/oai</request><ListRecords><record><header><identifier>oai:escholarship.mcgill.ca:5m60qv46f</identifier><datestamp>2020-03-21T13:52:59Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>La formation du givre en vol sur un aéronef, traversant un nuage de gouttelettes surfondues, présente un sérieux risque pour la sécurité aérienne. Le système de protection contre le givre (IPS) est conçu afin de protéger l'aéronef durant ces conditions de vol dangereuses et ainsi satisfaire les normes de certifications de la FAA (Federal Aviation Administration) aux États-Unis et, des autorités équivalentes dans les autres pays. La plupart des gouttelettes surfondues rencontrées en vol sont sphériques et de petite taille et, par conséquent, adhèrent rapidement à la surface qu'elles frappent, ou en sont complètement éjectées. Toutefois, les grosses gouttelettes surfondues (SLD) se comportent différemment et doivent être traitées adéquatement dans l'enveloppe de givrage tel que conseillé par le FAA dans l'annexe O. En effet, lorsque ces gouttelettes frappent une surface, elles peuvent soit s'y coller, soit se fragmenter et s'éclabousser, ou rebondir et être entrainées dans l'écoulement qui peut les déposer à un endroit non protégé de l'aéronef. Par conséquent, bien modéliser la dynamique des grosses gouttelettes surfondues est primordial afin de prédire correctement son effet sur la formation du givre en vol. En effet, les informations sur le ratio de gouttelettes éjectées sur celles qui adhèrent à la surface, ainsi que sur la distribution de ces gouttelettes après impact permettront une amélioration de la modélisation numérique de l'impact de ces gouttelettes. L'objectif de cette thèse est d'améliorer la compréhension de la dynamique des collisions entre les grosses gouttelettes surfondues et une surface sèche ou humide, et ceci à des vitesses représentatives des applications aéronautiques. Pour cela et dans ce but, deux méthodes fondées toutes les deux sur le suivi de la particule sont présentées. D'abord, une approche numérique méso échelle, basée sur la méthode Quasi-Moléculaire (QMD), est proposée pour la modélisation de la dynamique des gouttelettes chaque quasi-molécule représentant une grande agglomération de véritables molécules. Fondées sur le théorème de l'équipartition de l'énergie, plusieurs approches pour extraire les quantités macroscopiques, tel que la température et les coefficients de transport, sont étudiées. En effet, un choix approprié des paramètres libres du modèle permet une meilleure prédiction des propriétés macroscopiques. Nous explorons par ailleurs, l'efficacité et la précision de l'approche proposée. Deux aspects sont par la suite évalués: l'ajout de l'effet de l'air dans le modèle multi-phases et un couplage hybride QMD-continuum.Ensuite, nous présentons comme alternative une méthode Smoothed Particle Hydrodynamics (SPH) qui est développée afin de modéliser les conditions SLD. En effet, la méthode SPH fournit une approximation particulaire pour la résolution des équations de Navier-Stokes et est ainsi adaptée aux fluides comportant de grandes déformations. Dans cette optique, un modèle multi-phases incorporant la tension superficielle et un algorithme shifting pour fluide légèrement compressible sont proposés afin de simuler la dynamique d'une seule gouttelette. La validité de l'approche est confirmée par la modélisation de problèmes références ainsi que par la comparaison avec d'autres données numériques et expérimentales. Enfin, nous discutons les avantages et limitations de la méthodologie proposée. Par ailleurs, l'impact de la gouttelette de même que sa déformation et sa rupture sur une fine pellicule liquide et une surface solide sont aussi modélisés.</description><description>In-flight ice accretion on aircraft flying through clouds of supercooled droplets poses a serious safety risk to air travel. Ice Protection Systems (IPS) are designed to protect the aircraft against these hazardous conditions and to meet the certification standards of the Federal Aviation Administration (FAA) in the USA and equivalent airworthiness authorities in other countries. Most of the supercooled droplets encountered during flights are small in size and are assumed to be spherical droplets that adhere quickly to the surface upon impact, or progress and result in the runback ice formation. However, larger droplets with a diameter greater than 50 microns, i.e. the so-called Supercooled Large Droplets (SLD), act differently and need to be addressed properly as indicated by the recently introduced icing envelope FAA's Appendix O. Once these SLDs impinge, they may stick, splash, or bounce back to the airstream and result in ice accretion in areas not covered by an IPS designed taking into account only small droplets. Therefore, modeling SLD dynamics is of great importance in accurately assessing in-flight icing effects. Obtaining information on the ratio of ejected to deposited water and the post-impact droplet distribution will improve the numerical modeling of the bulk of impinging droplets.In this dissertation, two particle-based methods are developed and employed to model the SLD dynamics. The goal is to improve the understanding of the dynamics of large droplets collisions over dry or wet surfaces at velocities typical of aeronautical applications. First, a mesoscale model for droplet dynamics based on the Quasi-Molecular Method (QMD) is proposed. It considers the interaction between quasi-molecules within a material, each quasi-molecule representing an agglomeration of a large number of actual molecules. Based on the Equipartition Theorem, approaches for extracting macroscopic quantities such as temperature and transport coefficients from the quasi-molecular method are discussed. A proper choice of the free parameters of the model that lead to accurate values for the macroscopic properties is also addressed. Approaches for improving the computational efficiency and numerical accuracy are explored. Then the possibility of including airflow effects within a multi-phase model and a hybrid continuum-QMD coupling is also investigated.As an alternative, Smoothed Particle Hydrodynamics (SPH) method is employed and developed to model SLD conditions. SPH provides a particle approximation of the Navier-Stokes equations and is suitable for flows with large deformations. A weakly compressible multi-phase model with shifting algorithm and surface tension model is presented to simulate the single droplet dynamics. The validity of the approach has been proved by modeling classical benchmark cases and comparing against other numerical and experimental data in the literature. The advantages and limitations of the method are investigated, and droplet impingement on a liquid film and solid surface are modeled, together with droplet deformation and breakup.</description><creator>Abdollahi, Vahid</creator><contributor>Wagdi George Habashi (Supervisor1)</contributor><contributor>Marco Fossati (Supervisor2)</contributor><date>2017</date><subject>Mechanical Engineering</subject><title>Particle modeling of supercooled large droplets dynamics for in-flight icing conditions</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/h702q876w.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/5m60qv46f</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:vt150m51h</identifier><datestamp>2020-03-21T13:53:00Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Cette thèse se penche sur la relation entre duperie de soi et conscience de soi. On a fait valoir que, si l'on adopte une position littéraliste en prenant la duperie de soi au pied de la lettre – c'est-à-dire en la comprenant comme duperie élaborée par et imposée à un même sujet conscient de lui-même – alors la duperie de soi est impossible. Elle entraîne en effet le problème suivant (connu sous le nom de « problème dynamique ») : étant consciente de mon intention de me duper moi-même, je ne pourrai d'emblée être dupe de la tromperie projetée, éliminant par là sa possibilité même. La duperie de soi soulèverait également le problème suivant (appelé « problème statique »). En tant que je me dupe, je crois non-P, mais – en tant que dupée – je crois P. Il nous faut donc expliquer ensuite comment on peut entretenir des croyances contradictoires en toute conscience de soi. Je soutiens que le rejet de la position littéraliste sur la duperie de soi repose sur deux erreurs. Premièrement, ce rejet résulte d'une mauvaise compréhension du littéralisme. Un littéralisme bien compris ne requiert pas l'adhésion simultanée à des croyances contradictoires. Deuxièmement, il fausse la relation entre duperie de soi et conscience de soi – ce sur quoi je m'attarderai tout particulièrement.La phénoménologie de la duperie de soi montre qu'une personne qui se dupe elle-même expérimente la tension suivante. Elle est d'une certaine façon consciente de sa duperie en tant que telle. Mais elle se représente également (à tort) cette duperie comme pleinement conforme à la vérité – et, en ce sens, elle n'est pas consciente qu'il s'agit d'une duperie. Afin de prendre en compte cette tension, il nous faut une théorie de la conscience de soi – théorie à part entière – qui satisfasse cette double exigence ; c'est-à-dire, qui permette à la personne se dupant elle-même de ne pas voir clair dans son propre jeu. Le rejet du littéralisme présuppose qu'aucune théorie de la conscience de soi ne peut satisfaire cette double exigence. Je soutiens que cette double exigence peut pourtant être satisfaite. La première partie de cette thèse élabore une théorie détaillée de la conscience de soi. La seconde partie montre que cette théorie de la conscience de soi explique la tension inhérente à la duperie de soi tout en évitant les problèmes dynamique et statique mentionnés plus haut. Cette seconde partie constitue par le fait même une tentative de défendre la position littéraliste.</description><description>This thesis examines the relation between self-deception and self-consciousness. It has been argued that, if we follow the literalist and take self-deception at face value – as a deception that is intended by, and imposed on, one and the same self-conscious subject – then self-deception is impossible. It will incur the Dynamic Problem that, being aware of my intention to self-deceive, I shall see through my projected self-deceit from the outset, thereby precluding its possibility. And it will incur the following Static Problem. Qua self-deceiver, I shall believe not-P, but – qua self-deceived – I shall believe P. We shall then have to explain how I can sustain contradictory beliefs in full self-awareness. I argue that this rejection of literalism about self-deception rests on error. First, it misunderstands what literalism holds. Properly understood, literalism does not require the simultaneous commitment to contradictory beliefs. Second, it misunderstands the relation between self-deception and self-consciousness – and that is my primary focus. The phenomenology of self-deception reveals that the self-deceiver experiences the following tension. She is somehow aware of her self-deception as such. Yet also, she misrepresents that self-deception to herself as being a sincere commitment to the truth – so, in that sense, she is not aware of her self-deceit as such. To capture this tension, we require a theory of self-consciousness – independently defensible in its own right – that will meet this twofold requirement, of permitting the self-deceiver not to see what is right before her gaze. The rejection of literalism presupposes that no theory of self-consciousness can meet this twofold requirement.I argue that this twofold requirement can be met. The first part of this thesis offers a detailed defence of a theory of self-consciousness. The second part shows how this theory of self-consciousness can faithfully capture the tension of self-deception, while eschewing the Dynamic and Static Problems. It thereby claims to vindicate the literalist's position.</description><creator>Jordan, Maiya</creator><contributor>David Davies (Supervisor2)</contributor><contributor>Alia Al-Saji (Supervisor1)</contributor><contributor>Ian Jeffrey Gold (Supervisor3)</contributor><date>2017</date><subject>Philosophy</subject><title>Self-awareness and self-deception</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/ft848t03n.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/vt150m51h</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Philosophy</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:n583xx51z</identifier><datestamp>2020-03-21T13:53:05Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Beta cell death is responsible for the pathogenesis of both Type-1 and Type-2 diabetes patients. Pancreatic beta cell regeneration, either through the induction of beta cell proliferation, or through beta cell neogenesis resulting from the differentiation of other cell types, is becoming an attractive prospect for achieving a true curative therapy. Islet Neogenesis-Associated Protein (INGAP), previously identified by Dr. Lawrence Rosenberg, has been documented to promote beta cell regeneration and improved glucose tolerance in animal models. A peptide derivative of INGAP has shown similar effects and is currently undergoing phase II clinical trials. We hypothesize that the effects of INGAP can be improved by continuous in vivo release, which can be achieved through secretion of INGAP by an implanted cell population. Mesenchymal stem cells (MSCs) represent a promising choice for INGAP delivery, owing to their advantageous "homing", regenerative and immunomodulatory properties, as well as the many benefits they have exhibited when injected into both diabetic animals and human patients. In this study, to prove that MSCs can express and secrete INGAP, C57BL/6 mouse bone marrow-derived mesenchymal stem cells were transfected with a plasmid encoding INGAP cDNA. The expression and secretion of INGAP in these cells was then confirmed both by Western Blot of conditioned media, and by fluorescence microscopy. To further optimize the detectability of secreted INGAP for use in animal studies, a new DDK (FLAG) tagged INGAP cDNA construct was designed, using a sub-cloning approach. In addition, INGAP-DDK cDNA was incorporated into a retroviral plasmid for the generation of retroviral particles for transduction applications. These experimental results demonstrate that the cellular machinery of MSCs is capable of secreting full length INGAP, and lay the foundation for producing reliable INGAP-expressing MSCs through viral transduction, for use in future in vitro and in vivo experimentation.</description><description>La mort des cellules bêta est à l'origine de la pathogénèse du diabète de Type 1 et de Type 2. La regénération de ces cellules, soit par la prolifération de cellules bêta existantes, ou par la neogénèse résultant de la différenciation de cellules provenant d'autres lignées, devient une proposition intéressante pour obtenir une véritable thérapie curative. L'injection de l' "Islet Neogenesis-Associated Protein" (INGAP), identifiée par le Dr. Lawrence Rosenberg, a réussi à promouvoir la regénération de cellules bêta, et à améliorer la tolérence au glucose, chez des animaux. Un peptide, dérivé de INGAP, a produit des effets comparables, et est couramment sous investigation dans le contexte d'études cliniques de phase II. On soumet l'hypothèse que les effets d'INGAP peuvent êtres accrus par une relache continue in vivo, ce qui peut etre accompli par la sécretion d'INGAP au moyen de cellules implantées. Les cellules souches mésenchymateuses (MSCs), derivées de la moelle osseuse, constituent un excellent choix pour la livraison d'INGAP, à cause de leurs propriétés regénératrices, immunomodulatrices, et migratrices, ainsi que les bénéfices préalables qu'elles ont apporté au cours d'études où elles ont été injectées chez des animaux et des patients humains. Au cours d'étude présenté ici,  pour établir que les MSCs peuvent exprimer et sécéter INGAP, des MSCs de souris C57Bl/6 ont été transfectées en utilisant un plasmide contenant l'ADNc de INGAP. L'expression et la sécretion d'INGAP dans ces cellules ont ensuite été confirmé par Western Blot et par immunofluoréscence. Pour optimizer la détection d'INGAP, en vue d'études futures sur des animaux, un nouveau plasmide contenant un marqueur DDK (FLAG) a été construit, utilisant une approche de sous-clonage. De plus, la portion INGAP-DDK a été introduit dans un plasmide rétroviral, pour servir à des applications futures de transduction rétrovirale. Ces résultats expérimentaux démontrent que la machinerie céllulaire des MSCs est capable de sécréter une protéine INGAP de pleine longueur, et établissent la base d'une méthode fiable pour la génération de MSCs exprimant INGAP, qui pourront servir au cours d'études subséquentes in vitro et in vivo.</description><creator>Violette-Deslauriers, Shaun</creator><contributor>Lawrence Rosenberg (Internal/Supervisor)</contributor><date>2017</date><subject>Surgery</subject><title>Generation of mesenchymal stem cells expressing islet neogenesis-associated protein (INGAP)</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/qb98mj00v.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/n583xx51z</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Surgery</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:d791sj82q</identifier><datestamp>2020-03-21T13:53:05Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Loss-of-function mutations in the X-linked immunoglobulin superfamily, member 1 (IGSF1) gene cause central hypothyroidism. IGSF1 is a transmembrane glycoprotein of unknown function. It is expressed in thyroid-stimulating hormone (TSH) producing thyrotrope cells of the anterior pituitary gland. The protein is co-translationally cleaved into N- and C-terminal domains (NTD and CTD). The CTD is trafficked to the plasma membrane, whereas the NTD is retained in the endoplasmic reticulum (ER). Most intragenic IGSF1 mutations in patients map to the CTD. To better understand IGSF1 function, we used the CRISPR-Cas9 system to introduce a loss-of-function mutation into the IGSF1-CTD in mice. The modified allele harbors a 312 bp deletion, removing part of exon 18 and intron 18. The resulting mRNA is expressed, though at greatly reduced levels relative to wild-type, and contains a novel hybrid exon. This exon introduces frame-shift and a premature stop codon, which affects the trafficking of the CTD. Igsf1 deficient mice show normal serum TSH levels and normal numbers of TSH-expressing thyrotropes. However, expression of the TSH subunits, Tshb and Cga, and TSH protein content are reduced in these animals relative to wild-type littermates. Hypothalamic thyrotropin-releasing hormone (TRH) stimulates TSH synthesis and release. Importantly, pituitary TRH receptor (Trhr) mRNA levels are reduced in Igsf1 deficient males. When challenged with exogenous TRH, Igsf1 deficient mice release TSH, but to a significantly lesser extent than wild-type animals. The mice show similar impairments when endogenous TRH release is increased in response to reduced thyroid hormone feedback. Collectively, these results suggest that IGSF1 deficiency leads to central hypothyroidism via impairments in pituitary TRHR expression and/or downstream signaling.</description><description>Les mutations causant une perte de fonction du gène membre 1 de la superfamille des immunoglobulines (IGSF1) qui se trouve sur le chromosome X, sont responsables d'un syndrome d'hypothyroïdie centrale. IGSF1 est une glycoprotéine transmembranaire ayant une fonction inconnue. Elle est exprimée dans les thyréotropes de l'hypophyse  produisant la thyréostimuline (TSH). IGSF1 est clivées au moment de la traduction et se sépare en deux domaines: le domaine du terminal N (NTD) et du terminal C (CTD). Le CTD est transporté à la surface cellulaire tandis que le NTD est retenu dans le réticulum endoplasmique (ER). La majorité des mutations intra-géniques de IGSF1, découverte chez les patients, se trouvent dans la région qui encode le CTD. Pour mieux comprendre le rôle de IGSF1, nous avons utilisé le système CRISPR-Cas9 pour introduire une mutation causant une perte de fonction de IGSF1-CTD. L'allèle modifié contient une perte de 312 paires de bases qui engendre la perte d'une partie de l'exon 18 et de l'intron 18. L'ARN résultant est exprimé, malgré un niveau d'expression très bas comparativement aux souris de type sauvage, et contient un nouvel exon hybride. Cet exon introduit une mutation qui affecte le cadre the lecture de l'ARN, ce qui cause l'introduction d'un codon-stop prématuré, qui affecte le transport du CTD. Les souris déficientes en Igsf1 démontrent des niveaux normaux de TSH en circulation ainsi qu'un nombre normal de thyréotropes produisant du TSH. Par contre, l'expression des différentes parties de TSH, Tshb and Cga, ainsi que le niveau de protéine de TSH contenu dans l'hypophyse sont réduits chez ces souris comparativement aux souris de type sauvage dans la même cage. L'hormone thyréostimuline (TRH) de l'hypothalamus stimule la synthèse et la sécrétion de TSH. Intéressement, l'ARNm des récepteurs de TRH dans l'hypophyse (Trhr) est réduit chez les souris mâles déficientes en Igsf1. Quand ces souris sont stimulées avec une dose exogène de TRH, elles sécrètent du TSH, mais en quantité moindre que les souris de type sauvage. Les souris déficientes en Igsf1 répondent semblablement à une hausse des niveaux de TRH endogène dû à une réduction des niveaux d'hormones thyroïde. Ensemble, ces résultats suggèrent qu'une déficience en IGSF1 engendre le syndrome d'hypothyroïdie centrale via une réduction de l'expression ou de la cascade de TRHR au niveau de l'hypophyse.</description><creator>Turgeon, Marc-Olivier</creator><contributor>Daniel Bernard (Internal/Supervisor)</contributor><date>2016</date><subject>Anatomy and Cell Biology</subject><title>Impaired pituitary actions of thyrotropin-releasing hormone underlie central hypothyroidism in immunoglobulin superfamily, member 1 deficiency syndrome</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/0r967625k.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/d791sj82q</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Anatomy and Cell Biology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:jm214r681</identifier><datestamp>2020-03-21T13:53:06Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>The presented work is an experimental study of detonation propagation in a converging conical channel.  We explore the amplification of the detonation wave in a conical tube with a small angle of convergence.  Experiments were carried out in a detonation tube which has a straight, constant area cylindrical section with inner diameter of 6.35 cm and length of 3 m.  At the end of the cylindrical tube is the conical tube.  A Chapman-Jouguet (C-J) detonation wave is first established in the constant area portion of the tube prior to entering the converging section.  Two explosive mixtures: stoichiometric acetylene-oxygen mixture (C_2 H_2+〖2.5O〗_2) and acetylene-oxygen mixture with 70% argon dilution (C_2 H_2+〖2.5O〗_2+70% Ar) were used.  The initial pressure range of the experiments are from 0.5 kPa to 10 kPa and from 4 kPa to 16 kPa respectively for the two mixtures.  It was found that for cone angles ≲20°, the Mach stem curves to merge with the incident detonation to give a smooth continuous detonation front.  The reflected shock also degenerates into a series of compression waves in the triple point region.  In the present study, the detonation velocity is measured via ionization probes spacing periodically in both the constant area section as well as the converging conical section.  Smoked foils are also used to determine the detonation cell structure in both the straight as well as the converging sections of the detonation tube.  A PCB Piezotronics Pressure Sensor is also placed at the end of the conical section to measure the reflected pressure of the detonation.  A continuous increase in the detonation velocity and decrease in the cell size were observed as the detonation propagates into the conical tube.  Smoked foil records indicate the absence of a distinct triple point trajectory separating the Mach stem from the incident wave, indicating a smooth continuous converging detonation front is obtained</description><description>Cette thèse rapporte les résultats d'une étude expérimentale sur la propagation de détonations dans un tube conique convergent. L'intérêt est particulièrement porté sur leurs amplifications alors qu'elles se propagent dans un tube conique à petit angle. Les expériences ont été conduites dans un tube à détonation formé de deux sections distinctes : un tube droit d'une longueur total de 3m avec un diamètre de 6.35 cm qui est connecté à la toute fin à un tube conique convergent.  Une détonation Chapman-Jouguet (C-J) est initialement obtenue dans la première section, un peu avant la section convergente. Deux mélanges explosifs ont été utilisé: un mélange stœchiométrique d'acétylène-oxygène (C_2 H_2+〖2.5O〗_2) et un mélange d'acétylène-oxygène concentré à 70% d'argon (C_2 H_2+〖2.5O〗_2+70% Ar). La pression de chaque mélange a été varié de 0.5 kPa à 10 kPa et de 4 kPa à 16 kPa respectivement. Il a été possible de démontrer que pour un cône avec un angle ≲20° le Mach stem, généralement perpendiculaire à la surface sur laquelle il se propage, devient courbe et se confond avec la détonation incidente. Ceci résulte en un détonation d'apparence lisse et continue au lieu d'une détonation avec un point triple défini.  L'onde qui est réfléchie dégénère en une série d'onde de compression dans la région du point triple.  La vitesse de détonations est mesurée à l'aide de capteurs d'ionisations séparés de manière périodique dans les deux sections du tube à détonation. Afin d'observer la structure cellulaire des détonations, des feuilles fumées en laboratoire ont été placé sur les surfaces internes au bout de la section linéaire de même que sur l'entièreté du tube conique convergent.  Un capteur de pression Piezotronics PCB a été inséré à la pointe du tube conique, afin de mesurer la pression lorsque la détonation est réfléchie.  Dans la section convergente, il y a une augmentation de la vitesse de détonations accompagné d'une réduction de la taille des cellules. Pour un angle ≲20°, les feuilles fumées ont montré l'absence d'un point triple qui sépare le Mach stem de l'onde incidente ce qui indique alors que le front de détonations et lisse et continue. </description><creator>Hung, Ivan</creator><contributor>John H S Lee (Supervisor1)</contributor><date>2017</date><subject>Mechanical Engineering</subject><title>Propagation of a detonation in a converging conical channel</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/jm214r699.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/jm214r681</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Mechanical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:t148fk54g</identifier><datestamp>2020-03-21T13:53:07Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Title: "Goal directed fluid therapy and gastrointestinal function after abdominal surgery"Introduction: Goal directed fluid therapy (GDFT) aims at optimizing oxygen delivery by administering intravenous fluids, with or without inotropes, based on the assessment of stroke volume or cardiac output. It has demonstrated to decrease perioperative morbidity mostly in high-risk patients. However, very few studies have primarily investigated the impact of GDFT on the occurrence of primary postoperative ileus (PPOI). PPOI in the absence of surgical complications constitutes an important economic burden for healthcare systems, since it increases postoperative morbidity and delays hospital discharge. GDFT can prevent the occurrence of both hypovolemia and fluid overload by administrating intravenous fluids based on more objective measures of the intravascular volume.The objectives of this thesis are 1) to review the evidence supporting the use of GDFT to facilitate the recovery of bowel function after abdominal surgery, 2) investigate whether GDFT compared to traditional fluid administration can reduce the incidence of PPOI after laparoscopic colorectal surgery in the context of an Enhanced Recovery After Surgery Program, 3) and determine the effect of GDFT on sub-lingual microcirculation, as a surrogate measure of splanchnic tissue perfusion. Methods: First, a systematic review of the literature and meta-analysis was performed to evaluate the effects of GDFT on the recovery of bowel function after abdominal surgery. Second, a randomized controlled trial comparing intraoperative GDFT with a traditional fluid administration technique was conducted in patients undergoing laparoscopic colorectal surgery in the context of an ERAS program; PPOI was the primary outcome. Finally, perioperative sub-lingual microcirculatory measurements were acquired in a subgroup of patients to analyze the microcirculatory effects of the 2 different fluid strategies. Results: the results of the systematic review and meta-analysis indicated that GDFT facilitated the recovery of bowel function, particularly in patients not treated within an ERAS program and in those undergoing colorectal surgery. Sub-group analysis including only high-quality studies showed limited gastrointestinal benefits with GDFT. Only a few trials primarily investigated the effect of GDFT on the recovery of bowel function. However, the validity of these results was influenced by a high degree of statistical and clinical heterogeneity. In the randomized controlled trial, GDFT did not reduce the incidence of PPOI when compared to fluid therapy based on traditional principles (21.9% in both groups, p=1.000), even though patients treated with GDFT had a more pronounced and sustained increase of stroke volume and cardiac output during surgery, and received less intravenous fluids. Sub-lingual microcirculation analysis demonstrated that GDFT improved the proportion of perfused vessels (PPV) (p = 0.023), but this effect did not translate into less PPOI and better bowel function. Patients who presented with PPOI exhibited a lower sub-lingual PPV than patients without PPOI, probably indicating suboptimal splanchnic perfusion in the former (82.76 ± 3.19 vs 87.29 ± 4.20, p = 0.026). Conclusions: GDFT might be beneficial to improve bowel function after abdominal surgery, mainly in patients not treated with an ERAS program. Despite increasing systemic perfusion and PPV, possibly indicating better splanchnic tissue perfusion and oxygenation, GDFT did not translate into better recovery of bowel function in patients undergoing colorectal surgery within an ERAS program. </description><description>Titre: « Thérapie des fluides ciblée par objectifs hémodynamiques et la fonction gastro-intestinale après une chirurgie abdominale »Introduction : La thérapie des fluides ciblée par objectifs hémodynamiques (TFCOH) a pour but l'optimisation de l'apport d'oxygène par l'administration de liquides intraveineux, avec ou sans inotropes, sur la base d'une évaluation du volume d'éjection systolique et du débit cardiaque. Cette technique a démontré la diminution de la morbidité peropératoire, surtout chez les patients qui ont un risque peropératoire élevé. Pourtant, seulement quelques études ont investigué comme objectif principal l'effet de la TFCOH sur l'incidence d'iléus postopératoire primaire (IPP). L'IPP, en l'absence de complications chirurgicales, constitue un problème économique qui affecte de façon très importante les systèmes de santé parce qu'il augmente la morbidité peropératoire et retarde le congé hospitalier des patients. La TFCOH peut prévenir l'occurrence d'hypovolémie et la surcharge de liquides intraveineux par l'administration de liquides intraveineux sur la base de paramètres physiologiques plus objectifs du volume intravasculaire. Méthodes : D'abord, une revue systématique de la littérature et une méta-analyse ont été effectuées pour évaluer les effets de la TFCOH sur la récupération de la fonction intestinale après la chirurgie abdominale. De même, une étude randomisée contrôlée, laquelle compare la TFCOH intra-opératoire avec une technique traditionnelle d'administration de liquides intraveineux pendant les chirurgies, a été effectuée chez des patients subissant des chirurgies colorectales par laparoscopie dans le contexte d'un programme de RRAC. Le principal objectif de cette étude était d'évaluer l'incidence d'IPP. Finalement, un sous-groupe de patients a été analysé pour évaluer l'effet des deux techniques d'administration intra-opératoires de liquides intraveineux sur la microcirculation sous-linguale. Résultats : Les résultats de la revue systématique de la littérature et la méta-analyse ont indiqué que la TFCOH facilitait la récupération de la fonction intestinale, en particulier chez les patients qui ne sont pas traités avec un programme de RRAC et chez les patients subissant une chirurgie colorectale. L'analyse des études par sous-groupe, incluant seulement ces études considérées comme étant de haute qualité, a démontré des avantages limités avec la TFCOH. Aussi, on a observé que seulement quelques études avaient comme objectif principal d'investiguer l'effet de la TFCOH sur la récupération de la fonction intestinale après la chirurgie. Cependant, la validité des résultats de l'étude a été influencée par le haut niveau d'hétérogénéité statistique et clinique. La comparaison de la TFCOH avec l'administration intra-opératoire de liquides intraveineux de façon traditionnelle, n'a pas réduit l'incidence IPP après la chirurgie colorectale par laparoscopie, même si les patients traités avec la TFCOH avaient une augmentation plus remarquable et soutenue du volume d'éjection systolique et du débit cardiaque pendant la chirurgie et ont reçu moins de liquides intraveineux. L'analyse de la microcirculation sous-linguale a démontré que la TFCOH améliorait la proportion de vaisseaux perfusés (PVP), mais cet effet n'entraînait pas moins d'incidence d'IPP ni une meilleure fonction intestinale. Conclusion : La TFCOH pourrait être utile pour améliorer la fonction intestinale après la chirurgie abdominale, principalement chez les patients qui ne sont pas traités avec un programme de RRAC. Malgré l'augmentation de la perfusion systémique a été plus remarquable et soutenue avec la TFCOH pendant la chirurgie et une PVP plus haute avec la TFCOH, ce qui indique probablement une meilleure perfusion du tissu splanchnique et une meilleure oxygénation, ces avantages n'ont pas entraîné une meilleure récupération de la fonction intestinale. </description><creator>Gómez Izquierdo, Juan Camilo Ernesto</creator><contributor>Liane S Feldman (Supervisor2)</contributor><contributor>Gabriele Baldini (Supervisor1)</contributor><date>2017</date><subject>Surgery</subject><title>Goal directed fluid therapy and gastrointestinal function after abdominal surgery</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/7m01bp14t.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/t148fk54g</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Surgery</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:z029p732k</identifier><datestamp>2020-03-21T13:53:08Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les canaux ioniques pentamériques ligand-dépendants (pLGICs) font partie d'un vaste groupe de protéines transmembranaires jouant un rôle fondamental dans la neurotransmission synaptique au sein du règne animal. Ces acteurs essentiels des mouvements neuromusculaires sont apparus il y a plus de 2 milliards d'années et ont, par le biais de duplications ou pertes génétiques, donné naissance à la grande famille des pLGICs actuellement connus. L'évolution des pLGICs a permis l'émergence d'une population complexe de récepteurs hétéromériques, en particulier dans la clade V des nématodes apparentés à l'espèce modèle libre Caenorhabditis elegans. Parmi eux, le récepteur à l'acétylcholine sensible au lévamisole (L-AChR) se démarque de part la présence de cinq sous-unités différentes chez C. elegans. L'apparition des nouvelles sous-unités de ce récepteur semble être en progression via le phénomène de duplication de gènes. En effet, l'expansion du gène unc-29 sous forme de quatre copies chez le nématode parasite des petits ruminants Haemonchus contortus, est proposé en tant que modèle de ce processus de manière plus générale. J'ai pu démontrer que malgré des séquences très similaires, ces quatre copies ont subit une divergence fonctionnelle particulièrement marquée au niveau de leurs interactions respectives avec les sous-unités voisines du récepteur. En principe, les interactions avec les sous-unités de type α limitent le rôle que les sous-unités non-α peuvent jouer au sein de récepteurs fonctionnels. En revanche, la combinaison de sous-unités provenant de différentes espèces au sein d'un même récepteur permet d'annihiler ces relations et de mieux comprendre leur rôle respectif. J'ai également pu établir l'organisation structurelle du L-AChR de C. elegans et démontré que les interactions entre sous-unités se déroulent parmi celles établissant un contact physique direct. Ce travail fournit une base solide qui permettra d'identifier les mécanismes précis qui déterminent la diversification des sous-unités au sein de la grande et complexe famille des pLGICs. </description><description>Pentameric ligand-gated ion-channels (pLGICs) represent a large family of transmembrane proteins that play a fundamental role in synaptic neurotransmission throughout the animal kingdom. These essential mediators of neuromuscular movement appeared more than two billion years ago and through the central mechanism of gene duplication and loss, have led to a diverse family of modern-day pLGICs. The evolution of pLGICs has given rise to a complex population of heteromeric receptors, particularly in the clade V nematode species, closely related to the free-living model Caenorhabditis elegans. Among them, the levamisole-sensitive acetylcholine receptor (L-AChR) stands out, composed of five different subunits in C. elegans. The appearance of new subunits of this receptor continues through gene duplication. Expansion of the unc-29 gene to four copies in the parasitic nematode of small ruminants, Haemonchus contortus, is proposed as a model of the process more generally. I was able to show that despite very similar sequences, the four copies have diverged in function, most clearly in their interaction with other subunits of the receptor. Interaction with α-type subunits limits the role that non-α type subunits can play in a functional receptor and these evolved interactions break down when subunits of different species are combined into the same receptor. I was also able to define the structural organization of the C. elegans L-AChR and so demonstrate these interactions occur between subunits in physical contact. This work provides a foundation that can be used to identify the precise mechanisms that determine subunit diversification in this complex family of pLGIC receptors.</description><creator>Duguet, Thomas</creator><contributor>Robin N Beech (Supervisor)</contributor><date>2017</date><subject>Parasitology</subject><title>Subunit diversity of the nematode levamisole-sensitive acetylcholine receptor</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/cf95jd802.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/z029p732k</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Institute of Parasitology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:jm214r702</identifier><datestamp>2020-03-21T13:53:09Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>L'indépendance des observations est une hypothèse fondamentale des modèles de variable latente. Cependant, ce postulat échoue lorsque les observations sont imbriquées dans des unités de haut niveau puisque ces structures de données imbriquées induisent une dépendance des données amassées. La version non paramétrique d'un modèle de structure latente multiniveau (MLCM) est une extension des LCM dans laquelle la dépendance des données est comptabilisée par les effets aléatoires discrets dans le modèle. Même si ce cadre de modélisation peut être utilisé dans plusieurs contextes, il reste que son application est encore quelque peu problématique. Cette dissertation comprend quatre manuscrits indépendants ; ceux-ci portent sur les différents enjeux reliés aux erreurs de spécification et de sélection, à la taille d'échantillon requise, et aux effets des covariables. Le premier manuscrit (chapitre 2) évalue les effets négatifs qui surviennent lorsqu'on ignore les structures imbriquées de haut niveau. En tenant compte de ces effets, on peut mieux justifier l'utilisation d'un modèle non paramétrique MLCM. Le deuxième manuscrit (chapitre 3) explore la performance relative des critères d'information (IC). Le troisième manuscrit (chapitre 4) examine systématiquement la taille minimale requise pour l'échantillon dans la modélisation des MLCM. Le quatrième manuscrit (chapitre 5) étudie la performance de diverses approches utilisées pour évaluer les effets simultanés des covariables sur deux niveaux. Finalement, à la fin de chaque manuscrit et dans la section de synthèse, on offre une discussion approfondie sur les contributions et des recommandations pratiques quant à l'utilisation des MLCM.</description><description>The fundamental assumption in any latent variable model is that observations are independent of one another, given the latent status. However, this assumption is often inadequate when observations are nested within higher-level units because such nested data structures induce dependencies in data. The nonparametric version of the multilevel latent class model (MLCM) is an extension of latent class models in which the dependencies in data are accounted for by discrete random effects in the model. To date, this modeling framework has been used in a wide variety of empirical applications. Nonetheless, several unresolved issues relating to the application of the MLCM require further investigation. This dissertation consists of four independent manuscripts addressing the issues of model misspecification, model selection, sample size requirements, and covariate effects in MLCM. The first manuscript (Chapter 2) evaluates the adverse impact of ignoring higher-level nesting structures to provide a rationale for the use of nonparametric MLCMs. The second manuscript (Chapter 3) investigates the relative performance of diverse information criteria (IC) in identifying the optimal number of latent classes. The third manuscript (Chapter 4) systematically examines the minimum sample size requirement for the nonparamtric MLCM. The fourth manuscript (Chapter 5) explores the performance of several approaches to evaluating the effects of covariate in situations in which level-1 and level-2 covariates are simultaneously included to predict the latent class membership at each level. A summary and discussion of the manuscript's contributions and practical recommendations regarding the application of the MLCM in empirical data analyses are presented at the end of each manuscript. </description><creator>Park, Jungkyu</creator><contributor>Fei Gu (Supervisor1)</contributor><contributor>Hsiu-Ting Yu (Supervisor2)</contributor><date>2017</date><subject>Psychology</subject><title>Latent class models for analyzing multilevel nested data: model misspecification, model selection, sample sizes, and covariates</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/5x21th96f.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/jm214r702</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Psychology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:cv43p0681</identifier><datestamp>2020-03-21T13:53:10Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>[18F]FDG positron emission tomography (PET) imaging in mesial temporal lobe epilepsy (MTLE) patients identifies glucose hypometabolism in the epileptic focus and beyond, including the temporal neocortex. However, the mechanisms underlying this phenomenon are still poorly understood. It is thought disruptions of glutamate cycling, which is coupled to glucose utilization, may be related.  Metabotropic glutamate receptor 5 (mGluR5) is a G-protein coupled receptor that shows upregulation in immunohistochemistry studies of hippocampi resected from MTLE patients. However, ex vivo studies are limited as they are conducted in drug-resistant patients needing surgery and convey information only on the seizure focus. [11C]ABP688 is a PET radioligand specific for the allosteric site of mGluR5, allowing for non-invasive in vivo whole brain quantification of mGluR5 availability. In this study, we determined if [11C]ABP688 binding abnormalities are more specific and focal than hypometabolism within the epileptic focus of MTLE patients and whether [11C]ABP688 binding abnormalities can be detected at a higher magnification in different hippocampal subfields as described in a immunohistochemistry study by Kandratavicius et al., 2013. Thirty one unilateral MTLE patients (13 right; 24 female; 40±14 yrs) and 30 healthy controls (12 female; 47±18 yrs) underwent structural magnetic resonance imaging and PET imaging with injection of [11C]ABP688; a subset of 15 patients also underwent [18F]FDG PET. Patients were classified as "normal volume" (NV) or "hippocampal atrophy" (HA) based on their degree of hippocampal volume and asymmetry. We compared partial volume corrected [11C]ABP688 non-displaceable binding potentials (BPND) between patients and controls in regions of interest (ROIs) in mesial temporal structures, temporal neocortex, and hippocampal subfields. For the subset of patients with [18F]FDG data, we performed asymmetry analyses and correlated [11C]ABP688 BPND and [18F]FDG standard uptake value ratios (SUVr) to determine if changes in one are associated with changes in the other. Finally, we compared mGluR5 availability with mGluR5 immunoreactivity (IR) in hippocampal subfields.mGluR5 availability was reduced in the hippocampal head and amygdala of patients compared to controls, with reductions also seen in the temporal neocortex in HA patients only. Patients who were seizure-free following surgery had lower [11C]ABP688 BPND in the ipsilateral hippocampal head and entorhinal cortex than patients who were not seizure-free. [18F]FDG hypometabolism was widespread in ROIs ipsilateral to the focus, while [11C]ABP688 BPND reductions were more prominent and more focal in the hippocampal head. Correlation between [11C]ABP688 BPND and [18F]FDG SUVr in presumably healthy supramarginal gyrus was low, while trends for positive correlations in both ipsilateral and contralateral mesial temporal and cortical regions were found, significant only in contralateral entorhinal cortex and in ipsilateral insula and temporal lobe. [11C]ABP688 PET subfield analyses demonstrated that in vivo reduced mGluR5 availability contrasted to reported ex vivo increased mGluR5 IR. This study is among the first to characterize mGluR5 availability in MTLE patients using [11C]ABP688 PET. [11C]ABP688 BPND reductions in the epileptic focus contrasting to previously reported increased mGluR5 IR in resected tissue suggests that overexpressed mGluR5 in the focus might be internalized or subject to other conditions impacting the binding of the radioligand to the allosteric site. Prominent and focal [11C]ABP688 BPND abnormalities supports mGluR5 imaging as a more specific biomarker for the seizure focus than hypometabolism that may help to predict post-surgical success. Trends for positive association between [18F]FDG SUVr and mGluR5 availability within the MTLE network might further support post-synaptic glutamatergic neurotransmission modulation underlying hypometabolism in MTLE.</description><description>La tomographie par émission de positons (TEP) au [18F]FDG réalisée chez les patients atteints de l'épilepsie du lobe temporal mésial (ELTM) permet de montrer l'hypométabolisme dans le foyer épileptique et des régions distantes. Les mécanismes de l'hypométabolisme sont peu connus mais on postule que les perturbations dans le cycle du glutamate, qui est associé à l'utilisation du glucose, y sont liées. Le récepteur métabotropique du glutamate type 5 (mGluR5) est un récepteur couplé aux protéines G dont l'expression est régulée à la hausse dans l'hippocampe épileptique réséqué de patients atteints de l'ELTM selon des études ex vivo. Cependant, ces études sont limitées car elles ont été effectuées chez des patients qui nécessitent de la chirurgie et elles nous informent que sur le foyer épileptique. [11C]ABP688 est un radioligand TEP spécifique pour le site allostérique de mGluR5 qui permet de quantifier la disponibilité du mGluR5 de façon in vivo et non-invasif. Pour l'étude courante, on a déterminé si les anomalies de la disponibilité du mGluR5 sont plus spécifiques et focales que l'hypométabolisme dans le foyer épileptique. De plus, on a évalué si elles pourraient être détectées dans les sous-zones de l'hippocampe comme décrit dans une étude immunohistochimique par Kandratavicius et coll., 2013. Trente et un patients atteints de l'ELTM (13 du côté droit; 24 femmes; 40±14 ans) et 30 sujets en santé (12 femmes; 47±18 ans) se sont soumis à la TEP au [11C]ABP688; 15 patients se sont également soumis à la TEP au [18F]FDG. Les patients ont été classés dans deux catégories, soit "volume normal" (VN) ou "hippocampe atrophié" (HA). On a comparé le moyen potentiel de liaison (non-displaceable binding potential; BPND) du [11C]ABP688 corrigé pour l'effet volume partiel entre les patients et les sujets en santé dans des régions d'intérêts (ROIs) qui comprennent le réseau ELTM et les sous-zones de l'hippocampe. On a aussi comparé l'asymétrie entre [11C]ABP688 et [18F]FDG. En outre, on a corrélé le BPND du [11C]ABP688 avec la fixation du [18F]FDG dans les ROIs afin de déterminer si les changements d'un sont associés avec les changements de l'autre. Enfin, on a comparé la disponibilité du mGluR5 à l'immunoreactivité du mGluR5 dans les sous-zones de l'hippocampe.La disponibilité du mGluR5 était réduite dans le foyer épileptique chez les deux groupes de patients et dans le cortex temporal que chez les patients HA. Le [11C]ABP688 BPND dans la tête de l'hippocampe et dans le cortex entorhinal ipsilatéralaux était réduit chez les patients qui n'avaient plus de crises suite à la chirurgie par rapport aux patients qui en avaient encore. L'hypométabolisme était présente dans tous les ROIs ipsilatéraux, tandis que les réductions du [11C]ABP688 BPND était plus focales dans la tête de l'hippocampe. On avait généralement des tendances de corrélations positives entre [11C]ABP688 BPND et la fixation du [18F]FDG dans tous les ROIs, statistiquement significatives dans certains ROIs ipsilatéraux et controlatéraux. Finalement, on a montré une relation inverse entre la disponibilité du mGluR5 et l'expression du mGluR5 mesuré par l'immunohistochimie. Cette étude est une des premières à examiner la disponibilité du mGluR5 chez les patients atteints d'ELTM en utilisant la TEP au [11C]ABP688. La réduction du [11C]ABP688 BPND, contrairement à ce qui a été décrit auparavant, suggère que le mGluR5 surexprimé dans le foyer épileptique aurait pu être internalisé ou avoir été soumis à d'autres conditions ayant la capacité d'influencer la liaison du radioligand. L'imagerie du mGluR5 pourrait être utilisée comme un biomarqueur plus spécifique du foyer épileptique et aider à prédire le succès après la chirurgie. Les tendances de corrélations positives entre la fixation du [18F]FDG et le [11C]ABP688 BPND soutiennent l'hypothèse que la modulation post-synaptique de la neurotransmission glutaminergique pourrait-être à l'origine de l'hypométabolisme dans l'ETLM.</description><creator>Lam, Jackie</creator><contributor>Eliane Kobayashi (Internal/Supervisor)</contributor><date>2017</date><subject>Neuroscience</subject><title>The relationship of metabotropic glutamate receptor type 5 (MgluR5) availability with hypometabolism in mesial temporal lobe epilepsy</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/hq37vr02n.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/cv43p0681</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Integrated Program in Neuroscience</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:rf55zb24b</identifier><datestamp>2020-03-21T13:53:11Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Plusieurs facteurs génétiques et environnementaux ont été liés au trouble du déficit de l'attention avec ou sans hyperactivité (TDAH). Cependant, une caractérisation plus approfondie concernant certains facteurs de risque causatifs est nécessaire. La thèse suivante démontrerait une analyse visée à identifier le rôle d'un polymorphisme dans le gène récepteur dopamine 4 (DRD4) chez les enfants atteints du TDAH. Premièrement, une révision compréhensive des études génétiques, fonctionnelles et comportementales sur DRD4 et le TDAH est présentée. Ensuite, nous avons performé deux analyses pour déterminer le rôle de DRD4 dans l'étiologie du TDAH. Dans notre première étude pharmacodynamique, nous avons examiné la réponse au méthylphénidate (MPH) en fonction du génotype DRD4 exon 3. Dans la deuxième étude, nous avons explorer l'interaction entre le génotype DRD4 et le stress maternel pendant la grossesse, et leurs effets sur la sévérité des symptômes chez les enfants atteints du TDAH. À date, ceci est la plus grande étude pharmacogénétique rapportée dans la littérature TDAH.Nous avons découvert une interaction significative entre le génotype DRD4 et le traitement avec placebo et MPH. Selon les parents, les enfants avec le génotype de deux allèles de 7 répétitions DRD4 ont une meilleure réponse au placebo et une symptomatologie moins sévère pendant les semaines placebo et médicamenté, (MPH). En outre, l'analyse sur l'interaction génique et environnementale a révélé une interaction significative sur les problèmes de comportement globaux, ainsi que sur l'internalisation, l'externalisation et l'attention. De plus, le groupe qui a eu une meilleure réponse au MPH a aussi été affecté plus significativement par rapport à l'exposition d'un stress très élevé pendant la grossesse. Les enfants qui ont le génotype de deux allèles de 7 répétitions DRD4 et qui ont été exposés à un stress très élevé pendant la grossesse ont eu un score de problème d'attention significativement plus élevé. Ces résultats suggèrent que le génotype DRD4 pourrait être utilisé pour prédire lesquelles des enfants atteints de TDAH répondront bien au traitement MPH.</description><description>Both genetic and environmental factors have been implicated in the etiology of attention deficit/hyperactivity disorder (ADHD), but there is a need for further characterization of the relevant risk factors. This thesis presents a comprehensive analysis of the role of dopamine receptor 4 (DRD4) gene polymorphism in childhood ADHD. First, genetic, functional and behavioral studies on DRD4 and ADHD are reviewed. Then, two studies were conducted to characterize the role of this genotype in etiology of ADHD. In the first study, we examined the effect of DRD4 exon 3 genotype on response to methylphenidate (MPH) with a pharmacodynamic design. In the second study, we explored an interaction between the genotype and exposure to maternal stress during pregnancy and their effects on symptom severity in children with ADHD. This is the largest pharmacogenetic study reported in the literature today.We discovered a significant interaction between DRD4 genotype and treatment course with placebo and MPH. According to the parents, children with two long 7-repeat alleles had a better response to placebo, and lower symptomatology at both placebo and active medication weeks, as evaluated by the parents. Furthermore, gene-by-environment analysis revealed a significant interaction on overall behavioral problems, as well as externalizing problems sub-scores. Interestingly, the same group that showed better response to treatment was differentially affected by exposure to high stress during pregnancy. Those with two long 7-repeat allele genotype exposed to high stress had significantly higher attention problem score. These results suggest that DRD4 genotype could be used to predict the strength of treatment and clinical outcomes in children with ADHD.</description><creator>Naumova, Darya</creator><contributor>Ridha Joober (Internal/Supervisor)</contributor><date>2017</date><subject>Human Genetics</subject><title>The role of dopamine receptor 4 polymorphism in Childhood Attention Deficit/Hyperactivity Disorder</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/k643b3579.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/rf55zb24b</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Human Genetics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:q524jr37s</identifier><datestamp>2020-03-21T13:53:12Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>In this thesis, nanostructured lithium titanate, Li4Ti5O12 (nano-LTO), is synthesized and characterized in terms of phase transformations during different stages of synthesis and cycling, aiming towards the design of stable and high-power performing anodes for Li-ion batteries. In this context, the following aspects are considered: (i) development of a sustainable aqueous-based process for nano-LTO synthesis; (ii) determination of the crystallization kinetics of 2D lithium titanate hydrate crystals (LTH; (Li2-xHx)Ti2O5•yH2O); (iii) probing the thermal transformation of LTH nanocrystals to nano-LTO; and (iv) understanding and controlling the capacity fade mechanism of nano-LTO upon cycling. The devised process is a two-step chemical/annealing process. Step 1 involves aqueous solution reaction of LiOH with TiCl4 at &lt; 10 °C, Li/Ti = 1.3  and pH 11.5-12 to produce a hydrous precipitate, which is aged in LiOH solution at 80 oC to promote 2D LTH crystal formation. In step 2, the intermediate LTH crystals are converted to nanostructured LTO via thermal conversion (annealing over the temperature range 400 oC to 600 oC). It is demonstrated that the 2D shape of LTH crystals is preserved via proper annealing, yielding sheet-like morphology to nano-LTO during thermal conversion. The 2D anisotropic growth of LTH nanocrystals, has been successfully studied via the employment of in situ Raman spectroscopy during isothermal solution aging. At the beginning of crystallization, mono-layered LTH nuclei are seen to form via oxygen diffusion; as the LTH nanosheet crystals grow anisotropically, they inter-block to limit their length to 50 nm. The LTH crystallization mechanism obeys the modified Johnson-Mehl-Avrami-Kolmogorov (JMAK) nucleation-growth kinetic model. The mechanism of topotactic transformation of lepidocrocite LTH crystals into spinel LTO crystals was elucidated by synchrotron X-ray and other types of characterization. It was revealed that upon annealing, dehydration of LTH leads to topotactic transformation from a C-centered orthorhombic to a body-centered orthorhombic system. Dehydrated LTH then transforms into spinel LTO nanosheets. More importantly, it was shown that residual strain in the thermally formed LTO nanosheets is associated with structural relaxation, which is triggered by excess Li-ion storage upon cointercalation. These phenomena adversely affect the Li-ion intercalation properties of nano-LTO, leading to performance deterioration. Via carefully selecting the annealing protocol, it is demonstrated the residual strain elimination/manipulation opens the path for highly stable and high-power LTO nanosheet anodes. Excessive Li storage in nano-LTO (over the stoichiometric Li¬7Ti5O12 of 175 mAh/g) is a consequence of Li-ion occupation at vacancies in the near-surface region, which are thermodynamically unfavorable in bulk LTO. Those excessively intercalated Li ions trigger formation of the relaxed LTO structure, which interferes with Li-ion diffusion/storage and eventually causes capacity fade.  A JMAK kinetic model was developed to describe the formation of relaxed LTO. Namely, the near-surface region of nano-LTO is completely converted into relaxed structure, via continuous nucleation at constant rate followed by isotropic growth. Once the near-surface zone of relaxed LTO is fully saturated, the capacity fade is then governed by electrolyte/LTO interfacial reaction instead. These findings would be very valuable as the Li-ion battery research community is working to understand and stabilize electrode materials from their synthesis to their long-term cycling performance. </description><description>Dans cette thèse, le titanate de lithium nano-structuré Li4Ti5O12 (nano-LTO) est synthétisé et caractérisé en termes de transformations de phase atomique lors de différentes étapes de synthèse et cycle charge décharge. La récente recherche vise à la conception d'anodes stables et performantes pour une batteries Li-ion. La procédure de synthétisation contient deux étapes différentes :(i) La réaction chimique en solution aqueuse de LiOH avec TiCl4 à la température &lt;10 °C, Li/Ti = 1.3 et pH = 11.5 - 12. Cette étape permet à produire un précipité hydraté dans une solution de LiOH à 80 °C. Celle-ci favorise la formation de cristaux de LTH en 2D. (ii) La deuxième étape consiste à réchauffer le produit intermédiaire de LTH à une température entre 400-600 °C. Cette étape permet de convertir LTH en LTO nano-structuré par la conversion thermique tout en conservant sa morphologique 2D des cristaux de LTH. La croissance anisotrope des nano-cristaux de LTH en 2D a été étudiée avec succès par l'entremise de la spectroscopie Raman in situ lors du vieillissement en solution isotherme. Au début de la cristallisation, les noyaux LTH monocouches se forment par diffusion des atomes d'oxygène. À mesure que les cristaux de LTH se développent de manière anisotrope, il est possible de former la structure atomique avec une longueur inférieure à 50 nm. En fait, le mécanisme de cristallisation LTH obéit au modèle cinétique de croissance nucléaire de Johnson-Mehl-Avrami-Kolmogorov (JMAK), avec modification pour en tenir compte de l'effet inter-blocage qui limite la croissance de cristaux LTH. Le mécanisme de la transformation Topotactique des cristaux LTH Lepidocrocite dans des cristaux LTO spinelle a été expliqué par synchrotron et d'autres types de caractérisation. Il a été démontré que, la déshydratation de LTH provoque une transformation Topotactique d'un système orthorhombique centré en C vers un système orthorhombique centré sur le corps. La LTH déshydratée se transforme ensuite en nano-feuille de LTO spinelle. Cependant, il a été aussi montré que la contrainte résiduelle dans les nano-feuilles de LTO formées thermiquement est associée à la relaxation structurale déclenchée par l'excès de stockage de Li-ion lors de la co-calcination pendant le cycle initial. Ces phénomènes affectent négativement la performance électrochimique de nano-LTO. Notre recherche optimise la procédure dans le but d'obtenir des anodes à feuille de nano-LTO à haute puissance et hautement stables. Le stockage excessif de Li dans le nano-LTO (sur le Li7Ti5O12 stœchiométrique de 175 mAh/g) est une conséquence de l'occupation des Li-ion à des sites atomique vacants dans la région surfacique, qui sont thermodynamiquement défavorables pour le LTO massique. Ces ions Li excessivement intercalés déclenchent la formation de la structure LTO relâchée, ce qui interfère avec la diffusion et le stockage des ions de lithium et provoque éventuellement une diminution de sa capacité. Un modèle cinétique JMAK a été développé pour décrire la formation de LTO relâché. Une réaction nucléaire continue à vitesse constante suivie d'une croissance isotrope pour convertir complètement la région proche de la surface du nano-LTO en une structure relâchée. Une fois que la zone proche de la surface du LTO relâché est complètement saturée, la perte de capacité est alors régie par une réaction d'interface électrolyte/LTO. Ces résultats seraient très importants car beaucoup de scientifiques travaillent à comprendre et à stabiliser les matériaux des électrodes durant leur synthèse dans le but d'obtenir une plus grande performance de cycle charge décharge.</description><creator>Chiu, Hsien-chieh</creator><contributor>George Demopoulos (Supervisor)</contributor><date>2017</date><subject>Mining and Materials</subject><title>Green synthesis and capacity fade mechanism of Lithium titanate nanosheet anode for Lithium-ion batteries</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/tq57nt49f.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/q524jr37s</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mining and Materials</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:9w032556r</identifier><datestamp>2020-03-21T13:53:13Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>L'analyse en composantes structurée généralisée (ACSG) est une approche de la modélisation par équations structurelles (MES) basée sur les composantes qui postule et examine divers liens directionnels entre des variables latentes et observées. L'ACSG construit des composantes ou des ensembles pondérés de variables observées pour représenter des variables latentes. Elle combine trois sous-modèles, soit le modèles de mesure, le modèle structurel et le modèle des liens pondérés dans une formulation unifiée et estime tous les paramètres des modèles simultanément via la méthode des moindres carrés. Durant les dix dernières années, l'ACSG a évolué pour traiter une plus grande gamme de types de données, notamment des données discrètes, multiniveaux, et longitudinales avec de nombreuses mesures sur une longue durée, ainsi que pour s'adapter à une plus grande variété d'analyses complexes telles que l'analyse de modération avec des variables latentes, la mesure de l'hétérogénéité au niveau d'un sous-groupe et l'analyse régularisée. À date, néanmoins, il n'y a pas encore eu de tentative de généralisation de l'ACSG dans le cadre bayésien. Dans cette thèse, une nouvelle extension de l'ACSG, appelée ACSG bayésienne, évaluant les paramètres à l'intérieur du cadre bayésien, est proposée. L'ACSG bayésienne peut se prouver plus attractive que l'ACSG à de nombreux égards. Premièrement, elle déduit la loi de probabilité des paramètres, en traitant les paramètres comme des variables aléatoires, ce qui facilite alors l'interprétation des paramètres. Deuxièmement, elle permet de spécifier plusieurs structures de terme d'erreur dans le modèle de mesure qui ne sont pas spécifiées dans l'ACSG. Troisièmement, elle fournit des mesures d'ajustement supplémentaires pour l'évaluation et la comparaison du modèle d'une perspective bayésienne. Enfin, elle permet d'incorporer de l'information externe sur les paramètres, qui peut provenir de recherches passées, d'opinions d'experts, de croyances subjectives ou de connaissances sur les paramètres, directement sous la forme de distributions préalables dans le processus de modélisation. L'ACSG bayésienne adopte une méthode de Monte-Carlo par chaînes de Markov, soit l'échantillonnage de Gibbs, pour mettre à jour les distributions postérieures des paramètres. La thèse commence par une description des deux fondations de l'ACSG bayésienne – l'ACSG et l'inférence bayésienne, avant d'examiner les fondements techniques de l'ACSG bayésienne. Elle démontre également l'utilité de l'ACSG bayésienne pour l'analyse de données simulées et  réelles.</description><description>Generalized structured component analysis (GSCA) is a component-based approach to structural equation modeling (SEM) that postulates and examines various directional relationships among latent and observed variables. GSCA constructs components or weighted composites of observed variables as proxies for latent variables. It combines three sub-models, such as measurement, structural, and weighted relation models, into a unified formulation, and estimates all model parameters simultaneously via least squares. Over the past decade, GSCA has been extended to deal with a wider range of data types including discrete, multilevel, or intensive longitudinal data, as well as to accommodate a more variety of complex analyses such as latent moderation analysis, the capturing of cluster-level heterogeneity, and regularized analysis. To date, nonetheless, there has been no attempt to generalize the scope of GSCA into the Bayesian framework. In this dissertation, a novel extension of GSCA, called Bayesian GSCA, is proposed that estimates parameters within the Bayesian framework. Bayesian GSCA can be more attractive than GSCA in numerous respects. Firstly, it infers the probability distributions of parameters, treating the parameters as random variables, which in turn facilitates the interpretation of the parameters. Secondly, it permits specifying various structures of error terms in the measurement model, which are left unspecified in GSCA. Thirdly, it provides additional fit measures for model assessment and comparison from the Bayesian perspectives. Lastly, it allows directly incorporating external information on parameters, which may be obtainable from past research, expert opinions, subjective beliefs or knowledge on the parameters, as the form of prior distributions in the modelling process. Bayesian GSCA adopts a Markov Chain Monte Carlo method, i.e., Gibbs Sampler, to update the posterior distributions for parameters. The dissertation begins by describing two building blocks of Bayesian GSCA – GSCA and Bayesian inference, and subsequently discusses the technical underpinnings of Bayesian GSCA. It also demonstrates the usefulness of Bayesian GSCA based on the analyses of both simulated and real data. </description><creator>Choi, Ji Yeh</creator><contributor>Heungsun Hwang (Supervisor)</contributor><date>2017</date><subject>Psychology</subject><title>Bayesian generalized structured component analysis</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/gb19f857j.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/9w032556r</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Psychology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:kk91fp252</identifier><datestamp>2020-03-21T13:53:14Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>MYSM1 is a chromatin-interacting deubiquitinase that acts as a transcriptional co-factor. Mysm1 deletion in mice results in severe deficiencies in hematopoietic stem cell (HSC) function and lymphopoiesis. p53 is a central regulator of cell stress responses and HSC homeostasis that acts primarily via transcriptional regulation. We recently established that Mysm1-deficiency is associated with aberrant p53 activation within all affected hematopoietic lineages, and that p53 deletion rescues all hematopoietic defects in Mysm1-/- mice. Functional studies of hematopoietic stem and progenitor cells (HSPCs) from Mysm1-deficient models showed that MYSM1 represses p53-mediated transcription of Bbc3/PUMA and Cdkn1a/p21, key mediators of apoptosis and cell cycle arrest. In this work, phenotypic characterization of Mysm1-/-Puma-/- mice revealed a restoration of multipotent progenitor (MPP) numbers and a significant rescue of the HSC dysfunction seen in Mysm1-deficiency. Interestingly, Mysm1-/-p21-/- mice showed no hematopoietic rescue. Transcriptional profiling of distinct HSPC subsets demonstrated differential manifestations of Mysm1-deficiency and p53 activation in HSC and MPP subpopulations. We establish a critical tissue-specific role for MYSM1 in restricting p53-mediated stress responses in HSPCs. We show that PUMA is essential for p53-mediated apoptosis in MPPs and is partially responsible for Mysm1-/- HSC dysfunction, while p21 does not play a role in Mysm1-deficiency. Persistent PUMA-independent defects in Mysm1-/- HSCs are likely mediated through other p53-regulated mechanisms. Overall, this work expands our understanding of the cell-type specific roles of p53 in HSPCs and identifies transcriptional networks that drive the Mysm1-deficiency phenotype.</description><description>MYSM1 est une déubiquitinase qui interagit avec la chromatine pour agir comme cofacteur transcriptionnel. L'effacement de Mysm1 dans les souris causent de graves déficiences dans la fonction des cellules souches hématopoïétiques (CSH) et dans la lymphopoïèse. p53 est un régulateur central dans la réponse au stress cellulaire et dans l'homéostasie des CSH qui agit principalement par régulation transcriptionnelle. Nous avons récemment établie que la déficience en Mysm1 est associé à l'activation aberrante dans les lignées hématopoïétiques affectées et que la délétion de p53 renverse tous les déficiences hématopoïétiques dans les souris Mysm1-/-. Des etudes fonctionnelles sur les cellules souches et progéniteur hématopoïétiques (CSPH) dans le modèle de souris déficient en Mysm1 démontrent que MYSM1 supprime la transcription induit par p53 de Bbc3/PUMA et Cdkn1a/p21, ceci sont des médiateurs importants dans l'apoptose et l'interruption du cycle cellulaire. Dans ce travail, la caractérisation phénotypique des souris Mysm1-/-Puma-/- révèle un rétablissement dans le nombre des cellules souches multipotentes (CSM) et dans la fonction des cellules souches hématopoïétiques (CSH) dans les souris déficient en Mysm1. Curieusement, les souris Mysm1-/-p21-/- montrent aucun rétablissement en fonction hématopoïétiques. Le profilage transcriptionnelle de certaines CSPH démontrent différentes manifestations dans la déficience de Mysm1 et l'activation de p53 dans certaines sous-populations de CSH et CSM. Nous avons pu établir qu'il y a un rôle critique au niveau du tissu pour MYSM1 en limitant la réponse au stress cellulaire induite par p53 dans les CSPHs. Nous démontrons que PUMA est essentiel pour la régulation de l'apoptose induite par p53 dans les CSMs et partiellement responsable pour le dysfonctionnement vue dans les CSHs Mysm1-/-, alors que p21 ne joue pa de rôle dans la déficience Mysm1. Le but de notre travail est d'élargire nos connaissances sur le rôle de p53 spécifique au CSPHs et identifie les réseaux transcriptionnels qui causent le phénotype déficient en Mysm1.</description><creator>Petrov, Jessica</creator><contributor>Anastasia Nijnik (Internal/Supervisor)</contributor><date>2016</date><subject>Physiology</subject><title>Mapping the MYSM1 transcriptional network in hematopoietic stem and progenitor cells</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/h989r565v.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/kk91fp252</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physiology</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:gt54kq66k</identifier><datestamp>2020-03-21T13:53:14Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Oral anticoagulants such as vitamin-K antagonists (VKA) are prescribed for the prevention and management of venous thromboembolism, and for the prevention of ischemic stroke in atrial fibrillation (AF). Although effective, VKA have a narrow therapeutic window, and may require cumbersome monitoring due to their potential for adverse bleeding. Hence, the introduction of novel oral anticoagulants (NOAC) in 2008 presented attractive treatment options for patients averse to VKA therapy. Using the UK's Clinical Practice Research Datalink, a database of electronic medical records, the aim of this thesis research was to determine how NOAC have been adopted in a primary care setting, and to evaluate the clinical effectiveness and safety of NOAC compared to VKA in non-valvular AF patients in routine clinical practice. In our first objective, we used Poisson regression to describe the trends in first-time prescriptions of NOAC and VKA in the UK, between 2009 and 2015. Over the study period, we observed a significant increase in the rate of any oral anticoagulant initiation (RR 1.58; 95% CI 1.23-2.03). The rate of new users of VKA decreased over this same period (RR 0.69; 95% CI 0.52-0.93), while the rate of new users of NOAC increased by a substantial 17-fold between 2012 and 2015 only (RR 17.68; 95% CI 12.16-25.71). Using multivariate logistic regression, we also found that, compared to VKA, new users of NOAC were less likely to have a history of coronary artery disease, congestive heart failure, and peripheral vascular disease, while more likely to have a history of ischemic stroke.In our second objective, we evaluated the effectiveness and safety of NOAC compared to VKA in a cohort of new users with non-valvular AF, between 2011, when NOAC were approved for these patients, and 2016. Up to 6,818 new users of NOAC were matched 1:1 to new users of VKA on high-dimensional propensity scores, age, and sex. In Cox regression analyses, and using an as-treated definition of exposure, the rates of ischemic stroke and systemic embolism were similar (HR 0.94; 95% CI 0.62-1.42) between NOAC and VKA, as were the rates of major bleeding (HR 0.86; 95% CI 0.56-1.33), and myocardial infarction and all-cause mortality. While NOAC tended to be associated with a lower risk of intracranial bleeding (HR 0.51; 95% CI 0.18-1.44), these new medications also increased the risk of gastrointestinal bleeding (HR 1.78; 95% CI 1.27-2.48). Our results were generally unchanged in time-dependent analyses, and when stratified by concurrent chronic kidney disease.  In the UK, NOAC have been widely prescribed since their introduction in 2008, and these medications are comparable to VKA in the prevention of ischemic stroke in AF. NOAC also appear to have improved the rates of oral anticoagulation in patients with AF, and the introduction of these medications therefore represents an important step forward for stroke prevention in AF.  The growing preference for NOAC may be partly explained by their established effectiveness and safety, as well as their relative ease of use. Future studies should evaluate the extent to which other factors may have played a role in the substantial and rapid uptake of these new medications, while continuing to monitor the trends in their prescription. </description><description>Les anticoagulants oraux, tels que les antagonistes de la vitamine K (AVK), sont utilisés en prévention des thromboses veineuses et des accidents ischémiques cérébraux (AIC) chez les patients présentant une fibrillation auriculaire (FA). Bien qu'ayant fait la preuve de leur efficacité, les AVK ont une fenêtre thérapeutique étroite et nécessitent donc une surveillance régulière en raison du risque hémorragique qui leur est associé. Pour cette raison, les nouveaux anticoagulants oraux (NACO), introduits en 2008, représentent une option thérapeutique intéressante, notamment pour les patients peu enclins à prendre des AVK. En utilisant le « Clinical Practice Research Datalink » britannique, une base de données de dossiers médicaux électroniques, l'objectif de cette thèse était d'évaluer les tendances de prescription, l'efficacité, et la sécurité des NACO en pratique clinique quotidienne.  Dans le premier objectif, nous avons utilisé une régression de Poisson pour décrire les tendances de prescription des NACO et des AVK au Royaume-Uni, entre 2009 et 2015. Au cours la période d'étude, nous avons observé une augmentation significative des taux d'initiation des anticoagulants oraux (RR 1,58; IC 95% 1,23-2,03). Le taux de nouveaux utilisateurs d'AVK a diminué au cours de cette même période (RR 0,69; IC 95% 0,52-0,93), tandis que le taux de nouveaux utilisateurs de NACO a été multiplié par 17 fois entre 2012 et 2015 (RR 17,68; IC 95% 12,16-25,71). A l'aide d'une régression logistique multivariable, nous avons également trouvé que, comparativement aux AVK, les nouveaux utilisateurs de NACO avaient moins d'antécédents de coronaropathie, d' insuffisance cardiaque congestive, et de maladie vasculaire périphérique, mais plus d'antécédents d'AIC au moment de leur première prescription. Dans le deuxième objectif, nous avons évalué l'efficacité et la sécurité des NACO comparativement aux AVK dans une cohorte de patients atteints de FA et nouvellement traités par un anticoagulant oral entre 2011 et 2016. Jusqu'à 6818 nouveaux utilisateurs de NACO ont été appariés en nombre égal à de nouveaux utilisateurs d'AVK en fonction de l'âge, du sexe, et du score de propension de haute dimension. En utilisant une définition d'exposition « as-treated » dans les analyses de régression de Cox, nous avons trouvé que les NACO et les AVK réduisaient le risque d'AIC et d'embolisme systémique d'une manière comparable (HR 0,94; IC 95% 0,62-1,42), avec des taux similaires de saignement majeur (HR 0,86; IC 95% 0,56-1,33), d'infarctus du myocarde, et de mortalité toute causes confondues. Alors que les NACO étaient associés à une diminution du risque de saignement intracérébral (HR 0,51; IC 95% 0,18-1,44), ils augmentaient le risque de saignement gastro-intestinal de manière significative (HR 1,78; IC 95% 1,27-2,48). Des résultats similaires ont été obtenus en utilisant une définition d'exposition dépendante du temps. Enfin, nous avons confirmé ces résultats chez les patients présentant une insuffisance rénale chronique. La préférence croissante pour les NACO observée au Royaume-Uni pourrait être liée à leur efficacité et à leur sécurité, ainsi qu'à leur facilité d'utilisation comparativement aux AVK. L'introduction de ces médicaments  sur le marché semble s'être accompagnée d'une amélioration des taux d'anticoagulation orale chez les patients présentant une FA au Royaume-Uni, et l'introduction de ces médicaments représente donc une avancée importante pour la prévention des AIC dans cette pathologie. Les études futures devraient permettre de préciser le rôle potentiel d'autres facteurs dans l'adoption rapide des NACOs en prévention des AIC dans la FA. Par ailleurs, il parait important de poursuivre la surveillance des tendances de prescription de ces molécules et leur impact sur la prise en charge des patients atteints de FA.</description><creator>Loo, Simone</creator><contributor>Christel Renoux (Internal/Supervisor)</contributor><contributor>Samy Suissa (Internal/Cosupervisor2)</contributor><date>2017</date><subject>Epidemiology and Biostatistics</subject><title>Novel oral anticoagulants in the UK: use, effectiveness, and safety in a primary care setting</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/f1881p606.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/gt54kq66k</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Epidemiology and Biostatistics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:n009w489m</identifier><datestamp>2020-03-21T13:53:15Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Une compréhension des matériaux polycristallins, allant des alliages à certaines céramiques et polymères et au-delà, revêt une grande importance pour la société moderne. Ces matériaux se forment généralement à travers le processus de nucléation, une transition de phase activée thermiquement. La modélisation numérique de cette transition de phase est problématique pour les techniques numériques traditionnelles: la résolution des méthodes de champ de phase ('phase field') couramment répandue ne s'étend pas aux échelles atomiques auxquelles la nucléation prend place, tandis que les méthodes atomiques telles que la dynamique moléculaire ('molecular dynamics')sont incapables de s'adapter au régime mésoéchelle où la croissance de la phase crystalline et la formation de la structure ont lieu suite à une nucléation antérieure. Entant que tel, il est intéressant d'examiner si le modèle de 'Phase Field Crystal' (PFC), qui tente de combler les régimes atomique et mésoéchelle, est capable de modéliser la nucléation. Dans ce travail, nous calculons numériquement les taux de nucléation et les temps d'incubation dans le modèle PFC. Nous montrons un accord qualitatif avec la théorie classique de la nucléation (CNT), un modèle stochastique à variable unique. Notamment, nous montrons que les taux de nucléation dans le modèle PFC dépendent du temps. Nous examinons également la forme et le comportement des noyaux aux premiers temps de formation, constatant un désaccord avec certaines hypothèses de base de CNT. Nous soutenons alors qu'une théorie de nucléation quantitativement correcte pour le modèle PFC nécessiterait d'étendre CNT à une théorie multi-variable.</description><description>An understanding of polycrystalline materials, ranging from alloys to certain ceramics and polymers and beyond, is of great importance for modern society. These materials typically form through the process of nucleation, a thermally activated phase transition. The numerical modeling of this phase transition is problematic for traditional numerical techniques: the commonly used phase field methods' resolution does not extend to the atomic scales at which nucleation takes places, while atomistic methods such as Molecular Dynamics are incapable of scaling to the mesoscale regime where late-stage growth and structure formation takes place following earlier nucleation. As such, there is interest in examining whether the Phase Field Crystal (PFC) model, which attempts to bridge the atomic and mesoscale regimes, is capable of modeling nucleation. In this work, we numerically calculate nucleation rates and incubation times in the PFC model. We show qualitative agreement with classical nucleation theory (CNT), a single-variable stochastic model. Notably, we show that nucleation rates in the PFC model are time-dependent. We also examine the form and behavior of nuclei at early formation times, finding disagreement with some basic assumptions of CNT. We then argue that a quantitatively correct nucleation theory for the PF Cmodel would require extending CNT to a multi-variable theory.</description><creator>Jreidini, Paul</creator><contributor>Nikolaos Provatas (Supervisor)</contributor><date>2017</date><subject>Physics</subject><title>Classical nucleation theory in the phase field crystal model</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/kh04ds132.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/n009w489m</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:9p290d08t</identifier><datestamp>2020-03-21T13:53:16Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Le micro-usinage par laser pulsé est un procédé sans contact durant lequel une surface est balayée par des impulsions laser pour provoquer un enlèvement de matière par ablation et modifier sa morphologie. Pour assurer la précision et la prédictibilité du processus, il est important de comprendre la dynamique du phénomène d'ablation. Les modèles actuellement disponibles ne décrivent que les impulsions super-imposées au même endroit sur une surface. Comme premier pas vers l'adaptation de ces modèles aux méthodes de fabrication actuelles, cette thèse a pour objectif le développement d'une description mathématique de l'accumulation d'énergie sur une surface se déplaçant à vitesse constante par rapport à un faisceau laser. Cette description définit la condition pour laquelle l'accumulation totale d'énergie sur une surface en mouvement est équivalente au cas stationnaire. Cette condition étant établie, la largeur sur laquelle l'ablation se produit dans les deux cas peut maintenant être directement comparée. Cela permet également la modification des modèles actuels pour décrire avec précision l'ablation de surfaces en mouvements. En considérant également la réduction de la fluence laser due à la modification de la surface, la profondeur sur laquelle l'ablation se produit dans les deux cas peut similairement être comparée. Ce nouveau cadre mathématique est utilisé pour analyser et expliquer avec succès des résultats expérimentaux obtenus sur du PET (polytéréphtalate d'éthylène), du verre et du titane.</description><description>Pulsed laser micromachining is a contactless method in which pulse lasers are scanned over a surface to induce ablation, removing material and altering the surface morphology.  Understanding the complex dynamics which result in surface damage is important to ensure precision and predictability in the laser machining process.  To date, models which predict surface damage have only been developed for pulses overlapping on the same spot on a surface. As a first step towards implementing these models for fabrication of surfaces with real-world applications, this thesis shows the development of a mathematical description of the manner in which energy accumulates during irradiation of surfaces moving with constant velocity relative to a beam.  Within this description, the condition for total deposited energy to be equivalent in the moving and stationary cases is derived.  This equivalence allows for both direct comparison of damage feature width for moving and stationary surfaces and extension of current models to moving surfaces.  With the additional consideration of fluence reduction due to surface damage, this equivalence is used to understand and compare damage feature depth in the two cases.  The mathematical framework derived is used to successfully analyze and explain results from experiments on PET (polyethylene terephthalate), glass, and titanium.</description><creator>Matus, Luke</creator><contributor>Anne-Marie Kietzig (Internal/Supervisor)</contributor><date>2017</date><subject>Chemical Engineering</subject><title>The difference in fluence accumulation and surface damage for moving and stationary surfaces in laser micromachining</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/8910jx17h.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/9p290d08t</identifier><degree><name>Master of Engineering</name><grantor>McGill University</grantor><discipline>Department of Chemical Engineering</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:x059c9925</identifier><datestamp>2020-03-21T13:53:17Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Ce mémoire a pour objet de montrer que la « manière mêlée » de Théodore de Banville constitue une condition d'accès à une pensée de la valeur dans Odes funambulesques (1857) et Nouvelles Odes funambulesques (1869). L'hybridité qui caractérise ces deux recueils en rend l'analyse indissociable d'une théorie de la manière, catégorie qui appartient au champ artistique. Le mélange engage tout d'abord les mutations du sujet, depuis les pseudonymes utilisés dans les prépublications du petit journal jusqu'à l'anonymat de l'œuvre en 1857. Dans un second temps, ce travail aborde les liens qui nouent le poème au journalisme satirique et au genre du théâtre. La reconception critique de la poème qui en résulte dévoile finalement l'enjeu majeur du funambulesque, celui d'une parole à la fois singulière et collective.</description><description>This thesis studies how Théodore de Banville's "mixed manner" provides the basis for an unusual interpretation of value in his Odes funambulesques (1857) and Nouvelles Odes funambulesques (1869). Indeed, the hybridity that characterises these two poetry collections renders their analysis inextricable from a theory of manner, a category drawing from the artistic sphere. Foremost, this thesis shall explore the ways in which the practice of mélange involves the author-figure's mutations, from the pseudonyms used in the prepublications of the petit journal to the finished work's anonymous publication. Furthermore, we will examine how Banville's work intertwines poetic forms with those of satirical journalism and of theatre. Finally, we will address how the critical reconception stemming from the poems' mixed manner reveals the principal issue at stake in the funambulesque project : the articulation of a concurrently individual and collective voice.</description><creator>Boju, Joseph</creator><contributor>Arnaud Bernadet (Internal/Supervisor)</contributor><date>2017</date><subject>French Language and Literature</subject><title>Du journal au poème, la «Manière Mêlée» des «Odes Funambulesques» et «Nouvelles Odes Funambulesques« de Théodore De Bainville</title><language>fre</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/7h149s228.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/x059c9925</identifier><degree><name>Master of Arts</name><grantor>McGill University</grantor><discipline>Department of French Language and Literature</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:mw22v809b</identifier><datestamp>2020-03-21T13:53:18Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>La présente thèse est basée sur un ensemble d'articles scientifiques publiés par l'auteur au cours de la période 2014-2016 dans le cadre de travaux de recherche doctorale se déroulant à l'Université McGill. Ces articles explorent des aspects théoriques et expérimentaux complémentaires de la mesure de qubits, une tâche essentielle pour le traitement de l'information quantique. Pour rendre les résultats plus abordables, le premier chapitre de la thèse introduit des outils de la théorie de la mesure quantique et de la théorie de la décision d'une manière pédagogique. Les chapitres restants consistent en les articles de recherche présentés dans un ordre cohérent plutôt que dans un ordre chronologique.Le premier chapitre introduit le traitement de l'information quantique, les mesures quantiques, et la théorie de la décision. Le second chapitre discute de l'optimisation de la fidélité des mesures "d'un coup" pour une classe générique de mesures de qubits. Trois différentes méthodes de traitement de signal sont analysées et comparées. De plus, une stratégie générale pour l'amélioration de ces mesures est proposée. Le troisième chapitre discute de l'optimisation de la rapidité de la mesure à l'aide de méthodes de la théorie des décisions adaptives. L'accélération maximale théoriquement réalisable est dérivée pour deux classes importantes de stratagèmes de mesure physiquement pertinents. De plus, des stratagèmes de mesure pour lesquels l'accélération due à la décision adaptive est sans borne sont proposés. Une accélération significative est expérimentalement démontrée pour la mesure de l'état de charge du centre azote-vacance dans le diamant, avec des ramifications directes pour l'amélioration de la métrologie quantique. Le quatrième chapitre explore l'application de méthodes de décodage souple à la mesure de multiples qubits. Il est montré que ces méthodes peuvent significativement réduire les ressources requises pour des tâches de traitement d'information quantique concrètes. D'une importance particulière est la démonstration que ces améliorations existent même en présence d'un petit nombre de qubits, d'un faible rapport signal-sur-bruit, et de modèles de bruit réalistes. En conséquence, il est soutenu que le décodage souple est un outil important pour le développement à court terme des technologies quantiques.</description><description>The present thesis is based on a collection of scientific articles published by the author in the period 2014-2016 as part of Ph.D. research conducted at McGill University. These articles explore complementary theoretical and experimental aspects of qubit readout, an essential task for quantum information processing. To make the results more easily approachable, the first chapter of the thesis introduces tools from quantum measurement theory and decision theory in a pedagogical way. The remaining chapters consist of the research articles presented in a coherent order rather than in a chronological order.The first chapter introduces quantum information processing, quantum measurements, and decision theory. The second chapter discusses the optimization of single-shot readout fidelity for a generic class of qubit readouts. Three different signal processing methods are analyzed and compared. Moreover, a general strategy for the improvement of these readouts is proposed. The third chapter discusses the optimization of the speed of the readout using methods from the theory of adaptive decisions. The maximal theoretically-achievable speedup is derived for two important classes of physically relevant readout schemes. In addition, readout schemes with an unbounded adaptive-decision speedup are proposed. A significant speedup is experimentally demonstrated for the readout of the charge state of the nitrogen-vacancy center in diamond, with direct ramifications for the enhancement of quantum metrology. The fourth chapter explores the application of soft-decision decoding methods to the readout of multiple qubits. These methods are shown to significantly reduce the resources required for concrete quantum information processing tasks. Importantly, the improvements are shown to exist even for small numbers of qubits, low signal-to-noise ratio, and realistic noise models. Accordingly, it is argued that soft-decision decoding is an important tool for the near-term development of quantum technologies.</description><creator>D'Anjou, Benjamin</creator><contributor>William Coish (Supervisor)</contributor><date>2017</date><subject>Physics</subject><title>Optimization of real-world qubit measurements</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/12579v82v.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/mw22v809b</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:7p88ck085</identifier><datestamp>2020-03-21T13:53:19Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Rationale: Risk assessment tools are frequently used to help surgeons and patients decide whether the potential benefits of cardiac surgery outweigh the risks. One of the most popular risk estimation tools, the 2000 Parsonnet score, has shown varying levels of predictive ability in different regions throughout the world. This risk estimation tool is based on 37 variables. According to the literature, institutional quality, medical personnel and patient ethnicity also have predictive value when measuring the risk of in-hospital mortality yet these are not included as risk factors in the model. It is hypothesized that a regional score based on the 2000 Parsonnet score would have greater accuracy in its patient population. This is because the 2000 Parsonnet score was developed using data from only one region (New Jersey) and the literature has shown that it has varying levels of accuracy in different regions. Methods: Patient level data from two Montreal hospitals were used to create two hospital-specific models based on the 2000 Parsonnet score using multivariable logistic regression. There were 1,162 patients included from one hospital and 2,656 patients from the other hospital. Risk factor coefficients of the hospital-specific models were compared to the coefficients of the Parsonnet model using z-tests. Additionally, the predictive accuracies of the hospital-specific models were compared to the Parsonnet model using receiver operating characteristic (ROC) analyses. The number of preventable deaths at different risk thresholds was also calculated between models.Results: Between the Montreal-based institutions, 2 variables had different risk factor coefficients. One hospital-specific model had 4 different risk factor weights and the other hospital-specific model also had 4 different weights when compared to the Parsonnet model, (p≤0.05). Only one model was found to be marginally more accurate with an area under the ROC curve of 0.882 compared to 0.868 for the Parsonnet model (p=0.049). No difference was detected for the other model with an area under the ROC curve of 0.804 compared to 0.798 for the Parsonnet model (p&gt;0.05). Additionally, in one hospital, for three out of four risk thresholds, the use of the hospital-specific model instead of the Parsonnet model would have prevented between 2 and 8 in-hospital mortalities. In the other hospital, for four out of four risk thresholds, the use of the hospital-specific model instead of the Parsonnet model would have prevented between 3 and 18 in-hospital mortalities.  Conclusions: These results demonstrate that the Parsonnet model has approximately the same risk factor weights and predictive accuracy as the hospital-specific models. Also, the potential clinical benefit of prevented mortality was too small to justify using hospital-specific models over the Parsonnet model. Future research in other regions and in a larger number of hospitals is required to validate this conclusion. </description><description>Raisonnement : Des outils d'évaluation de risque sont couramment utilisés par des chirurgiens et leurs patients afin de conclure si les avantages de la chirurgie cardiaque surpassent les risques. Il a été démontré que l'index Parsonnet 2000, un des outils les plus populaires, présente des niveaux de capacité prédictive variés à travers le monde. Cet outil de risque possède 37 variables. La littérature a aussi illustré que la qualité institutionnelle, le personnel médical et l'ethnicité des patients prédisent le risque de mortalité intrahospitalière. Pourtant, ces facteurs ne sont pas inclus dans l'index Parsonnet 2000. Notre hypothèse est qu'un index régional basé sur l'index Parsonnet 2000 aura une précision supérieure dans cette population de patients. La raison pour ceci est parce que l'index Parsonnet 2000 a été développé utilisant des données provenant d'une seule région (New Jersey) et la littérature a montré que la précision de cet index varie en fonction du lieu. Méthodes : Des données provenant de deux hôpitaux de Montréal ont été utilisés afin de créer deux modèles basés sur l'index Parsonnet 2000 spécifiques à ces hôpitaux à l'aide de la régression logistique multivariée. Les données de 1,162 patients d'un hôpital et celles de 2,656 patients provenant de l'autre hôpital ont été inclus. Les coefficients des facteurs de risque des modèles spécifiques aux hôpitaux ont été comparés à ceux de l'index Parsonnet à l'aide de tests z. De plus, les précisions prédictives des modèles spécifiques aux hôpitaux ont été comparés à celle de l'index Parsonnet à l'aide d'analyses Receiver Operating Characteristic (ROC). Le nombre de décès évitables à différents seuils de risque a aussi été calculé pour chaque modèle. Résultats : Les coefficients de deux variables différaient entre les modèles basés sur les hôpitaux Montréalais. Un index spécifique à un hôpital avait quatre facteurs de risque différents quoique l'autre index différait aussi de quatre poids (p≤0.05) lorsque celui-ci fut comparé à l'index Parsonnet. Un seul index, avec une aire sous la courbe ROC de 0.804, possédait une marginale meilleure précision prédictive comparée à l'index Parsonnet qui avait une aire de 0.798 (p&gt;0.05). En outre, l'utilisation de trois de quatre seuils de risque d'un hôpital aurait prévenu entre deux et huit décès intrahospitaliers quand ce modèle fut comparé à l'utilisation de l'index Parsonnet. Dans l'autre hôpital, l'utilisation des quatre seuils de risque aurait prévenu entre trois et 18 intrahospitaliers quand ce modèle fut comparé à l'utilisation de l'index Parsonnet. Conclusions : Ces résultats démontrent que l'index Parsonnet possède approximativement les mêmes poids et précision prédictive que les modèles spécifiques aux hôpitaux. De plus, l'avantage clinique potentiel (mortalité diminuée) ne suffisait pas pour justifier l'utilisation d'un modèle spécifique au lieu de l'index Parsonnet. Des recherches supplémentaires conduites dans des régions additionnelles et utilisant des échantillons d'hôpitaux plus élevés sont nécessaires afin de valider cette conclusion.</description><creator>Tomaras, Dimitrios</creator><contributor>John Sotirios Sampalis (Internal/Supervisor)</contributor><date>2017</date><subject>Surgery</subject><title>Comparison of the Parsonnet score and hospital-specific models using cardiac surgery patients from Montreal</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/ns064845r.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/7p88ck085</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Surgery</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:44558g660</identifier><datestamp>2020-03-21T13:53:20Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Conjugated polymer (CP) materials and their water-soluble counterparts, conjugated polyelectrolytes (CPEs), are currently used in a variety of applications including photovoltaics, light emitting diodes, and chemical/biological sensors.  Fundamental studies on the synthesis of ever more complex architectures and on their resulting optoelectronic properties have paved the way to the broad range of innovative applications associated with these materials.  A key discovery was the realization that the optical properties of CP/CPE materials, in particular energy migration along the polymer backbone, may be modulated by the polymer morphology and degree of intra/interchain association. CP and CPEs are thus highly sensitive to the nature of the solvating environment.  For CPEs, this also includes the ionic strength and the presence of surfactants.  This thesis explores the self-assembly of lipid membranes with CPEs as a means of tuning CPE optical properties, and exploits CPE/lipid membrane interactions as a platform to develop applications centering on the study of membrane biophysics and the advancement of biomimetic light harvesting materials.  This thesis additionally describes fundamental studies toward improving CPE photostability, providing protocols to prepare anti-fading cocktails that are required for emerging single particle/molecule bioanalytical and biophysical studies requiring CPE excitation at high duty cycles. Working with poly(phenylene vinylene) CPE adsorbed onto silicon dioxide nanoparticles, we initially show how the emission enhancement that accompanies a membrane-induced conformational change of a CPE is exploited as a novel means of detecting the dynamics of membrane rearrangement, a topic that has hitherto been difficult to study experimentally despite its importance in cell biology.  While in this study the manipulation of energy transfer in a CPE was used as a tool to study membrane biophysics, the ability of the membrane to organize CPEs is an interesting topic in its own right.  Along this line, in a second study lipid membranes are evaluated for their ability to direct the self-assembly of CPEs with the goal of achieving efficient energy migration through the membrane to develop a biomimetic light harvesting antenna.  Here we show that CPEs can be successfully embedded within the membrane of oppositely charged liposomes at a high density (&lt; 1 nm separation) without self-quenching.  Light harvested by the polymers is transferred via through-space mechanisms to a lipophilic energy acceptor, where homotransfer between polymers is shown to play role in funneling energy to the acceptor in a manner analogous to the chlorophyll antenna molecules present in the naturally occurring light harvesting antennas of green plants. Alongside highlighting opportunities for CPE/lipid interaction and constructs toward the development of applications, in a third and fourth study, different solution additives are evaluated toward the goal of improving the photostability of CPEs under microscopy imaging conditions in order to meet these demands.  By using an enzymatic oxygen scavenging system and/or adding small molecule triplet quenchers, the number of photons collected from PPE-CO2 increases by up to ca. 15-fold in aqueous solution and by up to 20-40-fold in lipid membranes. Overall, the work presented in this thesis highlights the breadth of opportunities available toward developing new single molecule assays and materials based on CPE/lipid interactions.</description><description>Les polymères conjugués (PC) et leurs homologues solubles dans l'eau les polyélectrolytes conjugués (PEC), sont actuellement utilisés dans plusieurs applications, notamment dans les cellules photovoltaïques, les diodes électroluminescentes ainsi que dans les capteurs chimiques/biologiques. Des études fondamentales sur la synthèse d'architectures de plus en plus complexes et sur les propriétés optoélectroniques des PC / PEC ont contribué à une vaste gamme d'applications innovantes associées à ces matériaux. Une découverte clé a été la réalisation que les propriétés optiques des PEC, en particulier la migration d'énergie le long du squelette du polymère, peuvent être modulées par la morphologie du polymère et son degré d'association intra- et interchaînes.  Les PC et les PEC sont donc très sensibles à la nature de l'environnement de solvatation, et pour les PEC en particulier, à la force ionique, et à la présence d'agents de surface.  Cette thèse explore l'auto-assemblage des membranes lipidiques avec les PEC comme un moyen pour régler leurs propriétés optiques, et exploite les interactions PEC / membrane lipidique comme une plateforme pour développer des applications focalisées sur l'étude de la biophysique des membranes et sur l'avancement des matériaux biomimétiques.  Cette thèse décrit en outre des études fondamentales visant à améliorer la photostabilité des PEC, et fournie des protocoles nécessaires pour les études bioanalytiques et biophysiques émergentes à l'échelle de la particule / molécule unique nécessitant l'excitation des PEC à des cycles élevés.  En travaillant avec du poly(phénylène vinylène) PEC adsorbé sur des nanoparticules de dioxyde de silicium, nous démontrons tout d'abord comment l'augmentation d'émission qui accompagne un changement de conformation du PEC induit par la membrane est exploitée comme un nouveau moyen de détecter la dynamique du réarrangement de la membrane, un sujet qui a déjà été difficile à étudier expérimentalement malgré son importance dans le domaine de la biologie cellulaire.  Alors que dans cette étude la manipulation du transfert d'énergie dans un PEC a été utilisé comme un outil pour élucider la biophysique de la membrane, la capacité de la membrane d'organiser les PEC est un sujet intéressant en soi.  De même, dans une deuxième étude les membranes lipidiques sont évaluées pour leur capacité à diriger l'auto-assemblage des PEC dans le but d'obtenir une migration d'énergie efficace à travers la membrane afin de développer une antenne biomimétique photosynthétique. Dans ce projet, nous démontrons que les PEC peuvent être intégrés avec succès dans la membrane de liposomes à charge opposée avec une densité assez élevée (&lt;1 nm de séparation) sans auto-extinction.  La lumière récoltée par les polymères est transférée à travers des mécanismes d'espace à un accepteur d'énergie lipophile, durant lesquels l'homo-transfert entre les polymères joue un rôle dans l'acheminement d'énergie vers l'accepteur d'une manière analogue aux molécules d'antenne de chlorophylle présentes dans les antennes photosynthétiques naturelles des plantes vertes. Parallèlement à la mise en évidence de nombreuses opportunités pour ces interactions de PEC/lipide pouvant assister au développement de nombreuses applications, dans une troisième et quatrième étude, différents additifs sont évalués dans le but d'améliorer la photostabilité des PEC dans des conditions utilisées en imagerie microscopique pour répondre à ces demandes.  En utilisant un système enzymatique de piégeage d'oxygène et / ou en ajoutant des désactivateurs d'état triplet, le nombre de photons collectés à partir de PPE-CO2 augmente d'environ 15 fois en solution aqueuse et jusqu'à 20 à 40 fois dans les membranes lipidiques.  Dans l'ensemble, le travail présenté dans cette thèse souligne la pléthore des opportunités disponibles vers le développement de nouveaux tests à l'échelle de la molécule unique et matériaux basés sur les interactions CPE/lipides.</description><creator>Calver, Christina</creator><contributor>Gonzalo Cosa (Supervisor)</contributor><date>2017</date><subject>Chemistry</subject><title>Conjugated polyelectrolytes and lipid membranes: from exciton transport to membrane dynamics</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/m900nx09k.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/44558g660</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Chemistry</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:v405sc95m</identifier><datestamp>2020-03-21T13:53:20Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Contexte: Les professionnels de la santé jouent un rôle primordial dans la promotion de la santé des adolescents. Beaucoup des comportements en matière de santé appris lors de l'adolescence peuvent à leur tour affecter l'état de santé à l'âge adulte. Toutefois, peu d'études ont considéré dans quelle mesure la disponibilité des professionnels de la santé au niveau national affecte la santé des adolescents. De même, l'effet de cette densité de professionnels de la santé sur les inégalités sociales de santé chez les adolescents n'a pas fait l'objet d'études jusqu'ici. Méthodes: Cette thèse est une analyse transversale multi-niveaux de données individuelles et nationales de 38 pays. Des données portant sur 218790 adolescents ont été extraites de l'étude internationale "Health Behaviour in School-Aged Children" de 2013/2014. Les modèles de régression multi-niveau examinent l'association entre la densité de ressources humaines en matière de santé et les inégalités de santé à l'adolescence entre différents groupes socio-économiques. Résultats: Des analyses multi-niveaux montrent qu'une densité nationale plus élevée de psychologues est associée avec un niveau plus élevé d'auto-déclarations de problèmes de santé mentale (p = 0.052), tandis que la densité de docteurs et de psychiatres n'est pas prédictive d'une meilleure santé mentale chez les adolescents (p &gt; 0.05). Les interactions inter-niveaux entre les ressources humaines en santé et le niveau socio-économique n'étaient pas significatives, indiquant que les pays avec une plus faible densité de ressources humaines en santé ne manifestaient pas de plus grandes inégalités socioéconomiques en santé mentale.Conclusions: La thèse a conclu que les adolescents dans les pays avec une densité plus élevée de psychologues ont une meilleure santé mentale. Cependant, les resources humaines pour la santé ne sont pas un prédicteur fort de santé mentale chez les adolescents. Globalement, les conditions sociales et environementales peuvent être plus pertinentes pour les politiques de santé mentale des adolescents que les caractéristiques des systèmes de santé mêmes.</description><description>Background: Health professionals play a pivotal role in promoting adolescent health, and many of the health behaviors learned in adolescence can, in turn, shape health in adulthood. However, little is known about how the country-level availability of health professionals affects adolescent health, and whether the benefits of a large health workforce extend equally to adolescents across the spectrum of socioeconomic status (SES).Methods: This thesis is a cross-sectional, multilevel analysis of individual and country data from 38 countries. Data on self-reported mental and psychosomatic health and family affluence from 218,790 adolescents were drawn from the 2013 / 2014 Health Behavior in School-aged Children international survey. Multilevel regression models of health were estimated to examine (1) the association between the density of human resources for health and adolescent mental health and psychosomatic health and (2) the association between the density of human resources for health and health inequalities between socioeconomic groups.Results: Controlling for country-level wealth and income inequality, a higher density of psychologists at the country-level was marginally associated with greater self-reported mental health in adolescents (p = 0.052), whereas the densities of physicians and psychiatrists were not predictive of better adolescent mental health or psychosomatic health (p &gt; 0.05). Cross-level interaction terms between human resources for health and SES were not significant, indicating that countries with a lower density of human resources for health did not have greater socioeconomic inequalities in adolescent health.Conclusions: This thesis found that adolescents in countries with a higher density of psychologists display a trend toward reporting better mental health, after differences in country wealth and income inequality were accounted for. Taken together, however, human resources for health were not a strong predictor of adolescent health. On the whole, social and environmental conditions may be more relevant for adolescent health policy than structural features of health care systems.</description><creator>Riehm, Kira</creator><contributor>Francis Jason Elgar (Internal/Supervisor)</contributor><date>2017</date><subject>Psychiatry</subject><title>Human resources for health: A cross-national, multilevel study of adolescent health outcomes and inequalities</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/r494vn47n.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/v405sc95m</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Psychiatry</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:j38609475</identifier><datestamp>2020-03-21T13:53:21Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>One of the important tests for the validity of the Standard Model of particle physics is the measurement of the top quark Yukawa coupling, which can be directlymeasured in the production of the top-quark pair in association with a Higgs boson, ttH. In this thesis, a likelihood-based fitting tool, known as KLFitter, is implemented to improve the separation power between the ttH signal and the dominant background contribution arising from the top quark pair production, tt, in the final states containing two same-sign electric charge leptons. NLO Monte Carlo samples of the ATLAS detector at centre of mass energy of 13 TeV is used to test the fit. Improvement of the separation power of the fitter is studied by modifying parameters in the likelihood definition of the fit. Different cases have been analyzed and a better discriminating variable, after modifications to the original KLFitter, has been achieved regarding the performance of the fit.</description><description>La mesure du couplage de Yukawa du quark top, en particulier via la productionde pair de quarks top en association avec un boson de Higgs, est très pertinentepour tester la validité du Modèle Standard. Cette thèse présente les performancede KLFitter, un outil probabiliste développé pour améliorer le pouvoir de discriminationentre le signal ttH et la contribution de bruit dominante: la production depair de quarks top dont la désintégration donne deux leptons ayant la même chargeélectrique. À l aide d'algorithmes Monte Carlo, des événements sont générés avecune précision au deuxième ordre, leur interaction avec le détecteur est simulée puisles signaux émulés sont reconstruits et utilisés pour tester KLFitter. L'impact desparamtres du fit de KLFitter sur le pouvoir de discrimination est étudié. Différentscas sont analysés et la variable discriminante optimale est choisie en fonction desperformances observées.</description><creator>Saha, Shreya</creator><contributor>Steven Robertson (Internal/Supervisor)</contributor><date>2017</date><subject>Physics</subject><title>Event reconstruction of tt and ttH using the Kinematic Likelihood Fitter in final states with two same-sign electric charge leptons</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/fb494b81r.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/j38609475</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Department of Physics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:v692t885k</identifier><datestamp>2020-03-21T13:53:22Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Myelin basic protein (MBP) is a key myelin protein that also is expressed during tooth development. Here, the impact of Mbp mutations on tooth development, especially on the mineralization stage, was investigated using different methods. In the study, two mouse models, including MBP-B6, where the LacZ gene is knocked into the endogenous map locus and the mutant Shiverer (Shi) in which most of the mbp gene is deleted. Micro-computed tomography (Micro ct.) scan was used to detect the effect of the MBP gene mutations on the volume and the density of the 1st and 2nd molar. Moreover, radiography was performed to observe and quantify the radiopaque area of the molars of the mice, which also gives information about their density. Histological analysis was conducted by performing a von Kossa van Gieson (VKVG) staining to quantify and analyze tooth mineralization. The possible expression of the reporter gene(lacZ) and thereby the MBP gene in the hard tissue was investigated using the X-GAL staining technique. The different analyses revealed that genetic modifications in the MBP gene affected adversely tooth development and mineralization as well as the weight of the mice. Using the Micro ct. scan technique, it was noticed that the volume and density of the teeth of the Shi mice were less when compared to those of the MBP-B6 and WT mice. MBP-B6 mice exhibited a smaller teeth volume than that of the WT, but with regard to the density, statistical analysis revealed no significant difference between the two pre-mentioned type of mice. The latter results were confirmed by the radiography image where the opacity of the teeth of the WT, and MBP-B6 mice were quite similar and higher than that of Shi mice. The VKVG stained showed less mineralized tissues in The Shi mice in comparison with the WT mainly. Performing the X-Gal staining, it was possible to evidence an expression of the LacZ gene in the mandibles and the maxilla but not in the tooth hard tissues.The above results suggest that knockout of the MBP gene disrupts tooth development and mineralization. Therefore, this gene may play an important role in odontogenesis. No expression of the MBP gene was observed in the dental hard tissues in this study, but further screenings of the expression during different stages of the development of some components of the hard tissues are required before a clear conclusion can be drawn.</description><description>La protéine basique de la myéline (MBP) est une protéine de la myéline clé qui est également exprimé au cours du développement de la dent. Ici, l'impact des mutations MBP sur le développement des dents, en particulier sur la phase de minéralisation, a été étudiée en utilisant des méthodes différentes. Dans cette étude, deux souris modèles ont été utilisées, y compris MBP-B6, où le gène LacZ est frappé dans le locus endogène mbp, et le mutant Shiverer (SHI), où la plupart du gène MBP est supprimé. Micro-tomodensitométrie (CT scan Micro.) a été utilisé pour détecter l'effet de la mutation du gène MBP sur le volume et la densité de la 1ère et 2ème molaire. De plus,la radiographie a été réalisée afin d'observer et de quantifier l'espace radio-opaque desmolaires de la souris, ce qui donne également des informations à propos de leurdensité. L'analyse histologique a été réalisée par l'exécution d'une coloration von Kossavan Gieson (VKVG) pour quantifier et analyser la minéralisation des dents. L'expression possible du gène rapporteur (lacZ) et donc du gène MBP dans le tissu dur a été étudié en utilisant la technique de coloration X-GAL. Les différentes analyses ont montré que les modifications génétiques au niveau du gène MBP, affectent négativement le développement et la minéralisation des dents ainsi que le poids de la souris. En utilisant le technique de CT Scan Micro, il a été remarqué que les dents de la souris Shi étaient moins volumineux et moins denses queceux des souris MBP-B6 et WT. La souris MBP-B6 a démontré un plus petit volume dedents que celui de la souri WT. En ce qui concerne la densité, l'analyse statistique n'a révélé aucune différence significative entre ces deux types de souris. Ces résultats on tété confirmés par l'image radiographique, où l'opacité des dents des souris WT et MBP-B6 était assez semblable et plus élevée que celle de la souris Shi. La coloration VKVG a montré moins de tissus minéralisés dans la Shi, principalement en comparaison avec la WT. L'exécution de la coloration X-Gal, a permis la preuve d'une expression du gène LacZ dans les mandibules et le maxillaire mais pas dans les tissus durs de la dent. Les résultats ci-dessus suggèrent qu'assommer le gène MBP perturbe le développement des dents, ainsi que la minéralisation. Par conséquent, ce gène pourrait jouer un rôle important dans l'odontogenèse. Dans cette étude, aucune expression du gène MBP n'a été observée dans les tissus dentaires durs, mais le dépistage de l'expression au cours des différentes étapes du développement de certaines composantes des tissus durs sont nécessaires avant de pouvoir en tirer une conclusion claire.</description><creator>Al Tounisi, Mohammed</creator><contributor>Simon Tran (Internal/Supervisor)</contributor><date>2017</date><subject>Dentistry</subject><title>Myelin basic protein and its effect on tooth development</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/c534fr41m.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/v692t885k</identifier><degree><name>Master of Science</name><grantor>McGill University</grantor><discipline>Faculty of Dentistry</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:5h73pz601</identifier><datestamp>2020-03-21T13:53:23Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>Les digues de terre sont construites dans le monde entier en tant qu'infrastructure de lutte contre les inondations et sont des composantes importantes de la sécurité publique dans les zones qu'elles protègent. Parmi les facteurs qui ont une incidence négative sur la stabilité et la facilité d'utilisation des digues de terre, les détériorations internes sont connues pour être le type le plus critique de problème structurel. Un type courant de détérioration interne, les terriers d'animaux creusés dans des digues en terre de levée sont rapportés comme étant la cause d'échec dans de nombreux cas de brèches. Globalement, le coût annuel des dommages résultant de l'échec des structures en terre et d'infrastructures connexes en raison d'activités envahissantes de la faune est estimé à plusieurs milliards de dollars. La compréhension des mécanismes de défaillance de levée de terre causée par la faune est un élément clé de la prévention des brèches futures. Une grande partie de la littérature dans le domaine de la faune nuisible enquête sur les impacts écologiques et environnementaux des activités animales, cependant, les études liées aux mécanismes de défaillance des structures de terre en raison des activités de la faune envahissante sont limitées et nécessitent une perspective d'ingénierie géotechnique. Cette recherche vise à identifier les mécanismes qui régissent la dégradation des digues causée par la faune. L'étude de l'impact des terriers sur la performance hydraulique et la stabilité des structures de digues est effectuée en utilisant la modélisation par centrifugation. Des modèles de digues de terre à échelle réduite avec des terriers terrestres et des terriers situés au bord de l'eau, ainsi qu'un modèle de digues intactes de référence sont construits et testés à 35g lors des expériences de centrifugation. Les expériences de centrifugation servent à surveiller et enregistrer les mesures de déformation, d'infiltration et de pression des pores. Les analyses de vélocimétrie d'image de particules (PIV) sont effectuées sur les milliers d'images capturées lors des vols de centrifugation pour calculer la déformation globale des modèles de levée. Les modèles d'éléments finis sont développés sur la base du travail de centrifugation expérimentale et utilisés pour effectuer des études paramétriques sur les impacts de la configuration des terriers sur la stabilité des digues détériorées. Les études évaluent les principaux paramètres qui déterminent la sécurité des digues: la longueur du terrier, la profondeur du terrier et le rapport de la pente latérale. Les détails et les résultats du travail expérimental et numérique sont présentés dans cette thèse ainsi que des conclusions et des recommandations pour des recherches futures.</description><description>Earthen levees are constructed worldwide as flood control infrastructure and are important components of public safety in the areas they protect. Among the factors which negatively impact the stability and serviceability of earthen levees, internal deteriorations are known to be the most critical type of structural problem. A common type of internal deterioration, animal burrows dug into earthen levee embankments are reported to be the cause of failure in many cases of earthen levees breaches. Globally, the annual cost of damage resulting from failure of earthen structures and associated infrastructure due to invasive wildlife activities is estimated to be many billions of dollars. Understanding the mechanisms of wildlife-caused earthen levee failure is a key component of preventing future breaches. Much of the literature in the area of nuisance wildlife investigates the ecological and environmental impacts of animal activities and habitat, however, studies related to failure mechanisms of earthen structures due to invasive wildlife activities are limited and require a geotechnical engineering perspective. This research aims to identify the mechanics that govern the progress of failure within wildlife-induced levees deteriorations. Investigation of the impact of animal burrows on the hydraulic performance and stability of levee structures is performed using centrifuge modeling. Scaled-down earthen levee models with both landside and waterside burrows as well as a benchmark intact levee model are built and tested at 35g during the centrifuge experiments. The centrifuge experiments are monitored and recorded for deformation, seepage, and pore pressure measurements. Particle Image Velocimetry (PIV) analyses are performed on series of images captured during the centrifuge flights to calculate global deformation of the levee models. Finite element models are developed based on the experiments and used to conduct parametric studies on the impacts of burrow configurations on the stability of the deteriorated levees. The studies investigate key parameters which governs levee safety: burrow length, burrow depth and levee side slope ratio. Details and results of the experimental and numerical work are presented in this thesis along with conclusions and recommendations for future research.</description><creator>Saghaee, Gholamreza</creator><contributor>Mohamed Meguid (Supervisor)</contributor><date>2017</date><subject>Civil Engineering &amp; Applied Mechanics</subject><title>Investigating the performance of earthen levee structures with induced internal deterioration</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/v692t886v.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/5h73pz601</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Civil Engineering and Applied Mechanics</discipline></degree></thesis></metadata></record><record><header><identifier>oai:escholarship.mcgill.ca:dn39x397n</identifier><datestamp>2020-03-21T13:53:24Z</datestamp></header><metadata><thesis xmlns="http://www.ndltd.org/standards/metadata/etdms/1-0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ndltd.org/standards/metadata/etdms/1-0/ http://www.ndltd.org/standards/metadata/etdms/1-0/etdms.xsd"><description>In combinatorial graph theory, decomposable graphs are such type of graphs that are guaranteed to be decomposable into conditionally independent components, known as maximal cliques. In statistics, decomposable graphs are widely used in the field of graphical models or Bayesian model determination, where the dependency structure among high dimensional data or model parameters is unknown. Decomposable graphs are hence used as functional priors over large covariance matrices or as priors over hierarchies of model parameters. One such example is the Gaussian graphical model (lauritzen 1996, whittaker 2009), which has seen success in a variety of applications. Beyond this framework, decomposable graphs are seldom used in statistical applications.Random graphs, on the other hand, have recently seen much research interest, where the focus is on developing methodologies for models on relational data in the form of random binary matrices. A principle component of such models is to assume a network framework by mapping the relations to edges of the network, and data sources to nodes. The likelihood of an edge is assumed to be driven by affinity parameters of the associated nodes. The first part of this work attempts to propose a framework for modelling random decomposable graphs, using similar tools as in random graphs. Rather than modelling edges between nodes, the framework models the bipartite links between the graph nodes and latent community nodes, through node affinity parameters. The latent communities are assumed to represent the maximal cliques in decomposable graphs. Under the proposed framework, simple Markov update rules are given with explicit lower bounds for its mixing time (time until convergence). Under a set of conditions, an exact expression for the expected number of maximal cliques per node is given. The second part of this work illustrates a new application of decomposable graphs that is motivated by the proposedframework. Combinatorially, there is a unique set of subgraphs of any maximal clique. Treating maximal cliques as latent communities allows the treatment of subgraphs of maximal cliques as sub-clusters within each community. The proposed framework is extended to incorporate a sub-clustering component, which enables the modelling of decomposable graphs and simultaneous modelling of the sub-clustering dynamics forming within each larger community.The final part of this work deals with the topic of link prediction in networks with presence-only data, where absence is only an indication of missing information and not a prohibited link. The work is motivated by a particular example of identifying undocumented or potential interactions among species from the set of available documented interactions, in an aim to help guide the sampling of ecological networks by identifying the most likely undocumented interactions. The problem is framed in bipartite graph structure, where edges represent interactions between pairs of species. The work first constructs a Bayesian latent score model, which ranks observed edges from the most probable down to the least certain. To improve scoring efficiency, and thus link prediction, the work incorporates a Markov random field component informed by phylogenetic relationships among species. The model is validated using two host-parasite networks constructed from published databases, the Global Mammal Parasite Database and the Enhanced Infectious Diseases database, each with thousands of pairwise interactions. Finally, the model is extended by integrating a correction mechanism for missing interactions in the observed data, which proves valuable in reducing uncertainty in unobserved interactions.</description><description>En théorie des graphes combinatoire, les graphes décomposables sont un type de graphe dont il est garanti qu'ils sont décomposables en composantes conditionnellement indépendantes, appelées cliques maximum. En statistiques, les graphes décomposables sont communément utilisés dans le champ des modèles graphiques ou dans la détermination de modèles Bayésiens, pour lesquels la structure de dépendence entre variables à haute dimensionalité ou des paramètres du modèle est inconnue. Les graphes décomposables sont ainsi utilisés comme précédents fonctionnels par rapport aux matrices à large covariance ou en tant que précédents par rapport aux hierarchies des paramètres du modèle. Un exemple de cette utilisation est celle du modèle graphique Gaussien (lauritzen 1996, whittaker 2009) qui a été appliqué avec succès dans un grand nombre de cas.Les graphes aléatoires ont généré beaucoup d'intérêt, en particulier, sur les données relationnelles en de matrices aléatoires binaires. Une composante principale de ces modèles est la définition d'un cadre de réseau en associant les relations aux  liens du réseau et les sources de données aux noeuds. La première partie de ce travail propose un cadre de modèlisation pour les graphes décomposables aléatoires  et utilise des outils similaires à ceux utilisés pour les graphes aléatoires. Plutôt que de modèliser les liens entre les noeuds, le cadre modèlise les associations bipartites entre les noeuds du graphe et les noeuds des communautés latentes, à l'aide des paramètres d'affinité entre les noeuds. L'hypothèse émise étant que les communautés latentes représentent les cliques maximum des graphes décomposables. Au sein de ce cadre proposé, les règles simples de mise à jour de Markov se voient attribuées une limite basse explicite pour leur temps mélangé (temps sous convergence). La seconde partie de ce travail illustre une nouvelle application des graphes décomposables s'appuyant sur le cadre proposé. Combinatoirement, il existe un ensemble unique de sous-graphes pour toute clique maximum. En traitant chaque clique maximum en tant que communauté latente il est possible de traiter les sous-graphes des cliques maximum en tant que sous-group au sein de chaque communauté. Le cadre proposé est étendu pour incorporer  une composante de sous-groupement, ce qui autorise la modélisation des graphes décomposables et simultanément la modélisation de dynamiques de sous-groupement qui se forment au sein de chaque communauté plus large.La dernière partie de ce travail traite du sujet des prédictions de lien pour les réseaux avec des données présence uniquement, où l'abscence est seulement une indication de données manquantes et non d'un lien interdit. Ce travail s'appuie sur un exemple specifique, celui de l'identification d'interactions non-documentées ou potentielles au sein d'espèces appartennant à l'ensemble des interactions documentées. L'objectif est d'aider à guider l'échantillonnage de réseaux écologiques en identifiant les relation non-documentées les plus vraisemblables. Le problème est cadré en structure bipartite de graphe où les liens représentent les interactions entre paires d'espèces. Le travail développe tout d'abord un modèle de score latent Bayésien qui ordonne les liens observés du plus probable au  moins certain. Pour améliorer l'efficience du score et partant la prédiction des liens, le travail incorpore un composant de champ aléatoire de Markov utilisant lesretations phylogéniques entre espèces. Le modèle est validé en utilisant deux réseaux hôte/parasite construits à partir de deux bases de données publiées; la base globale  mammifère parasiteet la base de données améliorée des maladie infectieuses, chacune contenant des milliers de paires d'interactions. Finalement, le modèle est étendu en intégrant un méchanisme de correction pour les interactions manquantes dans les données observées, qui s'avère efficace à diminuer l'incertitude dans les interactions inobservées.</description><creator>Elmasri, Mohamad</creator><contributor>David Stephens (Supervisor)</contributor><date>2017</date><subject>Mathematics and Statistics</subject><title>On decomposable random graphs and link prediction models</title><language>eng</language><publisher>McGill University</publisher><type>Thesis</type><rights>All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.</rights><identifier>https://escholarship.mcgill.ca/downloads/nk322g981.pdf</identifier><identifier>https://escholarship.mcgill.ca/concern/theses/dn39x397n</identifier><degree><name>Doctor of Philosophy</name><grantor>McGill University</grantor><discipline>Department of Mathematics and Statistics</discipline></degree></thesis></metadata></record><resumptionToken completeListSize="47894">oai_etdms.s(Collection:theses).f(2019-10-16T06:03:34Z).u(2020-07-23T18:55:55Z).t(47894):5975</resumptionToken></ListRecords></OAI-PMH>