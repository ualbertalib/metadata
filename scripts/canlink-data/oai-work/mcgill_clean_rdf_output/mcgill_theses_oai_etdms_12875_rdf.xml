<?xml version="1.0" encoding="UTF-8"?><rdf:RDF xmlns:oai="http://www.openarchives.org/OAI/2.0/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:ual="http://terms.library.ualberta.ca/" xmlns:bibo="http://purl.org/ontology/bibo/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:schema="https://schema.org/" xmlns:etdms="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A73666797g"><dcterms:title>Operational mitigation of ground clutter using information from past and near-future radar scans</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Atmospheric and Oceanic Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Anderson-Frey, Alexandra</ual:dissertant><dc:abstract>Lorsqu'un signal radar rencontre des obstacles, la précision des données de réflectivité est endommagée, ce qui réduit la qualité des outils de prévisions météorologiques tels les totaux de précipitation et les algorithmes qui surveillent l'évolution des orages. Dans cette étude, on recherche une solution optimale pour remplir en temps réel et dans un contexte opérationnel les trous d'information causés par les échos de sol, en explorant une variété d'algorithmes de plus en plus complexe basée sur une méthode géostatistique: le kriging ordinaire. Le résultat final est le développement d'un algorithme de kriging ordinaire "intélligent". Cette méthode remplace les pixels contaminés en utilisant la moyenne pondérée de pixels non-contaminés à proximité, où ces pixels sont sélectionnés specialement pour incorporer des données indépendentes et pour ne pas surcompliquer les calculs avec trop d'informations redondantes. Ces informations proviennent non seulement du même temps et du même niveau que la région qui doit être corrigée, mais aussi d'aux autres niveaux ainsi que du passé et du proche-futur. L'inclusion de la dimension temporelle en particulier offre grand valeur même aux algorithmes les plus simples, et aussi lorsqu'on considère seulement les informations du passé et non du futur. Les données du radar des temps antérieurs constituent alors une source inexploité d'informations qui pourraient permettre de générer (et de régénérer, en utilisant les données du proche-futur) des produits radar plus précis.</dc:abstract><dc:abstract>When a radar pulse encounters obstacles in its path, the accuracy of radar reflectivity data is adversely affected, which in turn decreases the quality of forecasting and nowcasting tools such as rainfall totals and cell-tracking algorithms. In this study, we seek an optimal solution for real-time, operational gap-filling in radar data contaminated by known areas of ground clutter, and explore a variety of algorithms of increasing complexity to that end, making use of a geostatistical method known as ordinary kriging. The final result is the development of a "smart" ordinary kriging algorithm. This method replaces clutter-contaminated pixels in radar data using the weighted average of a nearby collection of uncontaminated pixels, which have been specially selected to sample independent spatial and temporal information while avoiding bogging down calculations with redundant information. These data are obtained not only from the same reflectivity scan as the ground clutter to be corrected, but also from different heights and from both earlier and near-future times. The incorporation of the time dimension in particular adds a great deal of value to simplistic algorithms, even when only data from past times are considered. Radar scans from earlier times are thus shown to be a major untapped source of information that can be used to generate (and regenerate, using near-future data) more accurate radar products.</dc:abstract><ual:supervisor>Frederic Fabry (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/m900nx94c.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/73666797g</ual:fedora3Handle><dc:subject>Earth Sciences - Atmospheric Sciences</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A0z7091037"><dcterms:title>Earthworm interactions with denitrifying bacteria in riparian buffers: significance for nitrogen dynamics from the physiological to ecological scales</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Natural Resource Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Chen, Chen</ual:dissertant><dc:abstract>Dans les zones ripariennes (ZR), la dénitrification est la majeure source d'azote (N)  perdu sous forme de gaz. Il a été démontré que la présence de vers de terre (VDT) affecte la dénitrification dans le sol à différentes échelles temporelles et spatiales (en conditions contrôlées et au champs). Cependant, il est nécessaire d'étudier l'impact à petite échelle des VDT sur la dénitrification afin de pouvoir l'extrapoler à l'échelle du champs. Cette thèse a pour objectif de déterminer comment les interactions entre les VDT et les bactéries dénitrifiantes affectent les dynamiques de l'N au niveau physiologique et au niveau de la drilosphère pour déterminer ensuite si ces interactions sont détectables sur le terrain. Dans une étude au laboratoire (échelle physiologique), des VDT ont été nourris avec des substrats organiques de rapports C/N variés tout en maintenant un rapport C/N constant de leurs tissus musculaires (3,37 à 5,35). Le taux de dénitrification des L. terrestris adultes était plus élevé avec un mélange à base de soja riche en N plutôt qu'avec la mousse de tourbe (P &lt; 0.05). En revanche, les taux de dénitrification de A. tuberculata étaient plus variables. Ces résultats suggèrent que les VDT produisent des formes gazeuses d'N: les écosystèmes avec une population abondante de L. terrestris sont plus à risque de produire des flux élevés de N2O et de N2 en présence de substrats organiques riches en N. Une autre étude au laboratoire (drilosphère) de 69 jours a démontré que la présence de VDT augmentait de 50%  le cumul de N2O émis dans le sol sec mais le diminuaient de 34% dans le sol humide et le réduisaient de 82% en présence de cycles d'assèchement-réhumidification. La dénitrification potentielle (DEA) augmentait en présence de VDT (P &lt; 0.05). L'interaction des traitements VDT × humidité du sol a affecté l'abondance des gènes 16S rRNA, nirS et nosZ (P &lt; 0.05). À la vue de ces résultats, les diminutions des cumuls de N2O émis causées par les VDT, en conditions humides ou lors cycles d'assèchement-réhumidification, sont dues à une stimulation des bactéries consommatrices de N2O et à une modification de la composition des microorganismes dénitrifiants du sol. De plus, la présence de VDT pourrait diminuer les émissions de N2O des sols saturés en eau. À l'échelle du terrain, les données démographiques sur les VDT ont été récoltées du printemps à l'automne 2012, dans des ZR temporairement inondées (TR) et non inondées (NR) du Québec, au Canada. Les zones TR présentaient une plus grande diversité (9 espèces) ainsi qu'une biomasse plus importante de VDT que les zones NR (6 espèces). La population et la biomasse des VDT étaient plus élevées au printemps et à l'automne 2012 mais déclinaient en été 2012. En présence de VDT, la DEA était 1,5 fois plus petite dans les zones TR et 1,2 fois plus petite dans les zones NR. L'analyse causale des données suggère qu'au contraire des VDT, l'humidité, l'ammonium et le rapport C:N du sol influence directement la DEA. Les interactions entre les VDT et les microorganismes dénitrifiants dans les ZR seraient alors la résultante de l'humidité du sol et des concentrations en substrats disponibles. En conclusion, mes résultats indiquent que les effets mesurés au laboratoire, à l'échelle physiologique, ne peuvent pas être extrapolés à l'échelle du champ. Cependant, les travaux de laboratoire à l'échelle de la drilosphère sont plus pertinents  pour déterminer l'influence des VDT sur les émissions réelles de N2O. Finalement, la production de N2O des ZR résulte d'interactions multiples entre l'humidité du sol, les populations de VDT et les microorganismes : l'humidité du sol contrôle le produit final de la dénitrification tandis que les VDT diminuent l'activité des microorganismes dénitrifiants en conditions hydriques saturées. Les VDT influencent les pertes gaseuses azotées des ZR en agissant sur les microorganismes dénitrifiants et sur la disponibilités des substrats nécessaires à la dénitrification.</dc:abstract><dc:abstract>Denitrification is responsible for gaseous nitrogen (N) loss from in riparian buffers. Earthworms affect denitrification in controlled laboratory and field studies; however, the small-scale effects of earthworm on denitrification need to be extrapolated to the field scale. The general objective of this thesis was to determine how earthworm-denitrifying bacteria interactions could affect N dynamics at a physiological level (within the earthworm body), the individual level (earthworm drilosphere), then finally determine whether these small-scale effects could be detected at the field scale (in riparian buffers). In a microcosm study (physiological level), earthworms were fed with organic substrates with different C:N ratio, but earthworm maintained a constant C:N ratio of 3.37 to 5.25 in their muscular tissue, regardless of the food N content. Adult Lumbricus terrestris had a significantly greater denitrification rate with the N-rich soybean mixture than with peat moss. These results suggest that adult L. terrestris consuming N-rich organic substrates may contribute to N2O and N2 fluxes from soil. In a mesocosm study (drilosphere level), earthworm presence increased the cumulative N2O emissions by 50% in the dry soil treatment, but earthworms reduced the cumulative N2O emissions by 34% in the wet soil treatment and reduced N2O emissions significantly by 82% in soil with rewetting-drying cycles (WD). Denitrification enzyme activity (DEA) increased significantly when earthworms were present and the abundance of 16S rRNA, nirS, and nosZ genes was affected significantly by the earthworm × soil moisture interaction. These results suggested that the decrease in cumulative N2O emissions from wet soil and the WD treatment by earthworms was due to a general alteration of the denitrifying bacterial community composition. Moreover, the results implied that earthworms would decrease the N2O emissions from saturated soils. At the field scale, earthworm demographics were investigated in temporary flooded riparian region (TR) and non-flooded riparian region (NR) in Quebec, Canada, from spring to autumn, 2012. The TR had more earthworm diversity (9 species) and larger population and biomass than NR. Earthworm population and biomass were largest in spring and autumn but declined in summer. Path analysis indicated that soil moisture, NH4+ and soil C:N ratio, but not earthworm biomass, directly affected the DEA. This observation suggests that earthworm-denitrifier interactions in riparian buffers were the result of soil moisture content and available substrate concentrations. In conclusion, my results indicate that physiological scale effects cannot be extrapolated directly from the lab to the field. Studies at the mesocosm and field scales suggest that the N2O output from riparian soils is the result of the moisture-earthworm-microbial interaction: soil moisture act as a crucial controller on the final product of denitrification (N2O or N2), while earthworms influence the gaseous N losses from natural riparian buffers through both direct effects on denitrifiers and indirect effects on substrates required for the denitrification reaction.</dc:abstract><ual:supervisor>Joann Karen Whalen (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/pc289n30z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/0z7091037</ual:fedora3Handle><dc:subject>Earth Sciences - Biogeochemistry</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9k41zh769"><dcterms:title>Rethinking state responsiblity in international space "environmental" law: a case for collective responsibility for space debris prevention</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Laws</schema:inSupportOf><dc:contributor>Institute of Air and Space Law</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Ekweozoh, Irene</ual:dissertant><dc:abstract>This thesis assesses the current legal regime of State Responsibility in the exploration and use of outer space as it concerns space debris prevention. It argues that the proliferation of space debris is attributable to a lack of clarity in the current regime regarding the duty imposed on state and non-state actors for its prevention.  It is contended that this concern must be addressed through regulation in order to be meaningful. In support, it interrogates the current status quo of state responsibility for the space activities of non-state entities and how this differs from state responsibility in international environmental law.  The objective is to show that the vicarious responsibility of states for non-state actors under current international space law and the disharmony between international space law and environmental law reinforces irresponsible conduct in space activity. As a preventive measure, it advocates for a return to the jurisprudence on "Mankind" that was the basis of the Outer Space Treaty.  It suggests that concern for "all mankind" can be used to impose a duty of due regard on all space actors.  In this regard, it advances a case for collective responsibility of all space actors by recommending a "Protocol on Collective Responsibility in the Prevention of Space Debris" to be executed by all space actors as a mandatory mechanism to compel the pursuit of uniform space debris prevention measures.  </dc:abstract><dc:abstract>La présente thèse décrit le régime légal de responsabilité des Etats dans l'exploration et l'utilisation de l'espace en ce qui concerne la lutte contre les débris spatiaux. Un régime obscur quant aux devoirs des Etats et autres acteurs est à l'origine de la prolifération de ces débris. Ce problème doit être réglé par la voie réglementaire, afin d'avoir un impact efficace. Pour ce faire, le présent statu quo dans la responsabilité des Etats pour les activités de leurs nationaux dans l'espace est étudié. L'objectif est de démontrer que la responsabilité du fait d'autrui des Etats dans le cadre du présent droit international de l'espace ainsi que la discorde qui existe entre ce dernier et le droit environnemental encourage les comportements irresponsables dans les activités spatiales. Comme mesure préventive, la présente thèse propose un retour à la doctrine de « l'humanité » qui était à la base du Traité de l'espace. La prise en compte de « toute l'humanité » peut permettre d'imposer un devoir de respect à tous les acteurs de l'espace. A cette fin, cette thèse propose un régime de responsabilité collective de tous ces acteurs au travers d'un « Protocole sur la responsabilité collective pour la limitation des débris spatiaux » qui serait signé par ceux-ci. Ce mécanisme obligatoire imposerait l'adoption de mesures uniformes de lutte contre les débris spatiaux.</dc:abstract><ual:supervisor>Yaw Nyampong (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/n009w560v.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9k41zh769</ual:fedora3Handle><dc:subject>Social Sciences - Law</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Am613n182h"><dcterms:title>A receiver structure for frequency-flat time-varying rayleigh channels and performance analysis</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Shao, Xiaofei</ual:dissertant><dc:abstract>Dans la présente thèse, nous proposons une structure de récepteur pour des canaux de Rayleigh à variation temporelle et à réponse uniforme en fréquences, comprenant deux parties : un étage d'entrée du récepteur et un détecteur a posteriori maximum (MAP, pour Maximum A-Posteriori). La discrétisation du signal reçu et continu dans le temps représente une étape essentielle de l'étage d'entrée et pour une telle application, nous présentons un nouveau cadre pour la représentation discrète des signaux continus dans le temps. L'un des aspects clés de ce cadre est la représentation de la fonction d'autocorrélation de l'évanouissement sous la forme d'un noyau séparable dimensionnel fini. On utilise l'algorithme de la transformée de Haar rapide (FHT, pour Fast Haar Transform) à l'étage d'entrée dans le but d'atténuer la complexité. Une analyse de notre structure de récepteur pour canaux à évanouissement graduel révèle que cette structure convient de façon optimale à certains schémas de modulation. Les résultats de rendement, simulés par ordinateur et à l'aide de la méthode de Monte-Carlo, sont présentés pour trois schémas de modulation binaire appliqués à des canaux à évanouissement de Rayleigh à variation temporelle. Une comparaison avec la littérature scientifique démontre que notre récepteur peut offrir un rendement optimum dans le cas de la modulation orthogonale temporelle. En ce qui concerne la modulation à déplacement minimum (MSK, pour Minimum Shift Keying), notre récepteur, qui fait appel à quatre fonctions de base, est en mesure d'abaisser le plancher d'erreur d'un ordre de grandeur pour ce qui est des techniques signalées présentant une complexité similaire. La modulation par déplacement de fréquence orthogonale (FSK, pour Frequency Shift Keying) peut offrir le même rendement que la modulation orthogonale temporelle dans le casdes canaux à évanouissement graduel, mais son rendement est médiocre avec des canaux à évanouissement rapide, en plus de subir les répercussions d'un plancher d'erreur. Par contre, comparativement à la modulation à déplacement minimum, la modulation par déplacement de fréquence orthogonale procure un rendement supérieur.</dc:abstract><dc:abstract>This thesis proposes a receiver structure for frequency-flat time-varying Rayleigh channels, consisting of two parts: a receiver front-end and Maximum A-Posteriori (MAP) detector. Discretization of the received continuous time signal is an essential stage in the front-end, and for such application this thesis presents a new framework for discrete representation of continuous time signals. A key point of this framework is the representation of the fading autocorrelation function in the form of a finite dimensional separable kernel. The Fast Haar Transform (FHT) algorithm is used in the front-end to reduce complexity. Analysis of our receiver structure for slow-fading channels shows that it is optimal for certain modulation schemes. Computed and Monte-Carlo simulated performance results are presented for three binary modulation schemes over time-varying Rayleigh fading channels. By comparison with literature, it is shown that our receiver can achieve optimal performance for Time-Orthogonal modulation. For Minimum Shift Keying (MSK) modulation, our receiver using four basis functions can lower the error floor by more than one order of magnitude with respect to reported techniques of comparable complexity. Orthogonal Frequency Shift Keying (FSK) modulation can achieve the same performance as Time-Orthogonal for the slow-fading case but performs worse over fast-fading channels, and suffers from an error floor. However, compared to MSK, Orthogonal FSK has a better performance.</dc:abstract><ual:supervisor>Harry Leib (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/02871025z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/m613n182h</ual:fedora3Handle><dc:subject>Engineering - Electronics and Electrical</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A7h149s90d"><dcterms:title>Joint OFDM symbol detection and channel estimation over doubly selective channels</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Wang, Yi</ual:dissertant><dc:abstract>Le multiplexage par répartition en fréquence orthogonale (OFDM, pour Orthogonal Frequency Division Multiplexing) est une technique prometteuse pour la transmission de données à haut débit. Il est largement utilisé dans les systèmes de communication sans fil actuels en raison de ses bonnes performances sur les canaux sélectifs en fréquence. Toutefois, les systèmes reposant sur l'OFDM sont sensibles aux variations de temps de voie, qui entraînent un brouillage inter-porteuses (ICI, pour Inter-Carrier Interference). En l'absence de méthodes de détection appropriée, ce brouillage peut réduire les performances. Les informations d'état de canal (CSI, pour Channel State Information) constituent un élément essentiel des divers systèmes de détection de données basés sur l'OFDM et leur acquisition est un facteur crucial, surtout sur les canaux variables en temps. La présente examine une technique permettant d'intégrer l'évaluation de canaux reposant sur un filtre de Kalman à une variante du décodage par sphères (SD, pour Sphere Decoding) adaptée à la détection OFDM. En modélisant le système d'OFDM dans le domaine fréquentiel comme un système « entrées multiples, sorties multiples » (MIMO, pour Multiple Input Multiple Output), nous obtenons une puissante technique de SD pour la détection des données. En tenant pour acquis que la matrice de canal est une matrice bande, il est possible de maintenir une complexité recelant le potentiel pour une implantation réelle du SD et d'obtenir des performances concurrentielles comparativement à d'autres systèmes existants. En ce qui a trait à l'acquisition des CSI, on utilise la méthode de l'« expansion de base » (BE, pour Basis Expansion) pour modéliser les canaux variables en temps et on crée un filtre de Kalman pour le suivi des variations de temps des canaux. Le filtre de Kalman utilise les retours de décisions tirés du SD. Il ne nécessite ainsi qu'une faible densité de symboles pilotes, ce qui accroît l'optimisation de la bande passante. Les performances de ce système sont évaluées à l'aide de simulations informatiques reposant sur la méthode de Monte-Carlo. Les résultats montrent que par rapport à l'algorithme mis en concurrence, ce système permet des gains de performance. Quand il est utilisé sur des canaux LTE (Long Term Evolution) modérément variables en temps, ce système atteint une bonne performance, même en présence de brouillage de la phase des porteuses.</dc:abstract><dc:abstract>Orthogonal Frequency Division Multiplexing (OFDM) is a promising technique for high data rate transmission, that is widely used in modern wireless communication systems because of its good performance over frequency selective channels. However OFDM systems are sensitive to channel time variation resulting in Inter-Carrier Interference (ICI), that without suitable detection methods can degrade performance. Channel State Information (CSI) is essential to various OFDM data detection schemes and its acquisition is a critical factor, in particular over time-varying channels. This work considers a technique for integrating the Kalman filter channel estimation with a version of Sphere Decoding (SD) adapted to OFDM detection. By modelling the OFDM system in frequency domain as a Multiple Input Multiple Output (MIMO) system, we derive a powerful SD technique for data detection. By approximating the channel matrix as banded, we are able to maintain a feasible complexity for SD and deliver competitive performance when compared to other existing scheme. For acquisition of CSI, the Basis Expansion (BE) method is used to model the time-varying channels and a Kalman filter is constructed for tracking. The Kalman filter employs decision-feedback from the SD, requiring only a low pilot symbol density, and hence improves bandwidth efficiency. The performance of this scheme is evaluated by Monte-Carlo computer simulations. Results show that compared with competing algorithm, this scheme provides performance gains. When it is used over moderate time-varying LTE channels, this scheme shows good performance even in the presence of carrier phase noise. </dc:abstract><ual:supervisor>Harry Leib (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/rx913t137.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/7h149s90d</ual:fedora3Handle><dc:subject>Engineering - Electronics and Electrical</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Acr56n439j"><dcterms:title>Multilevel and algebraic multigrid methods for the higher order finite element analysis of time harmonic Maxwell's equations</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Aghabarati, Ali</ual:dissertant><dc:abstract>The Finite Element Method (FEM) applied to wave scattering and quasi-static vector field problems in the frequency domain leads to sparse, complex-symmetric, linear systems of equations. For large problems with complicated geometries, most of the computer time and memory used by FEM goes to solving the matrix equation. Krylov subspace methods are widely used iterative methods for solving large sparse systems. They depend heavily on preconditioning to accelerate convergence. However, application of conventional preconditioners to the "curl-curl" operator which arises in vector electromagnetics does not result in a satisfactory performance and specialized preconditioning techniques are required. This thesis presents effective Multilevel and Algebraic Multigrid (AMG) preconditioning techniques for p-adaptive FEM analysis. In p-adaption, finite elements of different polynomial orders are present in the mesh and the system matrix can be structured into blocks corresponding to the orders of the basis functions. The new preconditioners are based on a p-type multilevel Schwarz (pMUS) approximate inversion of the block structured system. A V-cycle multilevel correction starts by applying Gauss-Seidel to the highest block level, then the next level down, and so on. On the other side of the V, Gauss-Seidel iterations are applied in the reverse order. At the bottom of the cycle is the lowest order system, which is usually solved exactly with a direct solver. The proposed alternative is to use Auxiliary Space Preconditioning (ASP) at the lowest level and continue the V-cycle downwards, first into a set of auxiliary, node-based spaces, then through a series of progressively smaller matrices generated by an Algebraic Multigrid (AMG). The algebraic coarsening approach is especially useful for problems with fine geometric details, requiring a very large mesh in which the bulk of the elements remain at low order. In addition, for wave problems, a "shifted Laplace" technique is applied, in which part of the ASP/AMG algorithm uses a perturbed, complex frequency. A significant convergence acceleration is achieved. The performance of Krylov algorithms is further enhanced during p-adaption by incorporation of a deflation technique. This projects out from the preconditioned system the eigenvectors corresponding to the smallest eigenvalues. The construction of the deflation subspace is based on efficient estimation of the eigenvectors from information obtained when solving the first problem in a p-adaptive sequence. Extensive numerical experiments have been performed and results are presented for both wave and quasi-static problems. The test cases considered are complicated to solve and the numerical results show the robustness and efficiency of the new preconditioners. Deflated Krylov methods preconditioned with the current Multilevel/ASP/AMG approach are always considerably faster than the reference methods and speedups of up to 10 are achieved for some test problems. </dc:abstract><dc:abstract>La méthode des éléments finis (FEM) appliquée à la dispersion des ondes et aux problèmes de champ de vecteurs quasi-statique dans le domaine fréquentiel mène à des systèmes d'équations linéaires rares, symétriques-complexes. Pour de grands problèmes ayant des géométries complexes, la plupart du temps et de la mémoire d'ordinateur utilisé par FEM va à la résolution de l'équation de la matrice. Les méthodes itératives de Krylov sont celles largement utilisées dans la résolution de grands systèmes creux. Elles dépendent fortement des préconditionnement qui accélèrent la convergence. Toutefois, l'application de préconditionnements conventionnels à l'opérateur "rot-rot" qui surgit en électromagnétisme vectoriel n'aboutit pas à des résultats satisfaisants et des techniques de préconditionnement spécialisés sont exigées.Cette thèse présente des techniques de préconditionnement efficaces multiniveau et multigrilles algébrique (AMG) pour l'analyse p-adaptative FEM. Dans la p-adaptation, des éléments finis de différents ordres polynomiaux sont présents dans le maillage et la matrice du système peut être structurée en blocs correspondant aux ordres des fonctions de base. Les nouveaux préconditionneurs sont basés sur un type d'inversion approximative à multiniveau p Schwarz (pMUS) du système structuré de bloc. Une correction à niveaux multiples en cycle V débute par l'application de Gauss-Seidel au niveau du bloc le plus élevé, suivi par le niveau inférieur, et ainsi de suite. De l'autre côté du V, des itérations de Gauss-Seidel sont appliquées en ordre inverse. Au bas du cycle se trouve le système d'ordre le plus bas, qui est habituellement résolu exactement avec un solveur direct. L'alternative proposée est d'utiliser l'espace auxiliaire de préconditionnement (ASP) au niveau le plus bas et de poursuivre le cycle en V vers le bas, d'abord en un ensemble d'auxiliaires, basé sur les espacements de nœuds, à travers une série de plus en plus petites de matrices générées par un multigrille algébrique (AMG). L'approche de grossissement algébrique est particulièrement utile aux problèmes ayant de fins détails géométriques, nécessitant une très grande maille dans laquelle la majeure partie des éléments restent à un niveau plus bas.En outre, pour des problèmes d'onde, la technique "décalé Laplace" est appliquée, dans laquelle une partie de l'algorithme ASP/AMG utilise une fréquence complexe perturbée. Une accélération de la convergence significative est atteinte. La performance des algorithmes de Krylov est davantage renforcée au cours du p-adaptation par l'incorporation d'une technique de déflation. Cette saillie fait dépasser hors du système préconditionné, les vecteurs propres correspondants aux plus petites valeurs propres. La construction du sous-espace de déflation est basée sur une estimation efficace des vecteurs propres à partir d'informations obtenues lors de la résolution du premier problème dans une séquence p-adaptatif. Des expériences numériques approfondies ont été effectuées et les résultats sont présentés à la fois aux problèmes d'onde et quasi-statiques. Les cas de test sont considérés comme compliqués à résoudre et les résultats numériques montrent la robustesse et l'efficacité des nouveaux préconditionnements. Les méthodes de Krylov de déflation préconditionnés par l'approche multiniveaux/ASP/AMG actuelle sont toujours considérablement plus rapides que les méthodes de référence et des accélérations allant jusqu'à 10 sont atteintes pour certains problèmes de test.</dc:abstract><ual:supervisor>Jonathan P Webb (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/zk51vm29s.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/cr56n439j</ual:fedora3Handle><dc:subject>Engineering - Electronics and Electrical</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aqz20sx169"><dcterms:title>Quartz crystal microbalance studies of biomolecule binding to cranberry derived proanthocyanidins</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Chemical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Weckman, Nicole</ual:dissertant><dc:abstract>Tandis que la consommation de canneberge a été liée à la prévention d'infections bactériennes dans les voies urinaires pendant de nombreuses années, notre compréhension de la biodisponibilité et les mécanismes par lequel canneberge empêche les infections bactériennes est limitée. Malgré l'utilisation fréquente dans les essais cliniques, la biodisponibilité de matières dérivées de canneberges dans les voies urinaires peut être limitée en raison de leur interaction avec les protéines de sérum humain tels que l'albumine, α-1 glycoprotéine acide (AAG) et du fibrinogène. Dès qu'elles arrivent à la voie urinaire, les matières dérivées de canneberges peuvent interférer avec la pathogenèse bactérienne via l'interaction avec les lipopolysaccharides (LPS) sur la surface bactérienne et les biosurfactants sécrétées, rhamnolipide. Microbalance à quartz avec la surveillance de la dissipation (QCM-D) est un instrument capable de détecter de mass avec une haute sensibilité qui est utilisé dans cette étude d'enquêter directement sur les interactions entre les matières dérivées de canneberges et protéines de sérum humain ou des composants bactériens (LPS et rhamnolipide). Les liaisons entre les proanthocyanidines de canneberge (CPAC) et les trois protéines sériques, les rhamnolipides, et le LPS de Escherichia coli O111:B4 uropathogènique peut être décrite par les isothermes de type Langmuir permettant la détermination de la constante d'affinité apparente d'adsorption entre la CPAC et chaque biomolécule. CPAC interagit fortement avec le fibrinogène avec une constante de fixation de 2.2x108 M-1. CPAC a des interactions plus faibles avec l'albumine et AAG, avec les constantes de fixation de 2.4x106 M-1 et 1.5x106 M-1, respectivement. Ces interactions de liaison limiteront la biodisponibilité de la CPAC au site d'action, mettant ainsi en évidence la nécessité d'une meilleure compréhension de la biodisponibilité et la pharmacocinétique de la consommation de canneberge avant d'autres essais cliniques. De plus, CPAC interagit avec le LPS de Pseudomonas aeruginosa 10 de manière fondamentalement différente qu'il interagit avec le LPS de E. coli O111:B4 ou rhamnolipides de P. aeruginosa, soutenant la théorie selon laquelle il y a plusieurs mécanismes par laquelle canneberge empêche les infections bactériennes et la canneberge peut-être plus efficace pour prévenir certaines infections bactériennes.</dc:abstract><dc:abstract>While cranberry consumption has been linked with the prevention of bacterial infections in the urinary tract for many years, our understanding of the bioavailability and mechanisms by which cranberry prevents bacterial infection is limited. Despite frequent use in clinical trials, it is hypothesized that the bioavailability of cranberry derived materials in the urinary tract may be limited due to their interaction with human serum proteins such as albumin, α-1-acid glycoprotein (AAG), and fibrinogen. Upon reaching the urinary tract, cranberry derived materials may interfere with bacterial pathogenesis via interaction with the lipopolysaccharides (LPS) on the bacterial cell surface and the secreted biosurfactant, rhamnolipid. Quartz crystal microbalance with dissipation monitoring (QCM-D) is a sensitive mass sensor that is used in this study to directly investigate the interactions between cranberry derived materials and human serum proteins (albumin, AAG, and fibrinogen) or bacterial components (LPS and rhamnolipid). The binding of cranberry proanthocyanidins (CPAC) to all three serum proteins, the rhamnolipids, and LPS from the uropathogen Escherichia coli O111:B4 can be described by Langmuir-type isotherms allowing the determination of the apparent adsorption affinity constant between the CPAC and each biomolecule. CPAC interacts most strongly with fibrinogen with a binding constant of 2.2×108 M-1. CPAC exhibits weaker interactions with albumin and AAG, with binding constants of 2.4×106 M-1 and 1.5×106 M-1, respectively. These binding interactions will limit the bioavailability of the CPAC at the site of action thus highlighting the need for an improved understanding of the bioavailability and pharmacokinetics of cranberry consumption before further clinical trials. Furthermore, CPAC interacts with LPS from Pseudomonas aeruginosa 10 in a fundamentally different manner than it interacts with E. coli O111:B4 LPS or P. aeruginosa rhamnolipids, supporting the theory that there are multiple mechanisms via which cranberry prevents bacterial infections and that cranberry may be more effective at preventing certain bacterial infections.</dc:abstract><ual:supervisor>Nathalie Tufenkji (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/t722hd09b.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/qz20sx169</ual:fedora3Handle><dc:subject>Engineering - Chemical</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ak3569758j"><dcterms:title>An evaluation of the role of mapping in skill acquisition on digital musical instruments</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Schulich School of Music</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Ghamsari-Esfahani, Mahtab</ual:dissertant><dc:abstract>Les instruments de musique numérique (DMIs) sont de plus en plus populaires dans les laboratoires de recherche ainsi qu'auprès des artistes de la scène musicale expérimentale. Le concept du DMI est très attrayant, cependant peu d'instruments vont au delà de la recherche en laboratoire. Au contraire des instruments acoustiques, les DMIs sont dépourvus de contexte historique ou musical et il n'existe pas de pédagogie permettant d'évaluer l'intéraction avec de tels instruments. Il est possible d'emprunter les méthodes d'évaluation au domaine de l'interaction homme-machine (IHM). Cependant, pour améliorer leur pertinence dans le contexte de 'interaction musicale, ces dernières doivent être enrichies pour intégrer les trois points suivants: l'interface physique, le synthétiseur de son et la partie intermédiaire de mapping. Cette recherche a été conçue autour de l'évaluation d'un DMI appelé "Ballagumi" avec deux objectifs principaux: tout d'abord déterminer dans quelle mesure il est possible d'acquérir des compétences musicales dans la pratique du Ballagumi, en concevant des études qui utilisent des compositions musicales et un cadre d'improvisation; puis déterminer comment la conception du mapping affecte l'acquisition de ces compétences. Pour cela, deux études ont été menées: une étude portée sur la découverte des gestes ainsi que sur l'influence d'aspects purement technologique (par exemple, la  latence) sur la perception de l'instrument; et une seconde étude où des musiciens professionnels, familiers avec les instruments de musique électronique, étaient invités à interpréter des morceaux de musique composés pour l'instrument et aussi à improviser avec deux mappings. Les résultats de l'étude ont révélé un aspect intéressant de l'instrument, à savoir que l'interface elle-même donnait du retour sur l'interaction puisqu'elle possède des qualités haptiques propres. Par conséquent, des deux mappings conçus, le plus simple (un mapping à entrée d'énergie isotonique) s'est vu être privilégié par les participants, car perçu comme plus intuitif, pour l'improvisation plutôt qu'un mapping à entrée d'énergie continue. On a pu aussi noté que, bien que l'instrument et l'algorithme de synthèse sonore soient restés inchangés, le choix du mapping a donné lieu à des impressions clairement différentes pour les interprètes. Les résultats de ces deux études peuvent être utilisés pour améliorer le Ballagumi sur le plan technologique, ainsi qu'au niveau de l'esthétique de l'interface. En plus de démontrer l'importance du mapping pour acquérir des compétences permettant d'improviser avec l'instrument, ces expériences ont permis de montrer qu'il est pertinent de prendre en compte les retours d'informations propres à l'interface physique avant d'avoir recours à des configurations de mapping complexes.</dc:abstract><dc:abstract>Digital Music Instruments (DMIs) have become popular both in research laboratories and among performers of experimental music scenes. The concept of DMIs is very attractive, however few instruments make it beyond a laboratory environment. There is an apparent lack of historical and musical background enjoyed by acoustic instruments as well as a lack of pedagogy used to evaluate the effectiveness of DMIs and DMI players. While initial evaluation methods can be borrowed from the field of Human Computer Interaction (HCI), these methods need to be extended to evaluate the DMIs' integrated layers: the physical interface, the sound synthesizer and the abstract mapping block. This research was designed around the evaluation of a novel DMI called the "Ballagumi" with two main objectives: firstly determine the extent to which it is possible to obtain skills on the Ballagumi in a musical context (using musical compositions as tasks along with an improvisation setting) and secondly to determine how much the design of the mapping block affects the acquisition of skill on the instrument. For this purpose, two studies were conducted, a preliminary study that focused on discovering gestures on the Ballagumi and obtaining an initial perspective of the instrument features (e.g. latency), and a second study where professional musicians experienced in electronic music controllers were invited to play musical tasks and also improvise on the instrument with both mappings. The study results revealed interesting insights on the instrument itself, namely that the physical interface allowed for much of the learning in the interaction as it is built with material that have passive haptic feedback. The results also showed that of the two designed mappings, the simpler choice (a direct energy input mapping) was preferred for improvisation by participants. The choice of mapping resulted in very different impressions of the performance even though the instrument and the sound synthesis layer remained constant. Manipulating the instrument on an intuitive level was made easier by a direct energy input mapping since the physical interfaced required energy input and made use of both hands. The choice of mapping also affected the extent to which participants enjoyed playing the Ballagumi. The results from these two studies can be used to further improve the Ballagumi's sound and physical attributes to have it be used by more musicians. They also not only demonstrate the importance of the mapping block in acquiring skills to the point of improvising on the DMI (confirming in this regard past literature on the subject) but highlight the importance of accounting for the existing feedback before adding levels of complexity to the mapping. </dc:abstract><ual:supervisor>Marcelo Wanderley (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/r494vp27c.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/k3569758j</ual:fedora3Handle><dc:subject>Communications And The Arts - Music</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A4q77fv957"><dcterms:title>Is international commercial arbitration an autonomous legal system?</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Laws</schema:inSupportOf><dc:contributor>Faculty of Law</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Sadafi-Chaghooshi, Farshad</ual:dissertant><dc:abstract>Au cours des dernières décennies, l'arbitrage commercial international a subi de grandes transformations : longtemps utilisé comme simple méthode de résolution des différends internationaux, il est en voie de devenir un système de droit autonome. Avec la globalisation des échanges et des activités humaines et la décentralisation du pouvoir des États vers des acteurs privés, une nouvelle catégorie d'arbitres internationaux a fait son apparition, de nouveaux arbitres qui deviennent à leur tour des agents de changement.        La pluralité de leurs opinions a poussé ces nouveaux acteurs à se questionner sur la viabilité à long terme de la mise en place d'un nouvel ordre juridique arbitral. Diverses théories juridiques mises de l'avant par des experts issus de différents domaines du droit ont permis d'en étudier la légalité et la systématicité. Ce nouvel ordre juridique a ses défenseurs et ses détracteurs. Certains le défendent en invoquant la théorie positiviste du droit basée sur les règles de droit transnationales. D'autres refusent de le considérer comme un système autonome parce certaines règles de droit essentielles n'y sont pas définies et qu'il existe des lacunes structurelles flagrantes en arbitrage international. Ce sont là quelques-unes des grandes questions qui seront débattues dans le présent ouvrage. L'auteur y fera d'abord l'analyse des principaux courants théoriques traitant de la légitimité et de la systématicité de l'arbitrage commercial international et de la mise en place d'un régime juridique dans ce domaine, pour se concentrer ensuite sur les avantages et les désavantages que sa reconnaissance en tant que système de droit autonome pourrait représenter.</dc:abstract><dc:abstract>        In recent decades, the nature of international commercial arbitration has been transformed from a method of dispute resolution to an autonomous legal system.  Globalization and a shift of power from states to private actors have resulted in the emergence of an international arbitration community that eventually produced this kind of transition.         This movement has generated a dynamic discussion over the legality and systematicity of the arbitral legal system.  By applying various legal theories, scholars of different legal systems have analyzed the legality of the arbitral legal system. A few scholars have advocated the concept of this system based on a transnational legal positivism theory.  In contrast, others, because of a lack of essential qualities of law and structural deficiencies in international arbitration, refuse to recognize it as an autonomous legal system. The main objective of the present work is to study the major legal theories about the legality and systematicity of international commercial arbitration, and then to take an overview of the adverse and advantageous consequences of applying the concept of the arbitral legal system.      </dc:abstract><ual:supervisor>Fabien Gélinas (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/q524js13d.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/4q77fv957</ual:fedora3Handle><dc:subject>Social Sciences - Law</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Agt54kr34h"><dcterms:title>Refining the marine reservoir effect in the Northwest North Atlantic</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Geography</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Neulieb, Thomas</ual:dissertant><dc:abstract>Cette recherche vise à determiner si la datation par carbone 14 des grains de pollen peut être utilisée comme méthode alternative de datation des sédiments marins et si l'âge du pollen peut être utilisé pour raffiner la correction due à l'effet réservoir marin (Marine Reservoir Effect) appliquée aux carbonates marins des carottes prélevée le long de la marge de l'est canadien (plateaux continental de Terre-neuve et Néo-Écossais).  Une datation précise est d'une grande importance afin de situer dans le temps des évènements climatiques soudains comme le début et la fin de la période de refroidissement du 8.2 ka, l'oscillation du Dryas récent et l'oscillation du Préboréal et les sédiments correspondant à ces évènements ont été utilisés dans cette étude. Du pollen a été extrait de sédiments océaniques et de zones humides prélevés sur la marge est-canadienne, dans la région de la Baie James et dans les provinces maritimes du Canada et datés au carbone 14 en utilisant la méthode d`accélération de particules et le spectromètre de mass « Accelerator Mass Spectrometry (AMS) ». Les dates basées sur le pollen des sédiments marins ont été comparées avec celles obtenues sur des carbonates marins (coquilles de mollusques et foraminifères) provenant des mêmes niveaux dans les carottes, pour lesquelles le remaniement a été exclu. La validité des dates des carottes  a été évaluée via des corrélations avec des carottes provenant d`autres études, également datées. Des échantillons de pollen de trois zones humides de marrée ont été pris aux  niveaux qui ont été préalablement datés avec des profils de Cs 137 et Pb 210.  Lesâges du pollen de deux autres zones humides ont été comparés avec les âges carbone 14 de macrorestes botaniques. La plupart des dates carbone 14 basées sur le pollen diffèrent de celles basées sur les macrorestes ou les carbonates, avec des différences d'âge de plus de 250 ans, et même de 3000 ans dans un cas.  Dans quelques échantillons, les âges basées sur le pollen sont plus jeunes que ceux des carbonates du même niveau, tel que prévu.  Ces différences entre les âges des carbonates, qui sont affectés par l'effet réservoir marin et les âges du pollen, qui ne sont pas par l'effet réservoir marin, semble indiquer un effet réservoir marin, et la correction d`âge réservoir associée à cet effet pourrait altérer notre compréhension de la chronologie climatique de la marge continentale de l`est de Canada.  Cependant, ces nouveaux âges réservoir n'ont pas encore été validés avec les interprétations stratigraphiques des carottes avec lequelles ils sont associés. Toutefois, dans certaines carottes, les dates du pollen montrent des inversions d'âge.  Des proportions significatives des grains de pollen retravaillés, dans les milieux marins et les zones humides, ont été associées avec des dates polliniques trop âgées.  L'entreposage prolongé des carottes pourrait résulter en des dates carbone 14 trop jeunes, possiblement dû à la croissance de bactéries mais plus de recherche est nécessaire pour vérifier cette hypothèse.  Malgré les difficultés rencontrées, quelques dates basées sur le pollen sont consistentes avec les autres dates carbone 14 du même niveau dans la carotte, suggérant que cette méthode de datation peut fonctionner.  Pour l'instant, plus de recherche est nécessaire pour comprendre les résultats contradictoires obtenus. </dc:abstract><dc:abstract>This research examines whether 14C dating of pollen grains can be used as an alternative dating method for marine sediments and if the pollen ages can be used to refine the value of the Marine Reservoir Effect (MRE) applied to marine carbonates from cores retrieved from along the East Canadian Margin (Newfoundland and Scotian Shelves).  Precise dating is critically important to situate abrupt climate events, such as the onsets and conclusions of the 8.2 ka cooling event, Younger Dryas, and Preboreal Oscillation and sediments thought relevant to these events have been used in my study. Pollen was extracted from ocean and wetland sediments cored from the eastern Canadian margin, James Bay region and Maritime Provinces of Canada and 14C dated using Accelerator Mass Spectrometry (AMS). Pollen dates from ocean sediments were compared with marine carbonate (mollusk shells or foraminifera) dates from the same core levels for which reworking has been excluded. The validity of the core dates was assessed via correlations with cores from other studies. Pollen samples from three tidal wetlands were taken from levels dated with 137Cs and 210Pb profiles. Ages of pollen from two additional wetlands were compared with 14C dates of botanical macrofossils.Most pollen dates vary from 14C dates based on macrofossils or carbonates, with age differences typically exceeding 250 years and over 3000 years in one instance. In some samples pollen ages were younger than the corresponding carbonate, as expected, since pollen ages should not be affected by the MRE. These differences in age between these MRE affected carbonates and the non MRE affected pollen, suggest that a MRE and the associated reservoir age could alter our understanding of the climatic timeline for the eastern Canadian Margin. However, these new reservoir ages have yet to be validated with the stratigraphic interpretations of the cores with which they are associated. In some cores, however, pollen dates show age reversals. Significant proportions of reworked pollen grains in ocean and wetland samples are associated with pollen dates that are too old. Prolonged core storage could result in pollen 14C ages that are too young, possibly because of bacterial growth but more work is needed to verify this hypothesis. Despite the problems encountered, some pollen dates are consistent with other 14C dates from the same core levels, suggesting this dating method can work. At present however, more work is needed to understand the conflicting results obtained.</dc:abstract><ual:supervisor>Elisabeth Levac (Internal/Cosupervisor2)</ual:supervisor><ual:supervisor>Gail L Chmura (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/3r074z34w.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/gt54kr34h</ual:fedora3Handle><dc:subject>Earth Sciences - Palynology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Afj2365627"><dcterms:title>Effect of pressure support ventilation on maximum exercise capacity in individuals with COPD</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>School of Physical and Occupational Therapy</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Anekwe, David</ual:dissertant><dc:abstract>RATIONALE: Non-invasive mechanical ventilation (NIMV) has been used as adjunct to exercise in pulmonary rehabilitation but its acute effect on maximum exercise capacity is not fully understood. Whether NIMV delivered during exercise using a level of ventilatory support titrated to comfort is better than a fixed level of assist is also unknown. OBJECTIVE: The objective of the current study was to evaluate the effect of a fixed and an individually optimized level of pressure support ventilation (PSV) delivered during a symptom-limited incremental bicycle test on maximum exercise capacity, breathing pattern, end-expiratory lung volume (EELV), metabolic parameters, exercise limiting symptoms and respiratory comfort in individuals with COPD. METHODS: Individuals with stable COPD, who do not use supplemental O2 and without known conditions that could limit exercise were recruited. Participants performed three incremental bicycle exercise tests (workload increased by 5 watts/minute) until exhaustion. Tests, separated by 48 hours, were performed in a randomized order (i) without a ventilator (NV), (ii) with 10cmH2O PSV (PSV10) and (iii) with PSV level titrated to patient comfort (PSVt) using a Maquet SERVO-i ventilator in a crossover design. Maximum exercise workload (WLmax), breathing pattern, mouth pressure (Pmo), EELV, metabolic parameters, dyspnea, leg effort, and respiratory comfort were measured and compared using a one-way ANOVA. RESULTS: Eleven individuals (8 males, 3 females) with COPD (FEV1: 49±16% predicted; age: 64±7 years) were studied. The mean PSVt level was 8.2 ± 4.5 cmH2O. There was no difference in the WLmax achieved during the three tests. At rest, PSV10 increased tidal volume (p=0.001) compared to NV, whereas PSVt did not. Inspiratory duration was lower (p=0.009; p=0.004), minute ventilation higher (p=0.001; p=0.02), mean inspiratory flow higher (p&lt;0.001; p=0.006), EELV higher (p=0.003; p=0.002), and end-tidal CO2 lower (p&lt;0.001; p=0.005) with PSV10 and PSVt, respectively. These differences were not observed at peak exercise. Oxygen uptake, Carbon dioxide production, exercise limiting symptoms at peak exercise, and respiratory comfort were not different between the three tests. Mean inspiratory Pmo was lower at peak exercise compared to resting breathing (PSV10: p=0.034, PSVt: p&lt;0.001) and to 50% of WLmax (PSV10: p&lt;0.006, PSVt: p&lt;0.001), despite sustained peak Pmo values. These results suggest that the respiratory muscle unloading, evident with PSV at rest, progressively decreased with increasing exercise. CONCLUSION: Titrated and fixed levels of assist using non-invasive PSV did not improve WLmax due to a progressive decrease in respiratory muscle unloading as exercise intensity increased.</dc:abstract><dc:abstract>RATIONNEL: La ventilation mécanique non-invasive (VMNI) est utilisée comme thérapie complémentaire à la réadaptation pulmonaire. Toutefois, son effet sur la capacité maximale d'exercice n'est pas entièrement compris. En effet, l'impact de la VMNI fourni lors de l'exercice en utilisant un niveau de support ventilatoire ajusté au confort, plutôt qu'un niveau fixe demeure inconnu.  L'objectif de la présente étude est d'évaluer l'effet d'un niveau de ventilation de pression de support (VPS) fixe et optimisé individuellement durant un exercice de capacité maximal avec symptômes limités sur bicyclette, le profile respiratoire, les volumes pulmonaires en fin d'expiration, les paramètres métaboliques, les symptômes limitant l'exercice et le confort respiratoire chez les personnes atteintes de maladie pulmonaire obstructive chronique (MPOC). MÉTHODES: Les personnes avec MPOC stable, n'utilisant pas d'oxygène supplémentaire et sans autres problèmes médicaux connues affectant la capacité à faire de l'exercice ont été recrutées. Les participants ont effectué trois épreuves d'effort à vélo (charge augmentée de 5 watts / minute) jusqu'à l'épuisement. Les épreuves, effectués à intervalle de 48 heures, ont été réalisés dans un ordre aléatoire: 1) sans ventilateur (SV), 2) avec 10cm H2O VPS (VPS10) et 3) un niveau de VPS ajusté selon le confort du patient (VPSt) en utilisant ventilateur Maquet SERVO-i  utilisant un plan d'étude croisé. La charge de travail maximale d'exercice (WLmax), le profile respiratoire, la pression buccale, les volumes pulmonaires en fin d'expiration, les paramètres métaboliques, la dyspnée, l'effort de la jambe et le confort respiratoire ont été mesurés et comparés à l'aide d'une analyse de variance à un facteur. RESULTATS: Onze personnes (8 hommes, 3 femmes) avec MPOC (VEMS1 : 49 ± 16 % prédit; âge : 64 ± 7 ans) ont été étudiés. Le niveau VPSt moyen était de 8,2 ± 4,5 cm de H2O. Aucune différence n'a été mesurée dans la WLmax dans les trois tests. Au repos, avec VPS10 le volume courant a augmenté en comparaison avec le SV (p = 0.001), alors que le VPSt demeure in- changé. La durée inspiratoire était plus basse (p=0.009; p=0.004), la ventilation minute plus élevée (p=0.001; p=0.02), le débit inspiratoire moyen plus haut (p&lt;0.001; p=0.006), les volumes pulmonaires en fin d'expiration plus élevés (p=0.003; p=0.002), and CO2 de fin d'expiration plus bas (p&lt;0.001; p=0.005) avec le VPS10 et VPSt respectivement. Ces différences n'ont pas été observées lors de l'exercice maximal. La consommation d'oxygène, la production de dioxyde de carbone, les symptômes  limitant l'effort maximal de pointe et le confort respiratoire n'étaient pas différents entre les trois épreuves. La pression buccale inspiratoire moyenne était inférieure pour l'exercice maximal  comparé à la respiration au repos (VPS10: p = 0.034, VPSt: p &lt; 0.001) et à 50 % de WLmax (VPS10: p &lt; 0.006, VPSt: p &lt; 0.001), malgré des valeurs de pression buccale maximales soutenues. Ces résultats suggèrent que le déchargement des muscles respiratoires, évident avec le VPS au repos, diminue progressivement avec l'augmentation de l'exercice. CONCLUSION : Les niveaux d'assistance titrée et fixe utilisant une ventilation non-invasive n'a pas améliorés la WLmax dû à une diminution progressive de la décharge des muscles respiratoires avec l'augmentation de l'intensité de l'exercice. </dc:abstract><ual:supervisor>Jadranka Spahija (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/tb09j867j.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/fj2365627</ual:fedora3Handle><dc:subject>Health Sciences - Physical Therapy</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ahd76s3552"><dcterms:title>Monoamine oxidase A gene promoter methylation and impulsive aggression in an offender population with antisocial personality disorder</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Integrated Program in Neuroscience</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Checknita, David</ual:dissertant><dc:abstract>Le trouble de personnalité antisociale est une condition qui se caractérise par un niveau élevé d'agression impulsive ainsi qu'un risque accru de comportements criminels et d'incarcération. Il a été suggéré qu'une activation déficitaire du gène monoamine oxydase A (MAOA) contribuerait à une dysrégulation du système sérotogénique, qui est fortement associé à l'agression impulsive et au trouble de personnalité antisociale. La potentielle contribution des processus épigénétiques modulant l'expression génétique sans altérer le code génomique sous jacent dans la dysrégulation du MAOA chez les individus atteint du trouble de personnalité antisociale n'est pas encore comprise.  L'étude suivante avait comme objectif d'élucider le rôle des processus épigénétiques dans l'altération de l'expression du MAOA et de la régulation de la sérotonine dans une population incarcérée avec un trouble de personnalité antisociale, lorsque comparés à des contrôles sains. Les résultats suggérent que le promoteur d'hyperméthylation MAOA contribue à une réduction de l'expression génétique et à un niveau élevé de sérotonine sanguin chez les incarcérés avec un trouble de personnalité antisociale. Ces résultats sont cohérents avec la littérature suggérant que la dysrégulation du MAOA et de la sérotonine est présente dans les populations antisociales. De plus, nos résultats représentent la première évidence suggérant que les méchanismes épigénétiques pourraient contribuer à la dysrégulation du MAOA chez les incarcérés avec un trouble de personnalité antisociale.Le trouble de personnalité antisociale est une condition qui se caractérise par un niveau élevé d'agression impulsive ainsi qu'un risque accru de comportements criminels et d'incarcération. Il a été suggéré qu'une activation déficitaire du gène monoamine oxydase A (MAOA) contribuerait à une dysrégulation du système sérotogénique, qui est fortement associé à l'agression impulsive et au trouble de personnalité antisociale. La potentielle contribution des processus épigénétiques modulant l'expression génétique sans altérer le code génomique sous jacent dans la dysrégulation du MAOA chez les individus atteint du trouble de personnalité antisociale n'est pas encore comprise.  L'étude suivante avait comme objectif d'élucider le rôle des processus épigénétiques dans l'altération de l'expression du MAOA et de la régulation de la sérotonine dans une population incarcérée avec un trouble de personnalité antisociale, lorsque comparés à des contrôles sains. Les résultats suggérent que le promoteur d'hyperméthylation MAOA contribue à une réduction de l'expression génétique et à un niveau élevé de sérotonine sanguin chez les incarcérés avec un trouble de personnalité antisociale. Ces résultats sont cohérents avec la littérature suggérant que la dysrégulation du MAOA et de la sérotonine est présente dans les populations antisociales. De plus, nos résultats représentent la première évidence suggérant que les méchanismes épigénétiques pourraient contribuer à la dysrégulation du MAOA chez les incarcérés avec un trouble de personnalité antisociale.</dc:abstract><dc:abstract>Antisocial personality disorder (ASPD) is a condition characterized by elevated impulsive aggression and increased risk for criminal behaviour and incarceration. Deficient activity of the monoamine oxidase A (MAOA) gene is suggested to contribute to serotonergic system dysregulation strongly associated with impulsive aggression and ASPD. The potential contribution of epigenetic processes, which modulate gene expression without altering the underlying genomic code, towards the dysregulation of MAOA in ASPD is not yet understood.The current study aimed to elucidate the role of epigenetic processes in altered MAOA expression and serotonin regulation in a population of offenders with ASPD compared to healthy controls. Results suggest MAOA promoter hypermethylation contributes to downregulated gene expression and elevated whole-blood serotonin in offenders with ASPD. These results are consistent with prior literature suggesting MAOA and serotonergic dysregulation in antisocial populations. Further, our results offer the first evidence suggesting epigenetic mechanisms may contribute to MAOA dysregulation in antisocial offenders.</dc:abstract><ual:supervisor>Gustavo Turecki (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/sf268855b.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/hd76s3552</ual:fedora3Handle><dc:subject>Biology - Neuroscience </dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Anp193d23p"><dcterms:title>Crystallization kinetic investigations of calcium sulfate phases in aqueous CaCl2-HCl solutions</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mining and Materials</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Feldmann, Thomas</ual:dissertant><dc:abstract>La cinétique et la qualité cristalline d'un nouveau système de cristallisation réactive menant à la formation des phases de sulfate de calcium avec la génération simultanée de l'acide chlorhydrique (un acide fort, jusqu'à 9 M) ont été étudiées. Les études entreprises dans cette recherche sont reliées à un nouveau procédé de régénération de l'acide chlorhydrique présentement en développement pour des applications industrielles dans des opérations hydrométallurgiques des métaux non-ferreux. Cette thèse considère les aspects suivants: (a) la cinétique de cristallisation du sulfate de calcium α-hémihydraté (HH); (b) la cinétique des transitions de phase du sulfate de calcium; (c) les effets des impuretés sur la cristallisation réactive des phases du sulfate de calcium; et (d) les effets des modificateurs de cristaux dans la qualité cristalline du sulfate de calcium. Concernant (a), les taux de croissance des cristaux de sulfate de calcium α-hémihydraté, normalisés selon l'aire de surface, ont été déterminés dans deux types de solutions–à haute concentration en CaCl2 (HCl 1.4 M, CaCl2 2.8 M) et à haute concentration en HCl (HCl 5.6 M, CaCl2 0.7 M)–avec un réacteur semi-continu opéré à sursaturation constante dans une série de températures allant de 70°C jusqu'à 95°C. Il a été montré que le taux de croissance dépend uniquement du débit d'introduction des réactifs Ca2+ et SO42-, et qu'il obéit à la deuxième loi de cristallisation de von Weimarn reliant la taille du cristal avec le niveau de sursaturation. De plus, il a été trouvé que la largeur de la distribution de tailles des particules diminue lorsque le taux de croissance du cristal augmente. En (b), une étude sur la cinétique des transitions de phase du sulfate de calcium dihydraté (DH) et du HH a été réalisée dans des réacteurs discontinus sur un intervalle de concentrations de CaCl2 (de 0.25 M jusqu'à 4.97 M), de HCl (de 0 M jusqu'à 9.43 M), et de températures (de 40°C jusqu'à 90°C) incluant l'effet de la nucléation. Deux mécanismes de transition ont été identifiés: un mécanisme de dissolution-précipitation pour la transition du DH vers le HH, et un mécanisme à réaction topotactique pour la transformation du HH en AH. Une réduction de la durée de vie des phases métastables, DH et HH, a été trouvée lorsque la température a été augmentée et l'activité avec de l'eau a été réduite. En (c), les effets d'une série d'impuretés cationiques et anioniques (K+,Mg2+,Sr2+,Ba2+,Al3+,Fe2+,Fe3+,La3+,Y3+, F- (fluorure), PO43- (phosphate)) sur la cristallisation réactive des phases du sulfate de calcium dans des solutions à haute concentration de CaCl2 (1.82 M) et HCl (6.29 M) ont été déterminés à 40°C et à 80°C pour le DH et le HH respectivement. Parmi toutes les impuretés, l'incorporation de strontium dans le DH et le HH s'est montrée la plus élevée, par substitution du Ca2+ à cause de la similarité de leurs rayons ioniques. L'autre impureté qui a eu un effet négatif lorsqu'elle a été incorporée de façon significative durant la croissance du cristal est le phosphate. De plus, ces deux impuretés ont engendré une transition de phase du DH vers le HH lorsque certains niveaux de concentration ont été dépassés. Finalement en (d), les effets d'une vaste plage de modificateurs de la morphologie de cristaux ont été examinés dans des solutions de CaCl2-HCl à différentes concentrations. Les additifs les plus actifs ont été le bromure de cétyltriméthylammonium (CTAB), l'acide polyacrylique et l'acide polyvinyle sulfonique. L'utilisation du CTAB a favorisé la formation des plaques épaisses de cristaux, ce qui est d'intérêt industriel, alors que l'acide polyacrylique a engendré la formation des cristaux minces et allongés. Ce comportement avec l'acide polyacrylique, s'explique par la prédilection pour certains plans cristallographiques lors de l'adsorption. L'utilisation de l'acide polyvinyle sulfonique a nui à la nucléation et à la croissance des cristaux, ce qui s'explique par sa structure polymérique.</dc:abstract><dc:abstract>The kinetics and crystal quality of a novel reactive crystallization system leading to the formation of calcium sulfate phases with simultaneous generation of strong hydrochloric acid (up to 9  M ) were investigated. The investigations undertaken in this work relate to a new hydrochloric acid regeneration process currently under industrial development for application in non-ferrous metal hydrometallurgical operations. In particular the thesis is considering the following aspects: (a) the crystal growth kinetics of calcium sulfate α-hemihydrate (HH); (b) calcium sulfate phase transformation kinetics; (c) impurity effects; and (d) crystal modifier effects on calcium sulfate crystal quality. With reference to (a), the surface area-normalized crystal growth rates of calcium sulfate α-hemihydrate were determined in two types of solutions – high CaCl2 (HCl 1.4  M, CaCl2 2.8  M ) and high HCl (HCl 5.6  M, CaCl2 0.7  M ) in a semi-batch reactor operated at constant supersaturation over the temperature range 70°C to 95°C. The growth rate was found to depend only on the reagent supply rate of Ca2+ and SO42– and to obey von Weimarn's second law of crystallization that relates crystal size to supersaturation level. Moreover it was found the width of the particle size distribution to decrease with increasing crystal growth rate. In (b) an investigation of the phase transformation kinetics of calcium sulfate dihydrate (DH) and HH was done by batch experiments over a range of CaCl2 (0.25  M to 4.97  M ), HCl (0  M to 9.43  M ) concentrations and temperatures (40°C to 90°C) including the effect of seeding. Two different mechanisms were identified, a dissolution-precipitation mechanism for the DH to HH and a topotactic reaction mechanism for the HH to AH transformation. Increased temperature and reduced water activity were found to reduce the life-time of the metastable phases DH and HH. In (c) the effects of a wide range of cationic and anionic impurities (K+, Mg2+, Sr2+, Ba2+, Al3+, Fe2+, Fe3+, La3+, Y3+, F– (fluoride), PO43– (phosphate)) on the reactive crystallization of calcium sulfate phases in strong CaCl2 (1.82  M) HCl (6.29  M) solutions at 40°C for DH or 80°C for HH, were determined. It was found by far the uptake of strontium to be the highest in DH and HH among all impurities, caused by a substitution for Ca2+ due to their similar ionic radii. Phosphate was the other impurity that had a detrimental effect as it was taken up significantly during crystal growth. Furthermore, both impurities caused the phase transformation of DH to HH if certain concentration levels were exceeded. Lastly in (d), the effects of a broad range of crystal habit modifiers were investigated in CaCl2-HCl solutions of different strength. The most influential additives were CTAB (cetyltrimethylammonium bromide), polyacrylic acid and polyvinylsulfonic acid. CTAB favored formation of industrially interesting thick slab crystals. In contrast polyacrylic acid resulted in the formation of elongated and thinner crystals explained on the basis of preferential crystal plane adsorption. Polyvinylsulfonic acid had a detrimental effect on DH crystal nucleation and growth explained by its polymeric structure. The ineffectiveness of the majority of additives was attributed to a lack of ionization caused by the high acid solution strength.</dc:abstract><ual:supervisor>George Demopoulos (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/jw827f94m.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/np193d23p</ual:fedora3Handle><dc:subject>Engineering - Metallurgy</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aqv33s081b"><dcterms:title>Encouraging realistic expectations in STEM students: paradoxical effects of a motivational intervention</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Department of Educational and Counselling Psychology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Sverdlik, Anna</ual:dissertant><dc:abstract> University students in STEM disciplines are expected to successfully deal with academic stress while maintaining the well-being and motivation required to achieve superior performance. These students are at risk of overconfidence which can lead to disengagement when students are inevitably faced with disappointment. The present study evaluated the effects of a longitudinal motivational intervention encouraging downgrading expectations (Heckhausen, Wrosch, &amp; Schulz, 2010) for pre-medicine university students (N = 52) on self-reported expectancies (academic expectations and optimism), academic emotions (enjoyment and anxiety), psychological well-being (illness symptoms and depression), and academic achievement (sessional GPA). Contrary to study hypotheses, results showed students in the intervention condition to report higher expectations and optimism on post-test measures, as well as lower GPAs over five academic semesters following the intervention. These paradoxical effects highlight the importance of tailoring previously successful motivational programs to the unique psychological needs and aptitudes of students in STEM disciplines. Keywords: Motivational intervention, STEM, downgrading expectations</dc:abstract><dc:abstract>Les étudiants universitaires dans les disciplines de STEM sont censés de traiter le stress académique en préservant leur bien-être et la motivation nécessaire pour obtenir un rendement supérieur. Ainsi, ces élèves sont à risque d'être trop confiants, ce qui peut provoquer un désengagement lorsque les élèves rencontreront des expériences décevantes. Cette étude a évalué les effets d'une intervention qui encourage un déclassement de leurs attentes auto-déclarées à long terme (Heckhausen, Wrosch, et Schulz, 2010) pour les étudiants universitaires pré-médecine (n = 52) concernant leurs réussites scolaires et leur optimisme, les émotions académiques (jouissance et l'anxiété), le bien-être (symptômes des maladies et la dépression), et la réussite scolaire (AMP). Les résultats ont révélé que, contraire à les hypothèses, les élèves de la condition d'intervention ont démontré des attentes scolaires et des niveaux d'optimisme plus élever, ainsi que les AMP inférieurs pendant une période de plus de cinq semestres académiques après avoir reçu l'intervention. Ces effets paradoxaux soulignent l'importance d'adapter les programmes de motivation éprouvées aux aptitudes et besoins psychologiques spécifiques des élèves dans des disciplines de STEM. Mots clés: intervention de motivation, STEM, déclassement d'attentes</dc:abstract><ual:supervisor>Nathan Hall (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/d504rp57c.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/qv33s081b</ual:fedora3Handle><dc:subject>Education - Adult and Continuing</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Avt150n296"><dcterms:title>Impulsivity in mood disorders: the role of anxiety and substance use comorbidity</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Psychiatry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Iskric, Adam</ual:dissertant><dc:abstract>Contexte: Le trouble dépressif majeur (TDM) et le trouble bipolaire (TB) sont des troubles de l'humeur qui sont associés aux coûts économiques à long terme et la détérioration fonctionnelle. L'impulsivité a été impliquée dans les troubles de l'humeur, principalement dans TB. L'impulsivité est associée à l'agression, le risque de comportement suicidaire, et la détérioration fonctionnelle globale chez les personnes avec un trouble de l'humeur. Des études ont montré que les individus avec le TB ont des niveaux d'impulsivité plus élevés que les témoins, bien que peu d'études aient examiné le lien spécifique entre l'impulsivité et TB contre TDM. En outre, il y a eu peu de recherche relatif à la différenciation entre les niveaux d'impulsivité dans les sous-types TB, y compris le trouble bipolaire de type I (TB-I) et le trouble bipolaire de type II (TB-II). Aussi, l'impulsivité a été liée à d'autres comorbidités psychiatriques, dont les troubles anxieux ainsi que les troubles de toxicomanie. Cependant, le rôle de l'impulsivité chez les personnes avec un trouble de l'humeur et un trouble anxieux ou un trouble de toxicomanie comorbide n'a pas été établi.Objectifs: Il y a deux objectifs pour cette étude. Le premier est de comparer a) TDM et TB sujets et b) TDM, TB-I, et TB-II sur l'impulsivité, spécifiquement l'impulsivité total et les trois dimensions de l'impulsivité: cognitive, motrice, et non-planification. Le deuxième objectif est d'examiner si les troubles anxieux et de toxicomanie, modifient la relation entre les troubles de l`humeur et l`impulsivité. Méthodes: 115 patients en consultation externe avec un diagnostic de TDM (N = 45), TB-I (N = 53), ou TB-II (N = 17) du Programme des troubles de l'humeur du Centre Universitaire de Santé McGill ont été recrutés. L'Entrevues Cliniques Structurées pour le DSM-IV ont été menées pour diagnostiquer les troubles psychiatriques. L'impulsivité a été mesurée à l'aide du questionnaire de l'Échelle d'Impulsivité de Barratt (BIS- 11), qui divise l'impulsivité totale en trois dimensions: cognitive, motrice, et non-planification. Un examen des dossiers médicaux a été réalisée pour obtenir plus d`informations sur les troubles psychiatriques. Résultats: L'impulsivité totale, l'impulsivité motrice, et l'impulsivité non-planification étaient plus élevées chez les sujets avec le TB par rapport aux sujets avec le TDM. Les sujets avec le TB-I avaient l`impulsivité totale et l'impulsivité motrice significativement plus élevées par comparaison avec les sujets avec le TDM, et les sujets avec le TB-II avaient l'impulsivité totale, cognitive, motrice, et non-planification significativement plus élevées par rapport aux sujets avec le TDM. Enfin, l'impulsivité cognitive était significativement plus élevée chez les sujets avec le TB-II par rapport aux sujets avec le TB-I. Les sujets avec le TB et un trouble anxieux avaient des niveaux d'impulsivité cognitive plus élevés par rapport aux sujets avec le TDM et un trouble anxieux. À propos des diagnostics de toxicomanie, avoir une comorbidité de toxicomanie est associée aux niveaux d'impulsivité totale, motrice, et non-planification plus élevées chez les sujets avec le TB par rapport aux sujets avec le TDM. Il n'y avait pas d'interaction entre les comorbidités et le diagnostic de trouble de l'humeur sur l'impulsivité. Conclusions: L'impulsivité est un facteur important dans la présentation clinique des troubles de l'humeur. Nos résultats soulignent l'importance de l'impulsivité chez les individus avec le TB ainsi que chez les individus avec le TB et un diagnostic comorbide. On a besoin de plus de recherche sur l'impulsivité chez les patients avec les sous-types TB avec des comorbidités psychiatriques pour aider les professionnels de la santé à bien évaluer l'impulsivité chez les sujets avec le TB et à bien traiter l'agressivité impulsive, des comportements suicidaires, et d'autres corrélats cliniques de l'impulsivité.</dc:abstract><dc:abstract>Background: Major depressive disorder (MDD) and bipolar disorder (BP) are debilitating mood disorders that are associated with both long-term economic costs and functional impairment. Impulsivity has been implicated in mood disorders, and primarily in BP. Impulsivity is strongly associated with aggression, risk of suicidal behaviour, and overall functional impairment in mood disorders. Studies have found that BP individuals have significantly higher levels of impulsivity than controls, although fewer studies have examined the specific link between impulsivity and BP compared to MDD. Furthermore, there has been very little research pertaining to the differentiation between levels of impulsivity in BP subtypes, including bipolar type I (BPI) and bipolar type II (BPII). As well, impulsivity has been linked with other psychiatric comorbidities, including anxiety disorders as well as substance use disorders (SUDs). However, the role of impulsivity in mood disorders with anxiety disorder or SUD comorbidity has not been established. Objectives: There are two objectives for this study. The first objective is to compare a) MDD and BP subjects and b) MDD, BPI, and BPII subjects on impulsivity, specifically total impulsivity as well as each of the three dimensions of impulsivity: attentional impulsivity, motor impulsivity, and nonplanning impulsivity. The second objective is to examine if comorbid lifetime (a) anxiety disorders or (b) SUDs modify the association between mood disorder (BP and MDD) and impulsivity, specifically total impulsivity as well as each of the three dimensions of impulsivity. Methods: 115 euthymic outpatients with a primary DSM-IV diagnosis of MDD (N=45), BPI (N=53), or BPII (N=17) from the Mood Disorders Program of the McGill University Health Centre were recruited. The Structured Clinical Interview for DSM-IV (SCID) was conducted to diagnose psychiatric disorders. Impulsivity was measured using the self-report questionnaire - Barratt Impulsiveness Scale (BIS-11) - which assesses three dimensions of impulsivity: attentional, motor, and nonplanning impulsivity. A medical chart review was conducted to obtain socio-demographic and psychiatric disorder information. Results: BP subjects had significantly higher total impulsivity as well as motor and nonplanning impulsivity compared to MDD subjects. With regards to specific BP subtypes, BPI subjects had significantly higher total impulsivity and motor impulsivity than MDD subjects, and BPII subjects had significantly higher total impulsivity, as well as attentional, motor, and nonplanning impulsivity than MDD subjects. Finally, BPII subjects had significantly higher attentional impulsivity than BPI subjects. With regards to lifetime psychiatric comorbidity, mood disorder subjects with anxiety disorders had higher levels of attentional impulsivity. For lifetime SUD diagnosis, SUD comorbidity was associated with higher levels of total impulsivity as well as motor and nonplanning impulsivity in mood disorder subjects. There was no interaction between anxiety or SUD comorbidity and mood diagnosis on total or dimensional impulsivity. Conclusions: Impulsivity is an important factor in the clinical presentation of mood disorders. Our results emphasize the importance of impulsivity primarily in BP, as well as impulsivity in BP individuals with a comorbid anxiety disorder or SUD. Further research measuring impulsivity in BP subtypes with different psychiatric comorbidities (e.g., anxiety disorders, SUDs) would help guide health professionals and clinicians in the assessment of impulsivity and the treatment of impulsive aggression, suicidal behaviours, and other clinical correlates of impulsivity in BP subjects.</dc:abstract><ual:supervisor>Nancy Low (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/nz806336f.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/vt150n296</ual:fedora3Handle><dc:subject>Psychology - Clinical</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A3484zk976"><dcterms:title>Ovidian influences in Seneca's Phaedra</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Department of History and Classical Studies</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Mocanu, Alin</ual:dissertant><dc:abstract>The following thesis is an examination of the way Seneca constructs Phaedra, the main character of an eponymous tragedy. It aims to prove that the tragedian uses a mixing of mainly two literary genres, tragedy and elegy, and it analyzes the way the elegiac genre is transformed so it can fit this new generic hybrid. Seneca finds inspiration for the elegiac topoi in Ovid's love poems. The author uses the recurrent elegiac convention involving a soft man, the lover, and a dominant woman, the beloved, but he reverses this literary tradition: Phaedra becomes the lover while Hippolytus becomes the beloved. Besides a series of elegiac topoi such as fiery love metaphors, servitium amoris or symptoms of love, Seneca also deals with the erotic hunting. Roman love elegy often associates the lover, the feeble man, with a hunter, while it represents the beloved, the dominant woman, as his prey. In Phaedra, Hippolytus, a true hunter, becomes an erotic prey, while the female character takes on the role of the erotic predator, which causes the young man's tragic death.</dc:abstract><dc:abstract>Dans ce mémoire de maîtrise on examine la manière dont Sénèque construit Phèdre dans la tragédie portant le même nom. On prouve que pour créer son personnage, le tragédien romain mélange deux genres littéraires : la tragédie et l'élégie. On analyse aussi la façon dont Sénèque altère le genre élégiaque afin qu'il puisse créer un nouveau genre littéraire hybride. L'auteur trouve son inspiration pour les topoi élégiaques dans les poèmes érotiques ovidiens. En dépit de l'utilisation d'une convention élégiaque par excellence qui concerne la relation entre un amoureux, un homme faible, et une bien-aimée, une femme forte et dominante, Sénèque inverse ces éléments et Phèdre devient l'amoureux, tandis qu'Hyppolite se voit attribué le rôle du bien-aimé. À part une série de topoi élégiaques comme les métaphores érotiques du feu, le servitium amoris ou les symptômes de l'amour, le tragédien emploie aussi le lieu commun de la chasse érotique. L'élégie romaine associait très souvent l'homme faible à un chasseur et la femme forte à sa proie. Dans Phèdre, Hippolyte, un vrai chasseur, devient une proie érotique, tandis que le personnage féminin prend le rôle du prédateur, ce qui mène le jeune homme à une fin tragique.</dc:abstract><ual:supervisor>Charles Gladhill (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/sb397c553.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/3484zk976</ual:fedora3Handle><dc:subject>Literature - Classical </dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Apr76f6962"><dcterms:title>Dynamic performance of cemented rockfill under blast-induced vibrations</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mining and Materials</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Emad, Muhammad Zaka</ual:dissertant><dc:abstract>Dans cette thèse, une nouvelle méthodologie pour la conception de CRF sous des contraintes statiques et dynamiques est développée. Premièrement, un examen de la littérature existante à été accompli. Les opérations minières employant le CRF sont examinées ainsi que les programmes d'essai en laboratoire et les études de modélisation numérique effectuées pour le CRF.  Basés sur l'examen de la littérature existante, deux modèles numériques pour le CRF on étés développé en utilisant FLAC3D. Le premier modèle considère un pendage de 67 degrés pour les chantiers, lorsque l'autre considère des chantiers verticaux. Ces modèles numériques sont ensuite utilisés pour l'étude de différents aspects de la pratique du CRF tels que les conditions de chargement, les vibrations de tir, les propriétés du CRF, la méthode de placement du CRF, ainsi que la ségrégation du CRF. Un modèle pour la mine de l'étude de cas est ensuite construit en tenant en compte les conditions du site ainsi que les données geo-mecaniques fournies par la mine. Le modèle pour la mine de l'étude de cas est calibré selon les mesures des contraintes in-situ effectués à la mine. La séquence d'abattage et de remblayage est ensuite simulée avec le modèle numérique calibré et les chantiers secondaires sont extraits en trois étapes. Les vibrations de tir in-situ sont mesurées à la mine de l'étude de casé. Deux géophones sont installés : un à l'intérieur du CRF et l'autre à la surface du CRF, et les trois sautages de production du chantier adjacent sont enregistrés. La procédure de l'installation et les résultats obtenus avec le géophone sont présentés dans la thèse. Les résultats sont utilises pour valider le modèle numérique. Le chargement de tir ainsi que les coefficients d'amortissement sont extraits d'un modèle de cavité en calculant les vibrations de tir à la même position que celle du site. Le modèle est validé pour les 3 étapes de l'abattage du chantier et les résultats sont présentés. Finalement, une étude sur le contrôle de stabilité du CRF est effectuée. Cette étude englobe un scenario de base d'un chantier planifié à la mine de l'étude de cas, une stratégie d'abattage sélectif du minerai de haute teneure, une stratégie ou une couche de minerai est laissée dans le chantier, et une variation verticale planifiée des propriétés du CRF. De plus, une étude paramétrique est conduite pour améliorer la stabilité du CRF en variant les propriétés du CRF. Les tendances possibles sont présentées. Tous résultats comparent le chargement statique avec le chargement dynamique sur le CRF. Les résultats incluent une comparaison des contraintes du remblai ainsi que le profile de la vitesse de crête des particules. Les résultats de toutes les analyses sont présentés avec les constatations et conclusions.</dc:abstract><dc:abstract>In this thesis, a new methodology for the design of CRF under static and dynamic loading conditions is developed. First a comprehensive literature review of the backfill material is accomplished which included a review of CRF operations and laboratory testing programs. This is followed by a review of numerical modelling studies performed on CRF. Based on the findings from literature review, two numerical models for CRF are developed using FLAC3D code. One of the models considers stopes to be inclined at about 67 degrees and the other considers vertical stopes. The numerical models are then used to study different aspects of CRF practice including the loading conditions, blast-induced vibrations from adjacent production stopes, vertical block mining method, CRF properties, CRF placement method and segregation. A case study mine CRF model is then constructed in accordance with site conditions and the geomechanical data provided by the mine. The case study mine numerical model is calibrated with in-situ stress measurements previously conducted at the case study mine. Mining and backfilling sequence is simulated with the calibrated model and the secondary stope is mined out in three lifts. In-situ blast vibration monitoring experiment in CRF is performed at the case study mine. Two geophones are installed: one inside CRF and the other on the surface of the CRF, and all three production blasts of the adjacent secondary stope are recorded. The detailed procedure, installation and results are presented in the thesis. The results are also used for numerical model calibration. To calculate damping coefficient for the model and blast load magnitude an equivalent cavity model is constructed. The equivalent cavity model is applied with reduced borehole pressure and the model is compared with charge-weight scaling law. The blast load and damping coefficients are extracted from equivalent cavity model and applied as input parameters for the CRF model. The numerical model is calibrated using blast load and damping coefficients obtained from the equivalent cavity model by computing the blast vibrations at a location similar to the one on site. The model is calibrated for all three blast lifts and results are presented. Finally, a CRF failure control study is carried out which encompasses the base case scenario of a planned stope at the case study mine, selective mining strategy for mining high grade ore, strategy of leaving an ore skin, tactically varying CRF properties vertically in CRF. In addition a parametric study is conducted to improve CRF stability by varying CRF properties and possible trends are presented. All results compare static loading versus blast loading scenario on CRF. The results include comparison of backfill stresses and profiles of peak particle velocity. Results of all analyses are presented along with the findings, conclusions, suggestions for future work and statement of contribution.</dc:abstract><ual:supervisor>Hani Mitri (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/8g84mq76d.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/pr76f6962</ual:fedora3Handle><dc:subject>Engineering - Mining</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ank322h96p"><dcterms:title>Mapping Gaussian belief propagation on the graphics processing unit</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Hosseini-Doust, Zahra</ual:dissertant><dc:abstract>Avec l'introduction d'unités de traitement graphiques programmables (GPU) durant la dernière décennie, les systèmes informatiques hétérogènes sont devenus plus populaires. On pense qu'en exploitant la puissance des nombreux cœurs du GPU, de nombreuses applications pourront bénéficier de meilleures performances dans un avenir rapproché. Cependant, dans la plupart des cas le fait de porter des applications vers le GPU ne peut pas être automatisé à cause de l'architecture unique du GPU. Les problèmes de mappage sur le GPU ont été l'objet de recherches dans de nombreux domaines. Beaucoup de problèmes en science et en ingénierie consistent simplement à résoudre systèmes d'équations linéaires creux. Étant donné que les systèmes creux qui croissent à l'aide de solveurs itératifs classiques ne seront pas réalisables, un des nouveaux solveurs itératifs proposés dans la littérature récente est la méthode de croyance des propagations gaussiennes (GABP). Cette méthode utilise des messages de mise à jour récursifs sur un modèle graphique. Dans ce travail, une variante de l'algorithme GABP fut mise en place sur le GPU. La mise en œuvre a été testée avec succès avec des données FEM (méthode des éléments finis). Les implémentations parallèles ont atteint une amélioration du temps d'exécution jusqu'à 4 × par rapport à l'implémentation de série du CPU. </dc:abstract><dc:abstract>With the introduction of programmable graphical processing units (GPU) in the last decade, Heterogeneous computing systems have become more popular.  It has been predicted that by leveraging the power of the GPU's many cores, many applications can experience improved performance in the near future. However porting applications to the GPU in most cases cannot be automated due to the GPU's unique architecture. Mapping problems on the GPU has been researched in many diverse fields. Many problems in science and engineering come down to solving sparse systems of linear equations. Nevertheless conventional iterative solvers are not feasible tools for large sparse systems. One of the novel iterative solvers proposed in recent literature is the Gaussian belief propagation (GaBP) method. This method uses recursive message updates on a graphical model.  In this work a variant of GaBP algorithm was implemented on the GPU. The implementation was successfully tested with FEM (finite element method) data. The parallel implementations achieved up to 4× improvement in execution time, compared to the serial CPU implementation.  </dc:abstract><ual:supervisor>Warren Gross (Internal/Cosupervisor2)</ual:supervisor><ual:supervisor>Dennis Giannacopoulos (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/6m311s722.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/nk322h96p</ual:fedora3Handle><dc:subject>Engineering - Electronics and Electrical</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Az890rx86d"><dcterms:title>A multidimensional approach to food security and non-traditional export agriculture: a case study in Guatemala</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Natural Resource Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Méthot, Josée</ual:dissertant><dc:abstract>La sécurité alimentaire comporte quatre dimensions importantes: la disponibilité des produits alimentaires, l'accessibilité, la qualité de l'alimentation, et la stabilité des systèmes alimentaires. Bien que le concept de la sécurité alimentaire soit maintenant établi, il demeure encore difficile à mesurer. Dans le premier chapitre de cette thèse, je fais une revue des indicateurs utilisés pour évaluer les quatre dimensions de la sécurité alimentaire à l'échelle des ménages familiaux, et je mets de l'avant qu'une approche multidimensionnelle d'analyse et de mesure de la sécurité alimentaire est nécessaire pour comprendre les impacts des stratégies de développement sur la sécurité alimentaire des ménages. Ceci est un domaine de recherche important car beaucoup de stratégies visant à améliorer la sécurité alimentaire dans les pays en voie de développement se concentrent sur l'augmentation du revenu (considéré comme un indicateur de l'accès) et négligent souvent de considérer les trois autres dimensions. Une dimension qui est souvent ignorée est la stabilité des systèmes alimentaires. Dans le deuxième chapitre de cette thèse, je présente un cas d'étude de la culture de brocoli au Guatemala, destinée à l'exportation. J'explore les implications de cette culture non-traditionnelle d'exportation sur la sécurité alimentaire locale dans la perspective que cette culture est favorisée dans cette région en tant que moyen d'augmenter les revenus de petits exploitants et leur sécurité alimentaire. J'emploie une approche multidimensionnelle pour explorer les quatre dimensions de la sécurité alimentaire et je compare la sécurité alimentaire des producteurs de brocoli (adopteurs) avec celle des producteurs de maïs (non-adopteurs) dans la communauté de Chilascó, au centre du Guatemala. J'ai constaté que la disponibilité des produits alimentaires et l'utilisation des aliments n'étaient pas différentes entre les deux groupes. Bien que les revenus gagnés par les adopteurs étaient significativement plus élevés que les non-adopteurs (40% plus élevé), les gains de revenus ne se traduisaient pas par une amélioration de l'accès aux aliments selon les indicateurs utilisés. La majorité des adopteurs et des non-adopteurs confondus ont été classés comme modérément à extrêmement insécures face à l'alimentation selon l'Échelle de l'Accès déterminant l'Insécurité Alimentaire des Ménages.  Ce travail démontre que le revenu peut améliorer la sécurité alimentaire, mais ne le garantit pas. Les ménages dans le tercile supérieur du revenu avaient une diversité diététique significativement plus élevée que les ménages du tercile inférieur. En termes des indicateurs liés à la stabilité du système alimentaire, les adopteurs ont appliqué deux fois plus de fumier par hectare, trois fois plus d'engrais minéraux par hectare, et avaient un impact environnemental plus élevé associé à l'utilisation de pesticides. Dans l'ensemble, il y a des différences entre le succès des différentes dimensions de la sécurité alimentaire, où certains indicateurs s'améliorent (par exemple, le revenu), d'autres restent inchangés (par exemple, la production des cultures vivrières), et d'autres se dégradent (par exemple, le service écosystémique de la lutte biologique contre les ravageurs) pour les adopteurs comparés aux non-adopteurs. Mes résultats démontrent que les approches étroites de l'analyse de la sécurité alimentaire, souvent axées sur le revenu, peuvent masquer des différences importantes entre les quatre dimensions de la sécurité alimentaire. Les recherches sur les implications de l'agriculture d'exportation non traditionnelle sur la sécurité alimentaire doivent aller au-delà du concept dualiste de sécurité alimentaire (meilleur / pire) afin de mieux cibler les interventions. Cela nécessitera une prise en compte beaucoup plus systématique des quatre dimensions de la sécurité alimentaire dans les évaluations et la planification du développement.</dc:abstract><dc:abstract>There are four important dimensions of food security: food availability, food access, food utilization and food system stability. However, although the concept of food security has come of age, food security remains difficult to measure across these dimensions. In the first chapter of this thesis, I review the concepts and indicators used to assess the four dimensions of food security at the household level, and argue that a multidimensional approach to food security analysis is fundamental to understanding the overall impacts of development strategies on the food security of households. This is an important area of research because many strategies to improve food security in the developing world focus on income (supposedly an indicator of food access) and neglect to consider the other three dimensions of food security and associated indicators. Food system stability, in particular, is often overlooked in assessments of food security.In the second chapter of this thesis, I present a case study exploring the food security implications of farming broccoli for export in Guatemala, recognizing that broccoli, a non-traditional export crop, is widely promoted in this region as a means to increase smallholder incomes and food security. I use a multidimensional approach to explore the four dimensions of household food security, and compare the food security of broccoli farmers (adopters) and corn farmers (non-adopters) in the community of Chilascó, in Central Guatemala. Neither food availability nor food utilization differed significantly between adopters and non-adopters. Although adopters earned significantly higher income (40%) than non-adopters, income gains did not translate into improvements in food access according to outcome indicators. The majority of adopters and non-adopters alike were categorized as moderately to extremely food insecure according to the Household Food Insecurity Access Scale. Households in the top income tercile had significantly higher dietary diversity (an indicator of food access) compared to households in the bottom tercile. A nuanced conclusion from this work is that income can lead to positive food security outcomes, but does not guarantee them. In terms of food system stability, adopters applied twice as much manure per hectare, three times more inorganic fertilizer per hectare, and had a higher environmental impact associated with pesticide use. Taken together, there are trade-offs among the different dimensions of food security, whereby some indicators improve (e.g. income), others remain unchanged (e.g., staple crop production), and others degrade (e.g. the ecosystem service of biological pest control) for adopters relative to non-adopters. My results show that narrow, often income-oriented approaches to food security analysis may mask important differences among the four dimensions of food security. Future research into the food security implications of non-traditional export agriculture must move beyond the dualistic understanding of food security outcomes (better/worse) in order to better target interventions. This will require a systematic consideration of all four dimensions of food security in assessments and development planning. </dc:abstract><ual:supervisor>Nicolas Kosoy (Internal/Cosupervisor2)</ual:supervisor><ual:supervisor>Elena Bennett (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/bn999995s.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/z890rx86d</ual:fedora3Handle><dc:subject>Agriculture - General</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A8623j220z"><dcterms:title>A coupled finite-discrete element framework for soil- structure interaction analysis</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Civil Engineering and Applied Mechanics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Tran, Viet</ual:dissertant><dc:abstract>Modeling soil-structure interaction problems involving granular material and large deformation is a challenging task particularly for geotechnical engineering. Using standard finite element methods has been found to be inefficient to model soil-structure interaction at the soil particle scale. Soil-structure interactions such as erosion void around tunnel lining and geogrid reinforcement may not be properly captured using the finite element method. The discrete element method, on the other hand, has proven its efficiency in modeling the behavior of granular soil domains at the microscopic scale. It is however not suitable to model structural elements using this numerical method due to the continuum behavior of the structure. The coupling of the finite and discrete element methods, which takes advantages of the two methods, is a promising approach to model such geotechnical engineering problems. This thesis is devoted to develop a coupled Finite-Discrete element framework for soil-structure interaction analysis and validate the developed algorithm by comparing numerical simulations with experimental data. The research results have been published in refereed journals and conference proceedings amounting to 3 journal papers and 5 conference papers. These papers are compiled to produce 7 chapters and 1 appendix in this manuscript-based thesis. Experimental and discrete element investigations of earth pressure acting on cylindrical shaft are first presented along with a new gravitational packing technique that has been used to replicate the real sample packing process. Results from the numerical simulation and experimental work are then compared. The efficiency of the discrete element method in solving problems involving granular material and large deformation is demonstrated. The rest of the thesis is devoted to describe the development of a three-dimensional coupled Finite-Discrete element method and its implementations. To analyze a given soil-structure interaction problem using the developed coupled Finite-Discrete element framework, the structure is modeled using finite elements while the soil is modeled using discrete elements. Interface elements are used to ensure the force transmission between the finite and discrete element domains. Explicit time integration is used in both the finite and discrete element calculations. Different damping schemes are applied to each domain to relax the system. A multiple-time-step scheme is applied to optimize the computational cost. The developed coupled Finite-Discrete element framework is used to investigate selected soil-geogrid interaction problems including pullout test of biaxial geogrid embedded in granular material, strip footing over geogrid reinforced sand and geogrid-reinforced fill over strong formation containing void. The results of the numerical analysis are compared with experimental data. Micro-mechanical behavior of the soil domain is analyzed and displacements, stresses and strains developing in the geogrid are investigated. Conclusions and recommendations are made regarding the three-dimensional soil-structure interaction using the discrete element and coupled Finite-Discrete element methods. </dc:abstract><dc:abstract>La modélisation de l'interaction sol-structure présent plusieurs défis, particulièrement dans les sols granulaires et pour des grandes déformations. L'analyse par éléments finis est souvent utilisée mais celle-ci ne permet pas une modélisation à l'échelle des particules de sol. Ce dernier type d'analyse est requis pour la modélisation de problèmes d'interaction complexe tels que ceux de l'érosion des sols autour de l'enveloppe d'un tunnel ou du comportement d'une membrane géotextile. La méthode de modélisation par éléments discrets est un moyen efficace et reconnu pour modéliser le comportement granulaire des sols au niveau microscopique. Par contre, cette méthode n'est pas appropriée pour modéliser les structures solides et continues. Le couplage entre les éléments finis et les éléments discrets est une technique prometteuse qui combine les avantages des deux méthodes. Cette thèse est consacrée au développement de l'analyse couplée Éléments finis-Éléments discrets pour l'interaction sol-structure et à la validation des algorithmes par une comparaison des simulations numériques avec des résultats expérimentaux. Les résultats de la recherche ont été publiés dans les journaux avec comité de lecture (3 articles) m et dans des comptes-rendus de conférence (5 articles).  La thèse est soumise sous la forme d'une thèse avec manuscrits et comporte 7 chapitres principaux et une annexe. Le premier cas étudié est celui de la pression des sols sur un cylindre. Un nouvel algorithme de placement des particules par gravité et de compaction est proposé afin de mieux représenter le processus de préparation des échantillons en laboratoire. Les résultats de simulation sont comparés et validés par rapport aux résultats expérimentaux et démontrent l'applicabilité de la procédure pour l'analyse du comportement des matériaux granulaires pour des grandes déformations. Les chapitres suivants sont dédiés au développement d'analyses ridimensionnelles couplées et à leur validation. Les éléments structuraux sont modélisés par éléments finis tandis que le sol est modélisé par des éléments discrets. Des éléments spéciaux sont développés pour effectuer le couplage entre les deux domaines. Une intégration numérique du type explicite est utilisée pour tous les calculs dans le domaine temporel. Différents types d'amortissement sont utilisés pour chacun des domaines (finis ou discrets) afin de stabiliser le système. Une approche avec intervalles de temps multiple est utilisée afin d'optimiser le temps de calcul. La procédure est utilisée afin d'analyser la résistance à l'arrachement d'une membrane géotextile incorporée dans un milieu granulaire, et le comportement d'une semelle de fondation reposant sur un dépôt de sable renforcé avec des géotextiles au-dessus d'une cavité profonde. Les résultats des simulations numériques sont comparés aux données expérimentales. Les déplacements, les contraintes et les déformations dans le géotextile et le sol sont analysés. Des conclusions et des recommandations sont formulées pour l'analyse tridimensionnelle couplée de l'interaction sol-structure.</dc:abstract><ual:supervisor>Mohamed Meguid (Supervisor1)</ual:supervisor><ual:supervisor>Luc E Chouinard (Supervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/3197xp97z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/8623j220z</ual:fedora3Handle><dc:subject>Engineering - Civil</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Am326m518f"><dcterms:title>Behavior integration for Prometheus using real world ant colony algorithm</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>School of Computer Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Natanasihamani, Hariharan</ual:dissertant><dc:abstract>Prometheus aims to explore artificial intelligence in a controlled but flexible environment by mimicking the properties of the real world using a swarm intelligence implementation. Swarm Intelligence has been used for solving problems in the domain of self organization, complexity and collective intelligence for a group of agents. The collective behavior of the entity considered here - ants, are modeled as a decentralized and self-organized system in which the ants communicate indirectly and thrive by modifying the environment. This novel approach combines the widely established stigmergy theory with real-time fluid dynamics by using Pheromones and  the Navier-Stokes equations respectively to subject the environment to natural conditions like wind, and spread and decay of smell thus making the environment more suitable to real time conditions. The chosen real-time fluid dynamics method proves to be computationally fast, robust and far more realistic than traditional approaches. Also, for evaporation, instead of choosing a random fixed value for every timestep, we take into consideration the effect of temperature, vapor pressure, wind and humidity on evaporation and consequences of that. It is hoped that this model will be a step closer to achieving results substantially closer to the real world and also, observing the changes that the aforementioned natural properties might impose on the experimental world. </dc:abstract><dc:abstract>Le projet d'intelligence artificielle Prometheus vise à explorer, dans un environnement contrôlé mais  flexible, les propriétés du monde réel sur une intelligence en essaim. L'intelligence distribuée a été utilisée afin de résoudre les problèmes dans le domaine de l'auto-organisation, la complexité et l'intelligence collective d'un groupe d'agents. Le comportement collectif de l'entité considérée, ici la fourmi, est modélisé comme un système décentralisé et auto-organisé dans lequel les fourmis communiquent indirectement et prospèrent en modifiant l'environnement. Celle nouvelle approche combine la théorie de stigmergie avec la mécanique des fluides, utilisant respectivement les phéromones et les équations de Navier-Stokes, afin de soumettre à l'environnement des conditions naturelles comme le vent ou encore la propagation et la désintégration de l'odeur. Ainsi l'environnement correspond mieux à des conditions réelles. La méthode de mécanique des fluides en temps réel choisie, s'avère être rapidement calculable, robuste et beaucoup plus réaliste que les approches traditionnelles. De plus, pour modéliser l'évaporation, au lieu de choisir une valeur aléatoire fixée pour chaque itération, nous prenons en compte l'effet de la température, de la pression de la vapeur, du vent, de l'humidité de l'évaporation et leurs conséquences. Nous pensons que ce modèle contribuera à l'obtention de résultats nettement plus proches du monde réel et à l'observation des changements que les propriétés naturelles susmentionnées pourraient imposer à l'environnement expérimental.</dc:abstract><ual:supervisor>Joseph P Vybihal (Internal/Cosupervisor2)</ual:supervisor><ual:supervisor>Mathieu Blanchette (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/0z709104h.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/m326m518f</ual:fedora3Handle><dc:subject>Applied Sciences - Computer Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aq237hv899"><dcterms:title>The laws of Star Wars-the need for a 'manual of international law applicable to space warfare'</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Laws</schema:inSupportOf><dc:contributor>Institute of Air and Space Law</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Blake, Duncan</ual:dissertant><dc:abstract>Today there are over 1,000 active satellites in orbit and the number of States directly involved in launching or operating satellites has increased substantially since the dawn of the space age. Even States that have no direct involvement in launching or operating satellites rely heavily on such space infrastructure: for television, radio, banking, communications, transport, agriculture, mining, and especially for modern military services. Yet, those same satellites are under increasing threat from 100,000s of pieces of space debris and the actual or potential proliferation of weapons and other means capable of destroying or disrupting satellites. There is also increasing competition for use of the limited radio frequency spectrum that is essential for the operation of such satellites. Concurrently, there are a range of factors unsettling global security generally and the dominance of the US and Europe specifically. The proliferation of nuclear weapons and ballistic missiles, as their means of delivery, is a significant concern. Ballistic missiles have a trajectory through space and ballistic missile defence also rely on space-based infrastructure. Financial and other constraints have made global powers more inward-looking, less likely to deploy forces globally, except through the sort of 'remote reach' capabilities that rely on space infrastructure for their effectiveness (such as uninhabited aerial vehicles and cyber warfare). So space-based infrastructure is a key element in global security, yet it is also increasingly vulnerable to the threats described above – including space weapons. Warfare in space is becoming a real possibility. Outer space is not, though, a new, wild and lawless frontier. There is a legal framework of growing complexity. However, the treaties specific to the space domain, as well as recent initiatives to augment those treaties, barely contemplate warfare. The law of war is applicable to the space domain and potentially provides a comprehensive set of norms of State behaviour that restrains the recourse to force by States in the space domain. There is considerable uncertainty about how particular rules of the laws of war apply to the space domain. It would be in the strategic interests of all States to have a clear and authoritative statement on how the rules of the law of war apply to the space domain. The question is how to achieve such a clear and authoritative statement. Useful analogies can be found in the process and success of the San Remo Manual on International Law Applicable to Armed Conflict at Sea, the Harvard Manual on International Law Applicable to Air and Missile Warfare and the Tallinn Manual on International Law Applicable to Cyber Warfare. These manuals have been drafted by globally-recognised legal and technical experts in each domain expressing personal opinions on the lex lata. They have avoided many of the challenges of State negotiations on similar topics, yet the manuals have had a significant impact in each of their domains. There now needs to be a 'Manual of International Law Applicable to Space Warfare'.</dc:abstract><dc:abstract>Aujourd'hui, il ya d'autres 1.000 satellites actifs en orbite et le nombre d'États directement impliqués dans le lancement ou l'exploitation de satellites a augmenté considérablement depuis l'aube de l'ère spatiale. Même les États qui n'ont pas d'implication directe dans le lancement ou l'exploitation de satellites comptent beaucoup sur cette infrastructure spatiale: la télévision, la radio, les banques, les communications, les transports, l'agriculture, l'exploitation minière, et en particulier pour les services militaires modernes. Pourtant, ces mêmes satellites sont sous la menace croissante de 100.000 s de morceaux de débris spatiaux et la prolifération réelle ou potentielle d'armes et autres moyens capables de détruire ou de perturber les satellites. Il existe également une concurrence pour l'utilisation du spectre des fréquences radio limitée qui est essentiel pour le fonctionnement de ces satellites. Parallèlement, il existe une série de facteurs déstabilisant la sécurité mondiale en général et la domination des États-Unis et en Europe en particulier. La prolifération des armes nucléaires et des missiles balistiques, en tant que de leurs vecteurs, est une préoccupation importante. Les missiles balistiques ont une trajectoire à travers l'espace et de la défense contre les missiles balistiques compter également sur l'infrastructure spatiale. Les contraintes financières et d'autres ont fait des puissances mondiales plus introvertie, moins susceptibles de déployer des forces dans le monde, sauf par le genre de capacités »portée à distance» qui s'appuient sur l'infrastructure spatiale pour leur efficacité (tels que les véhicules aériens sans pilote et la cyberguerre). Alors infrastructure spatiale est un élément clé de la sécurité mondiale, mais il est aussi de plus en plus vulnérables aux menaces décrites ci-dessus - y compris les armes spatiales. La guerre dans l'espace devient une possibilité réelle. L'espace n'est pas, cependant, une nouvelle, sauvage et anarchique frontière. Il existe un cadre juridique de complexité croissante. Toutefois, les traités spécifiques au domaine de l'espace, ainsi que les récentes initiatives pour augmenter ces traités, à peine contempler la guerre. Le droit de la guerre est applicable au domaine de l'espace et fournit potentiellement un ensemble complet de normes de comportement de l'État qui restreint le recours à la force par les Etats dans le domaine spatial. Il existe une incertitude considérable sur la façon dont les règles particulières des lois de la guerre s'appliquent au domaine de l'espace. Il serait dans les intérêts stratégiques de tous les États à avoir une déclaration claire et faisant autorité sur la manière dont les règles du droit de la guerre s'appliquent au domaine de l'espace. La question est de savoir comment parvenir à une telle déclaration claire et faisant autorité.Analogies utiles peuvent être trouvés dans le processus et le succès du Manuel de San Remo sur le droit international applicable aux conflits armés sur mer, le Manuel de Harvard sur le droit international applicable à antiaérienne et antimissile guerre et le Manuel Tallinn le droit international applicable aux cyberguerre. Ces manuels ont été rédigés par des experts juridiques et techniques mondialement reconnues dans chaque domaine exprimer des opinions personnelles sur la lex lata. Ils ont évité les nombreux défis de négociations de l'Etat sur des sujets similaires, mais les manuels ont eu un impact significatif dans chacun de leurs domaines. Il doit maintenant être un «manuel de droit international applicable à la guerre de l'espace».</dc:abstract><ual:supervisor>Ram S Jakhu (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/5h73q0416.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/q237hv899</ual:fedora3Handle><dc:subject>Social Sciences - Law</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3An870zt915"><dcterms:title>Beyond protection: an informal economy perspective on labour law</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Laws</schema:inSupportOf><dc:contributor>Labour Law</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>McHugh-Russell, Liam Sean M.</ual:dissertant><dc:abstract>The large cohort of workers in the "informal economy" commonly described as lying "beyond the protection of labour law" represent a serious challenge (though not the only one) to the adequacy and legitimacy of labour law's normative tools and legitimating narratives. Drawing on a critical review of recent work at the ILO and by WIEGO (an international research-advocacy network focused on women in the informal economy), the thesis tries to provide insight into the nature of that challenge. The heterogeneity of informal work calls for prudence to avoid still-popular folk images rooted in the Fordist-era organization of work in the global North. Capturing that diversity instead requires "socio-economic" approaches attentive to the particulars of the networks of production that workers participate in, and the complex interaction between working practices and state regulation. Ultimately, however, providing a platform for workers to pursue their capabilities requires labour law to go beyond "protection" as a structuring discourse and embrace a broader normative horizon. </dc:abstract><dc:abstract>Les travailleurs de l'«économie informelle» souvent décrit comme «au-delà de la protection du droit du travail» représentent un défi sérieux (mais pas le seul) qui menace la pertinence et la légitimité des instruments et discours normatifs du droit du travail. En utilisant une analyse critique des textes récents produits par l'OIT et par WIEGO (un réseau international de recherche et de plaidoyer centré sur les femmes dans l'économie informelle), cette thèse offre une perspective quant à la nature de ce défi. L'hétérogénéité du travail informel demande de la prudence, afin d'éviter de rester dans les images folkloriques ancrées dans l'ère du travail "Fordiste" des pays du Nord. La réponse à cette diversité exige plutôt un approche «socio-économique» non seulement attentifs aux particularités des réseaux de production dans lesquelles les travailleurs participent, mais aussi alerte aux interactions complexes entre les pratiques de travail concrètes et la réglementation de l'État. En fin de compte, cependant, afin de  fournir aux travailleurs une plate-forme leur permettre de poursuivre leurs « capabilités », il faudrait que le droit du travail cherche au-delà de la «protection» pour ses discours de structuration, en  adoptant un horizon normatif plus large.</dc:abstract><ual:supervisor>Blackett, Adelle (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/gx41mn105.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/n870zt915</ual:fedora3Handle><dc:subject>Social Sciences - Law</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A1831cp18h"><dcterms:title>Inhibition of fibroblast growth factor receptor 3 (FGFR3) signalling to accelerate bone formation during distraction osteogenesis of mice tibiae</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Surgery</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Albishi, Waleed</ual:dissertant><dc:abstract>L'ostéogénèse par distraction (Distraction Osteogenesis, DO) est une technique chirurgicale largement utilisée pour le traitement des anomalies de la longueur des membres, des malformations, des fractures non-union, et de la perte osseuse suivant un traumatisme. La technique consiste à effectuer une ostéotomie puis à éloigner progressivement les deux segments osseux à l'aide d'un fixateur externe. Du nouveau tissu osseux se forme progressivement dans l'espace créé par la traction. Bien que montrant un taux de succès élevé, un des désavantages de cette technique est la longue période de temps où le fixateur externe doit être maintenu en place afin que l'os nouvellement formé se consolide. Cette période prolongée peut entraîner de nombreuses complications d'ordre social, psychologique ou médical. Des résultats récents de notre laboratoire montrent que l'absence de signalisation par le récepteur 3 des facteurs de croissance fibroblastiques (Fibroblast Growth Factor Receptor 3, FGFR3) au cours de la DO conduit à une augmentation de la formation osseuse chez les souris déficientes en FGFR3. Nous avons donc émis l'hypothèse que le blocage de la voie signalétique FGFR3 chez des souris de type sauvage pourrait être efficace afin d'augmenter la formation osseuse au cours de la DO. Afin de bloquer cette voie signalétique, nous avons utilisé un inhibiteur synthétique, PD173074, ou des anticorps anti-FGFR3. Des souris C57Bl/6 de type sauvage agée de 2 mois et divisée en 5 groupes. Les souris ont subi une ostéotomiechirurgicale et l'installation du fixateur externe (appareil de distraction). Après une période de récupération de 5 jours, la distraction a été amorcée (0,2 mm/12 heures pendant 12 jours). Les animaux ont été sacrifiés au jour 33 post-chirurgie (mi-consolidation). À l'aide de la tomographie haute résolution assistée par ordinateur (micro-computedtomography, μCT), nous avons comparé la formation osseuse dans l'espace créé par la traction entre le groupe contrôle (recevant seulement le véhicule) et les groupes traités avec une des trois doses croissantes d'inhibiteur ou avec l'anticorps anti-FGFR3. Nos résultats montrent une tendance dose-dépendante à l'augmentation de la formation osseuse chez les souris recevant l'inhibiteur, par rapport aux souris non traitées. Cette étude pilote est un jalon important dans la recherche translationnellevisant à accélérer la formation osseuse au cours de l'ostéogénèse par distraction.</dc:abstract><dc:abstract>Distraction osteogenesis (DO) is a surgical technique widely used for the treatment of limb length discrepancies, limb deformities, long bone nonunions, and bone loss. The technique involves performing an osteotomy and then gradually distracting the two bone segments with an external fixator. This generates new bone within the distracted gap. Although very successful, one of the limitations of this technique is the long period of time the external fixator needs to be kept on, until the newly formed bone in the distracted zone consolidates. This prolonged process may lead to numerous social, psychological, and medical complications. Recent studies from our laboratory showed that the absence of signalling through the fibroblast growth factor receptor 3 (FGFR3) in DO leads to an increase in bone formation in mice deficient for FGFR3. Thus, we hypothesize that exogenous blocking of the FGFR3 pathway in wild-type mice may be efficacious in promoting bone formation in DO. We thus planned to block this pathway using the small molecule inhibitor, PD173074 or using anti-FGFR3 antibodies. In this study, we used 2-month old wild-type C57BL/6 mice divided in 5 groups. The mice have undergone a surgical osteotomy and installation of an external fixator (distraction apparatus). Following a 5-day latency period, distraction (0.2 mm/12 hours for 12 days) was initiated. The animals were sacrificed at day 33 post-surgery (mid consolidation). Micro-computed tomography (μCT) of the wild-type control group tibiae (not treated) was compared to the tibiae of mice receiving one of three increasing doses of small molecule inhibitors and mice receiving a blocking dose of anti-FGFR3 antibodies. Our results showed a trend towards increased bone volume in mice receiving the higher doses of the small molecule inhibitor as compared to those of the untreated wild-type mice. This dose ranging experiment represents a critical study in this translational research effort to accelerate bone formation in DO.</dc:abstract><ual:supervisor>C Reggie Hamdy (Internal/Cosupervisor2)</ual:supervisor><ual:supervisor>Rene St-Arnaud (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/7s75dg91n.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/1831cp18h</ual:fedora3Handle><dc:subject>Health Sciences - Medicine and Surgery </dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A4j03d324x"><dcterms:title>Analytical optimization/verification schemes for finite- precision data-flow graphs</dcterms:title><ual:graduationDate>2014</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Sarbishei, Omid</ual:dissertant><dc:abstract>Finite-precision computing is an important topic, which has vast applications from computer arithmetic and Digital Signal Processing (DSP) to sensor fusion. The main goal in finite-precision computing is to deliver accurate enough computational results, while utilizing the minimum amount of available hardware resources such as arithmetic logic units, multiply-accumulator units, look-up-tables, logic gates, dedicated DSP blocks, memories, number of sensors, etc. Fixed-point, floating-point, and block-floating-point number representations are the most common data formats for finite-precision computing in DSP and computer arithmetic. In this thesis we explore various analytical optimization and verification schemes for finite-precision data-flow graphs. Our solutions are beneficial for a number of problems in the domain of DSP, computer arithmetic, sensor networks, and even operations research. Namely, we address the accuracy analysis and word-length optimization of fixed-point, floating-point and block-floating-point polynomial specifications. The proposed solutions are compared with previous work on several DSP benchmarks. Furthermore, we introduce a minimum mean-square-error, high precision data fusion algorithm with tolerance to multiple faults for an arbitrary central multi-sensor system. Experiments illustrate the results for temperature sensors as well as accelerometers and compare them with previous work.</dc:abstract><dc:abstract>L'arithmétique en précision finie est un sujet important qui permet à vastes applications pour le calcul arithmetique informatique dont le traitement de signal et la fusion de capteurs. L'objectif principal de l'arithmétique en précision finie est de fournir des résultats de calcul suffisamment précises tout en utilisant le minimum de ressources matérielles disponibles telles que des unités d'arithmétiques logiques, des blocs multiplicateur-accumulateurs, des tables de correspondances, des fonctions logiques, des blocs de traitement de signal dédiés, de la mémoire, des capteurs, etcLes formats à virgule fixe, virgule flottante, et bloc de calcul en virgule flottante sont les représentations numériques de données les plus courantes pour le calcul à précision finie en traitement de signal et en arithmétique informatique. Dans cette thèse, nous explorons les différentes techniques d'optimisations analytiques et systèmes de vérification pour les diagrammes de flux de données (DFD) contenant de l'arithmétique en précision finie. Nos solutions proposées sont bénéfiques pour un certain nombre d'applications dans le domaine du traitement de signal, de l'arithmétique informatique, des réseaux de capteurs, et même dans le domaine de la recherché opérationnelle. Notamment, nous nous adressons à l'analyse de la précision et de l'optimisation de la taille des mots informatique pour l'arithmétique en virgule fixe, virgule flottante, et bloc de calcul en virgule flottante avec specification de polynomes. Les solutions proposées sont comparées avec les travaux précédents en utilisant plusieurs critères de performances dans le domaine du traitement de signal. De plus, nous introduisons un algorithme de fusion de données à haute précision avec une erreur quadratique moyenne minimum. Cet algorithme est tolérante aux pannes pour un système multi-capteur central arbitraire. Des expériences illustrent les résultats pour les capteurs de température ainsi que des accéléromètres. Nous les comparons ensuite avec des travaux de recherche antérieurs.</dc:abstract><ual:supervisor>Zeljko Zilic (Supervisor1)</ual:supervisor><ual:supervisor>Katarzyna Radecka (Supervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/rf55zc042.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/4j03d324x</ual:fedora3Handle><dc:subject>Engineering - Electronics and Electrical</dc:subject></rdf:Description></rdf:RDF>