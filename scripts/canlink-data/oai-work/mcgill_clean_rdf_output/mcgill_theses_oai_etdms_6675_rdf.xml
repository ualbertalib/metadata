<?xml version="1.0" encoding="UTF-8"?><rdf:RDF xmlns:oai="http://www.openarchives.org/OAI/2.0/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:ual="http://terms.library.ualberta.ca/" xmlns:bibo="http://purl.org/ontology/bibo/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:schema="https://schema.org/" xmlns:etdms="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A2z10wt10s"><dcterms:title>Predictive filtering for automatic focus pulling and robot control</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>School of Computer Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Kia, Seyed-Ehsan</ual:dissertant><dc:abstract>Nous présentons un système de mise au point automatique tirant à l'aide du matériel de niveau des consommateurs et des filtres de Kalman prédictifs. Ce travail décrit le processus complet de la construction d'un tel système à partir de zéro, tant au niveau matériel et logiciel. Nous documentons comment modifier un objectif Canon pour communiquer directement avec elle, et la façon de communiquer avec l'aide d'un micro-contrôleur. Nous calibrons alors le moteur de mise au point de l'objectif et nous mesurons la latence du système. Nous présentons ensuite une série de filtres prédictifs visant une variété de cibles de mise au point qui pourraient survenir dans un film. Ces filtres compensent la latence dans le système et visent à maintenir la cible au point parfaite, tout en mouvement. Enfin, nous testons chacun de ces filtres en utilisant l'objectif modifiée pour suivre la position prédite de la cible. Nous évaluons la défocalisation dans les exemples à la fois synthétiquement à partir des données de mouvement enregistrées et numériquement en analysant la vidéo enregistrée pour le flou. Nous présentons également quelques autres utilisations de nos filtres de Kalman prédictifs en utilisation avec des quadrotors.</dc:abstract><dc:abstract>We present a system for automatic focus pulling using consumer level hardware and predictive Kalman filters. This work describes the complete process of building such a system from scratch, both in hardware and software. We document how to modify a Canon lens to communicate with it directly, and how to communicate with it using a micro-controller. We then calibrate the lens focus motor and measure the latency of the system. We next introduce a series of predictive filters aimed at a variety of focus targets that could arise in a film. These filters compensate for the latency in the system and aim to keep the target in perfect focus while in motion. Finally, we test each of these filters using the modified lens to track the predicted position of the target. We evaluate the defocus in the examples both synthetically from the recorded motion data and numerically by analyzing the recorded video for blur. We also showcase a few other uses of our predictive Kalman filters in use with quadrotors.</dc:abstract><ual:supervisor>Paul Kry (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/cf95jd845.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/2z10wt10s</ual:fedora3Handle><dc:subject>Computer Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Anc580q29b"><dcterms:title>Effect of intramammary infusion of chitosan hydrogels on bovine mammary gland involution after drying-off</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Animal Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Lanctôt, Samuel</ual:dissertant><dc:abstract>La transition entre la lactation et le tarissement est une période à risque élevé pour la vache laitière de contracter de nouvelles infections intra-mammaires. Ce risque est réduit lorsque l'involution de la glande mammaire est terminée. Par conséquent, une approche permettant d'accélérer le processus d'involution après le tarissement pourrait réduire l'incidence de la mammite. Notre programme de recherche vise à développer un modificateur de la réponse biologique pouvant être injecté dans les trayons de la vache afin de promouvoir la migration rapide des cellules immunitaires, accélérant ainsi l'involution mammaire. Le chitosan est un polysaccharide naturel dérivé de la chitine capable de stimuler l'immunité innée de l'hôte. Nous avons développé deux formulations; fabriqué à partir de chitosane ayant une viscosité faible ou élevé. Ces formulations sont liquides à température ambiante mais forment un hydrogel à la température du corps. Dans une première expérience, Chaque quartier de 7 vaches Holstein en fin de lactation a été assigné de manière aléatoire à une infusion intra-mammaire de 2.5ml ou 5ml de chitosan ayant une viscosité faible, 5ml de chitosan ayant une viscosité élevée ou 5ml d'eau. Des échantillons de lait (sécrétions mammaires) de chaque quartier ont été recueillis lors des jours précédents et suivants le tarissement afin de mesurer plusieurs marqueurs de l'involution mammaire. Les infusions d'hydrogel de chitosan ont toutes accéléré l'augmentation des teneurs des sécrétions mammaires en cellules somatiques, albumine sérique et lactoferrine, ainsi que de l'activité de la lactate déshydrogénase. Aucune  différence notable n'a été observée entre les différents traitements de chitosan. Ces résultats indiquent que la perfusion d'hydrogel de chitosan accélère le processus d'involution de la glande mammaire.  La compatibilité de cette approche avec un scellant à trayon interne a été vérifiée dans une seconde expérience. Chaque quartier de 8 vaches Holstein en fin de lactation a été assigné de manière aléatoire à une infusion intra-mammaire de 5 ml de chitosan ayant une viscosité faible, 4 g de scellant, une combinaison de sellant et de chitosan, ou 5 ml de l'eau. Des échantillons de lait (sécrétions mammaires) de chaque quartier ont été recueillis lors des jours précédents et suivants le tarissement. Les effets du chitosan sur les marqueurs d'involution et les réponses immunitaires ont été similaires à la première expérience. Ces effets n'ont  pas été affectés par la présence du scellant à trayon, ce qui montre que les deux approches sont entièrement compatibles et peuvent être utilisés en combinaison. Bien que cela devra être évalué, ces résultats suggèrent que l'administration d'un hydrogel de chitosan au tarissement pourrait réduire l'incidence des nouveaux cas de infection intra mammaire durant la période de tarissement. Finalement, cette approche pourrait être utilisée comme une alternative au traitement antibiotique  au tarissement pour les vaches non infectées.</dc:abstract><dc:abstract>The transition from lactation to the dry period in dairy cows is a period of high risk for acquiring new intramammary infections. This risk is reduced when the involution of the mammary gland is completed. Accordingly, approaches that speed up the involution process after drying-off could reduce the incidence of mastitis. The research presented in this thesis aimed at developing a biological response modifier that could be injected into cow teats to promote immune cell migration and speed up mammary gland involution. Chitosan, a natural polysaccharide derived from chitin, is able to triggers host innate immunity. We developed 2 formulations, made from either high- or low- viscosity chitosan. Both are liquid at room temperature but form a hydrogel at the body temperature. In the first experiment, each udder quarter of 7 Holstein cows in late lactation was randomly assigned at drying-off to receive one of the following intra-mammary infusions:  2.5 or 5 mL of low-viscosity chitosan hydrogel, 5 mL of high-viscosity chitosan hydrogel, or 5 mL of water. Milk (mammary secretion) samples of each quarter were collected on days before and after drying-off to measure different immune and involution markers. The chitosan hydrogel infusions significantly hastened the increases in somatic cell count, serum albumin and lactoferrin concentrations, and the lactate dehydrogenase activity in mammary secretions. No major differences between sources or volumes of chitosan were observed for the measured parameters. These results suggest that chitosan hydrogel infusion hastened mammary gland involution, which may reduce the risk of acquiring new intra-mammary infection during the dry period. The compatibility of chitosan hydrogel with an internal teat sealant was verified in the second experiment. Each udder quarter of 8 Holstein cows in late lactation was randomly assigned at drying-off to administration of an intra-mammary infusion of 5 mL of low viscosity chitosan hydrogel, 4 g of an internal teat sealant, combination of sealant and chitosan, or 5 mL of water. Milk (mammary secretion) samples of each quarter were collected on days before and after drying-off to measure different involution markers. As in the first experiment, chitosan induced changes in involution and immune responses markers. Those parameters were not affected by the presence of the teat sealant, showing that both could be used in combination. Further studies are needed to determine whether administration of chitosan hydrogel could also reduce the incidence of new cases of intra mammary infections during the dry period. Ultimately, this approach could be used as an alternative to dry cow antibiotic therapy for non-infected cows.</dc:abstract><ual:supervisor>Xin Zhao (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/k643b363f.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/nc580q29b</ual:fedora3Handle><dc:subject>Animal Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Arb68xf332"><dcterms:title>Mapping the clinical translation of successful and unsuccessful neurological drugs</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Medicine</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Hakala, Amanda</ual:dissertant><dc:abstract>BACKGROUND Neurology is one of the most failure-prone areas of drug development. However, almost nothing is known about the amount of evidence that is lost due to nonpublication of trials testing products that never receive regulatory approval, nor about the burdens to research participants of exposure to potentially unsafe or ineffective drugs. OBJECTIVES To evaluate nonpublication rates among trials of new successful and unsuccessful neurological drugs and to quantify the patient benefit and burden associated with their clinical translation trajectories. METHODS We created two drug cohorts: our 'licensed' drug cohort consisted of all novel drugs receiving FDA licensure 2005 to 2012 inclusive in seven prevalent neurological disorders. Our cohort of 'stalled' drugs included all experimental agents tested in the same domains that had at least one completed phase III trial in the same timeframe but failed to receive FDA approval. To assess publication rates, registered trials of these drugs were included in our sample if their primary outcome collection occurred before October 1st, 2010. We determined the publication status of eligible trials by searching public registries, PubMed, and Embase. The primary outcome was journal publication (or result reporting in other media). To measure the total patient burden incurred by and the efficiency of neurological drug development in these same drugs, we performed a systematic review and meta-analysis of the clinical literature. We piloted our extraction and analysis protocol in stroke and subsequently assessed Parkinson's disease and multiple sclerosis. We extracted demographic, outcome, efficacy, and safety data from published trials and quantified patient-years of involvement. For the stroke cohort, cumulative rates of serious adverse events and good outcomes were plotted as a function of time. RESULTS We found that 56% (91/163) of trials of licensed drugs were published within five years of completion, compared to 32% (64/203) of trials of stalled drugs in our sample. Trials of licensed drugs were 1.7 times more likely to publish results compared to trials of stalled drugs (adjusted hazard ratio for publication: 1.68, 95% confidence interval: 1.11 to 2.54). 14 092 and 33 882 volunteers participated in unpublished trials of licensed and stalled neurological drugs, respectively. Result data were not publicly available in any form for 10% (16/163) and 46% (94/203) of trials of licensed and stalled drugs, respectively. Our systematic review captured four licensed and twelve stalled neurological drugs. Our pilot project in stroke demonstrated that 10 979 participants were involved and 2612.8 patient-years were expended in developing unsuccessful drugs, but patients in treatment arms were not disadvantaged against comparator arms. Risk/benefit analyses for Parkinson's disease (N=24 306 patients for 10 393.2 patient-years) and multiple sclerosis (N=23 861 patients for 32 354 patient-years) are forthcoming, but preliminary efficiency analyses suggested indications that received licensure were generally identified early in development and that 11% of clinical trial participants may have been involved in studies of marginal value to the medical research community. CONCLUSION Despite their ethical and practical importance, results of trials for unsuccessful neurological drugs are heavily underreported. Consequently, research and care communities are deprived of feedback on biological premises driving drug development, pathophysiology, drug class effects, and the value of surrogate endpoints in trials. Nevertheless, our systematic review in stroke suggested that exposure to unsuccessful drugs was not harmful for participants relative to comparator arms. Forthcoming risk/benefit analyses in Parkinson's disease and multiple sclerosis will better indicate whether research subjects are unduly burdened by unsuccessful clinical translation in neurology.</dc:abstract><dc:abstract>CONTEXTE La neurologie subi un des taux d'échec les plus élevés parmi les majeurs domaines de développement de médicaments, mais nous savons encore peu de la quantité d'information qui est perdue en raison de nonpublication des essais cliniques analysant des molécules échouées, ni du fardeau des sujets de recherche qui y participent. OBJECTIFS Évaluer le taux de nonpublication des essais cliniques ayant éprouvé des molécules homologuées par la FDA ou non-autorisées à la commercialisation et quantifier les bienfaits et la charge associés au développement clinique de ces pharmacothérapies. MATÉRIEL ET MÉTHODE Nous avons créé deux cohortes de molécules: «réussies» (tous les nouveaux médicaments recevant l'homologation par la FDA entre 2005 et 2012 pour le traitement de sept troubles neurologiques courants) et «échouées» (tous les agents expérimentaux non-homologués faisant l'objet d'au moins un essai clinique de phase III contre ces mêmes maladies entre 2005 et 2012). Pour évaluer le taux de publication, nous avons capturé tous les essais cliniques enregistrés sur clinicaltrials.gov testant une de ces molécules qui ont terminé la collecte primaire des résultats avant le 1er Octobre 2010. Nous avons évalué l'état de publication de ces essais cliniques en cherchant dans les registres publics et les bases de données PubMed et Embase. Le principal indicateur de résultat a été la publication dans une revue académique.Afin de quantifier le fardeau des sujets de recherche et les inefficacités de la mise au point de nouvelles pharmacothérapies, nous avons entrepris un examen systématique et méta-analyse des essais cliniques publiés éprouvant les molécules de nos cohortes. Nous avons effectué une étude pilote dans le domaine de l'attaque cérébrale, suivi par la maladie de Parkinson et la sclérose en plaques. Nous avons extrait les données démographiques, de l'efficacité et de l'innocuité. Les années-patients d'exposition ont été quantifiées. RÉSULTATS 56% (91/163) des essais cliniques de médicaments homologués ont été publiés dans les cinq ans suivant l'achèvement de l'essai, comparé à 32% (64/203) des essais cliniques de molécules «échouées». Les essais cliniques de médicaments homologués étaient 1,68 fois plus susceptibles de publier leurs résultats (rapport de risque ajusté; intervalle de confiance à 95% : 1,11 à 2,54). 14,092 et 33,882 patients ont participé à des essais cliniques inédits de molécules «réussies» et «échouées». Les résultats n'ont pas été rendus publiques pour 10% (16/163) et 46% (94/203) des essais cliniques, respectivement.Notre examen systématique a compris 4 médicaments homologués et 12 molécules «échouées». 10,979 patients ont été impliqués au cours de 2612.8 années-patients dans le développement infructueux de molécules pour traiter l'attaque cérébrale, mais les groupes expérimentaux n'étaient pas désavantagés contre les groupes témoins. Les analyses risques-avantages pour la maladie de Parkinson (N=24,306 patients;10,393.2 années-patients) et la sclérose en plaques (N=23,861 patients; 32,354.2 années-patients) sont à venir. Cependent, nos résultats préliminaires ont démontré que les indications qui reçoivent ultimement l'homologation par la FDA sont identifiées très tôt au cours du développement d'un médicament et qu'approximativement 11% de participants dans notre cohorte ont assisté à des essais cliniques de valeur marginale à la communauté médicale. CONCLUSION Malgré leur importance éthique et pratique, les résultats des essais cliniques mettant à l'épreuve des molécules «échouées» sont sous-déclarés. Par conséquent, les communautés scientifiques et médicales sont privées du retour d'information sur les effets liés aux classes de médicaments et la valeur des critères d'évaluation substitutifs. Néanmoins, notre examen systématique pilote en attaque cérébrale a démontré que l'exposition à des molécules «échouées» ne semble pas nuire aux participants de rechereche par rapport aux bras témoins.</dc:abstract><ual:supervisor>Jonathan Kimmelman (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/8623j154k.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/rb68xf332</ual:fedora3Handle><dc:subject>Medicine</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aft848t13w"><dcterms:title>Interactions of engineered nanoparticles with biofilms and selected microorganisms in model saturated environments</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Natural Resource Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Mitzel, Michael</ual:dissertant><dc:abstract>Les nanoparticules manufacturées (NPM) présentes dans l'environnement posent des risques potentiels à la qualité du sol et de l'eau, de même qu'à la santé humaine. Comprendre le transport et le sort environnemental des NPM est primordial au développement et à la validation d'une réglementation pour encadrer l'usage de ces nouveaux matériaux dont l'utilisation est grandissante. Également, il doit y avoir évaluation des dangers que les NPM présentent ou non aux organismes dans l'environnement. Les réactions physiques individuelles d'organismes, telles que l'évitement, sont sans doute des indices de la sensibilité biologique face aux NPM. Conséquemment, j'ai conçu des expériences pour tenter de réduire le fossé entre la caractérisation de ces matériaux aux propriétés nouvelles et leur contamination de l'environnement. Dans un premier temps, la nature fondamentale de l'interaction entre les NPM et des biomatériaux communs, tels que les biofilms, fut étudiée grâce à des expériences de transport dans des colonnes de sable. Par la suite, des microbalances à quartz avec mesure de dissipation (QCM-D) furent utilisées pour observer les interactions entre le biofilm et le substrat dans le cadre du deuxième objectif de la recherche : celui d'identifier les possibles réponses physiques de la part des biofilms. Dans un troisième temps, un dispositif microfluidique développé pour l'étude de la chimiotaxie fût combiné avec la microscopie en chambre sombre pour évaluer individuellement le changement de la trajectoire de nage d'organismes en réponse à un contact critique avec du nanoargent et d'autres NPM afin, une fois de plus, d'enquêter sur les réactions physiques des organismes vivants. Les expériences de transport dans des colonnes de sable couvert de biofilm démontrèrent une réduction nette de la rétention de PVP-nanoargent stabilisé stériquement. Cette diminution de la rétention de PVP-nanoargent dans le sable comparativement au sable vierge est due aux forces électrostatiques répulsives entre la couche de PVP et le biofilm. D'autre part, le sable de quartz couvert de biofilm retint plus de nanoparticules manufacturées en latex stabilisées électrostatiquement que le sable vierge. Le rôle de l'hydrophobie fut également étudié en utilisant deux types de NPM en latex et deux types de biofilms. Il fut observé qu'en augmentant la force ionique, de nettes différences apparaissent dans les formes des courbes de percée de l'adsorption de NPM, ce qui pointe vers des différences fondamentales des mécanismes de déposition sur des biofilms hydrophobiques et hydrophiliques en raison des propriétés d'hydratation de ces biofilms.Les NPM ont une incidence sur le comportement des organismes qui y sont exposés. Ce constat découle des trajectoires de nage non aléatoires observées dans le dispositif microfluidique de même que des réponses différentielles dans le QCM-D. Somme toute, les individus ont des réactions physiques inconsistantes à des concentrations similaires de nanoargent, nanoparticules de latex et nanoparticules d'or, ce qui tend à suggérer que les organismes ne sentent pas les nanomatériaux directement. Ainsi, j'avance plutôt l'hypothèse que les ions positifs d'argent dissous se trouvant autour du nanoargent colloïdal indiquent la présence d'une substance toxique. Corollairement, les organismes réagiraient à la toxicité, et non au matériau spécifiquement. En définitive, les résultats des expériences décrites dans cette thèse de doctorat ont mené à une meilleure compréhension des NPM, de leurs interactions avec les microorganismes, en plus de leurs comportements potentiels dans l'environnement.</dc:abstract><dc:abstract>The presence of engineered nanoparticles (ENPs) in the environment represents a potential risk to soil health, water safety and human health. Understanding the environmental transport and fate of ENPs is of paramount importance for the development and validation of regulatory guidelines regarding these new and increasingly prevalent materials. In addition, the safety of ENPs to organisms in the environment must be evaluated. Physical responses such as avoidance behavior are likely an indication of biological sensitivity to ENPs that merits further study. Experiments about the transport, fate and physical responses to ENPs were designed to address these knowledge gaps. First, traditional sand-packed columns were used in transport experiments to determine how ENPs interact with prevalent biomaterial, such as biofilms. Then, a quartz crystal microbalance with dissipation monitoring (QCM-D) was employed to assess biofilm-substrate interactions and the potential for physical responses in biofilms as a second objective. A microfluidic device for chemotaxis study, coupled with enhanced darkfield microscopy, was used to assess changes in swimming trajectory of organisms, a direct measure of physical responses following acute exposure to silver nanoparticles (nAg) and other ENP. Transport experiments in biofilm-coated sand indicated significantly reduced retention of sterically-stabilized, polyvinylpyrrolidone-coated nAg (PVP-nAg). The decreased retention of PVP-nAg in biofilm-coated sand compared to clean sand is attributed to repulsive electrosteric forces between the PVP coatings and the biofilm. Biofilm-coated quartz sand was also found to retain more electrostatically-stabilized latex ENPs than clean, uncoated sand. The role of hydrophobicity in ENP transport was also investigated using two types of latex ENPs and two types of biofilms. As ionic strength increased, clear differences in the shape of the ENP breakthrough curves were observed, indicating that hydration properties of biofilms controlled ENP deposition onto sand particles coated with hydrophilic or hydrophobic biofilms. Physical behavior was affected in organisms exposed to ENPs, based on non-random swimming observed in the microfluidic device and differential responses in a QCM-D. However, this physical response to ENPs was not consistent when organisms were exposed to similar concentrations of nAg, nLatex and nAu, which appears to indicate that they do not sense nanomaterials directly. Rather, I hypothesize that the dissolved Ag+ in a colloidal nAg suspension can provide a stimulus, indicating the presence of a toxic substance, to which the organisms respond. In conclusion, the physico-chemical interactions of ENPs with environmental matrices will determine their potential for transport and detection by microorganisms, which can exhibit physical responses based on perceived toxicity. </dc:abstract><ual:supervisor>Joann Karen Whalen (Supervisor1)</ual:supervisor><ual:supervisor>Nathalie Tufenkji (Supervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/df65vb29z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/ft848t13w</ual:fedora3Handle><dc:subject>Natural Resource Sciences</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9k41zh07w"><dcterms:title>Adaptive cloud publish-subscribe services for latency-constrained applications</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Computer Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Gascon-Samson, Julien</ual:dissertant><dc:abstract>The publish/subscribe model is an efficient paradigm that can be leveraged in many contexts, as it provides a nice abstraction that allows for logically and efficiently decoupling content producers (publishers) from content consumers (subscribers). Publish/subscribe is typically provided as a service, in which subscribers register interest in (subscribe to) contents that they want to receive. Then, as publishers generate and submit contents in the form of publications to the service, the latter determines to which subscribers each publication should be sent to, and forwards each publication accordingly to the relevant subscribers. While multiple variants of the publish/subscribe paradigm have been described in the literature, this thesis is centered around topic-based publish/subscribe, which enjoys widespread usage in large-scale commercial systems. Supporting large-scale topic-based publish/subscribe applications brings interesting research challenges, notably regarding the scalability and load balancing aspects, as some applications built on these systems can generate high message volumes. In addition, some specific applications impose additional constraints, such as multiplayer online games (MOG), in which publication delivery latencies must be kept below a given threshold, which can be particularly challenging when clients are distributed around the world. The cloud can be leveraged in these contexts, as a publish/subscribe service deployed in the cloud can benefit from the large pool of resources that the cloud can provide in several geographical regions.This thesis proposes a set of contributions in the general area of scaling cloud-based topic-based publish/subscribe systems. Our first contribution, Dynamoth, provides a scalable topic-based publish/subscribe service that is tailored for latency-constrained applications. It provides a hierarchical scalability and load balancing model that exploits the intrinsic characteristics of the topic-based publish/subscribe paradigm. In addition, Dynamoth also provides availability and fault tolerance in the event of server failures and provides several levels of reliability and ordering guarantees. Our second contribution, MultiPub, provides a global-scale topic-based pub/sub service tailored for the needs of applications with many clients around the world, and having strict latency constraints. As such, it allows one to impose latency constraints. MultiPub then continuously makes sure that these constraints are satisfied (if possible), by generating optimal configurations of cloud deployments spanning across several of the available regions. As cloud usage incurs bandwidth-related costs, and that different cloud regions exhibit different costs, MultiPub also attempts to reduce such costs by selecting the most cost-efficient configuration that respects latency constraints. On the other end, our third contribution, DynFilter, proposes a game-oriented topic-based publish/subscribe service that aims at limiting bandwidth usage in multiplayer and massively multiplayer online games. As DynFilter is game-specific, it exploits the conceptual spatial model of such games in order to inhibit the dissemination of publications that are of a lesser importance in a game setting, in a dynamic way, in order to achieve target bandwidth savings.All of our experiments are run in the context of multiplayer online games, as the topic-based publish/subscribe paradigm fits well into the architectural model of such games. In addition, they are a good example of highly distributed, latency-constrained systems. As running experiments in the cloud is a challenging task, this thesis provides, as an additional contribution, a set of tools that were developed to assist in running large-scale, highly-distributed cloud-based experiments. Among these contributions is a full, reusable implementation of our Dynamoth platform, built according to software engineering principles.</dc:abstract><dc:abstract>Le modèle de publication/souscription orienté-sujet représente un paradigme efficace qui peut être exploité dans plusieurs contextes, puisqu'il fournit une abstraction qui permet de découpler de façon logique et efficace les producteurs de contenu (émetteurs) des consommateurs de contenu (souscripteurs). La prise en charge des applications à large échelle basées sur le modèle de publication/souscription orienté-sujet apporte des défis de recherche intéressants, notamment en ce qui a trait aux aspects d'évolutivité et de balancement de charge. De plus, certaines applications spécifiques imposent des contraintes additionnelles, telles que les jeux massivement multijoueurs en ligne (MOG), dans lesquels la dissémination des publications doit s'effectuer dans un délai strict, ce qui peut constituer un défi important surtout lorsque les clients sont géographiquement dispersés à travers la planète. L'utilisation du nuage informatique peut s'avérer bénéfique dans ces contextes en raison du large éventail de ressources que le nuage peut fournir dans plusieurs régions géographiques.Cette thèse propose un ensemble de contributions dans le domaine général de l'évolutivité des systèmes de publication/souscription orientés-sujet dans le nuage. Notre première contribution, Dynamoth, propose un service évolutif de publication-souscription orienté–sujet qui est optimisé pour les besoins des applications contraintes en latence. Dynamoth propose un modèle hiérarchique d'évolutivité et balancement de charge qui exploite les caractéristiques intrinsèques du paradigme de publication/souscription orienté-sujet. De plus, Dynamoth propose des propriétés de disponibilité et de tolérance aux pannes en cas d'échec de serveurs et propose différents niveaux de fiabilité et de garanties d'ordonnancement. Notre seconde contribution, MultiPub, propose un service de publication/souscription orienté-sujet à échelle globale conçu pour les besoins des applications avec des utilisateurs répartis à travers le monde, et ayant des contraintes strictes en latence. Dans cette optique, MultiPub permet aux applications d'imposer des différentes contraintes de latence. MultiPub s'assure alors sur une base continue que les contraintes définies sont respectées (si possible), en générant des configurations optimales de déploiements tirant parti d'un ensemble de régions infonuagiques disponibles. Puisque l'utilisation du nuage génère des coûts reliés à l'utilisation de la bande passante, et que ces coûts diffèrent au sein des différentes régions infonuagiques, MultiPub vise également à réduire les coûts en sélectionnant la configuration la plus économique qui respecte les contraintes en latence imposées. Dans un autre ordre d'idées, notre troisième contribution, DynFilter, propose un service de publication/souscription orienté-sujet conçu spécifiquement pour les besoins des jeux multijoueurs en ligne. DynFilter vise à limiter l'utilisation de la bande passante au sein de tels jeux. De par sa nature, DynFilter exploite le modèle conceptuel spatial propre aux jeux multijoueurs afin de limiter la dissémination des publications de moindre importance dans un tel contexte, de façon dynamique, afin d'atteindre des économies de bande passante ciblées.L'ensemble de nos expérimentations ont été menées dans le contexte de jeux multijoueurs contraints en latence, puisque le paradigme de publication/souscription orienté-sujet s'harmonise bien avec le modèle architectural de ces jeux. L'exécution d'expérimentations dans le nuage amène certes son lot de défis. Pour cette raison, cette thèse propose, en tant que contribution additionnelle, un ensemble d'outils qui ont été développés pour aider à l'exécution d'expérimentations à large échelle et hautement distribuées dans le nuage. L'une de ces contributions est une implémentation complète et réutilisable de notre plate-forme Dynamoth, qui a été construite selon des principles d'ingénierie logicielle.</dc:abstract><ual:supervisor>Bettina Kemme (Supervisor1)</ual:supervisor><ual:supervisor>Jorg Andreas Kienzle (Supervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/bc386n07d.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9k41zh07w</ual:fedora3Handle><dc:subject>Computer Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Abc386n08p"><dcterms:title>A study of distributed pulse-based synchronization in device-to-device communication</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Tétreault-La Roche, David</ual:dissertant><dc:abstract>Les bénéfices de la communication dispositif-à-dispositif (D2D) suscitent beaucoup d'intérêts dans les domaines académiques et industriels. La synchronization temporelle est un aspect clé de la communication D2D, particulièrement dans les réseaux complètement décentralisés où une référence de temps commune n'est pas disponible. L'algorithme de synchronisation nommé boucles à phases asservies distribuées (DPLL) est un algorithme bien adapté aux situations décentralisées. Dans ce mémoire, nous étudions l'implémentation d'un algorithme DPLL dans le contexte de réseaux sans-fil de cinquième génération (5G). Nous incluons dans notre analyse plusieurs contraintes et aspects pratiques de la communication D2D, tels que le délai de transmission, la propagation par trajets multiples, et la modulation à porteuse unique SC-FDMA. De plus, nous proposons des méthodes pour réduire l'impact de ces aspects sur l'algorithme de synchronisation. À travers nos simulations au niveau physique, qui capturent les effects des conversions analogique-numérique, nous démontrons que la synchronisation temporelle dans un réseau décentralisé est possible sous les contraintes spécifées par le 3 rd Generation Partnership Project (3GPP) pour les applications D2D.</dc:abstract><dc:abstract>The benefits of device-to-device (D2D) communication have garnered interest in both academic and industrial circles. Time synchronization is a key aspect of D2D schemes, particularly in decentralized networks where no reference time is available. Distributed phase-locked loops (DPLL) is a common synchronization algorithm that is suited for decentralized situations. In this work, we study the implementation of a DPLL algorithm in the context of fifth generation (5G) wireless networks, where we include in our analysis several limitations and practical aspects of D2D communication, such as transmission delays, wideband multipath propagation, and single-carrier frequency demodulation multiple access (SC-FDMA) modulation. We propose practical methods to compensate their effects, and introduce new performance metrics to evaluate the merits of the synchronization algorithm. Through simulations at the physical layer, which capture the effects of analog-digital conversions, we demonstrate that time synchronization in a decentralized setting is possible under the constraints specified by the 3 rd Generation Partnership Project (3GPP) for D2D applications.</dc:abstract><ual:supervisor>Benoit Champagne (Internal/Supervisor)</ual:supervisor><ual:supervisor>Ioannis Psaromiligkos (Internal/Cosupervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/6m311s02c.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/bc386n08p</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ajh343v624"><dcterms:title>Manga histories: Beyond the paradigms of modernization and modernism</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Department of East Asian Studies</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Harrisson, Annie</ual:dissertant><dc:abstract>This thesis examines histories of manga written in Japan. The first part provides a critical overview of influential accounts of manga. A close analysis of how these accounts locate the historical origins of manga reveals two large paradigms for understanding manga history: a modernization paradigm and a modernist or postmodernist paradigm. It also allows for a general hypothesis: because manga is a relatively new object of study, the primary goal of manga history is to legitimate it as an object of social importance. Recourse to historical paradigms allows writers to situate manga at the heart of important cultural and historical debates. Unfortunately, it also tends to flatten the diversity and specificity of manga. Thus the second part turns to two manga offering very different historical visions with the aim of showing how manga themselves may contribute to developing new perspectives for writing manga history.</dc:abstract><dc:abstract>Cette thèse examine l'histoire des mangas telle qu'écrite au Japon. La première partie donne un aperçu critique des récits influents concernant les mangas. Une analyse détaillée de la façon dont ces récits situent les origines historiques des mangas révèlent deux principaux paradigmes : un paradigme de modernisation et un paradigme moderniste ou postmoderniste. Ceci permet une hypothèse générale : puisque les mangas sont un objet d'étude relativement nouveau, le but premier de l'histoire des mangas est de légitimer le manga en tant qu'objet d'importance sociale. Le recours à des paradigmes historiques permet aux auteurs de situer le manga au cœur d'importants débats culturels et historiques. Malheureusement, cela tend à réduire la diversité et la spécificité des mangas. Ainsi, la deuxième partie se tourne vers deux mangas proposant des visions historiques différentes afin de révéler comment les mangas pourraient contribuer au développement de nouvelles perspectives concernant l'écriture de l'histoire des mangas.</dc:abstract><ual:supervisor>Thomas Lamarre (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/gt54kq730.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/jh343v624</ual:fedora3Handle><dc:subject>East Asian Studies</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Afb494b928"><dcterms:title>Synaptic defects and impaired locomotor behaviour in larval zebrafish following tdp-43 loss-of-function</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Integrated Program in Neuroscience</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Dzieciolowska, Stefania</ual:dissertant><dc:abstract>Amyotrophic lateral sclerosis (ALS) is a devastating neurodegenerative disease characterized by the progressive dysfunction and death of motor neurons. About 4% of familial ALS cases can be attributed to point mutations in the TAR DNA binding protein (TARDBP) gene, encoding the TDP-43 nuclear protein. To date, little is known about the pathophysiological deficits following the loss of TDP-43 as TDP-43 loss-of-function murine models die before birth. Here, we use a previously described mutant zebrafish line (obtained by TILLING mutagenesis) containing a premature stop codon (denoted 'Y220X') in the tardbp gene. tardbpY220X/Y220X zebrafish do not produce tdp-43 but develop normally as they are compensated by an alternative splicing of the tardbp-like ortholog. In order to obtain a true TDP-43 loss-of-function model, we injected homozygous tardbpY220X/Y220X fish with an antisense morpholino oligonucleotide (MO) targeting the tardbp-like gene. We then examined swimming activity, the structure of neuromuscular junctions (NMJs) and the synaptic input from motor neurons onto muscle fibers. tardbpY220X/Y220X  mutants injected with the tardbpl MO displayed decreased survival, gross morphological defects, impaired locomotor activity, impairments in passive muscle membrane properties and an increased frequency of miniature end-plate currents (mEPCs). These results indicate that TDP-43 is involved in synaptic vesicle release during vertebrate development and may be relevant for understanding synaptic dysfunction in ALS.</dc:abstract><dc:abstract>La sclérose latérale amyotrophique (SLA) est une maladie neurodégénérative caractérisée par la mort des motoneurones. Environ 4 % des cas familiaux de la SLA peuvent être attribués à des mutations dans le gène TARDBP, codant pour la protéine TDP-43. Jusqu'à présent, les défauts biochimiques liés à la perte de TDP-43 restent incompris. De plus, les recherches sont limitées dans les modèles animaux murins de la maladie car ceux-ci meurent durant l'embryogenèse. Pour étudier les conséquences de la perte de fonction de TDP-43, nous utilisons une lignée de poisson-zébré caractérisée précédemment contenant un codon stop prématuré (notée Y220X) dans le gène TARDBP.  Les poissons tardbpY220X/Y220X n'expriment pas la protéine tdp-43, mais ils se développent normalement. Ceci est expliqué par la présence d'un second gène  orthologue (nommé tardbp-like) dans le génome du poisson-zébré qui compense la perte de tdp-43. Afin d'obtenir un modèle plus rigoureux de la perte de TDP-43, nous avons injecté des embryons tardbpY220X/Y220X avec un morpholino oligonucleotide (MO) dirigé contre le gène tardbp-like. Nous avons observé chez les larves tardbpY220X/Y220X  injectés avec la MO contre tardbp-like une diminution de la survie, des défauts morphologiques, un sévère phénotype moteur, des défauts dans les propriétés passives de la membrane musculaire ainsi qu'une augmentation dans la fréquence des évènements miniatures synaptiques (mEPCs). Ces indices suggèrent que la protéine TDP-43 est impliquée dans la libération des vésicules synaptiques au cours du développement vertébré. Ainsi, ces nouvelles données peuvent être pertinentes pour comprendre le dysfonctionnement synaptique observé dans la SLA. </dc:abstract><ual:supervisor>Pierre Drapeau (Internal/Cosupervisor2)</ual:supervisor><ual:supervisor>Heather D Durham (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/br86b607d.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/fb494b928</ual:fedora3Handle><dc:subject>Neuroscience</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Arx913s441"><dcterms:title>Structural health monitoring of aircraft composite structures using ultrasonic guided wave propagation</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mechanical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Sherafat, Mohammad Hossein</ual:dissertant><dc:abstract>Malgré toutes les améliorations en termes de résistance et de rigidité amenées par l'utilisation des structures composites, leur sensibilité à des défauts internes reste un sujet qui soulève des préoccupations. L'objective de ce projet est d'étudier l'interaction des ondes guidées pour la détection des défauts tels que les délaminations, les décollements et les endommagements par impact. Une première configuration a été choisie pour évaluer l'effet d'un défaut artificiel et d'un endommagement réel sur la propagation de l'onde guidée. Les résultats montrent qu'un défaut artificiel peut correctement simuler un défaut réel d'impact, surtout lorsqu'il s'agit d'un défaut à peine visible. Cependant, la majorité de ce travail de cette thèse de doctorat se focalise sur l'évaluation des joints en composite. Un assemblage composé d'une peau raidie en composite et un joint de réparation ont été choisis pour représenter des configurations typiques d'assemblage aéronautique. Les comportements des ondes guidées planes en transmission, réflexion et en dispersion ont été étudié en fonction des modes, fréquence, l'angle d'excitation et l'état du joint. Pour la plaque à raidisseur, deux stratégies d'inspection ont été appliquées.  Dans la première stratégie, il a été conclu que le mode antisymétrique (A0) en transmission est très sensible à des défauts de décollement  pour des fréquences inférieures à 350 kHz, alors que pour le mode symétrique (S0) la réponse en réflexion autour de 200 kHz présente un bon paramètre pour la surveillance de ce type défaut. Concernant la détection de ce défaut en se basant sur la réponse en dispersion, le mode S0 parait le mieux adapté pour des fréquences inférieures à 350KHz puisqu'il introduit une augmentation de la dispersion d'environ 60%. Les résultats obtenus de la deuxième stratégie, à travers le collage, montrent que le mode A0 à un comportement plus directif alors que le mode S0 est beaucoup plus réfracté particulièrement pour des basses fréquences. Pour la détection d'endommagement, le mode S0 parait suffisamment sensible pour un défaut de décollement (une augmentation de 30% de la dispersion de l'onde) à environ 150kHz. La comparaison entre un joint réparé avec et sans défaut montre que la réflexion à la pointe de chaque couche du recouvrement (les réflexions en provenance des bords) peut être un indicateur sur la qualité du joint. Le mode antisymétrique  dans la configuration « pulse-echo » s'avère être un mode efficace et une bonne stratégie pour la détection des décollements dans les joints réparés en composite.</dc:abstract><dc:abstract>Despite enhancements in terms of specific strength and stiffness by using composite in aircraft structures, their susceptibility to hide damage is still a major point of concern. The objective of this work is to investigate guided wave propagation in composite structures to detect delaminations, disbond and impact damage.The majority of the work focuses on assessment of composite joints. Primarily, a simple composite structure configuration was chosen to evaluate the effect of artificial and real damage on guided wave behaviour. The results show that non-mid-plane artificial delamination can accurately represent real impact, particularly barely visible impact damage (BVID). Next, a composite skin-stringer assembly and a composite scarf repair were chosen in order to represent typical aerospace structural joint features. The reflection, transmission and scattering behaviour of the plane guided waves are studied as a function of mode, frequency, excitation angle and the quality of the joint. For the composite skin-stringer, two inspection strategies are applied. From the first strategy, the within-the-bond, it is concluded that the antisymmetric mode (A0) transmission is highly sensitive to the damage for frequencies below 350 kHz, while the symmetric mode (S0) reflection around 200 kHz could be employed for monitoring an echo induced by the disbond. For imaging the disbond based on the scattering of the waves, the S0 mode appears as the best candidate below 350 kHz, by inducing an increase of 60 % of the scattered field in the presence of a disbond. The results from the second strategy, the across-the-bond, indicate that the A0 mode behaves more directionally while S0 is more refracted, specifically at low frequencies. For damage imaging, the S0 mode appears to be sensitive enough to disbonds (an increase of 30 % of the scattered wave) at around 150 kHz. Comparison of the pristine and damaged repair joint indicates reflection at the tip of each layer in the scarf (the reflections from the steps' edges), which can be an indication for evaluation of the quality of the joint. The anti-symmetric mode in the pulse-echo configuration seems to be an efficient mode and strategy for disbond detection in composite repairs. </dc:abstract><ual:supervisor>Pascal Hubert (Supervisor2)</ual:supervisor><ual:supervisor>Larry Lessard (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/2514np110.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/rx913s441</ual:fedora3Handle><dc:subject>Mechanical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A0r9676338"><dcterms:title>Harmonizing customary justice with the rule of law? A sub-national case study of liberal peacebuilding in Sierra Leone and Liberia</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Political Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Sesay, Mohamed</ual:dissertant><dc:abstract>Une des plus grandes énigmes liées aux travaux de reconstruction à la suite d'hostilités dans les pays non occidentaux est la résilience des systèmes de droit coutumier dont les normes procédurales et substantives sont souvent incompatibles avec les normes internationales. Il y a également des préoccupations selon lesquelles la soumission des systèmes coutumiers aux règlements formels puisse miner les mécanismes vitaux de résolution des conflits dans ces sociétés ravagées par la guerre.Cependant, cette étude de cas de consolidation de la paix en Sierra Leone et au Libéria montre que des systèmes de justice traditionnels interagissent de façons complexes : ils soutiennent le processus d'une part et l'affaiblissent également, selon la configuration particulière des institutions, des normes, et des pouvoirs propres au contexte infranational local. Dans toute situation d'interaction entre la justice formelle et informelle  (qu'il y ait conflit ou coopération), il importe que le système  juridique d'État puisse offrir aux populations locales, des services juridiques accessibles, abordables et crédibles et que les normes de justice soient conformes à leurs besoins, à leurs priorités et à leurs attentes, en matière de résolution de conflits. Et pourtant, de telles interactions entre les institutions de justice et les normes sont soumises à la médiation par des dynamiques de pouvoir sous-jacentes, relatives aux autorités des politiques locales et à l'accès aux ressources locales.Les conclusions de cette étude ont été tirées d'un travail de recherche sur le terrain de six mois qui comprenait : la collecte d'éléments de preuve, l'observation des tribunaux coutumiers, des interviews en profondeurs avec un large éventail de parties intéressées, telles que des fonctionnaires judiciaires, des parajuristes, des autorités traditionnelles, ainsi que des résidents locaux qui cherchent à obtenir justice dans de nombreux forums. L'analyse comparative était en grande partie infranational, afin d'obtenir de multiples niveaux de complexité dans la structure du pouvoir traditionnelle, ainsi que les variations régionales importantes en Sierra Leone et au Libéria.</dc:abstract><dc:abstract>One of the greatest conundrums facing postwar reconstruction in non-Western countries is the resilience of customary justice systems whose procedural and substantive norms are often inconsistent with international standards. Also, there are concerns that subjecting customary systems to formal regulation may undermine vital conflict resolution mechanisms in these war-torn societies. However, this case study of peacebuilding in Sierra Leone and Liberia finds that primary justice systems interact in complex ways that are both mutually reinforcing and undermining, depending on the particular configuration of institutions, norms, and power in the local sub-national context. In any scenario of formal and informal justice interaction (be it conflictual or cooperative), it matters whether the state justice system is able to deliver accessible, affordable, and credible justice to local populations and whether justice norms are in line with people's conflict resolution needs, priorities, and expectations. Yet, such interaction between justice institutions and norms is mediated by underlying power dynamics relating to local political authority and access to local resources. These findings were drawn from a six-month fieldwork that included collection of documentary evidence, observation of customary courts, and in-depth interviews with a wide range of stakeholders such as judicial officials, paralegals, traditional authorities, as well as local residents who seek justice in multiple forums. Comparative analysis was largely sub-national in order to capture multiple layers of complexity in the traditional authority structure as well as important regional variation in Sierra Leone and Liberia.</dc:abstract><ual:supervisor>Rex J Brynen (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/zk51vk467.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/0r9676338</ual:fedora3Handle><dc:subject>Political Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A47429c94d"><dcterms:title>A numerical evaluation of the functionality of coronary bifurcation lesions</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mechanical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Pagiatakis, Catherine</ual:dissertant><dc:abstract>Cardiovascular disease remains the leading cause of death in the developed world. Coronary artery disease, characterized by the thickening and hardening of the vascular wall, and the narrowing of the arterial lumen, constitutes the highest proportion of all cardiovascular-related deaths. In clinic, up to 20 percent of all percutaneous coronary interventions are executed for the treatment of coronary bifurcation lesions (CBLs), which are defined as luminal narrowings that are in the vicinity of, and/or include a significant side branch. There is still much uncertainty and debate with regard to the assessment of CBL severity as well as the corresponding optimal interventional technique. In addition, the treatment of CBLs is associated with high rates of peri- and post-procedural clinical events, which renders them, to this day, a major limitation of interventional cardiology. Thus far, studies of CBLs have focused on the risk for the initiation and the progression of the disease relative to the disturbed flow hypothesis. There has not been an attempt to study CBLs from the functional perspective where an understanding of the factors that affect the clinical manifestation of the disease is sought, based on standard diagnostic indices. A functional evaluation of CBLs is important because the development of ischemia is more complex than that of isolated lesions due to the presence of haemodynamic interactions. Furthermore, whereas a gold-standard diagnostic index with a well-defined threshold for the onset of ischemia, namely the Fractional Flow Reserve (FFR), has been validated for the decision to revascularize, it is only utilized in less than 10 percent of catheterization labs worldwide. As such, cardiologists still rely heavily on an angiographic evaluation, which has been shown to have a poor correlation with haemodynamic impact. Therefore, insight on the functionality of CBLs is important in order to improve the current strategies and thus, reduce the risk associated with their treatment. Therefore, the main objective of the thesis is to provide insight on the functionality of CBLs. As such, a numerical model of a diseased coronary bifurcation that enables the computation of the gold-standard diagnostic index (FFR), is developed and subsequently utilized to systematically quantify and compare the effect of various geometric parameters on the corresponding haemodynamic impact. Specifically, the effects of lesion configuration and severity, and the effects of luminal eccentricity and bifurcation angle are investigated. The results showed that clinical manifestation of the disease is a complex factor of both local and global haemodynamic interactions. Specifically, the numerical simulations demonstrated that the number of lesions does not govern the functional severity of a CBL configuration and that the characteristics of the supplying vessel stenosis play an important role in the clinical manifestation of the disease. Within the limitations of the work, the findings have potentially important clinical implications which, with future validation, could help improve the current diagnostic guidelines.</dc:abstract><dc:abstract>La maladie cardiovasculaire est l'une des principales causes de mortalité dans les pays développés. La majorité des décès sont provoqués par une insuffisance coronarienne, qui consiste à un épaississement et un endurcissement de la paroi vasculaire, ainsi qu'un rétrécissement de la lumière artérielle. Jusqu'à 20 pourcents des interventions coronariennes percutanées (ICP) servent à traiter des lésions de bifurcation coronaires (LBC), qui sont sujettes à un rétrécissement de la lumière vasculaire situé dans une branche de bifurcation significative ou bien à proximité de celle-ci. Les incertitudes persistent quant au degré de sévérité des LBC et les traitements qui leurs sont associés. Ces derniers sont reconnus pour être à l'origine de conséquences cliniques néfastes aux niveaux péri- et post-procéduraux. Il s'agit donc d'une limitation majeure en cardiologie interventionnelle.Jusqu'à présent, les études traitant des lésions de bifurcation coronaires se concentraient principalement sur le risque de l'apparition de la maladie, ainsi que de sa progression, prenant en compte la perturbation locale du flux sanguin. Cependant, aucune étude ne porte actuellement sur la fonctionnalité des LBC, alors que cela pourrait amener à une meilleure compréhension des facteurs à l'origine des symptômes cliniques. En effet, une analyse fonctionnelle des LBC serait primordiale puisque le développement de l'ischémie y est plus complexe que celui des lésions isolées dû à la présence des interactions hémodynamiques. Une norme de référence étalon appelée Fraction de Flux de Réserve (FFR, mesure de la réserve coronaire) a été développée à partir du degré de sévérité de l'ischémie pour définir le besoin d'une intervention de revascularisation. Cependant, au niveau mondial, moins de 10 pourcents des laboratoires de cathétérisme cardiaque utilisent cette méthode et les cardiologues continuent à se fier à l'angiographie. Or, cette dernière a d'ores et déjà présenté de faibles corrélations avec les états hémodynamiques des patients. C'est pourquoi une compréhension de la fonctionnalité des LBC par rapport à la FFR est importante, et permettrait d'améliorer les diagnostics des patients (avec l'angiographie), réduisant ainsi les risques associés aux traitements. Pour conclure, l'objectif principal du projet de doctorat vise à mieux comprendre la fonctionnalité des LBC. Un modèle numérique de bifurcation coronaire athérosclérotique, permettant de calculer la FFR, a été développé. Il a ensuite été utilisé pour quantifier et comparer de façon systématique l'effet de divers paramètres géométriques sur l'état hémodynamique. Plus spécifiquement, le degré de sévérité des lésions, leurs configurations, l'excentricité de la lumière et l'angle de bifurcation, ont été étudiés. D'après les simulations numériques, les signes cliniques de la maladie seraient dépendants des interactions hémodynamiques locales et globales. En particulier, les résultats ont démontré que le nombre de lésions présentes n'affecte pas la fonctionnalité d'une configuration de LBC et que les caractéristiques de la lésion principale (dans le vaisseau mère) jouent un rôle central dans l'apparition des symptômes cliniques. Ainsi, malgré les limitations, les résultats ont potentiellement des implications cliniques importantes pouvant aider, après validation, à améliorer le diagnostic des patients.</dc:abstract><ual:supervisor>Jean Claude Tardif (Supervisor2)</ual:supervisor><ual:supervisor>Rosaire Mongrain (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/gx41mm50x.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/47429c94d</ual:fedora3Handle><dc:subject>Mechanical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Axs55mf510"><dcterms:title>Evaluation of pork quality by proton nuclear magnetic resonance</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Bioresource Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Burelle, Ian</ual:dissertant><dc:abstract>Consumers today are offered a diversified meat section at their local retail outlets. As the number of meat sources grows, consumers favour meat of higher quality. Competition is coming not only from other meat products but also from meats being imported internationally. There are growing concerns that pork quality is in fact struggling. After decades of research, issues such as pale soft exudative (PSE) pork and dark firm dry (DFD) pork remain prevalent. If such products reach the consumer, the industry risks the consumer no longer choosing pork in the future. To prevent economic losses to the industry, these products need to be detected and withheld. The problem lies in the fact that conventional techniques for meat quality measurement are slow, expensive, and destructive. Effective quality assurance programs that can keep up with market trends require a fast, non-destructive, method for measuring meat quality. The purpose of this study is to investigate low-field time-domain proton nuclear magnetic resonance (NMR) as a tool for rapid, non-destructive, multidimensional meat quality assessment. In the first section, a review of current literature with regards to meat science, meat quality, and meat quality measurement tools and techniques. This is followed by a review of current literature regarding NMR as a tool for meat quality assessment. NMR is then used in an experiment to investigate its applicability in measuring cooking loss, drip loss, and thaw loss. This was performed by using a transverse relaxometry experiment using a benchtop NMR at 6 MHZ at 4 °C of samples averaging 539 g – a size and temperature closer to what might be seen in industrial applications. Currently, the literature has exclusively investigated NMR as a tool for measuring meat quality on samples often around 1 x 1 x 5 cm in size and at room temperature. This study seeks to close that knowledge gap. Using multivariate analysis, a correlation of r = 0.686, 0.573, and 0.452 for cook, drip, and thaw loss, respectively, was obtained. The second part, NMR was used for the measurement of solid fat content of pork fat at nine separate temperatures in order to predict iodine value. A correlation of r = 0.87 was obtained. This shows potential applicability in industry, though this research raised important questions of what other chemical properties might influence SFC properties; these notions are investigated.In conclusion, NMR showed promising correlations and revealed that it does, upon further research and development, have the potential to measure meat quality attributes such as cooking loss, thaw loss, drip loss, and fat consistency.</dc:abstract><dc:abstract>Les consommateurs font maintenant face à une grande diversité de viandes dans leurs marchers locaux. Cela leur permet de choisir des viandes de plus en plus hautes en qualité. La compétition arrive non seulement de d'autre produits à base de viande, mais aussi de viandes importées de l'international. La qualité de porc n'est pas optimale et les producteurs de porc s'en rendent compte. Après des décennies de recherches, les problèmes de qualité tels que la viande pâle, molle et exsudative en plus de viande foncée, dure et sèche sont encore commun. Quand des produits de basses qualités sont consommés, il se peut que le consommateur décide de ne plus vouloir acheter de porc dans le futur. Une grande partie du problème se trouve dans le fait que les techniques conventionnelles pour mesurer la qualité de viande sont dispendieuses, lentes, et destructives. Pour un contrôle de qualité efficace, il faut des technologies rapides, économiques, et non-destructives. Le but de cette thèse est d'investiguer la relaxométrie par résonance nucléaire magnétique (RMN) comme outil pour mesurer la qualité de viande de manière multidimensionnel.Dans la première partie de cette thèse, je présenterai une revue de littérature à propos de la science et la qualité de viande, les techniques de mesure de qualité de viande et puis finalement, une revue de l'utilisation de l'RMN pour mesurer la qualité de viande. L'RMN est ensuite utilisée dans une expérience pour investiguer sa capacité de mesurer la perte en cuisson, perte en eau et en cycle de gèle-dégèle. Cette dernière a été réalisé par relaxométrie transversale par un RMN de 6 MHz sur des échantillons de 539 g, à une température moyenne de 4 °C – des valeurs plus près de ce qui se verraient en industrie. En ce moment, la littérature regarde uniquement des échantillons de viande d'environ 1 cm x 1 cm x 5 cm à température pièce. Cette étude cherche à approfondir nos connaissances dans le domaine de qualité de viande par RMN. Après une analyse multi variable, une corrélation de r = 0.686, 0.573, 0.452 a été obtenue en perte à la cuisson, perte en eau et ainsi qu'en cycle de gèle-dégèle. Dans la deuxième partie, RMN a été utilisée pour mesurer le pourcentage de gras en état solide à 9 températures différentes pour chaque échantillon dans le but de prédire la valeur d'iode. Une corrélation de r = 0.87 a été obtenue. Ceci démontre l'applicabilité de l'RMN pour l'industrie. Cela aussi nous permet d'investiguer quels autres facteurs pourraient influencer la consistance d'un gras.En conclusion, l'RMN pourrait devenir applicable dans l'industrie si les recherches et développements de l'RMN ainsi que la viande se poursuivent. Elle démontre le potentiel de mesurer la perte à la cuisson, perte en eau et en cycle de gèle-dégèle, ainsi que la consistance des gras.</dc:abstract><ual:supervisor>Michael O Ngadi (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/02870z61f.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/xs55mf510</ual:fedora3Handle><dc:subject>Bioresource Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ag732dc47s"><dcterms:title>Characterization of wood sheathed cold-formed steel diaphragms: Under in plane loading (phase 2 of diaphragm research program)</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Civil Engineering and Applied Mechanics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Latreille, Patrick</ual:dissertant><dc:abstract>There exists a desire to better understand the in-plane response of cold-formed steel (CFS) framed diaphragms subjected to seismic loading. At present, the diaphragm design information available in the North American design standards (AISI S240, AISI S400) is not applicable in Canada and is of limited scope in the United States and Mexico. These design provisions are based largely on basic principles of engineering and the extrapolation of design methods used for wood diaphragms. Few full-scale tests have been performed to validate the accuracy of these design values and equations for real world application. Furthermore, the effect of non-structural components on the in-plane strength and stiffness of the diaphragm component has yet to be explored. In an effort to provide insight into the complex nature of the diaphragm structure and the influence of non-structural components, a research program was initiated in the Jamieson Structures Laboratory at McGill University focusing on the characterization of the behaviour of CFS framed - wood sheathed diaphragms under in-plane loading. A total of six 3.7m x 6.1m CFS diaphragm specimens with oriented strand board sheathing were tested following the cantilever test method. Three of these diaphragms focused on the effects of structural changes, while the remaining three were used to investigate the effects of incorporating non-structural elements in the construction. This thesis contains a detailed account of these six tests, including their design, construction and the overall results and observations. The tests focusing on structural changes, demonstrated that the orientation of the joists with respect to loading had little effect on the overall diaphragm response, strap blocking for panel edges was shown to be just as effective as full blocking and an upper threshold for shear strength and stiffness was reached by reducing the spacing of the perimeter fasteners. The tests of diaphragms built having non-structural elements demonstrated that both the addition of a single layer of gypsum panels to the underside of the diaphragm and a 19mm gypcrete topping poured on top of the wood sheathing had a significant strengthening and stiffening impact on the overall diaphragms response. Design predictions were calculated using the AISI S400 Standard for both deflection and shear strength. Meaningful comparisons were only realized for design deflection when the equation for shear walls was used with the values adjusted to remove inelastic behaviour, while no meaningful comparisons were realized for shear strength due to the limited information available within the S240 and S400 standards.</dc:abstract><dc:abstract>Il y a actuellement un manque de compréhension concernant la réaction des diaphragmes en acier formé à froid (AFF) soumis à un chargement sismique. Les informations contenues dans les normes de conception nord-américaines (AISI S240, AISI S400) concernant la conception des diaphragmes ne sont pas applicables au Canada et sont limités à une utilisation aux États-Unis et au Mexique. Ces dispositions de conception sont basées en grande partie sur des principes fondamentaux d'ingénierie et l'extrapolation des méthodes de conception utilisées pour les diaphragmes en bois. Peu de tests à grande échelle ont été réalisés pour valider l'exactitude de ces équations pour une application en situation réelle. En outre, l'effet des éléments non structuraux sur la résistance latérale et la rigidité du diaphragme n'a pas encore été exploré. Dans le but de mieux comprendre la complexité de la structure des diaphragmes et l'influence des éléments non-structuraux, un programme expérimental a été initié dans le laboratoire de Structures Jamieson de L'Université McGill. Le programme est centré sur la caractérisation des diaphragmes avec cadre en AFF et platelage en bois soumis à des charges latérales. Un total de six spécimens de diaphragmes 3.7m par 6.1m en AFF avec des panneaux de lamelles orientées (OSB) ont été testés avec la méthode du porte-à-faux. Trois de ces diaphragmes ont été utilisés pour examiner les effets des changements structurels et les trois autres pour étudier les effets de l'intégration des éléments non structurels dans la construction. Cette thèse contient un compte détaillé de ces six essais incluant leur conception, leur construction ainsi que les résultats et observations globales. Les tests centrés sur les changements structurels ont démontré que l'orientation des solives par rapport au chargement a un effet mineur sur la réponse globale du diaphragme. Le blocage réalisé avec des sangles a été aussi efficace que le blocage complet. En diminuant les espaces entre les vis sur le périmètre des panneaux, la résistance et la rigidité ont été augmentées.  Les tests des diaphragmes construits avec des éléments non-structurels ont démontré que l'addition d'une seule couche de panneaux de gypse sur la face inférieure du diaphragme et une garniture de gypcrete de 19mm versée sur le dessus du platelage de bois a eu un impact significatif sur la résistance et la rigidité des diaphragmes. Un dimensionnement préliminaire a été effectué avec la Norme AISI S400 afin de comparer la résistance et les déformations obtenues lors des tests. Des résultats satisfaisant ont été atteint pour les déformations lorsque l'équation pour les murs de cisaillement a été utilisée, ajusté d'un comportement élastique. Par contre, les résultats des tests ne corroboraient pas aux valeurs théoriques fournies par la norme pour la résistance au cisaillement, notamment à cause du manque d'information disponible dans les normes S240 et S400. </dc:abstract><ual:supervisor>Colin Andrew Rogers (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/k930c100m.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/g732dc47s</ual:fedora3Handle><dc:subject>Civil Engineering &amp; Applied Mechanics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9p290d182"><dcterms:title>The predictive power of self-rated health for mortality across national, epidemiologic, and socioeconomic contexts</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Sociology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Falconer, James</ual:dissertant><dc:abstract>Pendant des décennies, la mesure de la santé auto-évaluée a été une caractéristique des enquêtes sur la santé de la population. Cet indicateur a été démontré comme un prédicteur robuste de la mortalité, dépassant souvent le pouvoir prédictif des facteurs médicaux dits plus « objectifs ». Toutefois, la polémique persiste quant à la comparabilité de cet indicateur entre des groupes sociaux, pour qui le pouvoir prédictif de la santé auto-évaluée semble varier. Plutôt que d'assumer que cette variation démontre le manque de validité de l'indicateur, cette thèse prend pour point de départ une théorie avancée quant aux processus cognitifs sous-tendant l'auto- évaluation de santé qui permet d'avancer des hypothèses quant aux effets modérateurs attendus de certains facteurs sociaux. Ainsi, cette thèse étudie l'effet modérateur de plusieurs de ces facteurs sociaux sur le pouvoir prédictif de la santé auto-évaluée au Canada, qui offre un cas intermédiaire unique entre les nombreuses études sur ce sujet provenant surtout d'Europe et des États-Unis. Quatre chapitres empiriques testent l'effet de (1) l'âge et le sexe, (2) le revenu et l'éducation, (3) le contexte national, en comparant le Canada avec les États-Unis, et (4) la présence d'un diagnostic préalable de la maladie indiquée comme étant la cause du décès. La santé auto-évaluée, la mortalité, et un ensemble de variables de contrôle, sont mesurées en utilisant des données longitudinales de haute qualité, représentatives au niveau national, de l'Enquête nationale sur la santé de la population (ENSP), qui a recueilli des données tous les deux ans de 1994 à 2010. Le pouvoir prédictif est modélisé en utilisant la régression logistique avec des équations d'estimation généralisées. Les résultats montrent que la santé auto-évaluée est un facteur prédictif significatif du risque de mortalité jusqu'à 14 ans avant le décès au Canada. Même en tenant compte de leurs différents niveaux généraux de santé auto-évaluée, les hommes et les femmes montrent un pouvoir prédictif pour la mortalité similaire. L'association prédictive diminue avec l'âge : les répondants de 80 ans ou plus ont montré une réduction de puissance prédictive par rapport aux répondants de 30-64 ou 65-79 ans, mais surtout parmi les femmes. Par ailleurs, le revenu et l'éducation ont un effet faible sur le pouvoir prédictif au Canada. Bien que nous ayons émis l'hypothèse que la puissance prédictive serait supérieure au Canada face aux États-Unis, les résultats ne montrent pas de différences statistiquement significatives entre ces pays. La trajectoire du pouvoir prédictif vers la fin de la vie pourrait donc s'avérer un phénomène plus universel qu'initialement anticipé. Cependant, le statut socio-économique a semblé modérer la pouvoir prédictif de la santé auto-évaluée plus fortement aux États-Unis qu'au Canada, en accord avec l'hypothèse que l'égalité socio-économique et l'accès aux soins de santé réduit l'impact de l'éducation et le revenu pour la pouvoir prédictif. Enfin, l'un des principaux mécanismes invoqués pour expliquer le pouvoir prédictif de la santé auto-perçue est directement testé. Ces analyses novatrices suggèrent en effet qu'une large proportion de la puissance prédictive dans les dernières années de la vie est expliquée par la connaissance parmi les répondants du diagnostic de la maladie indiquée comme la cause du décès. Ces résultats offrent un soutien indéniable pour la théorie cognitive expliquant la persistance de l'effet prédictif de la santé auto-évaluée.</dc:abstract><dc:abstract>For decades the measurement of self-rated health has been a mainstay of population health surveys. It has been consistently demonstrated as a reliable predictor for mortality, often exceeding the predictive power of other "objective" medical factors. Yet, because a growing number of studies have found this predictive power to vary depending on social characteristics (e.g., gender, socioeconomic status), debate still persists about the extent to which self-rated health can be validly used in the study of social inequalities in health. Drawing from a theoretical framework for the cognitive processes underlying the self-assessment of health, this dissertation seeks to test the hypothesis that social characteristics moderate the predictive power of self-rated health in systematic ways. Using the Canadian context as a unique intermediate case between the many studies originating primarily from Europe and the United States, four empirical chapters test the moderating effect for the predictive power of self-rated health of (1) age and sex, (2) income and education, (3) national context, by comparing Canada with the United States, and (4) disease diagnosis, which directly tests a theorized mechanism by which self-rated health predicts mortality. Self-rated health, mortality, and a large set of control variables are measured using nationally-representative longitudinal panel data from the National Population Health Survey (NPHS) collected biennially from 1994-2010. Predictive power is modelled using generalized estimating equation logistic regression. Findings show that self-rated health is a significant predictor of mortality up to 14 years prior to death in Canada. Despite their different overall levels of self-rated health, men and women show similar predictive power for mortality. The predictive association diminishes with increased age, as respondents over 80 showed reduced predictive power relative to respondents aged 30-64 or 65-79. However, distinguishing the age gradients by sex revealed that the age gradient in predictive power is only observable among women. Income and education only weakly moderated the predictive power of self-rated health in Canada, and the gradient was only observable between the highest and lowest education and income groups. Although I hypothesized better predictive power in Canada than in the United States, findings did not show statistically significant differences between the countries. This may suggest that the trajectory of the predictive power of self-rated health toward the end of life is a more general phenomenon across contexts than originally believed, which poses a challenge to several of the hypothesized social mechanisms that determine predictive power. However, SES appeared to moderate predictive power more strongly in the United States than in Canada, consistent with the hypothesis that better socioeconomic equality and access to health care reduces the impact of SES. Finally, I directly test a mechanism by which self-rated health is theorized to predict mortality, and find that a major portion of the predictive power of self-rated health in the final years of life is explained by respondent knowledge of the disease conditions which eventually cause their death. This novel finding supports one of the foremost theories putting cognition and knowledge at the root of why self-rated health is such a robust predictor of mortality.</dc:abstract><ual:supervisor>Amélie Quesnel Vallée (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/cr56n3713.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9p290d182</ual:fedora3Handle><dc:subject>Sociology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Axg94hs31q"><dcterms:title>Color measurement at low light levels</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Rezagholizadeh, Mehdi</ual:dissertant><dc:abstract>Ambient light level drives the human visual system into three phases: scotopic, mesopic and photopic vision. In photopic conditions, light level is far above the visual system activation threshold; cones are mainly responsible for our color vision and rods are saturated due to their higher sensitivity to light. Mesopic range refers to the condition where both cones and rods are active and contribute to color vision. In scotopic vision, the light level is very low such that cones are inactive (i.e. there is no color vision); however, rods are still able to contribute to our vision.Low light vision is of high importance in many computer vision and color science applications such as night time driving, display industry, consumer electronics, virtual reality devices, image sensors and photography at night mode. However, study of low light vision is acquainted with several challenges such as: first, the uncertainty and noise come into play; second, color perception mechanisms of the human visual system are not fully known; third, the number of existing research and models in the literature is small; fourth, computer vision field and industry are far behind not only the current findings, but also the existing well-known perceptual models in the domain.The human visual system is driven by photons. The details of the colorful journey of photons from triggering photoreceptors to the final visual perception stage inside the visual system is still to a high extent unknown. The methodology of this thesis involves: studying from first principles the physical rules governing the probabilistic nature of human vision at low light levels; modeling mesopic color perception using the maximum entropy based spectral theory of color vision; and then developing a real time bidirectional mesopic color appearance model to be used in the image rendering algorithms which are responsible for reproducing colors of the image as they appear in the original scene. Several simulations and computational tests on various datasets with real world and synthetic images are performed, and the methods proposed in this thesis are compared with other existing techniques. The results show the importance of mesopic color appearance modeling and the vital role of noise at low light levels. Moreover, the results support the feasibility of spectral modeling for mesopic vision, and this thesis suggest a bidirectional color appearance model for the purpose of luminance retargeting of images in the image rendering pipeline.</dc:abstract><dc:abstract>Le niveau de luminance ambiant entraine le système visuel humain dans trois domaines : la vision scotopique, mésopique et photopique. En condition photopique, la luminance est de loin supérieure au seuil d'activation des cônes ; les cônes sont principalement responsables de notre perception chromatique et les bâtonnets sont saturés à cause de leur plus grande sensibilité à la lumière. Le domaine mésopique se rapporte à la condition où les cônes et les bâtonnets sont actifs et contribuent à la perception chromatique : la luminance est au-dessus du seuil du cône et au-dessous de la marge de saturation du bâtonnet. Dans la vision scotopique, la luminance est très basse telle que les cônes sont inactifs ; cependant, elle est toujours au-dessus du seuil de détection des bâtonnets. Cette thèse se concentre sur la vision mésopique et scotopique et étudie la perception humaine des couleurs en ces deux domaines du système visuel humain. La vision en faible luminance est d'une grande importance dans plusieurs applications de la vision par ordinateur et de la colorimétrie tels que la conduite de nuit, l'industrie de l'affichage, l'électronique grand public, les dispositifs de réalité virtuelle, les capteurs d'images et la photographie en mode nocturne. Cependant, l'étude de la perception chromatique en faible luminance est accompagnée de plusieurs défis comme : premièrement, l'entrée en jeu de l'incertitude et du bruit ; deuxièmement, la méconnaissance de la plupart des mécanismes de la perception chromatique chez l'humain ; troisièmement, le faible nombre de recherches existantes et de modèles dans la littérature ; quatrièmement, le retard du domaine de la vision par ordinateur et de l'industrie non seulement en termes de résultats actuels, mais également au niveau des modèles perceptuels bien connus du domaine.Le système visuel humain réagit aux photons qui atteignent la rétine. Les détails du voyage haut en couleur des photons, du déclenchement des photorécepteurs à l'étape finale de perception visuelle à l'intérieur du système visuel, sont en grande partie toujours inconnus. La méthodologie de cette thèse comprend : l'étude des lois physiques qui régissent la nature probabiliste de la vision humaine en conditions de faible luminance ; la modélisation de la perception mésopique des couleurs en utilisant la théorie spectrale de la vision chromatique basée sur l'entropie maximale ; et le développement d'un modèle mésopique d'apparence de la couleur bidirectionnel et temps réel qui peut être utilisé par les algorithmes de rendu d'image qui sont responsables de la reproduction des couleurs en conditions de faible luminance de façon fidèle à la scène originale. Plusieurs expériences objectives sur divers ensembles de données avec des images réeles et synthétiques sont exécutées et les méthodes proposées dans cette thèse sont comparées à d'autres techniques existantes. Les résultats démontrent l'importance de la modélisation de l'apparence des couleurs (color appearance modeling) en condition de faible luminance et le rôle essentiel du bruit en de telles conditions. Plus encore, les résultats démontrent la faisabilité de la modélisation spectrale pour la vision mésopique et cette thèse suggère un modèle bidirectionnel d'apparence des couleurs à des fins de recalage de la luminance des images dans le pipeline de rendu d'image. </dc:abstract><ual:supervisor>James J Clark (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/37720g51z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/xg94hs31q</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A3484zk313"><dcterms:title>Surgical innovation: fracture fixation targeting device</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Surgery</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Rajkumar, Vijidha</ual:dissertant><dc:abstract>Ce projet d'innovation chirurgicale vise à répondre aux besoins cliniques présents dans le service d'orthopédie à l'Hôpital général de Montréal. Notre groupe multidisciplinaire se compose d'étudiants au niveau maîtrise en administration, ingénierie et chirurgie expérimentale. Ensemble, nous suivons le processus d'innovation: de la phase d'identification à la phase d'invention. D'un vaste éventail de besoins cliniques divers dans plusieurs aspects de la pratique clinique et chirurgicale orthopédique, nous concentrons notre étude à l'optimisation de la procédure de clouage intramédullaire (IM) pour les fractures des os longs. Nous produisons quatre solutions innovantes - fil de guidage, nouveau foret pour l'os, mini c-arm, et capteurs de stress des clous - pour accroître l'efficacité de la procédure: guidage du clou, dimensionnement, alésage du canal, et la partie de fixation des clous de la procédure. Dans le cadre de l'étape de sélection de concept de la phase d'identification, nous recueillons les commentaires de chirurgiens, d'innovateurs, de représentants d'entreprises de technologie médicale et de professeurs afin de choisir et d'optimiser notre solution finale à la procédure de clouage intramédullaires. Notre innovation est le Hawk-eye : Fracture Fixation Targeting Device qui utilise les fréquences radio pour localiser des trous distaux sur les clous intramédullaires. Ce dispositif vise à accroître l'efficacité du verrouillage distal pendant le clouage IM, tout en réduisant l'exposition à la radiation reçue à la fois par les chirurgiens et par le patient pendant la fixation.</dc:abstract><dc:abstract>This surgical innovation project aims to address the clinical needs present in the orthopaedics department at the Montreal General Hospital. This multidisciplinary group consists of business, engineering and experimental surgery Masters students. Together, the students underwent the process of innovation: from the identification phase to invention phase. From a broad spectrum of diverse clinical needs in various aspects of orthopaedic clinical and surgical practice, the group narrowed the scope to optimizing the intra-medullary (IM) nailing procedure for long bone fractures. The innovation team generated four innovative solutions - guide wire, novel bone drill, mini c-arm and nail stress sensors - to increase the efficiency of the procedure: the nail guiding, sizing, channel reaming, and the nail fixation portion of the procedure. As part of the concept-screening step in the identification phase, the team received feedback from surgeons, innovators, medical technology business representatives and professors to choose and optimize the final solution to the intra-medullary nailing procedure. The team's innovation is the Hawk-eye: Fracture Fixation Targeting Device that utilizes radio frequency to locate distal holes on the intra-medullary nails. This device aims to increase the efficiency of distal locking during IM nailing, while reducing the amount of radiation both surgeons and patient receives during fracture fixation. </dc:abstract><ual:supervisor>Edward Harvey (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/k643b364q.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/3484zk313</ual:fedora3Handle><dc:subject>Surgery</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Af7623g164"><dcterms:title>A randomized cluster trial to evaluate the effect of iodized salt exposure on birth outcome and infant development in Ethiopia</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Dietetics and Human Nutrition</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Mohammed, Husein</ual:dissertant><dc:abstract>Les objectifs principaux de cette thèse étaient d'examiner les effets du sel iodé introduit au sein d'une population d'Ethiopie modérément déficiente en iode (1) sur la grossesse et les issues de la naissance, (2) sur la croissance et le développement mental des enfants, (3) sur les fonctions thyroïdiennes et la médiation hormonale du développement cognitif.Soixante districts dans la région Amhara ont été aléatoirement assignés dans le groupe d'intervention (accès tôt au sel iodé) ou contrôle (accès plus tard à travers la pénétration du marché). Des 30 districts d'intervention et 30 districts contrôle, 22 ont été aléatoirement sélectionnés dans chaque groupe pour la collecte des données. Un total de 1220 femmes enceintes qui ont conçu après le début de l'intervention ont été évaluées sur leur statuts en iode et en fer, le sel de leur ménage testé pour sa teneur en iode. Leurs enfants ont été évalué entre 2 et 13 mois, des données collectées sur les caractéristiques socio-économiques, l'historique de gestation, les paramètres de grossesse et de naissance, la mortalité infantile, la stimulation psycho-sociale et l'insécurité alimentaire du ménage. L'alimentation de l'enfant, l'anthropométrie, la teneur en iode de l'urine, l'hémoglobine, le développement moteur mental (Echelles Bayley : cognitif, langage expressif et réceptif, la dextérité) ont également été évalués. Le sérum des enfants a été testé pour les hormones thyroïdiennes, la ferritine et l'inflammation. Une analyse en intention de traiter des covariables, contrôlant pour les grappes, la régression logistique et l'analyse de médiation ont été conduites.  Cette étude faisait partie d'un essai clinique randomisé enregistré sous clinicaltrials.gov No : NCT01349634.Un total de 1024 enfants ont été inclus dans l'analyse. Moins de ménages d'intervention utilisent le sel iodé par rapport aux contrôles (92% vs 95%, p=0,01) mais la teneur en iode de l'urine des mères était adéquate et significativement plus élevée dans les villages d'intervention que dans les contrôles (médiane 163 vs 121 µg/L, p&lt;0.0001). En plus, moins de mères (28% vs 41%, p&lt;0,05) et d'enfants (13% vs 20%, p&lt;0,05) étaient carencés en iode au seuil de 50 µg/L dans le groupe d'intervention comparé au contrôle.  Les mères étaient en moyenne âgées de 28 ans avec un faible niveau d'éducation (77% illettrées) et peu de ressources (score de 2,4 sur 10) et la plupart avaient des animaux d'élevage (94%). Leurs pratiques d'hygiène et d'accès à l'eau étaient pauvres (61% ont un score &lt;2 sur 3). L'intervention n'a pas significativement réduit les résultats négatifs de la grossesse (fausses couches aOR (95% IC) : 2,5 (0,9 ; 6,9)), les issues de naissances (mort-nés aOR (95% IC) : 1,0 (0,4 ; 2,8)), ou la mortalité infantile (aOR (95% IC) : 0,9 (0,5 ; 1,6)). Les enfants du groupe d'intervention ont obtenu un score cognitif plus élevé (33.3 ± 0.3 vs 32.6 ± 0.3; taille d'effet (d) = 0,17 ; points QI 103 vs 99 ; p=0,01) mais les autres scores du Bayley et les indicateurs de croissance n'étaient pas différents entre les deux groupes. Il y avait des interactions significatives entre l'effet de l'intervention et les niveaux de stimulation, les symptômes de dépression maternelle et les réserves en fer de l'enfant sur la cognition. Les enfants du groupe d'intervention avaient des niveaux moindres d'hormone de stimulation de la thyroïde (TSH) (2,4 ± 1,0 vs 2,7 ± 1,0, taille d'effet =0,8, p&lt;0,01) et la thyroglobuline (Tg) (41,6 ± 1,0 vs 45,1 ± 1,0, taille d'effet =0,14, p&lt;0,05). L'analyse de médiation a montré que le TSH était un médiateur partiel de l'effet de l'intervention sur le développement cognitif des enfants (Sobel z-score= 2,1 ± 0,06, p&lt;0,05).   L'exposition tôt au sel iodé pendant la grossesse n'a pas eu de conséquence négative sur la grossesse ni la naissance mais a amélioré le statut en iode des femmes enceintes, le statut en iode de leurs enfants, leur cognition et niveaux d'hormones thyroïdienne.</dc:abstract><dc:abstract>Iodine deficiency disorders (IDD) can result in physical and mental disabilities at all stages of human life. The main objectives of this thesis were to examine the effects of iodized salt introduced in a moderately iodine-deficient population of Ethiopia (1) on the pregnancy and birth outcomes, (2) on the growth and mental development of young children, and (3) on thyroid function and hormonal mediation of children's cognitive development. Sixty villages in 60 districts across six zones in the Amhara region of Ethiopia were randomly assigned to intervention (early access to iodized salt) or control (later access through normal market forces) arms. Out of the 30 intervention and 30 control districts, one village per district in 22 out of the 30 intervention and 22 out of the 30 control districts were randomly selected for data collection. A total of 1220 pregnant women who conceived after the intervention began were assessed for their iodine and iron status, and their household salt was tested for iodine. When their children were 2 to 13 months old, additional data were collected once on socio-demographic characteristics, gestational history, pregnancy and birth outcomes, infant mortality, child psycho-social stimulation in the home, and household food insecurity (HFI). Child's diet, anthropometry, urinary iodine, hemoglobin, motor milestones, and mental development (Bayley III scales: cognitive, expressive language, receptive language, and fine motor) were also assessed. Children's serum samples were tested for thyroid hormones, ferritin, and inflammation markers. An intention-to-treat analysis of covariance, controlling for clusters, logistic regression analysis and mediation analysis were conducted. The study was part of a randomized clinical trial to evaluate the effect of iodized salt on child development, registered at clinicaltrials.gov No: NCT01349634.A total of 1035 children were followed up, of which 1024 were included in the analysis. Even though fewer intervention than control households used iodized salt (92% vs 95%, p=0.01), the maternal urinary iodine levels were adequate and significantly higher in the intervention than in the control villages (median 163 vs 121 µg/L, p&lt;0.0001). Additionally, fewer mothers (28% vs 41%, p&lt;0.05) and children (13% vs 20%, p&lt;0.05) were iodine deficient at less than 50 µg/L in the intervention compared to the control group. The mothers were on the average 28 y of age with low education (77% illiteracy) and low assets (2.4 out of 10); most had livestock (94%). Their water and sanitation practices (61% scored &lt;2 out of 3) were poor. The intervention did not significantly reduce adverse pregnancy outcomes (miscarriage aOR (95% CI): 2.5 (0.9, 6.9), birth outcome (stillbirth aOR (95%Cl): 1.0 (0.4, 2.8)) or infant mortality (aOR (95%CI): 0.9 (0.5, 1.6)). The intervention children had a higher cognitive score (33.3 ± 0.3 vs 32.6 ± 0.3; effect size (d) =0.17; IQ points 103 vs 99; p=0.01) but other Bayley scores and child growth indicators did not differ from control children.  There were significant interactions between the intervention effect and levels of psychosocial stimulation, maternal depression, and child iron stores on cognition. Intervention children compared to control children had lower serum thyroid stimulating hormone (TSH) (2.4 ± 1.0 vs 2.7 ± 1.0, effect size=0.18, p&lt;0.01) and thyroglobulin (Tg) (41.6 ± 1.0 vs 45.1 ± 1.0, effect size=0.14, p&lt;0.05) levels. Mediation analysis showed that TSH was a partial mediator of the effect of the intervention on children's cognitive development (Sobel z-score= 2.1 ± 0.06, p&lt;0.05). Early exposure to iodized salt during pregnancy did not reduce adverse pregnancy and birth outcomes but improved pregnant women's iodine status, and their children's iodine status, cognition, and thyroid hormone levels. </dc:abstract><ual:supervisor>Grace Marquis (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/5t34sm966.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/f7623g164</ual:fedora3Handle><dc:subject>Dietetics and Human Nutrition</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Az890rx12m"><dcterms:title>New concepts in team theory: mean field teams and reinforcement learning</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Arabneydi, Jalal</ual:dissertant><dc:abstract>Cette thèse se compose de deux parties dans lesquels chaque partie introduit un nouveau concept dans la théorie de l'équipe. Dans la première partie, nous introduisons des systèmes avec des agents partiellement échangeables. Un système est appelé partiellement échangeable si elle peut être divisée dans des sous-populations où les agents sont échangeables. Une sous-population d'agents est appelé échangeables si la manière dont les agents sont indexés n'a aucune incidence sur la dynamique et le coût de celle-ci. En pratique, cette insensibilité à l'indexation émerge dans de nombreuses applications naturelles. Par exemple, dans les systèmes de puissance, la dynamique du système et le coût ne changerait pas si les maisons dans un quartier résidentiel ont été numérotés différemment; en robotique en essaim, la dynamique etle coût dépend de la position des robots, et non de la manière dont les agents sont indexés. Premièrement, nous montrons qu'un système avec des agents partiellement échangeables est équivalent à un système oùla dynamique et le coût sont couplés à travers le comportement global des agents (appelés champ moyen). Ensuite, nous enquêtons et identifions la solution optimale ---sous la structure du partage des information du champ moyen qui n'est pas classique---de deux modèles différents: la linéaire quadratique ainsi que la chaîne contrôlée de Markov. Nous démontrons que les solutions optimales, contrairement aux résultats existants dans la théorie de l'équipe, sont évolutives aux systèmes à grande échelle. Nous utilisons cette théorie pour résoudre des modèles idéalisés de réponse à la demande dans les systèmes d'alimentation et de l'allocation des ressources dans les réseaux.Dans la deuxième partie, nous étudions les systèmes avec une structure de partage d'information historique partielle---qui englobe une grande classe de problèmes de l'équipe, y compris les équipes à champ moyens---où les agents ne connaissent pas le modèle complet du système. Ces agents doivent apprendre les stratégies optimales en interagissant avec leur environnement en utilisant l'apprentissage par renforcement. Nous développons un algorithme de renforcement d'apprentissage qui garantit  solution epsilon-optimale performance. Comme une étape intermédiaire de ce développement, nous revissions le processus de décision  partiellement observable bien connu de Markov et nous proposons une nouvelle approche pour apprendre solution epsilon-optimale. La nouveauté de cette approche est d'identifier l'espace de planification basé sur la structure du modèle. Pour illustrer l'algorithme, nous développons un algorithme d'apprentissage de renforcement pour deux utilisateurs d'un accès multi canal de diffusion, qui est un exemple de référence.</dc:abstract><dc:abstract>This thesis consists of  two parts wherein each part introduces  a new concept in team theory. In the first part, we  introduce systems with partially exchangeable agents.  A system is called partially exchangeable if it can be partitioned into sub-populations where agents are exchangeable. A sub-population of agents is called exchangeable if the manner in which agents  are indexed  does not affect the dynamics  and  cost.  In practice,  this insensitivity to the index naturally  emerges in many  applications.  For example, in power systems, the system dynamics and cost would not change if the houses in a residential neighbourhood were numbered differently; in swarm robotics, the dynamics andcost depend on the position of the robots, not on how the agents are indexed.  We first show that a system with partially exchangeable agents  is  equivalent to a  system where agents are coupled in the dynamics and  cost through the aggregate behaviour of agents (called mean-field).  Then, we  investigate and identify the optimal strategy---under mean-field sharing information structure which is non-classical---for two different models: linear quadratic and controlled Markov chain. We show that the optimal strategies, unlike the existing results in team theory,  are scalable to large scale systems.   We use the theory to solve  idealized models of demand response in power systems and resource allocation in networks.In the second part, we  study systems with partial history sharing information structure---which encompasses a large class of team problems  including  mean-field teams---when  agents do not know the complete model of the system.  The agents  must learn the optimal strategies by interacting with their environment using  reinforcement learning. We develop a reinforcement learning algorithm that guarantees epsilon-team-optimal performance. As an intermediate step of this development, we revisit the well-known partially observable Markov decision process and propose a novel approach  to  find an  epsilon-optimal solution. The novelty of this approach is to  identify the planning space based on  the structure of the model.   To illustrate the algorithm, we develop a reinforcement learning algorithm for the benchmark example of two-user multi access broadcast channel and present numerical results. </dc:abstract><ual:supervisor>Mahajan, Aditya (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/ht24wm82m.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/z890rx12m</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9306t183f"><dcterms:title>Transfer and multitask learning methods for improving brain signal analysis</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Computer Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Wang, Boyu</ual:dissertant><dc:abstract>The human brain is one of the most complicated biological systems in the world. The brain activities measured by various signals such as electroencephalogram (EEG), electrocorticogram (ECoG), and functional magnetic resonance imaging (fMRI) provide avenues that can help understand the underlying mechanisms of the brain as well as diagnosis brain disorders and the related diseases.  However, without the proper techniques to analyze the brain signals, they are of limited value. In this thesis, we formulate the brain signal analysis as pattern recognition problems and emphasize the role of machine learning techniques in feature extraction and classification of EEG/ECoG signals, where we primarily consider two scenarios: epileptic seizure detection and translation of brain activities into control commands for a brain-computer interface (BCI) system.  The first part of this thesis focuses on online cost-sensitive learning problem arise from epileptic seizure detection, where the data is collected incrementally over time, and the seizures are relatively rare compared to non-seizure brain activities. We generalize a number of batch cost-sensitive ensemble learning algorithms to the online setting, and show that the convergence of the proposed algorithms is guaranteed under certain conditions. In the second part of this thesis, we handle another learning paradigm called transfer and/or multitask learning in the context of online learning, which is also of practical value to develop effective patient-specific seizure detection algorithms.  We follow the line of our work on online cost-sensitive ensemble learning, and present online boosting algorithms for transfer and multitask learning.  The third contribution of this thesis consists of introducing a novel learning framework called the multitask generalized eigenvalue program, which is originally motivated by the spatial filter design for BCIs. By assuming that leading eigenvectors of related generalized eigenvalue problems (GEPs) lie in some subspace that can be approximated by a sparse linear combination of basis vectors, our method jointly solves multiple related GEPs, which substantially enriches the possibilities of the traditional multitask learning framework.  The last piece of our work aims to handle nonlinear multitask classification of EEG signals for BCIs. Taking the advantages of both dictionary learning and gradient boosting, the proposed model can be applied to a variety of loss functions and can readily accommodate many choices of nonlinear base algorithms for multitask learning.    While our work is primarily motivated by brain signal analysis, the proposed algorithms are quite general and can be applied to many other related machine learning problems.</dc:abstract><dc:abstract>Le cerveau humain est l'un des systèmes biologiques les plus complexes dans le monde. Les activités cérébrales mesurées par différents tests tels que l'électro-encéphalographie (EEG), l'électrocorticographie (ECoG) et l'imagerie par résonance magnétique fonctionnelle (IRMf) offrent des indices nous permettant de comprendre les mécanismes internes du cerveau ainsi que permettent de diagnostiquer les troubles cérébraux et les maladies reliées. Par contre, sans technique pour analyser les signaux cérébraux captés, les résultats de ces tests sont sans valeur. Dans cette thèse, nous formulons le problème d'analyse des signaux cérébraux en tant qu'un problème de reconnaissance de formes et soulignons l'importance de techniques d'apprentissage automatique dans l'extraction de caractéristiques et dans la classification des signaux EEG/ECoG pour lesquels nous nous intéressons à deux scénarios particuliers: la détection de crises d'épilepsie ainsi que la traduction d'activités cérébrales en commandes pour un système d'interface cerveau-ordinateur (ICO). La première partie de cette thèse se concentre sur le problème d'apprentissage en ligne avec coûts variés survenant de la détection de crises d'épilepsie où les données sont recueillies au fur et à mesure et où les crises sont relativement rares par rapport aux autres activités normales du cerveau. Nous généralisons plusieurs algorithmes au contexte en ligne et montrons que notre algorithme a une garantie de convergence sous certaines conditions.Dans la deuxième partie de cette thèse, nous nous concentrons sur un autre paradigme d'apprentissage appelé transfert et/ou apprentissage multi-tâches dans le contexte d'apprentissage en ligne ce qui est aussi d'intérêt pratique dans le développement efficace d'algorithmes de détection de crises d'épilepsie pour patients individuels. %Nous suivons notre travail de online cost-sensitive ensemble learning (I DONT KNOW HOW TO TRANSLATE THAT) et présentons un algorithme boosting en ligne (I DON'T KNOW HOW TO TRANSLATE BOOSTING) pour les apprentissages transferts et multi-tâches.La troisième contribution de cette thèse consiste en l'introduction d'un nouveau cadre d'apprentissage appelé "multitask generalized eigenvalue program" qui a été motivé par la conception des filtres spatiaux pour l'ICO. En supposant que les "vecteurs principaux du problème de valeur propre généralisé" se trouvent dans un sous-ensemble qui peut être approximé par une combinaison linéaire "de moindre degré" de la base vectorielle. Notre méthode résout aussi plusieurs problème de vecteurs principaux reliées ce qui enrichit les possibilités des méthodes traditionnelles d'apprentissage multi-tâches.La dernière partie de notre travail concerne la classification multi-tâches non linéaire des signaux EEG pour les ICO. En utilisant le l'apprentissage automatique d'un dictionnaire, le modèle proposé peut être appliqué à une variété de fonctions d'erreur et peuvent facilement accommoder plusieurs choix d'algorithmes non linéaires de base pour l'apprentissage multi-tâches.Même si notre travail concerne principalement les signaux d'activités cérébraux, les algorithmes présentés sont généraux et peuvent être utilisés dans plusieurs problèmes d'apprentissage automatique similaires.</dc:abstract><ual:supervisor>Joelle Pineau (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/td96k5193.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9306t183f</ual:fedora3Handle><dc:subject>Computer Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A08612r23h"><dcterms:title>An examination of the trend-renewal process for use in recurrent events modelling in sports and medicine</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Mathematics and Statistics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Zhao, Meng</ual:dissertant><dc:abstract>The trend-renewal model for recurrent time-to-event data is seldom used outside of the reliability literature. This thesis thoroughly discusses the foundations of the trend-renewal process, emphasizing its applicability in the fields of sports injury and medicine. It proposes ways to better utilize a popular choice of parametric framework to address research questions in practical settings, in particular, an alternative to the classical Cox proportional intensities formulation of covariate effects. Simulation studies are carried out to evaluate the finite sample inference of parametric trend renewal models with unobserved heterogeneity. Finally, an application to a medical dataset is provided.</dc:abstract><dc:abstract>Le modèle renouvellement-de-tendance pour les données récurrentes de temps de survie est rarement utilisé en dehors de la littérature de fiabilité. Cette thèse examine en profondeur les fondements du processus renouvellement-de-tendance, mettant l'accent sur son application dans les domaines des blessures sportives et de la médecine. Elle propose des moyens pour mieux utiliser un cadre paramétrique populaire pour répondre aux questions de recherche en milieu pratique, en particulier, une alternative à la méthode classique Cox d'intensités proportionnelles. Des simulations sont effectuées pour évaluer l'inférence en échantillonnage fini de modèles renouvellements-de-tendance paramétriques avec hétérogénéité non observée. Enfin, une application à un ensemble de données médicales est fournie.</dc:abstract><ual:supervisor>Russell Steele (Internal/Supervisor)</ual:supervisor><ual:supervisor>Ian Shrier (Internal/Cosupervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/1c18dj193.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/08612r23h</ual:fedora3Handle><dc:subject>Mathematics and Statistics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ahd76s282r"><dcterms:title>Dynamic analyses for privacy and performance in mobile applications</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Computer Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Xia, Mingyuan</ual:dissertant><dc:abstract>Mobile applications (also called apps) have greatly extended and innovated users' daily tasks. The mobile programming model features event-driven execution, rapid changing APIs (about three generations per year) and ubiquitous accesses to user's personal data. These features enrich app functionalities but also give rise to many new software problems that impact performance or damage user privacy, many of which are not occasional programming mistakes. In this thesis, we systematically study these problems and develop dynamic program analyses to effectively detect, diagnose and fix these new problems. We start by researching the sensitive data leakage problem in apps. Since mobile apps can access various sensitive user data stored on the device, data leaks become a great concern for both end users and app market operators. Existing leak detecting approaches rely on static analysis that does not perform well on real-world apps with growing complexity, further limiting their adoption for real usage. We propose AppAudit, which embodies a novel dynamic analysis that can execute part of the app code while tracking the dissemination of sensitive data. AppAudit also has a static analysis to shrink analysis scope and boost analysis performance. The synergy of two analyses achieves higher detection accuracy, runs 8.3x faster and uses 90% less memory on real-world Android apps as compared to previous approaches. Based on the analysis building blocks from AppAudit, we further develop binary instrumentation to profile and improve app performance. We study 115 thousand apps and common performance anti-patterns from existing literature. Based on these understandings, we propose AppInspector, which instruments apps to profile a small set of methods while collecting various app runtime diagnostic data. These profiling data is transformed into a graph structure, where AppInspector programmatically diagnoses three common performance anti-patterns from this graph. We also develop AppSwift based on AppInspector, which transforms app code to automatically fix some performance anti-patterns and improve app performance. Both tools instrument app code automatically. Instrumented apps can run on unmodified Android OSes and thus being readily deployable to existing test environments. With extensive tests on real-world apps, AppInspector uncovers 22 performance issues per app, with detailed analysis results to guide developers to fix them; AppSwift automatically eliminates about 5 of such issues without any code modification from the app developer. We believe that the analysis methodologies, frameworks and tools developed in this thesis can assist developers in debugging various performance problems and better protecting user privacy.</dc:abstract><dc:abstract>Les applications mobiles (également appelés apps) ont considérablement étendu et innovée les  tâches quotidiennes des utilisateurs. Le modèle de programmation mobile dispose d'exécution événementielle, API évolution rapide (environ trois générations par an ) et omniprésente des accès aux données personnelles de l'utilisateur. Ces fonctionnalités enrichissent app fonctionnalités , mais aussi donner lieu à de nombreux problèmes nouveaux logiciels que la performance de l'impact ou de dommages utilisateur vie privée, dont beaucoup ne sont pas des erreurs de programmation occasionnelles. Dans cette thèse, nous étudions systématiquement ces problèmes et développons le programme dynamique des analyses pour détecter efficacement, diagnostiquer et résoudre ces nouveaux problèmes.% Nous commençons par rechercher le problème de fuite de données sensibles dans des apps. Comme les applications mobiles peuvent accéder à diverses données sensibles de l'utilisateur stockés sur l'appareil, les fuites de données devient une grande préoccupation pour les utilisateurs finaux et les opérateurs du marché de l'app. Les méthodes de détection de fuites existantes s'appuient sur l'analyse statique qui ne fonctionne pas bien sur les applications dans le monde r'el avec une complexité croissante. Nous proposons AppAudit, qui incarne une nouvelle analyse dynamique qui peut exécuter la partie de l'app code tout en effectuant le suivi de la diffusion des données sensibles. AppAudit possède également une analyse statique pour rétrécir l'analyse des performances de l'analyse et boost scopie.La synergie des deux analyses permet d'obtenir une plus grande précision de détection, 8.3x plus rapide et utilise exécute 90% moins de mémoire sur les applications Android dans le monde réel par rapport aux approches précédentes.Sur la base des blocs de construction de l'analyse d'AppAudit, nous développons l'instrumentation binaire au profil et améliorons les performances des applications. Nous étudions 115 mille applications et performance communs anti-modèles à partir de la littérature existante. Sur la base de ces accords, nous proposons AppInspector, qui instrumente applications au profil d'un petit ensemble de méthodes tout en recueillant des données de diagnostic différentes application d'exécution. Ces données de profilage se transforme en une structure de graphe, où AppInspector diagnostique trois performances commune anti-modèles à partir de ce graphique. Nous développons également AppSwift basé sur AppInspector, qui transforme le code de l'application pour corriger automatiquement certaines performances anti-modèles et d'améliorer les performances des applications.Les deux outils instrument code de l'application automatiquement. Les applications instrumentées peuvent fonctionner sur les systèmes d'exploitation Android non modifiés et donc être facilement déployable à des environnements de test existants. Avec des tests approfondis sur les applications du monde réel, AppInspector découvre 22 problèmes de performance par application, avec des résultats d'analyse détaillés pour guider les développeurs de les corriger; AppSwift élimine automatiquement environ 5 de ces questions sans aucune modification de code à partir du développeur de l'application. Nous croyons que les méthodes d'analyse, les cadres et les outils développés dans cette thèse peuvent aider les développeurs à déboguer divers problèmes de performance et une meilleure protection de la vie privée des utilisateurs.</dc:abstract><ual:supervisor>Xue Liu (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/x059cb00n.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/hd76s282r</ual:fedora3Handle><dc:subject>Computer Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ak0698b147"><dcterms:title>Diffuse field modeling: The physical and perceptual properties of spatialized reverberation</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Schulich School of Music</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Romblom, David</ual:dissertant><dc:abstract>Cette thèse porte sur les méthodes d'enregistrement et de reproduction du champ réverbérant diffus lors de représentations musicales dont les résultats peuvent aisément s'appliquer aux domaines de la réalité virtuelle, des jeux et des télécommunications. D'un point de vue scientifique la reproduction physique du champ acoustique vise à simuler le champ de pression à l'identique sur une surface aussi grande que possible. Tandis que la reproduction d'un point de vue artistique vise à permettre la manipulation de nombreux aspects du champ sonore afin de créer une présentation musicale convaincante pour les formats de distribution existants, tels que la stéréophonie. Entre ces deux approches, il est intéressant de créer un modèle acoustique virtuel à la fois, physiquement et perceptivement plausible, et qui puisse être manipulable selon les objectifs divers d'artistes, de chercheurs et d'ingénieurs.Les travaux de recherche de cette thèse ont donné lieux à quatre contributions. La première consiste en l'évaluation perceptive d'une technique d'enregistrement microphonique bien acceptée dans la communauté des ingénieurs du son et connue sous le nom de carré de Hamasaki. Cette technique qui peut s'interpréter d'un point de vue physique comme l'échantillonnage parcimonieux sur une surface limite est destinée, contrairement aux techniques de prise de son direct, à la prise du champ diffus réverbérant. La seconde est une expérience sur des seuils perceptifs démontrant que le système auditif humain est sensible à des différences directionnelles du champ réverbérant diffus, et que ces différences sont observées dans la pratique. Les résultats de ces deux expériences sont pris en compte dans la troisième contribution, qui met en place un système de synthèse de champ diffus. Ce système, fondé sur une approche physique, génère un grand nombre de canaux de réverbération diffuse à partir d'une seule réponse impulsionnelle de salle (RIS) de type B-Format.  La quatrième contribution est une évaluation perceptive de cette stratégie pour un large réseau de 20 haut-parleurs ainsi qu'une comparaison avec la technique du carré de Hamasaki pour un système de diffusion sur haut-parleur en 5.1.</dc:abstract><dc:abstract>This dissertation addresses methods of recording and reproducing the reverberant diffuse field for musical performances; it is equally applicable to virtual reality, gaming and telecommunications. A purely physical reproduction of a sound field might attempt to replicate identical pressure fields for as large an area as possible. A purely artistic reproduction may manipulate many aspects of the physical sound field to create compelling musical presentation for existing distribution formats such as 2-channel stereo. Intermediate to these approaches is a physically and perceptually plausible virtual acoustic model that can be manipulated for the manifold objectives of artists, researchers, and engineers. Four contributions are made. The first is a perceptual evaluation of a room microphone technique known as the Hamasaki Square that is well accepted in the sound recording community. This technique has physical interpretation as a sparse sampling of a bounding surface and, as opposed to direct sound techniques, is intended for the reverberant diffuse field. The second is a perceptual threshold experiment demonstrating that the human auditory system is sensitive to directional differences in the reverberant diffuse field and that such differences can be found in practice. The results of both of these experiments are considered in the third contribution, which is a physically-inspired strategy to generate a large number of channels of diffuse reverberation from a single B-Format Room Impulse Responses (RIR). The fourth contribution is a perceptual evaluation of the technique for a large 20-loudspeaker array and a comparison to the Hamasaki Square for a 5.1 loudspeaker array. </dc:abstract><ual:supervisor>Catherine Guastavino (Supervisor1)</ual:supervisor><ual:supervisor>Philippe Depalle (Supervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/rf55zb38p.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/k0698b147</ual:fedora3Handle><dc:subject>Music</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A2227ms50f"><dcterms:title>Advanced digital signal processing in coherent fiber optic communication systems</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Malekiha, Mahdi</ual:dissertant><dc:abstract>To satisfy the explosive growth in global Internet traffic, the development of transmission links that not only have high-capacity but are also flexible, reconfigurable, and adaptive is imperative. The advents of high speed digital-to-analog and analog-to-digital converters, and recent progress in complementary metal–oxide–semiconductor (CMOS) technology has facilitated the development of optical transceivers relying on coherent detection and digital signal processing (DSP) for the compensation of fiber impairments. Next generation coherent optical networks are anticipated to further approach the Shannon limit and deliver 400~Gb/s or 1~Tb/s data rates per channel, while providing flexibility and agility to maximize the utilization of the network resources. This thesis explores novel system architectures and advanced DSP algorithms to fulfill these design targets. Currently, chromatic dispersion (CD) compensation and forward error correction (FEC) decoders are the major power consuming (more than 50%) blocks of a conventional transceiver application-specific integrated circuit (ASIC). This thesis presents the concepts and architectures of multi-sub-band (MSB) signaling for mitigation of CD, and eliminating the need for a CD compensating equalizer in reduced-guard-interval (RGI) orthogonal frequency-division multiplexing (OFDM) and single carrier systems. The performance of the proposed techniques are experimentally evaluated using a leading-edge optical long-haul transmission test-bed. It is shown that the MSB technique, in addition to the evidently lower computational complexity, allows for a highly efficient adaptive rate smart transceiver implementation with lower system overhead and simplified parallelism, while attaining the same or better transmission reach and performance as the conventional transceiver. This is due to its higher tolerance to fiber nonlinearity.With coherent technology and advanced FEC, it is known that the capacity of current fiber optic transmission systems is fundamentally limited by fiber nonlinearities. We have optimized the perturbation based nonlinearity compensation (PB-NLC) equalization scheme and proposed a novel adaptive nonlinear equalizer. The performances of the aforementioned DSP equalization schemes are numerically and experimentally studied. It is found that the optimized technique demonstrates lower computational complexity over conventional PB-NLC. In addition, the proposed adaptive nonlinear equalizer does not require prior calculations of perturbation coefficients and detailed knowledge of the transmission link parameters. It achieves comparable performance to the PB-NLC. Unlike previously studied adaptive nonlinear equalization techniques, our algorithm takes advantage of common symmetries, avoids replication of operations, and only uses a few adaptive nonlinear coefficients. Finally, its computational complexity is smaller than previously proposed adaptive nonlinear equalization schemes, which meets the requirements of next generation optical networks.</dc:abstract><dc:abstract>Pour satisfaire la croissance explosive du trafic Internet mondial, le développement des liaisons de transmission qui ont non seulement une grande capacité, mais qui sont aussi flexibles, reconfigurables, et adaptables est impératif. L'avancement en vitesse des convertisseurs numériques-analogiques et analogiques-numériques, et les progrès récents dans la technologie des semi-conducteurs complémentaires à l'oxyde de métal (CMOS) ont facilité le développement d'émetteurs-récepteurs optiques utilisant la détection cohérente et le traitement numérique du signal (DSP) pour la compensation des détériorations de la fibre. Il est prévu que les réseaux optiques cohérents de la prochaine génération approcheront la limite de Shannon en livrant des débits de données de 400 ~Gb/s ou 1~Tb/s par canal, tout en offrant la flexibilité et l'agilité pour optimiser l'utilisation des ressources du réseau. Cette thèse explore de nouvelles architectures de systèmes et algorithmes DSP pour répondre à ces objectifs de conception.Présentement, la compensation pour la dispersion chromatique (CD) et la correction d'erreur directe (FEC) sont les principales consommatrices d'énergie (plus de 50%) des blocs d'un circuit intégré spécifique (ASIC) d'un émetteur-récepteur. Cette thèse présente les concepts et architectures de signalisation multi-sous-bande (MSB) pour diminuer l'effet de la CD et éliminer la nécessité d'un égaliseur CD dans les systèmes à garde-intervalle réduite (RGI) à multiplexage par répartition en fréquence orthogonale (OFDM). La performance des techniques proposées est expérimentalement évaluée à l'aide d'un banc d'essai de transmission optique à longue distance. Il est démontré que la technique MSB, en plus de réduire la complexité de calcul, permet le traitement en parallèle et rend possible de réaliser un émetteur-récepteur intelligent très efficace avec une faible surcharge. Tout cela en atteignant la même ou une meilleure portée de transmission et de performance que l'émetteur-récepteur classique. Ceci est dû à sa plus grande tolérance à la non-linéarité de la fibre.Avec la technologie cohérente et FEC moderne, il est connu que la capacité des systèmes de transmission à fibre optique est essentiellement limitée par la non-linéarité de la fibre. Nous avons optimisé la technique de compensation de non-linéarité à base de perturbation (PB-NLC) et proposé un nouvel égaliseur de non-linéarité adaptable. La performance de ces algorithmes DSP est numériquement et expérimentalement étudiés. On démontre que la technique optimisée a une complexité de calcul plus faible que la PB-NLC classique. En outre, cet égaliseur non linéaire adaptable ne nécessite pas de calculs antérieurs des coefficients de perturbation et de connaissance détaillée des paramètres de liaison de transmission. Il réalise des performances comparables à la PB-NLC classique. Contrairement aux techniques non linéaires d'égalisation adaptable étudiées précédemment, notre algorithme tire parti des symétries communes, évite la répétition des opérations, et utilise seulement quelques coefficients adaptables. Enfin, sa complexité de calcul est plus petite que des systèmes non linéaires proposés précédemment, il est donc conforme aux exigences des réseaux optiques de la prochaine génération.</dc:abstract><ual:supervisor>David V Plant (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/c534fr49t.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/2227ms50f</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Abz60d0220"><dcterms:title>AMPK-dependent regulation of foraging behaviours in Caenorhabditis elegans</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Biology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Ahmadi, Moloud</ual:dissertant><dc:abstract>L'homéostasie énergétique est d'une importance capitale pour la santé métabolique et la survie au cours d'un stress énergétique et cela doit se produire à la fois au niveau cellulaire et organismal. Inanition est donc souvent accompagnée de comportements compensatoires qui, pense-t-on, accroîent la probabilité de rencontrer une source d'alimentation. On ne fait que commencer à élucider comment les conséquences de la faim en termes de changements dans l'activité neuronale peut déclencher des réponses comportementales spécifiques d'adaptation. La protéine kinase activée par l'AMP (AMPK) est un capteur métabolique qui a été également proposé comme un régulateur clé de réponses comportementales d'adaptation à la privation de nourriture, bien que les mécanismes par lesquels cette kinase affecte ces résultats restent incertains. Nous avons observé que les larves de C. elegans affamées qui n'ont pas d'AMPK affichent un défaut locomoteur qui révèle leur incapacité à répondre de manière appropriée à l'épuisement des ressources. Nous avons observé que la reconstitution de l'AMPK dans les interneurones AIB et AIY qui sont impliqués dans les différentes réponses locomotrices associés à la disponibilité alimentaire réduite sauve complètement les défauts locomoteurs des mutants AMPK. En utilisant une combinaison d'analyse génétique classique, des approches optogénétiques et des techniques d'imagerie de calcium, nous avons constaté que, bien que l'AMPK ne participe pas à des aspects essentiels de la fonction neuronale, il joue un rôle clé dans la modulation de l'activité neuronale lors de la famine qui garantit à son tour des comportements adaptatifs en réponse à ce stress énergétique aiguë. De plus, nous avons découvert que l'AMPK médie cet effet en régulant les niveaux de récepteur type AMPA du glutamate GLR-1 et du récepteur métabotropique du glutamate MGL-1 dans les interneurones AIB et AIY respectivement, qui module en fin de compte la force synaptique dans des conditions de famine. Dans l'ensemble, notre étude suggère que, outre sa fonction bien connue dans le contrôle métabolique au niveau cellulaire, l'AMPK agit également comme un déclencheur moléculaire au niveau organismal pour réguler l'activité neuronale et éventuellement susciter des comportements adaptatifs en réponse à la faim.</dc:abstract><dc:abstract>Energy homeostasis is of paramount importance for metabolic health and survival during energy stress and this must occur at both the cellular and organismal level. Starvation is therefore often accompanied by compensatory behaviours that are believed to increase the probability of encountering a food source. But how hunger results in changes in the neural activity to trigger specific adaptive behavioural responses has yet to be elucidated. AMP-activated protein kinase (AMPK) is a key metabolic sensor that acts as a regulator of adaptive behavioural responses during acute starvation, although the mechanisms by which this kinase affects such outcomes remain unclear. We observed that the starved AMPK-deficient C. elegans larvae display a locomotory defect that reveals their inability to appropriately respond to resource depletion. We observed that reconstitution of AMPK in the AIB and AIY interneurons that are involved in the various locomotory responses associated with reduced food availability completely rescues the locomotory defects of AMPK mutants. Using a combination of classic genetic analysis, optogenetic approaches and calcium imaging techniques, we found that although AMPK is not involved in essential aspects of neural function, it does play a key role in the modulation of neuronal activity upon starvation which in turn ensures adaptive behavioural outcomes in response to acute starvation. Furthermore, we discovered that AMPK mediates this effect by regulating the levels of AMPA-type glutamate receptor GLR-1 and Metabotropic glutamate receptor MGL-1 in the AIB and AIY interneurons respectively, which ultimately modulates synaptic strength under conditions of starvation. Overall, our study suggests that besides its well-known function in metabolic control at the cellular level, AMPK also acts as a molecular trigger at the organismal level to regulate neuronal activity and eventually to elicit adaptive behavioural outputs in response to hunger.                          </dc:abstract><ual:supervisor>Richard D W Roy (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/wd376011j.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/bz60d0220</ual:fedora3Handle><dc:subject>Biology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ang451m246"><dcterms:title>New statistical methods for using confounders measured in a validation sample to enhance time-to-event analyses of large databases</dcterms:title><ual:graduationDate>2017</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Epidemiology and Biostatistics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Burne, Rebecca</ual:dissertant><dc:abstract>MOTIVATION Unmeasured confounding is a common, and important, problem in many observational studies. Pharmacoepidemiological studies, which typically rely on large administrative databases, are especially prone to this source of bias. Often, however, smaller validation samples are available, such as clinical datasets collected as part of a study into a particular disease or drug, which provide information on additional confounders, not measured in the large 'main' database. In the last decade, several methods have been proposed to utilize such validation samples in order to correct for unmeasured confounding bias in the analyses of large main databases. However, most recent methods do not extend to time-to-event analyses, and none have as yet been developed which deal with time-varying unmeasured confounding and mediation. OBJECTIVES, METHODS and RESULTS The over-arching aim of this thesis is, therefore, to develop and validate new methods for enhancing time-to-event analysis in the presence of unmeasured confounders available only in a small validation sample. The original contributions of the thesis consist of three manuscripts. In the first manuscript, I rely on comprehensive simulations to assess the performance, in time-to-event analyses, of an existing method for utilizing validation sample data to reduce unmeasured confounding bias, propensity score calibration (PSC), (Stürmer et al., 2005). The second manuscript proposes a novel, martingale residual (MR)-based imputation approach to this problem for time-to-event analysis, and validates it in simulations. In a real-life application, with an external validation sample, the method suggested a substantially weaker association between glucocorticoid therapy and risk of type II diabetes mellitus than conventional analysis that adjusted for only fully measured confounders. In the third manuscript, I extend the MR-based imputation to Cox Marginal Structural Models (MSM), with time-varying confounders/mediators, and demonstrate that it yields more accurate estimates than alternative methods based on regression calibration. I then apply the proposed method in Cox MSM analyses to re-assess the association between DPP-4 inhibitor use and risk of hypoglycemia, using a large database where HbA1c results, a potential time-varying confounder/mediator, is available only for a small subset of study subjects.</dc:abstract><dc:abstract>MOTIVATION Les facteurs de confusion non mesurés sont un problème commun, et important, dans de nombreuses études d'observation. Les études pharmacoépidémiologiques, qui reposent généralement sur de grandes bases de données administratives, sont particulièrement sujettes à cette source de biais. Souvent, cependant, des échantillons de validation plus petits sont disponibles, tels que des bases de données cliniques recueillies dans le cadre d'une étude sur une maladie ou un médicament particulier, ce qui fournit des informations sur des facteurs de confusion supplémentaires, non mesurés dans la grande base de données «principale». Dans la dernière décennie, plusieurs méthodes ont été proposées pour utiliser de tels échantillons de validation afin de corriger le biais dû aux facteurs de confusion non mesurés dans les analyses de grandes bases de données principales. Cependant, la plupart des méthodes récentes ne s'appliquent pas aux analyses de survie, et aucune n'a encore été développée pour traiter les facteurs de confusion non mesurés qui varient dans le temps. OBJECTIFS, MÉTHODES ET RÉSULTATS L'objectif global de cette thèse est, par conséquent, de développer et valider de nouvelles méthodes pour améliorer l'analyse de survie en présence de facteurs de confusion non mesurés disponibles uniquement dans un petit échantillon de validation. Les contributions originales de la thèse se composent de trois manuscrits. Dans le premier manuscrit, je compte sur des simulations exhaustives pour évaluer la performance, dans des analyses de survie, d'une méthode existante pour utiliser les données d'un échantillon de validation pour réduire le biais dû aux facteurs de confusion non mesurés, soit la calibration du score de propension (CSP) (Stürmer et al., 2005). Le deuxième manuscrit propose une nouvelle approche d'imputation basée sur les résidus de martingale (RM) pour ce problème dans le cas de l'analyse de survie, et la valide avec des simulations. Lors d'une application à une étude réelle, avec un échantillon de validation externe, la méthode a suggéré une association considérablement plus faible entre la corticothérapie et le risque de diabète de type II qu'une analyse conventionnelle ajustant uniquement pour les facteurs de confusion pleinement mesurés. Dans le troisième manuscrit, j'adapte la méthode d'imputation basée sur les RM au modèle structurel marginal de Cox (MSM Cox), avec des facteurs de confusion, et/ou médiateurs, variant dans le temps, et démontre qu'ils produisent des estimations plus exactes que les méthodes alternatives basées sur la calibration par régression. J'ai ensuite appliqué la méthode proposée dans des analyses avec MSM Cox pour réévaluer l'association entre l'utilisation d'un inhibiteur de la DPP-4 et le risque d'hypoglycémie, en utilisant une grande base de données où des résultats d'HbA1c, un facteur de confusion ou médiateur potentiel variant dans le temps, sont disponibles seulement pour un petit sous-ensemble des sujets de l'étude.</dc:abstract><ual:supervisor>Michal Abrahamowicz (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/m900nx178.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/ng451m246</ual:fedora3Handle><dc:subject>Epidemiology and Biostatistics</dc:subject></rdf:Description></rdf:RDF>