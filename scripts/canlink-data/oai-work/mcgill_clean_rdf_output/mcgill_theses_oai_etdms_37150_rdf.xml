<?xml version="1.0" encoding="UTF-8"?><rdf:RDF xmlns:oai="http://www.openarchives.org/OAI/2.0/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:ual="http://terms.library.ualberta.ca/" xmlns:bibo="http://purl.org/ontology/bibo/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:schema="https://schema.org/" xmlns:etdms="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A4x51hp76q"><dcterms:title>Mesoscale Eddy Transport across the Antarctic circumpolar current: a model-observation comparison</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Atmospheric and Oceanic Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Gauthier, Audrey-Anne</ual:dissertant><dc:abstract>La circulation méridienne de retournement joue un rôle important dans la redistribution de chaleur, de carbone et de nutriments à travers l’océan global. Le transport méridien des eaux profondes à travers les fronts puissants du courant circumpolaire antarctique (ACC) est une partie intégrante de cette circulation globale. En effet, ces eaux remontent près des côtes antarctiques, où elles absorbent de grandes quantités de chaleur et de carbone d’origine anthropique présents dans l’atmosphère. Jusqu’à récemment, les études quantifiant le transport au travers des fronts de l’océan Austral ont été principale- ment réalisées à l’aide de modèles numériques, les données d’observation étant éparses dans cette région. Ces études ont montré que les tourbillons de méso-échelle sont les principaux agents du transport de masse vers le sud à travers les fronts de l’ACC au-dessus des seuils topographiques. Désormais, le nombre de données recueillies par le réseau de flotteurs Argo permet la reconstruction du transport des tourbillons dans l’ACC. Cette étude examine le transport de masse à travers le Front Polaire, estimé entre la surface et environ 2 km de profondeur avec des simulations de contrôle d’une série de modèles climatiques différant uniquement par la résolution horizontale de la composante océanique (1◦, 0.25◦ et 0.1◦). Ces transports sont comparés avec des données d’observations du projet Argo. Les analyses montrent que la composante tourbillonnaire est sous-estimée par les modèles par rapport aux observations d’au moins 50 %. La paramétrisation du transport des tourbillons de méso-échelle utilisée dans le modèle à 1◦ corrige l’intensité du transport tourbillonnaire résolu, de sorte que le transport tourbillonnaire net est plus proche de celui provenant des observations par rapport aux autres modèles. Dans chaque modèle, le transport tourbillonnaire est principalement limité aux régions associées à une topographie abrupte en accord avec les observations. Les modèles à 0.1◦ et 0.25◦ produisent exclusivement des transports tourbillonnaires dans les zones à topographie escarpée tant dis que le modèle à 1◦ simule également un transport significatif entre ces zones comme le montrent les observations. Le modèle à 1◦ s’avère donc être le modèle le plus réaliste de la suite en termes de magnitude et de distribution spatiale</dc:abstract><dc:abstract>As part of the Meridional Overturning Circulation, deep waters cross the strong fronts of the Antarctic Circumpolar Current (ACC) to upwell near the Antarctic coast, where they take up large amounts of excess heat and anthropogenic carbon from the atmo- sphere. Until recently, studies quantifying cross-frontal transport in the Southern Ocean have been mostly carried out with numerical models as observational data were too sparse in this region. Modelling studies have shown that mesoscale eddies are the main agents for the southward mass transport across the fronts of the ACC above major topographic sills. Recently, the number of measurements collected by the Argo array of profiling floats has become sufficient to allow reconstruction of eddy transport across the ACC. This study examines the transport of mass across the Polar Front, calculated between the surface and around 2 km depth from control simulations of a suite of climate models differing solely in the resolution of the ocean component (1◦, 0.25◦, 0.1◦). These transports are compared to transport estimates from Argo float measurements. All models underesti- mate the eddy transport across the Polar Front, relative to the observation-based results by at least 50 %. The mesoscale eddy transport parameterization used in the 1◦ model is found to compensate for the weak resolved eddy transport so that the magnitude of the resulting eddy transport is the closest to that from observational estimates across the model suite. In all models, eddy transport is amplified at regions associated with sharp topography in agreement with observations. The 0.1◦ and 0.25◦ models exclusively produce transport at topographic features whereas the 1◦ model also simulates significant eddy transport in between those features in agreement with observations. The 1◦ model is found to be the most realistic model within the suite in terms of magnitude and zonal pattern</dc:abstract><ual:supervisor>Carolina Dufour (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/6h440z086.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/4x51hp76q</ual:fedora3Handle><dc:subject>Atmospheric and Oceanic Sciences</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aww72bh090"><dcterms:title>The development of a thermoelectric generator (TEG) concept for recycling waste heat into electricity</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Mining and Materials</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Riggio, Justin</ual:dissertant><dc:abstract>There is a significant amount of waste heat from industrial processes in the form of liquid or gaseous streams. With a growing demand for electrical power, there is a need for solutions to make use of this heat and increase the overall energy efficiency of the processes in question. Although the most practical solution may be to recuperate the waste heat for heating and/or cooling purposes, its transformation into electricity allows for versatility in its end-use. The thermoelectric generator (TEG) presented in this work consists of heat pipes to capture and concentrate the waste heat, coupled with thermoelectric modules to convert some of the heat directly into electricity. A bench-top prototype was designed and built as a proof-of-concept. The experimental testing of the prototype involved the measurement of power output to a load as a function of the temperature and flow rate of the inlet air, which represented the waste heat stream. The efficiency as a function of temperature and flow rate was then obtained by estimating the heat captured by the TEG using an empirical relation for a cylinder in a cross-flow. The results obtained for power output compared well with a model developed for thermoelectric module performance in both a simplified and detailed approach. However, as the steady-state temperature measurements from the hot-side and cold-side of the thermoelectric modules were input to the model, the effect of the heat pipes was not fully accounted for. There are factors, including thermal contact resistance at the interfaces and heat transfer limitations of the heat pipes that can be further investigated and improved in order to maximize the power per unit area of the TEG. Furthermore, recommendations for industrial application of this concept are discussed on the subject of performance optimization when scaling up and life-cycle analysis. To conclude, a strategy for the further development of this concept is presented</dc:abstract><dc:abstract>Les procédés industriels proiduisent une quantité importante de chaleur résiduelle sous forme d’écoulements liquide ou gazeux. Dû à la demande croissante en puissance électrique, il est nécessaire de trouver des solutions pour utiliser cette chaleur afin d'augmenter l'efficacité des procédés en question. Par exemple, la chaleur résiduelle pourrait être récupérée pour le chauffage et/ou le refroidissment. Cependant, la transformation de cette chaleur en electricité permettrait une plus grande polyvalence en terme d’utilisation finale. La génératrice thermoélectrique présentée dans ce mémoire est composée de caloducs pour capter et concentrer la chaleur résiduelle couplés de modules thermoélectriques afin de convertir une portion de la chaleur directement en énergie électrique. Un prototype a été conçu et construit afin de valider le concept. Le prototype a été testé en obtenant la puissance électrique en fonction de la température et du débit de l’air entrant, qui répresentait la chaleur résiduelle. L'efficacité énergétique en fonction de la température et du débit a été calculée par l’estimation de la chaleur capturée par la génératrice avec une relation empirique pour un cylindre soumis à un débit tranversal. Les résultats sont comparables à ceux obtenus à partir d’un modèle conçu pour la performance des modules thermoélectriques. Par contre, puisque le modèle requiert des mesures de température uniquement des côtés chaud et froid des modules thermoélectriques, l’effet complet des caloducs n’était pas pris en compte. Cependant, des facteurs comme la résistance de contact thermique et les limitations de transfert de chaleur pourraient être explorés en détail afin d’augmenter la puissance par superficie de la génératrice thermoélectrique. De plus, des recommandations sont décrites pour l'application industrielle de ce concept, plus précisément au sujet de l'optimisation de la performance avec accroissement d’échelle et de l’analyse du cycle de vie. En conclusion, une stratégie potentielle pour le développement de cette génératrice thermoélectrique est presentée</dc:abstract><ual:supervisor>Faramarz P Hassani (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/pn89db79c.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/ww72bh090</ual:fedora3Handle><dc:subject>Mining and Materials</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A3484zn200"><dcterms:title>Combining high-density electrical source imaging and hemodynamic responses to epileptic discharges</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Biomedical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Hedrich, Tanguy</ual:dissertant><dc:abstract>Electrical source imaging (ESI) and simultaneous electroencephalography and functional magnetic resonance imaging (EEG-fMRI) are two imaging techniques that are useful for the localization of the epileptogenic focus during the pre-surgical evaluation of patients with drug-resistant epilepsy. Whereas ESI and EEG-fMRI are usually employed separately, EEGrecorded in the MRI scanner can, in theory, be used to perform ESI. The purpose of this thesis was to assess the performance of joint ESI and EEG-fMRI analysis in the context of epilepsy. The thesis is organized around three contributions, forming the three manuscripts of this document.The goal of the first study was to evaluate and compare the spatial resolution of four source localization methods, based on high density EEG (hdEEG) or magnetoencephalography (MEG) data. The intrinsic spatial properties of the methods were studied through the analysis of their resolution matrices. Overall, the study showed that the method developed by our group, entitled coherent maximum entropy on the mean (cMEM), exhibited excellent performance in terms of localization error and spatial dispersion. Moreover, we obtained similar levels of spatial accuracy on MEG and hdEEG, further confirmed by real data EEG/MEG acquisitions of electrical median nerve stimulation. Our overall findings indicated that cMEM was a reliable and robust source localization technique, and was therefore a suitable candidate for the analysis of EEG inside the MRI scanner.In the second study, we specifically evaluated the performance of the source reconstruction of two ESI methods during a visual stimulation paradigm in two conditions: using hdEEG data acquired inside and outside the MRI scanner. We found that, even if EEG signals were distorted when acquired in the presence of a high magnetic field, ESI using cMEM performed inside the scanner remained accurate, exhibiting similar performance to ESI applied on EEG data acquired outside the scanner. This study demonstrated the feasibility of using ESI jointly with EEG-fMRI analysis.Finally, for the last study, we evaluated how ESI could be applied with simultaneous EEG-fMRI to guide fMRI analysis. Considering eight patients with epilepsy who underwent EEG-fMRI recordings, we used hdEEG reconstructions to automatically classify interictal epileptic discharges, to propose better regressors for fMRI statistical analyses. The classes of epileptic discharges identified with the automatic clustering from ESI analysis were used to build regressors for the fMRI analysis and compared to regressors obtained with the manual classification of epileptic discharges. We found that ESI results were overall spatially concordant with fMRI responses, and that the automatic classification of epileptic discharges provided similar results to the manual classification. This result opens the door for less operator-dependent approach for EEG-fMRI investigations in epilepsy.To summarize, we demonstrated in this thesis that cMEM was a source imaging technique exhibiting excellent spatial resolution, even in noisy environments such as EEG data acquired in the MRI scanner. This technique was used on a visual protocol inside the scanner and was proven to be robust to MR-related noise. Finally, when applied on epilepsy data, cMEM exhibited an excellent concordance with the fMRI clusters and was able to classify epileptic discharges in function of their source localization results, providing relevant information to propose a more accurate regression model for fMRI analysis</dc:abstract><dc:abstract>L’imagerie de sources électriques (ISE) et l’acquisition simultanée de l’électroencéphalographie et l’imagerie par résonance magnétique (EEG-IRMf) sont deux techniques d’imagerie utiles à la localisation du foyer épileptogène pendant l’évaluation pré-chirurgicale des patients présentant une épilepsie réfractaire aux médicaments. Bien que l’ISE et l’EEG-IRMf soient généralement employées séparément, l’EEG enregistrée dans le scanner IRM peut en théorie être utilisée pour effectuer l’ISE. L’objectif de cette thèse a été de tester la faisabilité et la performance de l’analyse jointe de l’ISE et de l’EEG-IRMf dans le contexte de l’épilepsie. Cette thèse s’organise autour de trois contributions, formant les trois manuscrits de ce document.Le but de la première étude a été d’évaluer et de comparer la résolution spatiale de quatre méthodes de localisation de sources, basées sur des données d’EEG haute densité (EEGhd) ou de magnétoencéphalographie (MEG). Les propriétés spatiales intrinsèques des méthodes ont été étudiées à travers l’analyse de leurs matrices de résolution. Globalement, l’étude a montré que la méthode développée par notre groupe, intitulée maximum d’entropie sur la moyenne cohérent (MEMc), présentait d’excellentes performances en termes d’erreur de localisation et de dispersion spatiale. Nos observations ont indiqué que MEMc était une technique de localisation de sources fiable et robuste, et qu’il était ainsi un candidat adapté pour l’analyse de l’EEG à l’intérieur du scanner IRM.Dans la deuxième étude, nous avons évalué spécifiquement la performance de la reconstruction de sources de deux méthodes d’ISE pendant une expérience de stimulation visuelle dans deux conditions: en utilisant les données EEGhd à l’intérieur ou à l’extérieur du scanner IRM. Nous avons trouvé que, même si les signaux EEG ont été déformés dans l’environnement de haut champ magnétique, l’ISE utilisant MEMc effectuée à l’intérieur du scanner demeurait précise, présentant des performances similaires à l’ISE appliquée à des données EEG à l’extérieur du scanner. L’étude a démontré la faisabilité de l’utilisation de l’ISE conjointement avec l’analyse EEG-IRMf. Finalement, dans la dernière étude, nous avons évalué comment l’ISE pourrait être appliquée avec l’EEGhd-IRMf pour guider l’analyse IRMf. En considérant huit patients atteints d’épilepsie qui ont réalisé des enregistrements EEG-IRMf, nous avons utilisé les reconstructions ISE pour classifier automatiquement les décharges épileptiques interictales dans le but de proposer de meilleurs régresseurs pour l’analyse statistique IRMf. Les classes de décharges épileptiques identifiées avec le clustering automatique ont été utilisées pour construire des régresseurs pour l’analyse IRMf et ont été comparées aux régresseurs obtenus avec la classification manuelle des décharges épileptiques. Nous avons trouvé que les résultats ISE étaient spatialement concordant avec les réponses IRMf, et que la classification automatique des décharges épileptiques apportait des résultats similaires à la classification manuelle. Il s’agit d’un résultat important permettant de considérer une approche moins dépendante de l’opérateur pour les analyses EEG-IRMf en épilepsie.Pour résumer, nous avons démontré dans cette thèse que le MEMc était une technique d’imagerie de sources présentant une excellente résolution spatiale, même dans un environnement bruité tel que les données EEG acquises dans le scanner IRM. Cette technique a été utilisée avec un protocole de stimulation visuelle à l’intérieur du scanner et a démontré sa robustesse au bruit lié aux forts champs magnétiques. Finalement, avec des données d’épilepsie, MEMc a présenté une excellente concordance avec les clusters IRMf et a pu classifier les décharges épileptiques en fonction de leurs résultats de localisation de sources, apportant des informations pertinentes afin de proposer un modèle de régression plus précis pour l’analyse IRMf</dc:abstract><ual:supervisor>Christophe Grova (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/fb494d77k.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/3484zn200</ual:fedora3Handle><dc:subject>Biomedical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ag732df32k"><dcterms:title>More powerful together: collaborative theorizing with social movements about decolonizing and decarbonizing Canada</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Natural Resource Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Gobby, Jennifer</ual:dissertant><dc:abstract>While the climate and inequality crises intensify in Canada, the federal government continues to approve pipelines and other oil and gas projects, violating commitments to both climate action and reconciliation with Indigenous nations. In response to this, grassroots groups, non-governmental organizations, and Indigenous communities are working hard to contest these projects and the colonial capitalist logics that underlie them. This thesis reports on four years of qualitative research developed and conducted in collaboration with people actively involved in the Indigenous land defense, climate/environmental justice, and anti-pipeline movements in Canada.The research explores, as a first central theme, the ways that movement actors understand the crises and their underlying causes and how they envision the worlds they want. People in these movements are seeing that climate change and inequality are both driven by colonial capitalism, which is undergirded by western worldviews that promote domination of people over nature and of people over other people. These systems have bred systemic disconnection from land and from each other, cutting us off from the communities, tools, and knowledges we need to get ourselves out of this mess. Their visions of the worlds they want conjure up a future of flourishing networks of decentralized, self-determining communities, powered by renewable energy, and learning from the land. This is a future where a hard process of decolonizing relations renders us all much more capable of living and making decisions together – decisions that benefit all beings. This future depends on a fundamental restructuring of our systems and a massive redistribution of wealth, power, and land.A second major theme of this work is the ways that people conceptualize how large-scale change happens. This theme is engaged first through a literature review of theories of change (TOCs) from various bodies of academic literature including Indigenous scholarship, Historical Materialism, Intersectional Feminism, Social Movement Studies, and Social-Ecological Systems Transformation.  This thesis also brings together the TOCs held by activists in these movements. Though their TOCs are diverse and conflicting at times, when brought together, they emphasize that transformation happens through a complex convergence of 1) the contexts in which we act, 2) people’s understanding, worldviews, and values and 3) building power through collective action and directing that power in various ways.  4) Undergirding these 3 factors is how we relate to each other and how all our efforts combine in mutually supporting ways.  The third major theme in this thesis examines the obstacles to change being faced. Activists and land defenders identified barriers that are external to the movement, including lack of public will, the capitalist-driven economic system, corporate influence over public decision-making, criminalization of activists, and others. People I spoke with also identified barriers more internal to the movements. These include fragmentation, internal tensions, the trends towards NGOization, hierarchical, centralized organizing structures, as well as the insular, overly-critical activist culture and activist burn-out. This project culminates in a collective strategizing on what can be done to overcome the barriers and increase the transformative power of these movements to successfully contest and weaken colonial capitalism and offer viable and inspiring alternatives. To overcome all these daunting barriers, we need much bigger, much stronger movements and to do so we need to develop more capacity for thinking and working across differences. A shared understanding that emerges from all the chapters is that forging stronger, more just relations within and across movements is crucial for strengthening our collective ability to bring radical change to Canada’s economic, political, and social systems</dc:abstract><dc:abstract>Alors que la crise climatique et des inégalités s’intensifient au Canada, le Gouvernement fédéral continue à approuver des projets de construction de d'oléoduc et d’expansion de l’extraction de combustibles fossiles (pétrole et gaz naturel), en violation de ses engagements en actions climatiques et de réconciliation envers les Premières Nations. En réponse, des mouvements populaires travaillent à contester ces projets. Cette thèse est le résultat de recherches qualitatives qui se sont déroulées sur quatre années et développées en collaboration avec des personnes engagées dans la défense des territoires autochtones, la justice climatique et les mouvements anti-pipeline au Canada. Le premier thème central de cette recherche consiste dans l’étude des modes d’après lesquels les acteurs des mouvements environnementaux comprennent les crises ainsi que leurs causes sous-jacentes et comment envisagent-ils le monde qu’ils désirent. Ces individus estiment que la crise climatique et les inégalités sont toutes deux produites par le capitalisme colonial, lequel est soutenu à son tour par les ontologies occidentales qui valident la domination de la nature par les humains et entre eux. Ces systèmes ont engendré une déconnexion systémique des humains avec la Terre, nous éloignant des communautés, des outils et des savoirs nécessaires pour nous sortir de ces systèmes. Les visions du monde futur évoquées par les militant.e.s sont constituées de réseaux de communautés florissantes, décentralisées et auto-déterminées, alimentées par les énergies renouvelables et attentives aux enseignements de la Terre. Ce futur en est un où le processus difficile de décolonisation nous rend tous et toutes davantage capables de vivre et prendre des décisions ensembles – décisions au bénéfice de l’ensemble des êtres. Ce futur dépend d’une restructuration fondamentale de nos systèmes et d’une redistribution massive de la richesse, du pouvoir et de la terre.Un second thème majeur de cette thèse concerne les modes d’après lesquels les individus conceptualisent comment les changements à grande échelle se produisent. Cette thèse rassemble les théories du changement soutenues par les militant.e.s des mouvements étudiés. Bien que ces théories soient variées et parfois contradictoires, lorsque rassemblées, elles mettent en évidence que les transformations se produisent à travers une convergence complexe : 1) de contextes d’action, 2) de compréhensions, d’ontologies, de visions du monde et de valeurs, 3) de construction de rapports de force à travers l’action collective ainsi que d’une direction de ces rapports de force dans des directions diverses, 4) des rapports entretenus entre individus ainsi que les efforts combinés de manière mutuellement constructive.Le troisième thème de cette recherche étudie les obstacles au changement. Ceux-ci incluent les obstacles externes aux mouvements, incluant le manque de volonté au niveau social, l’économie capitaliste, l’influence des intérêts corporatifs sur la prise de décision publique, la criminalisation des militant.e.s et autres. Les obstacles internes incluent la fragmentation, les tensions internes, la tendance à la transformation des mouvements sociaux en ONG hiérarchiques et centralisées.Cette thèse culmine par l’élaboration d’une stratégie collective sur ce qui peut être fait pour dépasser ces barrières et accroître le pouvoir de transformation de ces mouvements pour contester et affaiblir avec succès le capitalisme colonial et offrir des alternatives viables et inspirantes. Pour surmonter ces barrières intimidantes, nous avons besoin de mouvements beaucoup plus grands et puissants.Le thème principal émanant de ces chapitres est qu'il est nécessaire de forger des relations plus fortes et plus justes entre mouvements pour renforcir notre habileté collective d’apporter des changements radicaux aux systèmes économiques, politiques et sociaux du Canada</dc:abstract><ual:supervisor>Peter Gilbert Brown (Supervisor1)</ual:supervisor><ual:supervisor>Patricia Perkins (Supervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/hd76s4604.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/g732df32k</ual:fedora3Handle><dc:subject>Natural Resource Sciences</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9593v079g"><dcterms:title>Refinement of connections in the absence of synaptic activity</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Integrated Program in Neuroscience</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Chong, Yumaine</ual:dissertant><dc:abstract>Au début du développement postnatal, les circuits neuronaux sont initialement imprécis; les axones innervent trop de cellules cibles et celles-ci reçoivent un excès d'axones convergents. Par conséquent, plusieurs connexions qui se chevauchent sont formées. Après la naissance, les circuits neuronaux deviennent de plus en plus actifs et améliorent leur performance en se réorganisant : certaines connexions sont renforcées, tandis que d’autres sont éliminées. De plus, en l’absence de transmission synaptique, ce raffinement des connections n’aura pas lieu. Il est généralement accepté que ce processus est dirigé par l’activité postsynaptique en générant des signaux moléculaires qui agissent localement sur les terminaisons présynaptiques. Cependant, l’abolissement de l’activité synaptique aux connections à un niveau d’un circuit neuronal aura des répercussions sur l'activité des cellules qui suivent. La possibilité que ces cellules qui suivent pourraient générer des signaux jouant un rôle important dans le raffinement des connections précédentes est rarement considérée. Ces signaux pourraient être essentiels dans l'établissement de circuits qui innervent les cibles distantes avec spécificité et précision. L’objectif principal de ma recherche doctorale était d'améliorer notre compréhension des mécanismes qui dirigent le raffinement (i) en examinant comment les circuits neuronaux se développent en l’absence d’activité postsynaptique, (ii) en déterminant si les cibles terminales jouent un rôle dans le développement des circuits neuronaux précédents, et (iii) en identifiant les mécanismes moléculaires impliqués dans le raffinement des connexions.Afin d’approfondir ces sujets, j’ai comparé le développement des circuits sympathiques chez 3 modèles de souris. Dans le premier modèle, les souris possèdent une mutation du gène α3 (α3 KO), une sous-unité essentielle pour la formation des récepteurs nicotiniques de l'acétylcholine. Par conséquent, les circuits sympathiques des souris α3 KO ne fonctionnent pas. Pour le deuxième modèle, les gènes 4E-BPs ont été supprimés pour examiner le rôle de la traduction de l’ARNm dépendante de la coiffe (α3/4E-BP DKO). Troisièmement, j’ai généré un nouveau modèle de souris : les neurones sympathiques avec des récepteurs et les neurones sans récepteurs sont aléatoirement mélangés pour générer des ganglions mosaïques. Avec ce modèle, j’ai étudié (a) comment les axons préganglionnaires innervent une mosaïque des neurones actifs et inactifs; et (b) comment les neurones sympathiques se développent sans l'activité postsynaptique quand ces cibles reçoivent l’innervation fonctionnelle.Dans l’absence d'activité des circuits sympathiques, l’extension des dendrites est diminuée, les synapses sont mal dirigées vers le corps cellulaire, et les axones préganglionnaires ne se raffinent pas. D’autre part, les neurones inactifs peuvent se développer normalement si la traduction dépendante de la coiffe est augmentée, ou si leurs cibles reçoivent de l’innervation fonctionnelle. Selon ces résultats, je propose un modèle où les cibles actives envoient, de façon rétrograde, des signaux moléculaires qui jouent un rôle essentiel dans l’innervation et la différenciation des neurones sympathiques. Pour supporter cette hypothèse, les neurones sympathiques des souris α3 KO innervent mal leurs cibles tandis que les neurones inactifs des ganglions mosaïques innervent normalement leurs cibles. De plus, j’ai identifié un ensemble de gènes qui sont déréglés avec l’absence d'activité synaptique chez les souris α3 KO, mais sont restaurés à la normale dans les neurones inactifs des ganglions mosaïques.Collectivement, mes résultats suggèrent un modèle où l’innervation fonctionnelle des cibles génère des signaux rétrogrades qui modulent l’extension des dendrites, la distribution des synapses et le raffinement des connexions, grâce, en partie, à la régulation de l’expression des gènes impliqués dans le développement des circuits neuronaux</dc:abstract><dc:abstract>At the onset of early postnatal development, neural circuits are initially imprecise; axons innervate an excessive number of targets, and target neurons receive inputs from a surplus of converging axons, resulting in diffuse circuits with overlapping connections. As activity increases over time, these circuits improve their performance by refining their connections, strengthening some and eliminating others. Moreover, if there is no activity in the circuit, then the refinement of connections does not occur. It is generally thought that the increase in synaptic transmission drives refinement by stimulating the postsynaptic neuron to release retrograde signals that act locally on presynaptic inputs. However, abolishing synaptic transmission at one node of a circuit alters synaptic activity in downstream target neurons and organs. Often overlooked is the possibility that activity-dependent, target-derived factors play an essential role in mediating the refinement of connections on upstream neurons. Such long-range factors would coordinate refinement of upstream connections in a retrograde manner to establish circuits that innervate distal targets with specificity and precision. The main objective of my doctoral research was to improve our understanding of the mechanisms that underlie refinement by (i) investigating how neural circuits develop in the absence of postsynaptic activity; (ii) determining whether downstream target organs have a role in the retrograde regulation of neural development; and (iii) identifying molecular mechanisms involved in refinement.To address these issues, I compared the development of sympathetic circuits in three mouse models. In one model, mice have a deletion in the α3 nAChR subunit gene (α3 KO), an essential gene for the assembly of postsynaptic receptors on autonomic neurons. As a result, sympathetic circuits in α3 KO mice are silenced. In a second model, 4E-BP genes were deleted (α3/4E-BP DKO) to examine the role of cap-dependent mRNA translation. Third, to test the role of postsynaptic activity while maintaining functional innervation to sympathetic targets, I generated a novel mouse model in which sympathetic neurons with postsynaptic receptors and those without receptors are randomly intermingled to generate mosaic ganglia. Using this new mouse model, I investigated (a) how preganglionic axons innervate a mosaic population of active and inactive neurons, and (b) how sympathetic neurons develop in the absence of postsynaptic activity when their targets receive functional innervation.When sympathetic circuits are silenced as in α3 KO mice, dendritic growth is impaired, synapses are mistargeted to the cell soma, and preganglionic axons do not refine. On the other hand, when cap-dependent translation is enhanced, or when their targets receive functional innervation, synaptically silent neurons develop normally without postsynaptic activity. Most strikingly, preganglionic inputs onto these inactive neurons refine in the absence of synaptic transmission. I propose that activity-dependent, target-derived factors play an essential role in the innervation and differentiation of sympathetic neurons. In support of this idea, sympathetic neurons in α3 KO mice innervate their targets poorly, whereas inactive neurons in mosaic ganglia innervate targets normally. Furthermore, I identified a number of genes whose expression levels were misregulated in synaptically inactive sympathetic neurons of α3 KO mice, and restored to normal levels in synaptically inactive sympathetic neurons of mosaic mice.My results overturn a widely held belief that the refinement of connections and the extension of dendrites require postsynaptic activity. Insights from my experiments suggest a model in which activity-dependent, target-derived factors mediate dendritic growth, synaptic targeting and refinement, at least in part, by regulating the expression of genes involved in neuronal development and differentiation</dc:abstract><ual:supervisor>Ellis Cooper (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/7m01bq88t.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9593v079g</ual:fedora3Handle><dc:subject>Neuroscience</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A5m60qx10g"><dcterms:title>Peripheral quality control of mutant hERG potassium channels: contribution to long-QT syndrome pathogenesis and pharmacological correction</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Physiology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Foo, Brian</ual:dissertant><dc:abstract>Proteostasis of membrane proteins involves distinct quality control (QC) machinery acting at multiple cellular compartments. Newly synthesized plasma membrane (PM) proteins that either fail to fold in a timely manner or are irreversibly misfolded by mutations and environmental factors are retained in the endoplasmic reticulum (ER) and targeted for elimination by ER associated degradation pathways (ERAD). Cargoes that escape the ER QC or unfold following trafficking to the PM are rapidly internalized and delivered to lysosomes in a ubiquitination-dependent manner. Although clearance of aggregate-prone and potentially cytotoxic non-native molecules is a vital cellular function, the destruction of partially folded yet-functional PM proteins also contributes to the loss-of-function phenotype of numerous conformational diseases including cystic fibrosis and nephrogenic diabetes insipidus.The Human ether-a-go-go related gene (hERG) encodes the kv11.1 voltage-gated K+-channel responsible for the rapidly activating delayed rectifier K+-current (IKr) involved in cardiac repolarization. Loss of hERG function is associated with long-QT-syndrome type-2 (LQT2) which is characterized by impaired ventricular repolarization and increased risk of life-threatening cardiac arrhythmia. Most disease-associated mutations minimally affect channel gating or conductance; instead, premature degradation of partially misfolded channels by protein QC systems underlies the loss-of-functional expression phenotype. Impaired hERG functional expression has been primarily attributed to retention and degradation of nascent channels by the ER QC machinery. The contribution of peripheral QC systems to the disease phenotype, as well as their underlying molecular machinery remain poorly established. In this study, I characterize the cellular processing of a panel of hERG mutations in the N-terminal cytosolic Per-Arnt-Sim (PAS) domain. These mutations reduce hERG PM expression by variably compromising both their ER conformational maturation and cell surface stability. Elimination of PAS-mutant hERG from the PM involves accelerated internalization and lysosomal delivery at the expense of endosomal recycling. In contrast to other misfolded membrane proteins, PAS-mutant hERG are processed by a novel ubiquitin- and clathrin- independent mechanism. I also performed a high-throughput drug screen to identify potential rescuers of hERG PM expression. Using biochemical and functional cell-based assays, we identify two compounds, Anagrelide and DCEBIO, which rescue mutant hERG functional expression by inhibiting their internalization and lysosomal delivery. Previously-described hERG modulators either enhance channel folding efficiency at the expense of concurrent pore block, or potentiate hERG gating in the absence of established folding/expression rescue. Our compounds act via a novel mechanism and could potentially contribute to the treatment of LQT2 either alone or in combination with hERG ‘activator/potentiator’ compounds. Taken together, our findings describe the cellular processing of a panel of hERG mutants which are jointly regulated by both the established ER QC system and a novel Ub-independent PM QC mechanism. We demonstrate the importance of the peripheral QC machinery in conformational disease pathology: not only as a significant contributor to the loss-of-expression phenotype, but also as potential pharmacological target(s) for novel rescuers such as Anagrelide and DCEBIO</dc:abstract><dc:abstract>L’homéostasie des protéines membranaires (ou protéostasie) comporte une machinerie de contrôle de qualité (CQ) agissant au niveau de plusieurs compartiments cellulaires. Les protéines de la membrane plasmiques (MP) nouvellement synthétisées qui ne se replient pas correctement en temps voulu ou qui sont mal repliées de façon irréversible à cause de mutations ou de facteurs environnementaux sont retenues dans le réticulum endoplasmique (RE) et ciblées pour l’élimination par la voie de la dégradation associée au RE (ERAD). Les cargos qui échappent au CQ du RE ou qui se replient mal à la suite du trafic vers la MP sont rapidement internalisés et livrés aux lysosomes par la voie de dégradation ubiquitine dépendante. Bien que la clairance des molécules susceptibles de s’agréger ou des molécules cytotoxiques non-natives soit une fonction cellulaire vitale, la destruction des protéines de la MP partiellement repliées et pourtant fonctionnelles contribue également à la perte de fonction phénotypique de nombreuses maladies de conformation y compris la fibrose kystique et le diabète insipide néphrogénique. Le gène « Human ether-a-go-go related » (hERG) code pour kv11.1, un canal potassique voltage-dépendant responsable du courant K+ retardé impliqué dans la repolarisation cardiaque. La perte de la fonction hERG est associée avec le syndrome du QT long (LQT2) ce qui se caractérise par une repolarisation ventriculaire altérée et un risque accru d’arythmie cardiaque. La plupart des mutations associées à la maladie affectent très peu l’ouverture ou la conductance du canal ; mais la dégradation prématurée des canaux partiellement repliés par les systèmes de CQ des protéines entraine une perte de fonction d’expression phénotypique. L’expression fonctionnelle altérée de hERG a été principalement attribuée à la rétention et à la dégradation des canaux naissants par la machine CQ du RE. La contribution des systèmes de CQ périphériques au phénotype de la maladie, ainsi que leurs mécanismes moléculaires restent mal établis. Dans cette étude, je caractérise le processus cellulaire d’un ensemble de mutations du canal hERG dans le domaine cytosolique N-terminal Per-Arnt-Sim (PAS). Ces mutations réduisent l’expression à la MP de hERG en compromettant à la fois la maturation conformationnelle dans le RE et la stabilité à la surface cellulaire. L’élimination des mutants PAS de hERG de la MP entraine une internalisation accélérée et la livraison lysosomale au détriment du recyclage endosomal. Contrairement à d’autres protéines membranaires mal repliées, les mutants PAS de hERG sont pris en charge par un nouveau mécanisme indépendant de l’ubiquitine et de la clathrine. J’ai également effectué un criblage à haut débit de drogues afin d’identifier des correcteurs potentiels de l’expression a la MP de hERG. En realisant des essais biochimiques et fonctionnels sur des cellules, nous avons identifié deux composés, l’Anagrélide et DCEBIO, qui restaurent l’expression fonctionnelle des mutants hERG en inhibant leur internalisation et leur élimination lysosomale. Les modulateurs de hERG précédemment décrits améliorent l’efficacité de repliement des canaux au détriment d’un blocage fonctionnel ou modulent les propriétés de voltage de hERG. Nos composés agissent via un nouveau mécanisme et pourraient potentiellement contribuer au traitement du LQT2 seul ou en combinaison avec des composés &lt;&lt;activateurs&gt;&gt; de hERG. Ainsi, l’ensemble de nos résultats décrivent le processus cellulaire d’un ensemble de mutants du canal hERG qui sont régulés à la fois par le système de CQ du RE déjà établi et par un nouveau mécanisme de CQ de la MP ubiquitine indépendant. Nous avons démontré l’importance de la machinerie périphérique du CQ dans le contexte d’une pathologie conformationnelle : non seulement en tant que contributeur significatif à la perte d’expression phénotypique, mais également comme une cible pharmacologique potentielle pour des nouveaux correcteurs</dc:abstract><ual:supervisor>Gergely Lukacs (Supervisor2)</ual:supervisor><ual:supervisor>Alvin Shrier (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/5t34sp74k.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/5m60qx10g</ual:fedora3Handle><dc:subject>Physiology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Agm80j098p"><dcterms:title>A novel model of optogenetic kindling of neocortex</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Integrated Program in Neuroscience</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Cela, Elvis</ual:dissertant><dc:abstract>L'épilepsie est une maladie neurologique majeure qui fait l'objet d'intenses activités de recherche scientifique et médicale. L'émergence des crises est associée à d'autres comorbidités et diminue la qualité de vie des patients. En outre, environ 30% des patients ne répondent pas à la ligne de traitement standard utilisant des DEA. Cependant, on ne sait toujours pas comment les crises surviennent dans un cerveau par ailleurs sain. Par conséquent, il est essentiel d'avoir des modèles bien développés d’où peut émerger une compréhension causale de l'épilepsie. Alors que le développement de crises a été étudié dans de nombreux modèles animaux, tels que ceux utilisant l'induction électrique, l’interprétation des résultats de ces études a été difficile en raison de l'incertitude de la population cellulaire ciblée ainsi que d’autres variables confusionnelles telles que les lésions potentielles associées à la procédure elle-même. Le modèle de l’allumage de l'épilepsie, un modèle d'induction électrique durable, a été utilisé au fil du temps pour disséquer les étapes du développement des crises chez les animaux. Nous avons utilisé le modèle de l’allumage classique comme base pour développer de nouveaux modèles et aborder les désavantages associés aux modèles d'induction actuels.Cette thèse décrit le développement d'un nouveau modèle de l’allumage optogénétique des crises d'épilepsie avec un homologue deux coups qui peut être utilisé séparément, ainsi que des méthodes d'analyse de circuit avec lesquelles ces modèles peuvent être combinés afin d’étudier l'épileptogenèse. En utilisant une combinaison de stimulation optogénétique et d'enregistrements EEG chez des animaux éveillés se déplaçant librement, nous avons testé l'hypothèse que l'activité seule montre que la stimulation répétée provoque des crises. Les crises présentent plusieurs caractéristiques classiques de l’allumage, comme une diminution du seuil épileptique, une aggravation des symptômes de crises et une rétention élevée de la susceptibilité aux crises chez les animaux modèles. En utilisant des enregistrements de cellules-entières en paires simultanément à de l’imagerie à deux photons et une manipulation optogénétique ex-vivo de l'activité cellulaire, nous avons trouvé que l'excitabilité intrinsèque des cellules provenant des cerveaux du l’allumage ne différait pas des témoins. Pour contourner les contraintes de temps et de travail imposées par notre protocole de l’allumage optogénétique, nous avons utilisé des injections de rimonabant pour tester si la manipulation de CB1R pouvait accélérer les crises d'épilepsie optogénétique. Enfin, nous avons effectué des expériences de preuve de principe en utilisant la stimulation optogénétique de tranches fraîches de cerveau qui pourrait servir d'alternative à l'enregistrement en paires pour cartographier rapidement le circuit après des crises.Dans l’ensemble, nos résultats démontrent que la manipulation spécifique du promoteur dans les populations de cellules excitatrices peut donner lieu à des crises, un modèle pour accélérer le développement des crises et des méthodes de cartographie des circuits pour disséquer les changements de circuit suite à des crises. Je prévois que mes résultats pourraient servir d'inspiration pour de futures études utilisant l'optogénétique et d'autres techniques modernes pour sonder les mécanismes potentiels du développement des crises et augmenter la probabilité de développer de nouvelles options de traitement</dc:abstract><dc:abstract>Epilepsy is a major neurological disorder that is under intense scientific and medical investigation. The emergence of seizures is associated with other comorbidities and decreases the quality of life of patients. In addition, around 30% of patients do not respond to standard line of treatment using anti-epileptic drugs (AEDs). Furthermore, it is still unclear how seizures arise in the otherwise healthy brain. Therefore, it is critical to have well developed models where a causal understanding of epilepsy can emerge. While the development of seizures has been studied in many animal models, such as those using electrical induction, deciphering the results of such studies has been difficult due to the uncertainty of the cell population being targeted as well as potential confounds like brain damage from the procedure itself. The kindling model of epilepsy, an enduring electrical induction model has been used over time to dissect the steps of seizure development in animals that underlie epilepsy. We used the classical kindling model as a base to develop novel models from and addressed the disadvantages associated with current induction models.This thesis describes the development of a novel optogenetic kindling model of seizures along with a two-hit counterpart which can be used separately, as well circuit analysis methods these models can be combined with to study epileptogenesis. Using a combination of optogenetic stimulation and EEG recordings in awake freely moving animals, we tested the hypothesis that activity alone from repeated stimulation eventually causes seizures. The resulting seizures display several hallmarks features consistent with classical kindling such as a decrease in seizure threshold, worsening seizure symptoms and an elevated retention of seizure susceptibility in kindled animals. Using two-photon targeted paired whole-cell recordings and ex-vivo optogenetic manipulation of cellular activity we found that intrinsic excitability of cells from kindled brains did not differ from controls. To circumvent the labour and time constraints imposed by our optogenetic kindling protocol, we developed a two-hit model using rimonabant injections to test if CB1R blockade could accelerate optokindled seizures. Finally, we ran proof of principle experiments using optogenetic stimulation in acute slices that could serve as an alternative to paired recording for rapid circuit mapping following seizures.  Taken together, our results demonstrate promoter-specific manipulation of excitatory cell populations can give rise to seizures, a variant model for accelerating seizure development and possible circuit mapping methodologies to dissect circuit changes following seizures. I anticipate my findings may serve as the inspiration for future studies using optogenetics and other modern techniques to probe potential mechanisms of seizure development and increase the probability of developing novel treatment options</dc:abstract><ual:supervisor>Per Jesper Sjostrom (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/gf06g678r.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/gm80j098p</ual:fedora3Handle><dc:subject>Neuroscience</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A3197xq95m"><dcterms:title>Defining the roles of STING and TRAF7 in the Type I interferon antiviral response</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Medicine</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Liu, Yiliu</ual:dissertant><dc:abstract>Le système immunitaire inné offre une première ligne de défense contre les agents pathogènes envahissants. Au cours d'une infection virale, les récepteurs de reconnaissance de formes (PRR) reconnaissent les modèles moléculaires associés aux pathogènes viraux (PAMPS) atypiques et déclenchent des cascades de signaux résultant en l'induction d'IFNs et de gènes codant des médiateurs antiviraux ou proinflammatoires, favorisant ainsi un état antiviral intracellulaire. La voie RLR-MAVS et la voie cGAS-STING sont deux voies typiques de détection pour l'ARN et l'ADN cytosoliques dérivés des virus. Ces dernières années, STING s’est révélé être un régulateur important dans de nombreux aspects des réponses immunitaires innées déclenchées par l'ARN et l'ADN cytosoliques. Comprendre les mécanismes de régulation de STING, ainsi que l’interaction entre la détection virale de l’ARN et de l’ADN, peut fournir de nouvelles informations sur l’équilibrage de réponses antivirales efficaces et pour développer des stratégies thérapeutiques pour le contrôle de l’infection virale et de l’immunité. Dans la première étude, présentée au Chapitre 3, nous avons caractérisé le mécanisme de la régulation positive de STING via la stimulation de la signalisation de RIG-I, et avons démontré la nécessité de STING lors de la restriction du virus à ADN HSV-1 par l'agoniste de RIG-I. Cette étude a fourni un nouveau modèle d'élimination de l'infection virale à l'ADN via l'activation de la voie de détection de l'ARN en contrôlant l'expression de STING, et a renforcé l'interaction entre les systèmes de détection de l’ARN et de l'ADN d’agents pathogènes. Bien que l'activation robuste du système immunitaire par la détection de l'ARN et de l'ADN soit essentielle au contrôle de l'infection virale, une régulation négative des réponses antivirales doit avoir lieu pour éviter les réactions immunitaires excessives et assurer la survie de l'hôte. Dans l’étude présentée au Chapitre 4, nous dévoilons un mécanisme régulateur inhibiteur de la réponse antivirale à l’IFN par la protéine TRAF7. L'expression ectopique de l'infection virale supprimée par TRAF7 a déclenché l'activation de l'IRF3 et la transcription IFNB1. Alors qu'un déficit de TRAF7 a amélioré la signalisation de l'IFN de type I, ainsi qu’augmenté les réponses antivirales contre les virus à ARN et à ADN. TRAF7 a notamment affecté la solubilité du TBKI et la localisation subcellulaire dans les systèmes de surexpression. Cette étude a identifié un rôle méconnu de TRAF7 comme étant un médiateur négatif de la signalisation IFN de type I.Dans l’ensemble, les travaux présentés dans cette thèse ont contribué à notre compréhension de l’interaction entre les réseaux de détection de l’ADN et de l’ARN dans la lutte contre l’infection virale, et de la régulation négative de la réponse immunitaire innée dans le maintien de l’homéostasie de l’hôte, par le biais de l’étude respective de STING et de TRAF7</dc:abstract><dc:abstract>The innate immune system provides the first line of defense against invading pathogens. During viral infection, pattern recognition receptors (PRRs) recognize atypical viral pathogen-associated molecular patterns (PAMPs), and trigger signaling cascades that result in the induction of IFNs and genes encoding antiviral or proinflammatory mediators, thus promoting an intracellular antiviral state. The RLR-MAVS pathway and the cGAS-STING pathway are two paradigmatic sensing pathways for cytosolic virus-derived RNA and DNA, respectively. STING has emerged in recent years as an important regulator in many aspects of both cytosolic DNA- and RNA-triggered innate immune responses. Understanding the regulation mechanisms of STING, as well as the interplay between RNA and DNA viral sensing, may provide new insights into balancing efficient anti-viral responses and develop therapeutic strategies for the control of viral infection and immunity. In the first study, presented in Chapter 3, we characterize the mechanism of STING up-regulation via RIG-I signaling stimulation, and demonstrated the necessity of STING during the RIG-I agonist 5’pppRNA-mediated restriction of the DNA virus HSV-1. This study provided a new model for eliminating DNA viral infection via activation of the RNA sensing pathway by controlling STING expression, and reinforced the interplay between the RNA and DNA pathogen-sensing systems. While robust activation of immune systems by RNA and DNA sensing is essential for controlling viral infection, negative regulation of antiviral responses must take place to avoid over-reacted immune responses and to ensure host survival. In the study presented in Chapter 4, we unveil an inhibitory regulatory mechanism of the IFN antiviral response by the TRAF7 protein. Ectopic expression of TRAF7 suppressed viral infection triggered IRF3 activation and IFNB1 transcription, whereas a deficiency of TRAF7 enhanced type I IFN signaling, as well as antiviral responses against both RNA and DNA viruses. Notably, TRAF7 affected TBK1 solubility and subcellular localization in overexpression systems. This study identified a previously unrecognized role for TRAF7 as a negative mediator of type I IFN signaling. Altogether, the work presented in this thesis has contributed to our understanding of the crosstalk between DNA- and RNA-sensing networks in fighting against viral infection, and the negative regulation of the innate immune response in maintaining host homeostasis, through the studies of STING and TRAF7, respectively</dc:abstract><ual:supervisor>Rongtuan Lin (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/05741x08k.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/3197xq95m</ual:fedora3Handle><dc:subject>Medicine</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3As1784q79w"><dcterms:title>Survival and failure patterns in WHO grade I-III meningiomas: a single-center experience of surgery and radiotherapy</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Medicine</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Jooya, Alborz</ual:dissertant><dc:abstract>IntroductionLa deuxième tumeur la plus commune du système nerveux central est le méningiome intracrânien et représente environ 15-20% de toutes les tumeurs cérébrales primaires chez les adultes . Il y a constamment des débats au sujet de la sélection optimale des patients, le type et l’horaire de traitements, sans approche standardisée dans tous les établissements. La présente étude traite de l'expérience institutionnelle avec les patients atteints de méningiome de grades OMS I à III. Les patients ont été traités à l'Hôpital Général Juif de Montréal (HGJ).Méthode et matériel Le présent texte est une analyse rétrospective de 137 patients diagnostiques avec un méningiome et traités à HGJ de 2008 à 2018. Des caractéristiques cliniques et la modalité de traitement sous forme de chirurgie et de radiothérapie (RT) ont été obtenues. L'analyse de survie de Kaplan-Meier de la survie sans progression (SSP) et de la survie globale (SG) a été exécutée.Résultats L'âge médian des patients était de 63,0 ans. 20 patients (14,6%) ont été observés et n'ont reçu aucun traitement, 74 (54,0 %) patients ont été traités uniquement avec la chirurgie, 33 (24.1%) ont été traités avec la chirurgie plus radiothérapie adjuvante et 10 (7.3%) n'ont reçu que de la radiothérapie. Dans notre cohorte, le grade de l'OMS avait un impact important sur la survie globale, avec un taux de 92, 79 et 0 % à 5 ans pour les tumeurs de grade I, II et III, respectivement (p-0,03). Une différence statistiquement élevée dans la SSP a été démontrée dans les patients qui ont subi la résection totale contre la résection sous-totale indépendamment de RT (p-0.04).  En outre, nous avons observé un avantage statistiquement significatif pour la SG (p-0.028) et SSP (p-0.05) chez les patients de grade II qui ont reçu le traitement de RT adjuvant suivant la résection de tumeur.ConclusionsLes méningiomes de grade II et III sont considérées comme des néoplasmes plus agressifs et le grade de l'OMS a un impact important sur la survie globale et la survie sans progression. Notre étude suggère que l’addition de radiothérapie postopératoire pourrait être considéré dans tous les patients de catégorie II, indépendamment du degré de résection réalisé. Le suivi à long terme est important car les répétitions et la progression locales peuvent se développer des années après le traitement initial</dc:abstract><dc:abstract>IntroductionIntracranial meningiomas are the second most common tumor of the central nervous system, and they account for approximately 15-20% of all primary brain tumors in adults. There is persistent debate regarding optimal patient selection for possible treatment modalities, their timing and delivery, without any consistent management approach to treatment across institutions. The present study reports the institutional experience with WHO grades I to III meningioma patients treated at Montreal Jewish General Hospital.Materials and methodsA retrospective analysis of 137 patients with diagnosis of meningioma that were treated at the Montreal Jewish General Hospital (JGH) from 2008 to 2018 was performed. Clinical characteristics and treatment modality in form of surgery and radiotherapy (RT) were obtained. Kaplan-Meier survival analysis of progression-free survival (PFS) and overall survival (OS) was performed. ResultsThe median age of the patients was 63.0 years. 20 patients (14.6%) were observed and did not receive any treatment, 74 (54.0%) patients were treated with surgery alone, 33 (24.1%) patients were treated with surgery plus Adjuvant RT and 10 (7.3%) patients only received RT. WHO grade has been shown to have significant impact on OS, with 92, 79 and 0% rate at 5 years for grade I, II and III tumors, respectively (p=0.03). A statistically significant difference in PFS has been demonstrated in patients that underwent gross total versus subtotal resection regardless of the radiation treatment status (p=0.04).  Also, we observed a statistically significant advantage in OS (p=0.028) and PFS (p=0.05) in grade II patients who received RT treatment following gross tumor resection. ConclusionsHigh-grade meningiomas are considered more aggressive neoplasms with lower OS and PFS, and histopathological grade is an important prognostic factor. Our study suggests that postoperative adjuvant RT could be considered in all grade II patients, regardless of the degree of resection achieved</dc:abstract><ual:supervisor>Thierry Muanza (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/5t34sp75v.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/s1784q79w</ual:fedora3Handle><dc:subject>Medicine</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9s161b65w"><dcterms:title>Heterogeneous uptake and reactions of atmospheric gaseous elemental mercury and its application to remediation technologies</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Atmospheric and Oceanic Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Kurien, Uday</ual:dissertant><dc:abstract>Mercury (Hg) is a persistent global toxicant that is recognized as a priority contaminant by the United Nations Environment Program (UNEP). To combat the hazards associated with the environmental inputs of Hg, the Minamata treaty was signed by 128 countries in 2013. The treaty calls for a reduction in the anthropogenic release of Hg and for research to better understand Hg’s behaviour in the environment and atmosphere. This thesis addresses some of these challenges, by (1) identifying and characterizing potentially important gas phase and surface reactions of Hg (Theme 1) and (2) developing a proof-of-concept energy neutral technology to remove and recycle Hg from consumer electronics (Theme 2). The specific issues addressed within Theme 1 are: (1) Gaseous elemental mercury’s (Hg0(g)) uptake on the reactive components (iron(oxyhydr)oxide particles) of mineral dust and (2) particle surface mediated Hg0(g)-O3 reactions. Theme 2 addresses the development of an energy neutral technology for the recycling of Hg from Compact Fluorescent Lights (CFLs).In the first part of Theme 1, the uptake of Hg0(g) on iron (oxyhydr)oxides nanoparticles was investigated. Loss of gas phase Hg0(g) was observed when the iron (oxyhydr)oxides were irradiated with ultra violet and visible radiation (300 nm ≤ λ ≤ 700 nm).  Irradiation caused the rates of the uptake reactions to increase by up to 900-fold relative to dark reactions. Humidity was found to inhibit the uptake reaction. Possible mechanisms of the radiation enhanced uptake reactions and its inhibition by humidity are discussed. Adsorption isotherms for the visible radiation assisted uptake of Hg0(g) on α-Fe2O3 were evaluated and found to fit the Langmuir-Hinshelwood model.  In the second part of Theme 1, the effect of α-Fe2O3 particles on the oxidation of Hg0(g) by O3 was investigated. Depending on the mass loading of the particles, the rates of the surface mediated oxidation reaction were found to be ~ 80 -2000-fold larger than those of the pure gas phase reaction, re-investigated in this study using particle free air. This evidence supports the longstanding belief that Hg0(g)-O3 oxidation kinetics are highly surface sensitive. The rates of the surface mediated reactions were found to be independent of O3 concentrations (1 – 40 ppm) and 0 % ≤ RH ≤ 75 %. At RH = 100 % the Hg0(g) loss profiles shifted from an exponential decay, typical of pseudo first order reactions, to a linear decay. The reaction rates were, however, still elevated relative to homogeneous reactions. Possible mechanisms of the surface mediated reaction and the effect of humidity are discussed.In Theme 2, we exploit the uptake of Hg0(g) on iron oxide surfaces to develop a laboratory scale energy neutral technology for the remediation of Hg0(g) from Compact Fluorescent Lights (CFLs). The application of this technology is intended to replace the presently employed high energy thermal desorption remediation techniques. Hg0(g) from CFLs was successfully trapped onto iron oxide nanoparticles (Fe3O4 and α-Fe2O3) of high crustal abundance, using batch and flow-through systems. Recovery (~ 40 % - 90 %) of Hg from the iron oxide nanoparticles and regeneration of the iron oxide nanoparticle surfaces were achieved via electrolysis in an aqueous solution of NaCl. The post-electrolysis iron oxide nanoparticles were reused to trap Hg0(g) in further remediation experiments. Even after 4 adsorption-electrolysis cycles, the iron oxides’ capacity to adsorb Hg0(g) did not diminish</dc:abstract><dc:abstract>Le mercure (Hg) est un composé toxique reconnu par l’ONU en étant un contaminant prioritaire. Afin de combattre les hazards liés au rôle néfaste du mercure sur l’environnement, le traité de Minamata a été signé par 128 pays en 2013. Le traité vise à réduire l’échappement de Hg d’origine humaine et à encourager la recherche dans le domaine pour améliorer la compréhension des effets et des contraintes entrainés par la présence de Hg dans l’environnement et l’atmosphère. La thèse qui suit addresse quelques unes des contraintes en (1)identifiant et caractérisant les réactions du Hg en phase gaseuse et à la surface (Thème 1) et (2)en développant une technologie à valeur énergétique neutre pour éliminer et recycler le Hg issu de produits de consommation électronique. Les sujets d’étude faisant l’objet du Thème 1 sont les suivants: (1)l’adsorption du mercure élémentale en phase gaseuse (Hg0(g)) à la surface de composants réactifs (oxide (oxyhydroxide) de fer) constituant la poussière minérale et (2)les réactions Hg0(g)-O3 facilitées par la présence de surface de particules. Dans la première partie du Thème 1, l’adsorption du Hg0(g) à la surface de nanoparticule d’oxide (oxyhydroxide) de fer a été evaluée. Une diminution de Hg0(g) en phase gaseuse est observée à la suite de l’irridiation des particules de fer avec rayons UV and visible (300 nm ≤ λ ≤ 700 nm).  L’irraditaion a causé l’accéleration des réactions d’adsorption en attaignant 900 fois celle observée dans des conditions au noir. L’humidité empêche les réactions d’adsoprtion. Les différents mécanismes expliquant le rôle qu’a l’irradiation sur l’accéleration de  l’adsorption et le rôle de l’humidité à l’empecher sont discutés. L’évaluation des isothermes d’adsorption de Hg0(g) au α-Fe2O3 dans la lumière visible démontrent que ces réactions correspondent au model Langmuir- Hinshelwood.    Dans la deuxième partie du Thème 1, l’effet des particules de α-Fe2O3 sur l’oxidation du Hg0(g) par l’O3 a été évalué. Dépendamment de la quantité adsorbée à la surface des particules, la vitesse des réactions d’oxidation à la surface notée était 80 – 2000 fois plus rapide qu’en phase gaseuse. Les réactions en phase gaseuese ont été re-évaluées dans une atmosphère exempte de particules. Ce résultat est en accord avec les hypothèses parvenues auparavant qui suggèrent que la cinétique de la réaction d’oxidation entre le Hg0(g)-O3 était fortement affectée par la présence de surface. La vitesse des réactions à la surface est indépendente de la concentration d’ O3 (1 – 40 ppm) et 0 % ≤ HR ≤ 75 %. Au niveau HR = 100%, le profil de perte en Hg0(g) a évolué d’une perte exponentielle, à une réaction de type pseudo-premier ordre, à une perte linéaire. Les vitesses de réactions observées sont restées tout de même élévée en comparaison avec les réactions homogènes. Les mécanismes de réactions facilitées par la présence de surface et de l’humidité sont abordés. Dans le Thème 2, nous exploitons le fait que Hg0(g) s’adsorbe à la surface des nanoparticules d’oxide de fer pour développer une technologie de valeur énergétique neutre à l’échelle du laboratoire pour assainir le Hg0(g) provenant de lampes fluorescentes compactes. L’application de cette technologie vise à remplacer la technologie présentement utilisée qui emploie une forte énergie thermique pour entrainer la désorption thermique. Le Hg0(g) provenant de lampes fluroescentes a été piègé par les nanoparticules d’oxide de fer (Fe3O4 and α-Fe2O3) abondant en cristaux dans un système en lot ainsi qu’en flux. La récupération (~ 40 %-90 %) de Hg piégé par les nanoparticules d’oxide de fer et la regénération de surface de ces dernières ont été realisées par électrolyse dans une solution aqueuse de NaCl. À la suite de l’électrolyse, les nanoparticules d’oxide de fer ont été reutilisées pour pièger davantage de Hg0(g). À la suite de 4 cycle d’adsoprtion-electrolyse, la capacité des nanoparticules d’oxide de fer pour adsorber le Hg0(g) n’avait pas diminué</dc:abstract><ual:supervisor>Parisa A Ariya (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/9306t3577.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9s161b65w</ual:fedora3Handle><dc:subject>Atmospheric and Oceanic Sciences</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Az029p910z"><dcterms:title>Analysis of novel genetic factors in the sporadic form of Alzheimer’s disease</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Integrated Program in Neuroscience</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Miron, Justin</ual:dissertant><dc:abstract>IntroductionAlthough genome-wide association studies (GWASes) on dozens of thousands of individuals have found dozens of genetic variants related to the risk of developing Alzheimer’s disease (AD), recent studies have shown that these single nucleotide polymorphisms (SNPs) account for only 30% of the genetic variance present in this illness. Since twin studies have attributed 60 to 80% of the variance in AD to genetic factors and because the discovery of these factors could serve for the screening of presymptomatic subjects at risk of developing AD for treatment or research purposes or could lead to the identification of molecular pathways implicated in this illness that could be used as potential therapeutic targets, there is still a need to discover the polymorphisms present in the remaining 70% of unknown genetic variance in AD. MethodsBecause founder populations have helped in the past to discover genetic variants correlated with complex diseases or traits, we performed a GWAS in the Québec Founder Population (QFP) in order to identify new loci linked to AD. Once these SNPs were found, genes in their vicinity were evaluated based on their expression quantitative trait loci (eQTLs), which are associations between polymorphisms and gene expression levels, in the BRAINEAC cohort. In order to observe the potential impact of genetic variants on AD biomarkers in the presymptomatic phase of this disorder, we used the PREVENT-AD cohort, which enrolled asymptomatic participants with an increased risk of AD due to a parental history. ResultsAmong the polymorphisms that are the most strongly linked to the risk of developing AD in the GWAS performed on the QFP, the minor allele of a genetic factor located on chromosome region 9q33.2 (rs10984186), which is related to an increased risk of developing the disease, reduced hippocampal volumes, as well as higher NFTs densities, is in eQTL correlation with the CDK5RAP2 gene. Because the TLR4 gene is located close to the CDK5RAP2 locus, we assessed, in brains from the Douglas-Bell Canada Brain Bank, the differences in the gene expression levels of TLR4 and its related pro-inflammatory cytokines (IL1B, IL6, and TNF), and found that mRNA levels transcribed from the IL6, TNF, and TLR4 genes are higher in brain regions affected by AD. Furthermore, after surveying the literature in order to find a genetic variant located in the TLR4 gene that is related to aging and dementia, we discovered the nonsynonymous coding rs4986790 polymorphism, whose minor allele is linked to a higher longevity and a reduced risk of developing AD in two independent Italian cohorts respectively. In presymptomatic subjects from the PREVENT-AD cohort, this SNP is associated with an increased cortical thickness in the frontal lobe, the fusiform gyrus, and the occipital lobe, improved visuospatial performances, as well as stable CSF IL-1β levels. When we conducted a GWAS on 751 pairs of control and AD subjects from the QFP, we found another genetic factor that is related to an increased risk of developing the disorder: the rs10406151 polymorphism on chromosome 19q13.41. ConclusionIn the future, we hope that the discovery of these AD-associated novel genetic variants will allow the screening of presymptomatic subjects with a higher risk of developing the disease for treatment or research purposes and will help unravel mechanisms implicated in AD that could serve as potential therapeutic targets</dc:abstract><dc:abstract>IntroductionBien que des études d’association pangénomique (EAP) sur des dizaines de milliers d’individus ont permis la découverte de dizaines de variantes génétiques reliées au risque de développer la maladie d’Alzheimer (MA), des études récentes ont démontré que ces polymorphismes d’un seul nucléotide (PSN) comptent pour seulement 30% de la variance génétique présente dans la maladie. Parce que les études sur des jumeaux ont attribué de 60 à 80% de la variance génétique aux facteurs génétiques et puisque la découverte de ces facteurs pourrait servir au dépistage de sujets présymptomatiques à risque de développer la MA à des fins thérapeutiques ou de recherche ou pourrait mener à l’identification de voies moléculaires liées à la maladie qui pourraient être utilisées comme de potentielles cibles thérapeutiques, il y a toujours un besoin de découvrir les polymorphismes présents dans le reste des 70% de variance génétique inconnue dans la MA.MéthodesParce que les populations fondatrices ont permis dans le passé de découvrir des variantes génétiques corrélées à des maladies et à des traits complexes, nous avons effectué une EAP dans la Population Fondatrice du Québec (PFQ) dans le but d’identifier de nouveaux loci liés à la MA. Une fois les PSNs découverts, les gènes à proximité ont été évalués selon leurs loci de caractère quantitatifs d’expression (LCQe), qui consistent en des associations entre des polymorphismes et des niveaux d’expression génétique, dans la cohorte BRAINEAC. Dans le but d’observer l’impact potentiel des variantes génétiques sur des biomarqueurs de la MA dans la phase présymptomatique de la maladie, nous avons utilisé la cohorte PREVENT-AD, qui a recruté des participants avec un risque accru de MA dû à un historique familial.    RésultatsParmi les PSNs les plus fortement liés au risque de développer la MA dans l’EAP effectuée dans la PFQ, l’allèle mineur d’un polymorphisme situé sur la région chromosomique 9q33.2 (rs10984186), qui est relié à un risque accru de maladie, des volumes réduits d’hippocampe, ainsi que des densités augmentées d’enchevêtrements neurofibrillaires, est en corrélation LCQe avec le gène CDK5RAP2. Étant donné que le gène TLR4 est situé à proximité du locus CDK5RAP2, nous avons évalué, dans nos cerveaux provenant de la banque de cerveaux Douglas-Bell Canada, les différences d’expression génétique de TLR4 et des cytokines pro-inflammatoires qui lui sont reliées, telles que IL1B, IL6 et TNF, et avons découvert que les niveaux d’ARNm transcrits à partir des gènes IL6, TNF et TLR4 sont élevés dans des régions du cerveau affectées par la MA. Par ailleurs, après avoir examiné la littérature afin de trouver une variante génétique située dans le gène TLR4 qui est reliée au vieillissement, ainsi qu’à la démence, nous avons découvert le polymorphisme codant non synonyme rs4986790, dont l’allèle mineur est lié à une longévité accrue, ainsi qu’à un risque réduit de développer la MA dans deux cohortes italiennes indépendantes respectivement. Chez des sujets présymptomatiques de la cohorte PREVENT-AD, ce PSN est associé à une plus grande épaisseur corticale dans le lobe frontal, le gyrus fusiforme et le lobe occipital, des performances visuospatiales améliorées, ainsi que des niveaux stables d’interleukine-1 bêta dans le liquide céphalo-rachidien. Lorsque nous avons effectué une EAP sur 751 paires de sujets contrôles et atteints de la MA provenant de la PFQ, nous avons découvert une autre variante génétique qui est reliée à un risque accru de développer la maladie : le polymorphisme rs10406151 sur le chromosome 19q13.41.  ConclusionDans le futur, nous espérons que la découverte de ces nouveaux polymorphismes associés à la MA permettra de dépister des sujets présymptomatiques avec un risque accru de développer la maladie à des fins thérapeutiques ou de recherche et servira à démêler les mécanismes impliqués dans la MA qui pourraient servir de potentielles cibles thérapeutiques</dc:abstract><ual:supervisor>Judes Poirier (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/sf2689637.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/z029p910z</ual:fedora3Handle><dc:subject>Neuroscience</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ap5547w83f"><dcterms:title>Low-dimensional semiconductor heterostructures and device applications in optoelectronics and renewable energy</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Tran, Hong</ual:dissertant><dc:abstract>Le développement de dispositifs optoélectroniques à haute efficacité dans le spectre ultraviolet (UV), en particulier la bande UV-C (200 - 280 nm), est fondamentalement limité par de hautes densités de dislocations et une faible conduction de type p dans les hétérostructures planaires en AlGaN. En utilisant des hétérostructures à base de nanofils par épitaxie par jet moléculaire (MBE), la qualité cristalline et le dopage de type p dans les matériaux à base des III-Nitrures peuvent être considérablement améliorés. Dans cette thèse, nous avons élucidé que les concentrations inhabituellement élevées de trous à la température ambiante allant jusqu’à environ 6  1017 cm-3, mesurées dans des nanofils en AlN dopés au Mg, résultent de la conduction efficace de sauts de trous dans la bande d’impuretés de Mg, entraînée par l’incorporation du Mg dans ces nanostructures en AlN presque exemptes de défauts. Des études détaillées de transistors à base de nanofils en AlN révèlent les signatures distinctes de la conduction de sauts de trous dans la bande d’impuretés de Mg, incluant une énergie d’activation relativement faible pour la conductivité électrique et une augmentation de la mobilité des trous avec une hausse de la température.Dans le spectre des longueurs d'onde visibles et infrarouges, nous avons également étudié les dichalcogénures de métaux de transition (TMDC), qui, en couches bidimensionnels, se sont révélés être un excellent candidat pour les dispositifs optoélectroniques souples et transparents. Nous avons d’abord étudié l’efficacité quantique des monocouches en MoS2 par des études détaillées de photoluminescence. Ces analyses démontrent des efficacités quantiques internes maximales pouvant atteindre 45% et 8,3% dans ces monocouches en MoS2 à 83 et 300 K, respectivement. Il est à noter que l'efficacité est réduite à cause d’un coefficient de recombinaison Auger exceptionnellement élevé, calculé comme étant d'environ 10-24 cm6s à température ambiante, soit près de six ordres de grandeur plus élevé que celui des semiconducteurs classiques. Par ailleurs, le MoS2 comprenant quatre couches présente une réduction beaucoup plus faible, et est donc utilisé pour fabriquer, pour la première fois, des dispositifs laser à température ambiante avec une puissance de seuil remarquablement faible d’environ 5 W.Nous avons davantage exploité l'application des hétérostructures à base de nanofils à base des III-Nitrures dans le domaine des énergies renouvelables, en particulier de la production d'hydrogène, et avons démontré la séparation photocatalytique de l'eau à grande échelle sous une lumière solaire naturelle concentrée. La synthèse de réseaux de nanofils p-GaN/InGaN sur un substrat de Si (3 pouces de diamètre) par MBE a fait l'objet d'études, résultant à une efficacité quantique apparente allant jusqu'à environ 28,8% dans le spectre visible de 400 à 480 nm. Par conséquent, un rendement de conversion d'énergie solaire en hydrogène d'environ 1,1% peut être obtenu sous une lumière solaire naturelle concentrée. Cette réalisation est une étape importante vers un système de production pratique de combustibles à base d’hydrogène à grande échelle</dc:abstract><dc:abstract>The development of high efficiency optoelectronic devices operating in the ultraviolet (UV) range, especially UV-C band (200 – 280 nm), has been fundamentally limited by large dislocation densities and poor p-type conduction in AlGaN planar heterostructures grown on lattice mismatched substrates. By employing nanowire heterostructures using molecular beam epitaxy (MBE) technique, the crystalline quality and p-doping in group III-nitride materials can be significantly improved. In this dissertation, we have elucidated that the unusually high room-temperature hole concentrations up to ~6  1017 cm-3 measured in Mg-doped AlN nanowires stem from the efficient hole hopping conduction in the Mg impurity band, driven by the significantly enhanced Mg-dopant incorporation in nearly defect-free AlN nanostructures. Detailed temperature- and gate-dependent studies of AlN nanowire transistors reveal the distinct signatures of hole hopping conduction in the Mg impurity band, which include a relatively small activation energy for electrical conductivity and an increase in hole mobility with temperature.In visible and near-infrared wavelength range, we also study two-dimensional layered transition metal dichalcogenides (TMDCs), which have emerged as an excellent candidate for flexible and transparent optoelectronic devices. We first investigate the quantum efficiency of monolayer MoS2 via detailed temperature- and power-dependent photoluminescence studies. From rate equation analysis, the maximum achievable internal quantum efficiencies in monolayer MoS2 at 83 and 300 K are determined to be 45% and 8.3%, respectively. Noticeably, efficiency droop is clearly observed at high carrier injection concentrations due to the unusually large Auger recombination coefficient, which is calculated to be ~10-24 cm6s at room temperature, nearly 6 orders of magnitude higher than that of conventional bulk semiconductors. In contrast, four-layer MoS2 exhibits much weaker efficiency droop, and is therefore used to fabricate, for the first time, room-temperature laser devices with a remarkably low threshold power of ~5 W under continuous wave operation.We have further exploited the application of III-nitride nanowire heterostructures in the field of renewable energy, specifically hydrogen production, and demonstrated large-scale photocatalytic water splitting under concentrated natural sunlight. The synthesis of double-band p-GaN/InGaN nanowire arrays on large Si substrate (3” in diameter) by MBE has been studied and resulted in an apparent quantum efficiency up to ~28.8% in the visible range of 400 – 480 nm. Consequently, a solar-to-hydrogen energy conversion efficiency of ~1.1% can be derived under concentrated natural sunlight. The achievement brings us one step closer towards a large-scale practical hydrogen fuel generation system</dc:abstract><ual:supervisor>Zetian Mi (Supervisor2)</ual:supervisor><ual:supervisor>Thomas Szkopek (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/v692tb66s.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/p5547w83f</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A05741x09v"><dcterms:title>Linguistic embodiment of affect: Influence of valence, arousal, and dominance on cognitive and motor processes</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Communications Sciences and Disorders</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Krusanova, Anna</ual:dissertant><dc:abstract>Contrairement aux émotions discrètes et aux mots sémantiquement liés aux actions, les concepts affectifs généraux de valence, d’éveil et de dominance et les concepts linguistiques d’abstraction et de concrétude n’ont pas été systématiquement examinés à partir de la perspective fondée sur le corps humain, qui relie le traitement d’informations subtiles codées linguistiquement et de réponses réflexes physiologiques dans le corps humain. Le projet en cours a examiné l’incarnation linguistique de la valence, de l’éveil, de la dominance et de la concrétude lors du traitement lexico-sémantique dans deux contextes non-verbaux différents, conçus pour susciter différents niveaux d’engagement dans la tâche, qui peuvent être directement proportionnels à différents niveaux d’éveil cognitif, tandis que des mesures électromyographiques (EMG) de la surface de la peau ont été enregistrées à partir de plusieurs muscles faciaux au cours d’une tâche de décision lexicale. L’étude 1 (Expérience 1, Expérience 2) a utilisé des pseudo-mots phonotactiquement de type anglais qui rendaient la tâche de décision lexicale plus difficile, mais également plus intéressante, et a trouvé un fort support pour l’incarnation de la valence et de l’éveil et aussi un support pour l’incarnation de la concrétude. L'étude 2 (Expérience 3, Expérience 4) a utilisé des non-mots à chaîne de consonnes, ce qui a rendu la tâche de décision lexicale beaucoup plus facile, mais également moins intéressante, et a trouvé un fort support pour l’incarnation de la valence et de la concrétude, tout en suggérant une dépendance du concept affectif d’éveil au niveau de l’engagement des participants dans la tâche. En outre, l’étude 2 a mis en évidence l’incarnation de concept de dominance. Ces découvertes suggèrent que des concepts subtils de valence, d’éveil, de dominance et de concrétude, codés linguistiquement, sont incarnés lors du traitement lexico-sémantique, que l’incarnation de l’éveil diffère qualitativement de celle de valence, de dominance et de concrétude, et que les mots et la tâche non-émotionnels peuvent toujours susciter des réponses réflexes physiologiques dans le corps humain</dc:abstract><dc:abstract>Unlike discrete emotions and action-related words, general affective concepts of valence, arousal, and dominance and linguistic concepts of abstractness/concreteness have not been systematically investigated from the bodily-grounded perspective, which bridges the processing of subtle linguistically encoded information and reflex physiological responses in the human body. The current project examined linguistic embodiment of valence, arousal, dominance, and concreteness during lexico-semantic processing in two different non-word contexts that were tailored to elicit different levels of task engagement, which may be directly proportional to different levels of cognitive arousal, while skin-surface electromyographic (EMG) measurements were recorded from several facial muscles during a lexical decision task. Study 1 (Experiment 1, Experiment 2) used phonotactically English-like pseudowords, which made a lexical decision task harder but also more engaging, and found strong support for the embodiment of valence and arousal and some support for the embodiment of concreteness. Study 2 (Experiment 3, Experiment 4) used consonant-string non-words, which made the lexical decision task much easier but also less engaging, and found strong support for the embodiment of valence and concreteness, while also suggesting dependence of the affective concept of arousal on the level of participants’ engagement in the task. In addition, Study 2 found some support for the embodiment of the concept of dominance. These findings suggest that subtle, linguistically encoded concepts of valence, arousal, dominance, and concreteness are embodied during lexico-semantic processing, that the embodiment of arousal qualitatively differs from that of valence, dominance, and concreteness, and that non-emotionally laden stimuli and task can consistently elicit reflex bodily-grounded responses</dc:abstract><ual:supervisor>Theodore Milner (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/bn999c051.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/05741x09v</ual:fedora3Handle><dc:subject>Communications Sciences and Disorders</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A4t64gs60k"><dcterms:title>The impact of transposable elements on the epigenome: A look at the functional TE-ome</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Human Genetics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Su, Zhaohui</ual:dissertant><dc:abstract>Background: Transposable elements (TEs) are DNA sequences that are able to cut and paste or copy and paste themselves throughout the human genome. TEs make up around 50% of the human genome, and are the main contributor to the repetitive region of the genome. After TEs were first discovered, some scientists labelled them as “junk DNA”, but TEs were later found to play a part in gene regulation, partially through their role with the epigenome, i.e., the chemical modifications to DNA or to the histones that DNA segments wrap around to regulate the expression of genes along that stretch of DNA. Generally, TEs are mutated or epigenetically silenced so they cannot propagate throughout the genome. When a TE pastes itself into a relatively important genetic region, such as a gene, a deleterious outcome may occur; so, organisms have evolved epigenetic mechanisms to counteract this. On the other hand, some TEs harbour important binding sites for active regulatory regions. In both cases, TEs have been reported to be associated with epigenetic changes in their flanking regions, thus affecting the genome beyond the simple act of insertion. This study looks to further this area of research. Methodology: In this study, we look at the associations TEs have with their local epigenome. We do this via a two-pronged approach. First, we compare the flanking regions of TEs found only in the human genome to the corresponding flanking regions of other species. For this work, we filter a database of human reference TEs to look only for those present in human, but which also have sufficiently orthologous flanking regions to make a meaningful epigenetic comparison. Then the epigenetics are compared. Secondly, we do a genome-wide association study to compare the effects that TEs have on the local epigenome to the effects that non-TE, single nucleotide polymorphisms (SNPs) have on the local epigenome. Here we take a publicly available, published dataset and repeat some of the analysis done on SNPs. Key Results: The combination of these two approaches reveals a significant association to the local epigenome from young TEs. These TEs, generally methylated in the human genome, have flanking regions that are methylated to a higher degree than the background. Young human-specific TEs tend to have a greater level of methylation in their flanking regions than corresponding flanking regions in gorillas and chimpanzees. However, there is no evidence showing that either TEs or SNPs play a greater effect than the other on levels of nearby histone modifications</dc:abstract><dc:abstract>Contexte: Les éléments transposables (ÉT) sont des séquences d'ADN capables de couper et coller ou de copier et coller eux-mêmes partout dans le génome humain. Représentant environ 50% du génome humain, ces séquences sont également le principal contributeur aux régions répétitives du génome. Après la découverte des ÉT, de nombreux scientifiques les qualifiaient d'«ADN indésirable», mais il a ensuite été démontré qu’ils jouent un rôle dans la régulation des gènes, en partie via leur rôle dans l'épigénome, les modifications chimiques de l'ADN ou les histones et l'expression de gènes. Généralement, les ÉT ont des mutations ou ne sont pas exprimés via des mécanismes de répression épigénétique de sorte qu'ils sont incapables de se propager à travers le génome. Cependant, quand un ÉT s’insère dans une région génétiquement importante, disons un gène, un résultat délétère peut se produire. Pour éviter cela, les organismes ont développé des mécanismes épigénétiques pour contrer ce processus. D'un autre côté, certains ÉT hébergent des sites de liaison importants pour les sites régulateurs actifs. Dans les deux cas, les ÉT ont été associés à des changements épigénétiques correspondants dans leurs régions adjacentes, affectant ainsi le génome au-delà du simple acte d'insertion. Cette étude cherche à approfondir ce domaine de recherche.Méthodologie: Dans cette étude, nous examinons les associations que les ÉT ont avec leur épigénome local. Nous explorons cela en utilisant une approche à deux volets, en comparant d'abord les régions adjacentes des ÉT trouvées seulement dans le génome humain aux régions adjacentes correspondantes des autres espèces. Pour ce travail, nous filtrons une base de données d'ÉT de référence humaine pour ne rechercher que celles qui sont présentes uniquement chez l'humain mais qui ont aussi des régions adjacentes suffisamment orthologues pour effectuer une comparaison réaliste. Ensuite, le profil épigénétique est comparé. Pour la deuxième partie de notre travail, nous faisons une étude d'association pangénomique pour comparer les effets que les ÉT ont sur l'épigénome local aux effets que les polymorphismes mononucléotidiques (SNP) non-ÉT ont sur l'épigénome local. Ici, nous prenons un ensemble de données publiquement disponible et nous répétons certaines des analyses effectuées sur les SNP.Résultats principaux: La combinaison de ces deux approches révèle un impact significatif sur l’épigénome local des jeunes ÉT. Ces ÉT, généralement méthylés dans le génome humain, ont des régions adjacentes qui sont méthylées à un degré supérieur au bruit de fond. De plus, les jeunes ÉT spécifiques à l'homme ont tendances à avoir un niveau de méthylation plus élevé sur leurs régions adjacentes que les régions adjacentes correspondantes du gorille et du chimpanzé. Cependant, leurs effets épigénétiques ne semblent pas plus importants que les autres SNP du génome par rapport à ce que l'on pourrait attendre</dc:abstract><ual:supervisor>Guillaume Bourque (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/xd07gz041.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/4t64gs60k</ual:fedora3Handle><dc:subject>Human Genetics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A7w62fd582"><dcterms:title>Critical mass: collectivity and collaboration in the history of multi-agent intelligent systems</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Department of Art History and Communications Studies</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Bergmann, Rachel</ual:dissertant><dc:abstract>This thesis examines the intellectual history of collaboration and collective action in multi-agent systems AI research. Drawing on archival work, including computer science research papers, conference proceedings, technical reports, and journalism, it considers the cultural, institutional, and intellectual forces that shaped this approach in artificial intelligence research and the teams that worked on them. The first chapter examines the cultural history of the Stanford AI Laboratory and Stanford Research Institute and considers how countercultural movements in the Bay Area from 1969-1973 influenced their institutional cultures. The second chapter outlines a network of women in AI research who, together, expanded the range of methodologies and disciplines usually included in AI and MAS research. Borrowing from Michelle Murphy’s concept of protocol feminism, I examine their “feminist AI protocol” and outline the sets of practices and techniques they used in their research objectives, scientific method, areas of specialization, and academic service. The third chapter focuses on the work of one researcher, Barbara Grosz, and places her ideas in conversation with academic developments at the same time from philosophy of mind, STS, and feminist critiques of AI. The thesis concludes by considering the limits of this feminist AI protocol, multi-agent or not, without a deeper commitment to feminist epistemologies</dc:abstract><dc:abstract>Ce mémoire examine l’histoire intellectuelle de la collaboration et de l'action collective dans la recherche sur l’IA et les systèmes multi-agents. S’appuyant sur des documents d’archives, des travaux de recherche en informatique, des comptes rendus de conférences, des rapports techniques et des publications journalistiques, il prend en compte les forces culturelles, institutionnelles et intellectuelles qui ont façonné les systèmes multi-agents et les équipes qui y ont travaillé. Le premier chapitre se penche sur l’histoire culturelle de SAIL et du SRI et considère la manière dont les mouvements de la contre-culture dans la région de la baie de San Francisco entre 1969 et 1973 ont influencé leurs cultures institutionnelles. Le deuxième chapitre décrit un réseau de femmes dans la recherche sur l’IA qui a élargi l’éventail des méthodologies et des disciplines communément incluses dans la recherche sur l’IA et le MAS. Empruntant à Michelle Murphy son concept de féminisme protocolaire, j’examine leur « protocole d’IA féministe » et décrit l’ensemble des pratiques et des techniques qu’elles ont utilisées dans leurs objectifs de recherche, leur méthode scientifique, leurs domaines de spécialisation et leur service académique. Le troisième chapitre se concentre sur les travaux de la chercheuse Barbara Grosz et met ses idées en dialogue avec les développements académiques issus à la fois de la philosophie de l’esprit, de la STS et des critiques féministes de l’IA. Le mémoire conclue en considérant les limites de ce protocole d’IA féministe, multi-agents ou non, dans l’absence d’un engagement plus approfondi avec les épistémologies féministes</dc:abstract><ual:supervisor>Jonathan Sterne (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/qn59q8690.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/7w62fd582</ual:fedora3Handle><dc:subject>Art History and Communications Studies</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Az890rz949"><dcterms:title>Understanding child supervision through a different lens: perspectives of South Korean caregivers and adolescents in Toronto, Canada</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Psychiatry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Park, Sol</ual:dissertant><dc:abstract>Child supervision is integral to the wellbeing and healthy development of children and adolescents. Parenting decisions and practices, including on child supervision, are greatly influenced by society’s norms and values, hence the need to explore child supervision norms and practices before and after families migrate to a new country. There has been limited scholarly attention to child supervision practices among migrant families, particularly how immigrant caregivers and children perceive adequate child supervision and the influence of different supervision practices on the wellbeing of children and adolescents. Fifteen South Korean caregivers and ten South Korean adolescents living in the Greater Toronto Area (GTA) were interviewed about their perceptions of adequate child supervision, how child supervision norms and practices changed after migration to Canada, and their perspectives of children home alone without adult supervision in South Korea and Canada. The study results are presented in two sections – the first one focuses on caregivers and adolescents’ understandings and practice of supervision in South Korea and in Canada, and the second one explores their views and experiences regarding children home alone in these countries. South Korean immigrant caregivers and adolescents have different perceptions of what constitutes adequate supervision and children being home alone. Families experience challenges providing supervision in Canada due to lack of extended family supports and a different educational system. Various factors, such as the child’s growing age and level of responsibility, and caregivers’ mistrust of their neighbours seem to determine whether children can stay home alone in Canada. Both sections end with recommendations for future research and improved services and community supports</dc:abstract><dc:abstract>La surveillance des enfants fait partie intégrante du bien-être et du développement sain des enfants et des adolescents. Les normes et les valeurs d’une société ont une grande influence sur les décisions et les pratiques parentales, y compris en ce qui concerne la surveillance des enfants. Puisque l’attention portée par les chercheurs sur les pratiques de surveillance des enfants dans les familles migrantes est limitée, il est nécessaire d’explorer les normes et les pratiques de la surveillance des enfants avant et après l’émigration des familles dans un nouveau pays. Effectivement, il n’y a pas beaucoup de recherches scientifiques sur les pratiques de surveillance des enfants parmi les familles migrantes, en particulier la manière dont les parents/tuteurs immigrants et les enfants perçoivent une surveillance adéquate des enfants et l'influence de différentes pratiques de surveillance sur le bien-être des enfants et des adolescents. Ainsi, quinze parent/tuteurs sud-coréens et dix adolescents sud-coréens vivant dans la grande région de Toronto ont été interviewés. Les participants ont été invités à parler de leurs perceptions d'une surveillance adéquate des enfants, la manière dont les normes et pratiques en matière de surveillance des enfants ont changé après la migration au Canada et leurs perspectives sur les enfants seuls à la maison sans encadrement d’un adulte en Corée du Sud et au Canada. Les résultats de l’étude sont présentés en deux parties: la première porte sur la compréhension et la pratique de la surveillance des parents/tuteurs et des enfants en Corée du Sud et au Canada, et la seconde porte sur leurs points de vue et expériences concernant les enfants laissés seuls à la maison dans ces pays. Les parent/tuteurs immigrants et les adolescents Sud-Coréens ont une perception différente de ce qui constitue une surveillance adéquate et des enfants laissés seuls à la maison. Les résultats de l’étude ont également démontré que les familles éprouvent des difficultés à assurer la surveillance adéquate des enfants au Canada en raison d’un manque de soutien de la famille élargie et d'un système d'éducation différent. Divers facteurs, tels que l’âge, le niveau de responsabilité de l’enfant et la méfiance des parents/tuteurs à l’égard de leurs voisins semblent déterminer si les enfants peuvent rester seuls à la maison au Canada. Les deux sections sont conclues par des recommandations pour la recherche future et l'amélioration des services et des soutiens communautaires</dc:abstract><ual:supervisor>Monica Ruiz Casares Yebenes (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/rf55zd11p.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/z890rz949</ual:fedora3Handle><dc:subject>Psychiatry</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A3j333675r"><dcterms:title>Fatigue structural testing enhancement research (FASTER)</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Mechanical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Fortune, Robyn</ual:dissertant><dc:abstract>This thesis focuses on the modelling and optimal control of a load-controlled fatigue structural testing rig. The modelling phase involves first attempting to analytically model the test system and the test controller, then using these models to estimate test cycle times. Afterwards, a system identification approach is taken to generate a more reliable numerical model using data. Open-loop and closed-loop methods are discussed, although only closed-loop experiments can be performed on fatigue testing rigs to prevent unnecessary damage to the valuable test article. The direct, indirect, and dual-Youla closed-loop system identification methods are applied to measurement data from a fatigue testing rig at the National Research Council of Canada (NRC). The identified models are validated then used in various controller synthesis methods. First, two methods for generating “optimal” proportional-integral (PI) gains are presented. The first uses H∞-optimal static output feedback, and the second employs the Bounded Real Lemma, iteration, and bisection method. Next, a single-input-single-output (SISO) approach to designing two degree-of-freedom (2DOF) controllers is presented. The feedback controller can be a PI or H∞ controller, for example, and the feedforward controller is designed using an approximate inverse of the plant transfer function. Finally, a multi-input-multi-output (MIMO) 2DOF H∞-optimal controller synthesis method is described. The alternative controllers are implemented on the test rig and used to perform tests. Tracking results and their comparison to the standard PI controller are presented</dc:abstract><dc:abstract>Cette thèse porte sur la modélisation et le contrôle optimal d’un banc d’essai structurel de fatigue à charge contrôlée. La phase de modélisation consiste d’abord à tenter de modéliser analytiquement le système de test et le contrôleur, puis à utiliser ces modèles pour estimer la durée des cycles de test. Ensuite, une approche d’identification du système est adoptée pour générer un modèle numérique plus fiable utilisant des données mesurées. Les méthodes en boucle ouverte et en boucle fermée sont discutées, bien que seulement des expériences en boucle fermée puissent être effectuées sur des bancs d’essais de fatigue, afin d’éviter les dommages inutiles à l’article de test de valeur importante. Les méthodes directe, indirecte et dual-Youla d’identification de systèmes en boucle fermée sont appliquées aux données mesurées d’un banc d’essai de fatigue du Conseil National de Recherches du Canada (CNRC). Les modèles identifiés sont validés puis utilisés dans différentes méthodes de synthèse de contrôleur. Premièrement, deux méthodes pour générer des gains proportionnel et intégral (PI) ≪optimaux≫ sont présentées. La première utilise la rétroaction à sortie statique H∞-optimal, et la seconde utilise la méthode du lemme réel borné (Bounded Real Lemma), de l’itération et de la bissection. Ensuite, une approche à entrée simple et sortie simple (SISO) pour la conception de contrôleur à deux degrés de liberté (2DOF) est présentée. Le contrôleur de rétroaction peut être un contrôleur PI ou H∞, par exemple, et le contrôleur adaptatif par action anticipatrice (≪feedforward≫) est conçu en utilisant un inverse approximé de la fonction de transfert du procédé. Enfin, une méthode de synthèse de contrôleurs H∞ optimaux à deux degrés de liberté, à entrées multiples et sorties multiples (MIMO) est décrite. Les contrôleurs alternatifs sont implémentés sur le banc d’essai et utilisés pour effectuer des tests. Les erreurs de poursuite et leur comparaison avec le contrôleur PI standard sont présentées</dc:abstract><ual:supervisor>James Forbes (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/tt44ps19c.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/3j333675r</ual:fedora3Handle><dc:subject>Mechanical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A6q182q554"><dcterms:title>Influence of the flanges of concrete I-girders on shear resistance</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Civil Engineering and Applied Mechanics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Benson, David</ual:dissertant><dc:abstract>An analytical study was conducted in which shear test results reported in the literature were compared with the results from three prediction methods for 82 I-girder specimens. The three prediction methods used were the Sectional Method, creating models in the Response 2000 program, and non-linear finite element analyses. The principal objective was to investigate the influence of the flanges of concrete I-girders on the ultimate shear resistance of the specimens and the ability of each prediction method to capture this influence. It was concluded that the predictions found using the sectional method and the Response 2000 program provided good estimations of the general web yielding shear, indicated by large stirrup strains and the sudden development of wide shear cracks. However, these two approaches gave very conservative predictions of the failure shears. The predictions made using non-linear finite elements provided good predictions of the entire shear response, including the failure shear. The non-linear finite element analyses accounted for the beneficial effects of the flanges, displaying behaviours similar to the actual tested specimens</dc:abstract><dc:abstract>Une étude analytique a été menée dans laquelle les résultats des tests de cisaillement rapportés dans la littérature ont été comparés aux résultats de trois méthodes de prédiction pour des échantillons de 82 I-poutrelles. Les trois méthodes de prévision utilisées étaient la méthode sectionnelle, la création de modèles dans le programme Response 2000 et les analyses non linéaires par éléments finis. L'objectif principal était d'étudier l'influence des brides des poutres en I en béton sur la résistance ultime au cisaillement des éprouvettes et la capacité de chaque méthode de prévision à capturer cette influence. Il a été conclu que les prédictions obtenues en utilisant la méthode des sections et le programme Response 2000 fournissaient de bonnes estimations du cisaillement générant le voile de la bande, indiqué par les grandes souches d'étriers et le développement soudain de larges fissures de cisaillement. Cependant, ces deux approches ont donné des prévisions très conservatrices du cisaillement à l'échec. Les prédictions effectuées à l'aide d'éléments finis non linéaires ont fourni de bonnes prédictions de la réponse de cisaillement dans son ensemble, y compris le cisaillement par rupture. Les analyses par éléments finis non linéaires ont pris en compte les effets bénéfiques des brides, affichant des comportements similaires à ceux des spécimens réellement testés</dc:abstract><ual:supervisor>Denis Mitchell (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/nk322k06x.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/6q182q554</ual:fedora3Handle><dc:subject>Civil Engineering and Applied Mechanics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A1z40kz28s"><dcterms:title>Inhibitory regulation of plasticity across the lifespan in the rat primary auditory cortex</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Integrated Program in Neuroscience</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Cisneros-Franco, Jose Miguel</ual:dissertant><dc:abstract>Neuroplasticity refers to the brain’s ability to modify its connections and function in response to experience. This experience-dependent plasticity is necessary for the acquisition of new abilities during early development or in adult life, and plays a crucial role in recovery after a neurological injury. During early developmental epochs known as critical periods (CPs), passive experience alone can have profound and long-lasting effects in cortical sensory representations. In contrast, plasticity in the adult brain occurs almost exclusively in the context of perceptual learning (PL); i.e., the process whereby attention and repetition lead to long-lasting improvements in stimulus detection or sensory discrimination.Whether it occurs as a result of passive experience, PL, or other experimental interventions, cortical plasticity ultimately entails a change in activity patterns driven by a shift in the local levels of excitation and inhibition. And although cortical inhibitory interneurons constitute a clear minority compared to the number of excitatory neurons, they are instrumental in regulating both juvenile and adult experience-dependent plasticity. This thesis consists of three experimental studies that addressed critical and interrelated knowledge gaps regarding the inhibitory regulating mechanisms of experience-dependent plasticity, both in the context of changes in the environment and during PL. Using the rat primary auditory cortex (A1) as a model, we combined electrophysiological, anatomical, chemogenetic, and behavioral methodologies to address each study’s main hypotheses. In the first study we examined the role of inhibition in A1 plasticity across the lifespan. We found that reduced cortical inhibition in older adults was associated with an increased but poorly regulated plasticity when compared to younger adults. In older brains, however, changes elicited by auditory stimulation and training were rapidly lost, suggesting that such increased plasticity might be detrimental, as the older brains were unable to consolidate these changes. Importantly, increasing inhibition artificially with clinically available drugs restored the stability of sensory representations and improved the retention of plastic changes associated with PL.In the second study, we turned our attention to parvalbumin-positive (PV+) cells, the most common type of inhibitory neurons in the brain. Bidirectional manipulation of PV+ cell activity affected neuronal spectral and sound intensity selectivity, and, in the case of PV+ interneuron inactivation, was mirrored by anatomical changes in PV and associated perineuronal net expression. In addition, we showed that the inactivation of PV+ interneurons is sufficient to reinstate CP plasticity in the adult auditory cortex. In the third study, we investigated the role of PV+ cells in auditory PL. As previously reported in other cortical areas, training was associated with a transient downregulation of PV expression during early stages of training. We then examined the effects of prolonged PV+ cell manipulation throughout the training period. Our results suggest that, although reduced PV+ cell function may facilitate early training-related modifications in cortical circuits, a subsequent increase in PV+ cell activity is needed to prevent further plastic changes and consolidate learning. Taken together, our findings underscore the importance of sustained inhibitory neurotransmission in ensuring high fidelity discrimination of sensory inputs and in maintaining the stability of sensory representations. Our behavioral studies further suggest that such stability is necessary for the consolidation of complex skills that are built on basic sensory representations. Finally, the experimental work presented in this thesis also highlights the potential of pharmacological and chemogenetic approaches for harnessing cortical plasticity with the ultimate goal of aiding recovery from brain injury or disease</dc:abstract><dc:abstract>La neuroplasticité est la capacité du cerveau de modifier ses connections et de changer sa fonction suite à une expérience. Cette plasticité est nécessaire pour l'acquisition de nouvelles compétences durant le développement précoce ou durant la vie adulte et joue un rôle crucial dans le rétablissement après une lésion neurologique. Durant des fenêtres temporelles du développement précoce nommées périodes critiques, l’expérience passive en elle-même peut avoir des effets profonds et durables sur les représentations sensorielles corticales. En revanche, la plasticité du cerveau adulte se passe presque exclusivement dans le contexte de l’apprentissage perceptuel, c.-à-d. le processus par lequel l’attention et la répétition mènent à des améliorations à long terme dans la détection de stimuli ou la discrimination sensorielle.Que ce soit par l’expérience par exposition passive, l’apprentissage perceptuel ou autres interventions expérimentales, la plasticité corticale comporte nécessairement une modification des patrons d’activation entraînés par un changement des niveaux d’excitation et d’inhibition. Cette thèse est constituée de trois études expérimentales conçues pour combler les manques de connaissances au sujet des mécanismes inhibiteurs régulant la plasticité dépendante de l’expérience dans le contexte des modifications du milieu et durant l’apprentissage perceptuel. En se servant du cortex auditif primaire des rats comme modèle, nous avons combiné des méthodes électrophysiologiques, anatomiques, chimiogénétiques et comportementales pour répondre aux hypothèses principales de chaque étude.Dans la première étude, nous avons examiné le rôle de l'inhibition dans la plasticité du cortex auditif primaire à différentes étapes de la vie. Nous avons trouvé que la réduction de l’inhibition corticale chez les adultes âgés était associée avec une augmentation médiocrement régulée de la plasticité en comparaison avec les jeunes adultes. Toutefois, pour les cerveaux âgés, les changements causés par la stimulation auditive et l'entraînement furent rapidement perdus, suggérant que l'augmentation de la plasticité pourrait être nuisible puisque les cerveaux âgés furent incapables de consolider ces changements. De manière importante, il est aussi à noter qu’augmenter l’inhibition artificiellement avec des substances disponibles en milieu clinique a rétabli la stabilité des représentations sensorielles et a amélioré la rétention des changements plastiques associés à l’apprentissage perceptuel.Dans la deuxième étude, nous avons penché notre attention sur les cellules positives à la parvalbumine (PV+), le type de neurone inhibiteur le plus commun dans le cerveau. Des manipulations bidirectionnelles de l’activité des cellules PV+ ont affectées la sélectivité spectrale et à l’intensité des neurones, et, dans le cas de l’inactivation des interneurones PV+, furent reflétés par les changements anatomiques dans l’expression de PV et des réseaux péri-neuronaux associées. De plus, nous avons démontré que l’inactivation des interneurones PV+ fut suffisante pour rétablir la plasticité due à la période critique dans le cortex auditif adulte.Dans la troisième étude, nous avons étudié le rôle des cellules PV+ dans l’apprentissage perceptuel. L’entraînement fût associé à une régulation négative transitoire de l’expression de PV dans les premières étapes de l'entraînement. Nous avons ensuite examiné les effets de la manipulation de longue durée des cellules PV+ au travers la durée de l’entraînement. Nos résultats suggèrent que malgré la réduction fonctionnelle des cellules PV+ pourrait faciliter des modifications liées à l'entraînement dans les circuits neuronaux, une augmentation subséquente de l’activité des cellules PV+ est nécessaire pour prévenir des changements additionnels et consolider l’apprentissage</dc:abstract><ual:supervisor>Etienne De Villers-Sidani (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/73666922t.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/1z40kz28s</ual:fedora3Handle><dc:subject>Neuroscience</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Adz010v30c"><dcterms:title>Hot and already bothered: exploring effects of warming waters on an imperiled freshwater fish, Pugnose Shiner «Notropis anogenus»</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Biology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Potts, Lindsay</ual:dissertant><dc:abstract>Les changements climatiques représentent une menace importante pour les systèmes d'eau douce et peut affecter les poissons par des effets directs sur leur physiologie thermique, mais aussi indirectement, en modifiant l'ampleur des interactions biotiques et/ou l'abondance relative des espèces voisines. Comprendre les effets directs et indirects de la température constitue une lacune importante qui peut être vitale pour établir des stratégies de conservation pour les poissons d'eau douce. Cela est particulièrement vrai pour les espèces déjà en péril car ces taxons ont tendance à être plus sensibles aux changements de leur environnement, et qui interagissent déjà avec d'autres facteurs. Dans cette thèse, j'ai utilisé une combinaison d'écophysiologie et de techniques de terrain pour examiner les effets de l’elevation de la température sur le méné camus, Notropis anogenus, une espèce menacée au Canada. Dans le premier chapitre, j’ai étudié les effets de la température sur le maximum thermique critique (CTmax) et le taux métabolique standard (SMR) chez le méné camus juvénile, élevé pendant 4 mois à cinq températures différentes. La CTmax a été mesurée sous normoxie et exposition aiguë à l'hypoxie afin de tester la sensibilité à l'oxygène de la limite thermique supérieure chez cette espèce. La CTmax a augmenté avec la température. La température d'agitation (température à laquelle les poissons montrent des signes de stress) a également augmenté avec la température d'élevage et s'est établie en moyenne 4.32°C au-dessus des températures d'acclimatation. La CTmax était plus basse lorsque les poissons étaient exposés à une hypoxie aiguë et ne présentait aucune augmentation entre 19 et 25°C. La CTmax sous hypoxie aiguë a fortement augmenté à la température la plus élevée. Le SMR a augmenté entre 16 et 25°C, puis a diminué de manière significative à 28°C, indiquant que la capacité des individus à inverser les effets thermodynamiques sur le RSM est limitée. La taille des branchies augmente avec l'exposition à long terme à une température élevée, ce qui peut augmenter la capacité d'absorption de l’oxygène et faciliter la réponse des individus à des demandes métaboliques accrues. Au chapitre 2, j’ai exploré le rôle de différents facteurs, tels que la température, d’autres facteurs abiotiques et de l’abondance d’espèces concurrentes, sur la répartition à petite échelle du méné camus dans la baie Thompson (fleuve du Saint-Laurent, Ontario). Des analyses multivariées et une modélisation de l'occupation ont été utilisées pour évaluer la manière dont les variables environnementales et la cooccurrence d'espèces expliquaient la répartition du méné camus. Les variables environnementales, y compris la température, étaient des prédicteurs significatifs de la composition de la communauté. Le méné camus était associé avec des sites clairs et frais éloignés de la baie intérieure, où la densité de la végétation était élevée, y compris Chara vulgaris. Le méné camus était négativement associé avec le crayon d'argent (Labidesthes sicculus), l'achigan à grande bouche (Micropterus salmoides), le crapet soliel (Lepomis gibbosus) et des jeunes Lepomis spp. Ces observations ont des implications importantes pour la conservation car le centrage de certaines centrarchidés  augmente avec le réchauffement climatique, ce qui peut menacer la persistance de cyprinidés rares et en péril. En termes d’action de conservation, l'élevage en captivité peut permettre le durcissement thermique (« thermal hardening ») des individus dans des efforts de rapatriement / réintroduction. De plus, la chasse ciblée d'espèces en expansion peut protéger le méné camus des effets négatifs de la température élevée de l'eau. Nos recherches ont mis en évidence l’importance d’examiner des traits à plusieurs échelles d’organisation biologique, afin de quantifier les effets de l’élévation de la température de l’eau sur les poissons d’eau douce, et de guider les recommandations de conservation</dc:abstract><dc:abstract>Climate change has emerged as an increasingly critical threat to freshwater systems that may affect freshwater fishes through direct effects on their thermal physiology, but also indirectly by altering the magnitude of biotic interactions and/or the relative abundance of co-habiting species. Understanding the direct and indirect effects of temperature represents a significant knowledge gap that may be vital to effective conservation strategies for freshwater fishes. This is especially true for already imperiled species because these taxa tend to be more sensitive to changes in their environment and are already contending with other, and potentially interactive, stressors. In this thesis, I used a combination of ecophysiology and field-based techniques to examine effects of elevated water temperature on Pugnose Shiner, Notropis anogenus, an endangered fish species in Canada. In Chapter 1, I investigated effects of temperature on critical thermal maximum (CTmax) and standard metabolic rate (SMR) in juvenile Pugnose Shiner reared for 4 months across five different temperatures. CTmax was measured under normoxia and acute exposure to hypoxia to test for oxygen sensitivity of upper thermal limits in this species. CTmax increased with elevated water temperature. Agitation temperature (temperature at which fish show behavioural signs of thermal stress) also increased with rearing temperature and occurred on average 4.32°C above acclimation temperatures. CTmax was lower when fish were acutely exposed to hypoxia during CTmax trials and showed no increase between 19 and 25°C. Surprisingly, CTmax under acute hypoxia increased sharply at the highest temperature. SMR increased between 16 and 25°C, and then declined significantly at 28°C, indicating that the ability of Pugnose Shiner to reverse thermodynamic effects on SMR is limited and there was no evidence of long-term thermal compensation. Interestingly, gill size (e.g. total gill filament length, hemibranch area) increased with long-term exposure to high temperature, which may increase oxygen uptake capacity and help to fuel increased metabolic demands. In Chapter 2, I explored the role of temperature, other abiotic factors, and co-occurring species in shaping the fine-scale distribution of Pugnose Shiner within Thompson’s Bay in the upper St. Lawrence River, Ontario. Multivariate analyses and occupancy modelling were used to assess how environmental variables and species co-occurrence accounted for variation in the distribution of Pugnose Shiner. Environmental variables, including temperature, were significant predictors of community composition. Pugnose Shiner was associated with clear, cool sites far from the inner bay with high vegetation density, including Chara vulgaris. Pugnose Shiner was negatively associated with Brook Silverside (Labidesthes sicculus), Largemouth Bass (Micropterus salmoides), Pumpkinseed (Lepomis gibbosus), and juvenile Lepomis spp. This has important conservation implications because some native centrarchids are increasing in abundance with climate warming, which may threaten persistence of rare and imperiled cyprinids, including Pugnose Shiner. Although Pugnose Shiner showed little evidence of thermal compensation (SMR increased with acclimation temperature), it did show acclimation capacity in terms of thermal tolerance and increased gill size. Captive rearing may provide opportunities for thermal hardening in reintroduction efforts, and targeted hunting of expanding species may buffer Pugnose Shiner from the negative effects of elevated water temperature. Overall, my research highlights the importance of examining multiple traits at multiple scales of biological organization to quantify effects of elevated water temperature on freshwater fishes and guide conservation recommendations for imperiled species</dc:abstract><ual:supervisor>Nicholas E. Mandrak (Supervisor2)</ual:supervisor><ual:supervisor>Lauren Chapman (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/nz806449q.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/dz010v30c</ual:fedora3Handle><dc:subject>Biology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A0c483p65r"><dcterms:title>Candidate indistguishability obfuscation for differential privacy: Expanding the use of the k-approximate multilinear map</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>School of Computer Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>White Bear, Javona</ual:dissertant><dc:abstract>L’av`enement de l’`ere de l’information et du ”Big Data” a apport ́e des progr`es  ́etonnants dans la compr ́ehension et l’analyse des donn ́ees, y compris les donn ́ees biologiques, les dossiers m ́edicaux et les achats commerciaux, pour n’en nommer que quelques-uns. Toutefois, l’accessibilit ́e massive de ces donn ́ees, en particulier avec l’av`enement des m ́edias sociaux, a conduit `a une mon ́etisation extrˆeme de la publicit ́e et a` ce que beaucoup consid`erent comme une rupture de confiance entre les institutions et les services qu’elles fournissent. Ces tendances sont exacerb ́ees par l’adoption r ́ecente en masse de l’intelligence artificielle et de l’apprentissage au- tomatique, ce qui am`ene de nombreuses personnes a` conclure que les atteintes `a la vie priv ́ee dans les secteurs public et priv ́e n ́ecessitent une r ́e ́evaluation de la valeur de la vie priv ́ee. Par cons ́equent, de nombreuses institutions cherchent des moyens de tirer parti de la vaste quantit ́e de donn ́ees disponibles tout en pr ́eservant la vie priv ́ee. Bien que la protection de la vie priv ́ee et la s ́ecurit ́e soient depuis longtemps les piliers de la recherche en cryptographie, de nombreuses applications n’ont pas la capacit ́e de fonctionner efficacement `a grande  ́echelle. Souvent pr ́esent ́ee comme une solution de rechange, la protection diff ́erentielle de la vie priv ́ee prend de plus en plus d’importance dans l’application de la protection de la vie priv ́ee en pr ́eservant les analyses de donn ́ees sensibles. Cependant, il lui manque beaucoup des fondements de ses racines cryptographiques et s’appuie plutt fortement sur une obfuscation bas ́ee sur des statistiques. Pour r ́esoudre ce probl`eme, nous proposons une m ́ethodologie capable de tirer efficacement parti de bon nombre des caract ́eristiques positives desvideux m ́ethodes. Dans ce travail, nous discutons et posons les  ́el ́ements de base pour comprendre `a la fois l’indistinguibilit ́e de l’obfuscation (IO) et la vie priv ́ee diff ́erentielle. Ensuite, nous construisons sur cette base et pr ́esentons un m ́ecanisme diff ́erent priv ́e bas ́e sur les principes de l’indistinguibilit ́e obfuscation (IO) sous k- cartes multilin ́eaires approximatives. Ensuite, nous montrons plusieurs m ́ethodes pour assurer la protection de la vie priv ́ee pr ́eserver l’analyse sous un candidat (IO) qui r ́epond aux garanties diff ́erentielles de protection de la vie priv ́ee. Enfin, nous discutons des implications modernes et pratiques de notre m ́ethodologie</dc:abstract><dc:abstract>The advent of the information age and ”Big Data” has brought amazing ad- vances in the understanding and analysis of data including biological data, medical records, and commercial purchases to name only a few. However, the mass accessi- bility of this data, particularly with the advent of social media, has led to extreme monetization in advertising and what many see as a breach of trust between insti- tutions and the services they provide. These trends are further exacerbated by the recent mass adoption of artificial intelligence and machine learning, leading many to conclude that the breaches of privacy in public and private sectors necessitate a reevaluation of the value of privacy. As a result, many institutions are seeking ways to leverage the vast amount of data available while preserving privacy. While privacy and security have long been the mainstays of cryptographic research, many implementations lack the ability to perform efficiently at scale. Often offered as an alternative, differential privacy has a growing foothold in the application of privacy preserving analyses to sensitive data. However, it lacks many of the underpinnings of its cryptographic roots and instead relies heavily on statistically based obfuscation. To address this problem, we propose a methodology that is able to efficiently leverage many of the positive features of both methods. In this work, we discuss and lay the foundational elements for understanding both indistinguishability obfuscation (IO) and differential privacy. Next, we build on this foundation and present a differen- tially private mechanism based on the principles of indistinguishability obfuscation (IO) under k-approximate multilinear maps . Then, we show several methods for ensuring privacy preserving analysis under a candidate (IO) that meets differential privacy guarantees. Finally, we discuss both the modern and practical implications of our methodology</dc:abstract><ual:supervisor>Prakash Panangaden (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/q524jt13m.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/0c483p65r</ual:fedora3Handle><dc:subject>Computer Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A2f75rd30c"><dcterms:title>After the music box: A history of automation in real-time musical performance</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Schulich School of Music</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Michaud, Alyssa</ual:dissertant><dc:abstract>After the Music Box: A History of Automation in Real-Time Musical Performance investigates the reception and use of automation in music over the past century. Employing methods from musicology and media studies, I examine the shifting discourse around music technologies in the early years before their functions and meanings had solidified, and locate the human agency underpinning each automated process. This dissertation explores three historical moments that illuminate the relationship between automation and human music-making: player piano technology in the early twentieth century, synthesizers and sequencers in the mid-twentieth century, and vocal synthesis technology in the early twenty-first century. In chapters 1 and 2, I examine three related piano-playing devices. Drawing on advertising materials, trade publications, and discussions in the popular press, I investigate the anxiety around these new technologies and examine early efforts at defining the purpose of these machines. I correct certain historical misconceptions about player pianos, restore an understanding of the importance of human creativity in the use of these devices, and situate these instruments in the context of other labour-saving technologies. Chapter 3 analyzes the reception history of two early techno-pop groups, in order to map anxieties about dehumanized music-making onto the globalized landscape of 1970s pop music. Finally, in chapters 4 and 5, I investigate Vocaloid singing synthesis software and holographic concerts. Examining Vocaloid performances in the context of research on creative online communities and fan culture, I argue that the contributions of users online and the structured participation of fans at concerts are evidence of a new type of performance enabled by automation in the twenty-first century. This study argues for an understanding of human agency in performances that include automation, and shows the importance of amateur music-making in the development of new music technologies</dc:abstract><dc:abstract>Après la boîte à musique : une histoire de l’automation dans la performance musicale en temps réel prend pour objet de recherche la réception et l’usage de l’automation dans la musique au cours du dernier siècle. Faisant appel à des méthodes empruntées à la musicologie ainsi qu’aux études médiatiques, j’examine la transformation du discours entourant les technologies musicales au cours de leurs toutes premières années d’existence, cette période qui précède la cristallisation de leurs fonctions et de leurs significations, et j’y situe la démarche humaine qui sous-tend chaque processus d’automation. Cette thèse explore trois périodes historiques qui font la lumière sur la relation entre l’automation et la création musicale par l’humain : la technologie des pianos mécaniques au début du vingtième siècle, les synthétiseurs et les séquenceurs au milieu de ce même siècle, ainsi que la technologie de synthèse vocale au début du vingt-et-unième siècle. Au cours des chapitres 1 et 2, j’examine trois modèles comparables de pianos mécaniques. À partir de matériaux publicitaires, de publications commerciales et de discours tirés de la presse populaire, je me penche sur l’anxiété générée par ces nouvelles technologies et sur les efforts faits en vue de définir la fonction de ces machines. J’y corrige certaines incompréhensions historiques au sujet des pianos mécaniques, rétablis une certaine compréhension de l’importance accordée à la créativité humaine en lien avec l’utilisation de ces appareils, et situe ces derniers dans un contexte technologique favorable à la facilitation du travail. Dans le chapitre 3, j’analyse l’histoire de la réception de deux groupes qui furent parmi les premiers à proposer une musique techno pop, de manière à identifier les différentes formes d’anxiété engendrées cette fois par la déshumanisation de la création musicale dans le panorama globalisé de la musique populaire des années 1970. Enfin, dans les chapitres 4 et 5, je m’intéresse au logiciel de synthèse vocale Vocaloid et aux concerts holographiques. En examinant des performances réalisées à l’aide de ce logiciel dans le contexte de recherches sur les communautés créatives en ligne et la culture des admirateurs [fan culture], je défends l’idée selon laquelle les contributions d’utilisateurs en ligne et la participation structurée des admirateurs à l’occasion des concerts sont la preuve d’un nouveau type de performance, rendu possible par les technologies d’automation au vingt-et-unième siècle. Cette étude soutient une compréhension et une reconnaissance de la démarche humaine au cœur de performances intégrant l’automation, et démontre l’importance que prend la création musicale amatrice dans le développement des nouvelles technologies musicales</dc:abstract><ual:supervisor>Jonathan Sterne (Supervisor2)</ual:supervisor><ual:supervisor>Lloyd Whitesell (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/r781wm64z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/2f75rd30c</ual:fedora3Handle><dc:subject>Music</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aws859k907"><dcterms:title>Model-based reuse of framework APIs: Bridging the gap between models and code</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Computer Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Schöttle, Matthias</ual:dissertant><dc:abstract>Reuse is considered key to software engineering and is very common at the implementation level. Many reusable libraries and frameworks exist and are widely reused. However, in the context of Model-Driven Engineering (MDE) reuse is not very common. Most modelling approaches do not support reuse, requiring a user to start their modelling activity either from scratch or copy and paste pieces from other models.This thesis provides a bridge for reusable units between implementation and modelling. We apply the principles of Concern-Oriented Reuse (CORE), a next-generation reuse technology, to lift existing frameworks up from the programming level to the modelling level. The level of abstraction of the API of existing frameworks is raised to the modelling level to facilitate their reuse within design models that are integrated within an MDE process. In addition, the benefits of the higher level of abstraction are exploited to formalize otherwise informally provided information, such as which features the framework provides, the impact of each feature on high-level goals and non-functional qualities, how to adapt the framework to the reuse context, and how the API of each feature is to be used. This thesis defines an automated algorithm that analyses the code of a framework and example code that uses the framework to produce an interface that 1) lists the user-perceivable features of the framework organized in a feature model, and 2) modularizes the API of the framework API according to each feature. The algorithm is implemented and validated on two small frameworks and the Android Notifications API along with an empirical user study.To smoothen the transition from a high-level abstraction to a low level of abstraction, i.e., from models to code, this thesis addresses the difficulty caused by the finality of signatures. We identify and discuss four difficult situations for defining high-level interfaces at the modelling level, and present evidence that shows that these situations also exist at the implementation level. The signature extension approach is introduced to CORE allowing interfaces to encompass diverse implementation variants and to be evolved at a fine level of granularity across groups of features. We re-design two reusable concerns to show that the approach addresses the four difficult situations</dc:abstract><dc:abstract>La réutilisation est essentielle dans l’ingénierie logicielle. La réutilisation de librairies et de frameworks (cadre d’application) est très utilisée lors du niveau de l’implémentation. Il existe un nombre important de frameworks et ils sont réutilisés fréquemment. Cependant, dans le contexte de l’ingénierie dirigée par les modèles (Model-Driven Engineering (MDE)), la réutilisation de modèles n’est pas très présente. La plupart des méthodes de modélisation ne supportent pas la réutilisation et demandent à l’utilisateur de soit commencer la modélisation de zéro ou de reprendre des morceaux d’autres modèles.Cette thèse crée un lien entre l’implémentation et la modélisation pour des unités réutilisables. Nous utilisons les principes de la réutilisation par préoccupation (Concern-Oriented Reuse (CORE)), une nouvelle génération de technologies de réutilisation, pour monter les frameworks du niveau de l’implémentation vers la modélisation. Le niveau d’abstraction de l’interface de programmation d’application (API) de framework existant est monté au niveau de la modélisation pour faciliter leurs réutilisations dans un modèle de conception qui est intégré dans le processus MDE. De plus, les avantages d’un niveau d’abstraction plus élevé sont utilisés pour formaliser de l’information qui autrement n’existe que d’une manière informelle. Celle-ci nous permet de savoir quelles fonctionnalités le framework propose, l’impact de chaque fonctionnalité sur des buts de haut niveau et des qualités non-fonctionnelles, comment adapter le framework dans un contexte de réutilisation et comment l’API de chaque fonctionnalité doit être utilisée. Cette thèse définit un algorithme automatisé qui analyse le code d’un framework et les exemples de code qui utilisent l’API pour produire une interface qui 1) liste les fonctionnalités utilisateur du framework organisée dans un modèle de fonctionnalité, et 2) modularise l’API du framework selon chaque fonctionnalité. L’algorithme est implémenté et validé avec deux petits frameworks ainsi qu’avec une étude utilisateur empirique de l’API de notification d’Android.Pour faciliter la transition depuis un haut niveau d’abstraction vers un niveau d’abstraction plus bas, c’est-à-dire, des modèles vers le code, cette thèse aborde les difficultés causées par la finalité des signatures de méthode. Nous identifions quatre situations complexes lors de la création d’interfaces haut niveau au niveau du modèle et présentons des preuves qui montrent que ces situations existent également au niveau de l’implémentation. La méthode de l’extension de signature est introduite à CORE, permettant aux interfaces d’intégrer plusieurs variantes d’implémentation et d’évoluer dans le détail à travers un groupe de fonctionnalités. Nous redéfinissons deux concerns réutilisables pour montrer que cette méthode répond à ces quatre situations complexes</dc:abstract><ual:supervisor>Jorg Andreas Kienzle (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/44558j53b.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/ws859k907</ual:fedora3Handle><dc:subject>Computer Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Atx31qp24g"><dcterms:title>Flexural response of flat plate edge slab-column connections</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Civil Engineering and Applied Mechanics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Tack, Ryan</ual:dissertant><dc:abstract>The CSA A23.3-14 Standard and ACI 318-14 Code require that the slab reinforcement for the negativemoment at edge connections be placed in a band width, bb, extending a distance 1.5 times the slab thicknesspast the sides of the column. To investigate the effectiveness of the top slab reinforcement in width bb, fourfull-scale edge slab-column specimens with varying column dimensions and top slab reinforcement arrangementswere designed, constructed, and tested. The experimental results of this program revealed that notall reinforcing bars in width bb were effective, especially when the distance from the inner column face to thefree edge was less than 1.5 times the slab thickness. A method for determining the effectiveness of top slabreinforcing bars was developed based on the formation of torsional yield lines in the slab. Compared to theCSA A23.3-14 and ACI 318-14 approach, better agreement between the predicted and experimental resultswas achieved with this proposed method</dc:abstract><dc:abstract>Les normes CSA A23.3-14 et ACI 318-14 exigent que l’armature de dalle pour le moment négatif auxconnexions de rive soit placée dans une bande de largeur, bb, égale à 1.5 fois l’épaisseur de la dalle de chaquecôté du poteau. Pour examiner l’efficacité de l’armature supérieure dans la bande de largeur, bb, quatrespécimens de connexion dalle-poteau de rive à grande échelle dont les dimensions du poteau et la dispositionde l’armature variaient ont été conçus, construits et testés. Les résultats de ce programme expérimental ontrévélé que les barres d’armature dans la bande de largeur, bb, n’étaient pas toutes éfficaces, surtout quand ils’agissait des spécimens dont la distance entre la face intérieur du poteau et le bord libre était moins de 1.5fois l’épaisseur de la dalle. Une méthode pour déterminer l’efficacité de l’armature supérieur a été développéebasée sur la formation des lignes de rupture dans la dalle. Par rapport à la méthode de CSA A23.3-14 etACI 318-14, les résults prédits par la méthode proposée convenaient mieux aux résultats expérimentaux</dc:abstract><ual:supervisor>Denis Mitchell (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/zp38wh89r.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/tx31qp24g</ual:fedora3Handle><dc:subject>Civil Engineering and Applied Mechanics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Atq57nw09w"><dcterms:title>The quantification of radiation induced pulmonary fibrosis</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Medicine</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Wang, Li Ming</ual:dissertant><dc:abstract>La fibrose pulmonaire causée par la radiothérapie (FPPR) est une complication tardive et permanente de la radiothérapie thoracique. Elle est caractérisée par le développement de tissus cicatriciels permanents aux poumons des patients traités pour le cancer du poumon, qui présentent déjà un réserve et fonction pulmonaire réduit. Présentement, l’évaluation de la FPPR dépend des examens qualitatifs et subjectifs fait par les médecins et souvent cette évaluation n’est pas faite sauf si les patients signalement des symptômes, la FPPR est généralement asymptomatique. La FPPR est présentement mal identifiée et elle évaluée avec une grande variabilité. Les études récentes suggèrent que quantifier des résultats diagnostiques qui sont traditionnellement qualitatives, comme par exemple l’évaluation visuelle de la FPPR, peut produire un système de notation plus objectif et reproductible. Dans cette étude, on explore la quantification de la FPPR chez un modèle animal (rat) en utilisant l’histologie et dans une étude clinique prospective, en utilisant l’imagerie de la tomodensitométrie de patients avec l’objectif de remplacer les évaluations traditionnellement qualitatives. On a validé deux modèles quantitatifs qui analysent des volumes spécifiques d’une couleur spécifique en histologie et la mesure d’une radio-densité spécifique avec l’imagerie de la tomodensitométrie dans le cadre de deux études cliniques pertinentes et on a comparé les résultats de nos méthodes quantitatives à ceux des médecins. Chez le modèle animal, les régions du poumon colorées en bleu avec la coloration Trichrome de Masson, indiquant la fibrose, ces régions sont utilisées pour quantifier la fibrose. Dans l’étude prospective avec des patients, le volume de l’espace occupé par une gamme de Hounsfield Unit définie empiriquement, qui est considérée une gamme de la fibrose, est utilisée pour quantifier la fibrose. Ces méthodes quantitatives se sont révélées prometteuses comme système de cotation de la FPPR, et sont compétents, objectifs et reproductibles. Par contre, la quantification des caractéristiques comme la couleur et la radio-densité ne sont pas assez pour évaluer complètement la FPPR. Pour la FPPR, il est primordial d’utiliser une composante quantitative qui analyse spécifiquement les changements structurels causés par la FPPR afin de reproduire un résultat similaire à l’évaluation d’un médecin. Les techniques qui peuvent quantifier les dommages anatomiques, dont les changements structurels anatomiques et la déformation d’entités anatomiques, peuvent améliorer davantage les résultats des autres techniques quantitatives et peuvent simuler une plus grande variété de caractéristiques que celles qui sont considérés par les médecins. La méthode dans son état actuel est apte pour être utiliser dans des études futures de la FPPR, afin de générer des résultats plus cohérents et objectifs à analyser</dc:abstract><dc:abstract>Radiation induced pulmonary fibrosis (RIPF) is a late and permanent complication of thoracic radiotherapy. It is characterized by the formation of permanent scar tissue in the lungs, often reducing the already depleted pulmonary reserves and decreased pulmonary function of lung cancer patients. Currently, the assessment of RIPF is dependent on the qualitative and subjective appraisals of physicians, evaluated only after patient self-reports, despite RIPF presenting asymptomatic in many cases. As a result, RIPF is poorly identified and assessed with high variability. Recent studies have suggested that quantifying traditionally qualitative diagnostic outcomes, such as visually appraised RIPF, can yield a more replicable and objective scoring. In this work, we explore the quantification of RIPF in an animal (rat) model through the use of histology and in a prospective clinical study using patient computed tomography imaging as a surrogate for traditional qualitative appraisals. We validated two quantitative models, which analyse specific volumes of a given color in histology and the extent of a given radio-density in computed tomography imaging, in the context of two clinically relevant problems and compared the performance of our quantitative methods to that of physicians. In the animal model, the area of Masson’s Trichrome stained regions of blue, indicative of fibrosis was used as a quantitative surrogate for fibrosis scoring. In the prospective patient study, the volume of space occupied by an empirically determined Hounsfield Unit range, deemed to be the density range for fibrosis, is used as a quantitative surrogate for fibrosis scoring. These quantitative methods showed promise as a capable, objective and reproducible method of scoring RIPF. However, quantifying a single feature such as color or radio-densities is not enough. For RIPF, a quantitative component which specifically analyzes structural changes due to RIPF is necessary in order to produce more physician-like performance. Techniques which can quantify anatomical damage, such as the change in anatomical structures or overall distortion of anatomical entities, can further improve the quantitative technique and simulate the variety of additional features that physicians take into account. The method in its current state is still capable of being applied for future studies of RIPF to generate more consistent and objective outcomes for analysis</dc:abstract><ual:supervisor>Norma Ybarra (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/z603r291f.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/tq57nw09w</ual:fedora3Handle><dc:subject>Medicine</dc:subject></rdf:Description></rdf:RDF>