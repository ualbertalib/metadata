<?xml version="1.0" encoding="UTF-8"?><rdf:RDF xmlns:oai="http://www.openarchives.org/OAI/2.0/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:ual="http://terms.library.ualberta.ca/" xmlns:bibo="http://purl.org/ontology/bibo/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:schema="https://schema.org/" xmlns:etdms="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3An583xx09z"><dcterms:title>Hippocampal shape and volume alterations in youth born with a congenital heart defect</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Integrated Program in Neuroscience</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Fontes, Kimberly</ual:dissertant><dc:abstract>There is a high prevalence of neurodevelopmental impairments in individuals living with congenital heart disease (CHD) and the neural correlates of neurodevelopmental impairments are not yet fully understood. Recent studies have shown that hippocampal volume and shape differences may provide unique biomarkers for neurodevelopmental disorders. The hippocampus is vulnerable to early life injury, especially in populations at risk for hypoxia or ischemia such as in neonates with CHD.  Therefore, we compared hippocampal grey and white matter volume and morphometry between youth born with CHD aged 16-24 years and healthy peers. We also explored whether hippocampal volume and morphometry are associated with executive function and self-regulation deficits. To do so, participants underwent 3T brain magnetic resonance imaging and completed the self-reported Behaviour Rating Inventory of Executive Function, Adult version.  Hippocampal circuitry was segmented using the Multiple Automatically Generated Templates brain segmentation algorithm to extract volume and morphometry. We found that youth with CHD had smaller hippocampal volumes (all statistics corrected for False Discovery rate; q&lt;0.05) as compared to controls. We also observed significant smaller surface area bilaterally and inward displacement on the left hippocampus predominantly on the ventral side (q&lt;0.10) in the CHD group that were not present in the controls. Left CA1 and left CA2/3 volumes were negatively associated with working memory performance (p&lt;0.05). No associations between hippocampal morphometry and executive function and self-regulation were found. This study is the first report of hippocampal morphometric alterations in youth born with CHD when compared to healthy peers, as well as, structure-function relationships between hippocampal volumes and EF and self-regulation. These differences may reflect long-lasting early life brain alterations that are present in these at-risk neonates with CHD. </dc:abstract><dc:abstract>Le taux de troubles neurodéveloppementaux chez les personnes vivant avec une cardiopathie congénitale est très élevé, cependant, les mécanismes neuraux de ces troubles neurodéveloppementaux ne sont pas encore bien compris. Des projets de recherche ont montré que les différences de volume et de morphométrie de l'hippocampe peuvent fournir des biomarqueurs uniques pour certain de ces troubles neurodéveloppementaux. L'hippocampe est susceptible aux lésions en début de vie, surtout chez les populations à risque d'hypoxie, telles que les nouveau-nés atteints de cardiopathie congénitale. Par conséquent, nous avons comparé le volume de la substance grise et blanche et la morphométrie de l'hippocampe entre des jeunes nés avec une cardiopathie congénitale âgés de 16 à 24 ans et des jeunes adultes en bonne santé. Nous avons également examiné si le volume et la morphométrie de l'hippocampe étaient associés à des difficultés de fonction exécutive et d'autorégulation. Les participants ont été soumis à une imagerie par résonance magnétique du cerveau et ils ont complété un questionnaire de FE et d'autorégulation. L'hippocampe a été segmenté à l'aide de l'algorithme de « segmentation cérébrale de modèles générés automatiquement » pour obtenir le volume et la morphométrie. Les jeunes atteints de cardiopathie congénitale présentaient avec des volumes d'hippocampe plus petits comparé aux contrôles (q&lt;0.05). Un plus petit volume des surfaces bilatérales de l'hippocampe et des déplacements internes de l'hippocampe gauche, principalement sur le côté ventral (q&lt;0.10) ont été observé uniquement chez les jeunes atteints de cardiopathie congénitale. Le volume de la région 1 de la corme d'Ammon (CA) gauche et de la CA2/3 gauche étaient associés avec une moins bonne mémoire de travail (p&lt;0.05). En ces résultats, nous rapportons, pour la première fois, des altérations morphométriques de l'hippocampe chez des adolescents et des jeunes adultes nés avec une cardiopathie congénitale qui ne sont pas présents chez les contrôles, ainsi que des relations entre les volumes de l'hippocampe et la FE et l'autorégulation. Ces différences pourraient refléter des altérations persistantes du maldéveloppement cérébral précoce de ses enfants à haut risque.</dc:abstract><ual:supervisor>Megha Chakravarty (Internal/Cosupervisor2)</ual:supervisor><ual:supervisor>Marie Brossard-Racine (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/jw827d83w.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/n583xx09z</ual:fedora3Handle><dc:subject>Neuroscience</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Az316q367q"><dcterms:title>MAIT cell recovery impossible despite early ART</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Medicine</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Kokinov, Nikola</ual:dissertant><dc:abstract>Chez les personnes vivant avec le VIH, l'activation immunitaire persistante reste le talon d'Achille à l'ère des antirétroviraux. L'activation immunitaire provient des réservoirs de VIH situés dans les ganglions lymphatiques et des lésions intestinales induites par l'infection. La perméabilité accrue de l'épithélium intestinal au cours de l'infection par le VIH permet la translocation de produits microbiens de la lumière intestinale vers le sang. Dans l'intestin, lescellules T invariantes associées aux muqueuses (MAIT) jouent un rôle protecteur contre les infections bactériennes et fongiques en produisant les cytokines IL-17 et IL-22. Les cellules MAIT sont des cellules T de type inné représentant jusqu'à 10% du nombre total de cellules TCD8 circulantes et sont caractérisées par la forte expression de CD161 et du récepteur de cellulesT semi-invariant Vα7.2, restreint par la protéine MR1. Il a été démontré que MR1 présentait des métabolites de la vitamine B exclusivement produits par les bactéries et les champignons. Dans la phase primaire de l'infection par le VIH, les cellules MAIT ont un phénotype épuisé dans le sang et ne sont pas restaurées malgré le début du traitement antirétroviral au stade chronique (traitement tardif). La diminution des cellules MAIT a été associée à un faible ratio de lymphocytes T CD4 / CD8, suggérant un rôle des cellules MAIT dans la progression de la maladie. Il reste à déterminer si l'initiation d'un traitement antirétroviral au cours des premières phases de l'infection par le VIH (traitement précoce) pourrait conduire à la récupération des cellules MAIT dans le sang. Nous avons émis l'hypothèse que, dans le sang, les cellules MAITsont restaurées grâce au traitement précoce. Nous avons découvert que les cellules CD8 MAIT dans le sang ne sont pas récupérées deux ans après le début du traitement antirétroviral. Les cellules CD8 MAIT restantes présentaient un phénotype hypofonctionnel (PD1 + CD39 +) nettement plus prononcé que chez les témoins non infectés. De plus, nous avons confirmé la déplétion et l'épuisement des cellules MAIT chez un nombre important d'élite contrôleurs.</dc:abstract><dc:abstract>The persistent immune activation remains the Achilles heel of the current ART era and it is thought to originate from the HIV reservoirs within the lymph nodes and the HIV induced gut damage. The increased permeability of the gut epithelium during the HIV infection, allows the translocation of microbial products from the intestinal lumen to the systemic circulation. In the gut, the mucosal-associated invariant T (MAIT) cells play a protective role against invading bacteria and fungi by producing effector cytokines such as IL-17 and IL-22. MAIT cells are innate-like T cells representing up to 10% of total circulating CD8 T cells and are characterized by the high expression of CD161 and of the semi-invariant T-cell receptor Vα7.2, restricted to the MHC-related protein 1 (MR1). MR1 has been shown to present unstable vitamin B metabolites exclusively produced by bacteria and fungi. During HIV infection, MAIT cells are depleted in blood during the primary stages and are not recovered despite ART initiation in the chronic stages (late ART). MAIT cell depletion has been associated with low CD4/CD8 T cell ratio suggesting a role for MAIT cells in clinical outcome. It remains unknown, whether ART treatment initiation within the primary stages of the HIV infection (early ART) could lead to MAIT cell recovery in the blood. We hypothesized, that in the blood MAIT cells are restored upon early ART. We discovered that blood CD8 MAIT cells are not recovered despite early ART and the remaining CD8 MAIT cells were having a significantly more pronounced hypofunctional phenotype (PD1+CD39+) in comparison to uninfected controls. In addition, we confirmed the depletion of MAIT cells in a relatively larger number of elite controllers. Surprisingly, the remaining MAIT cells in EC were hypofunctional.</dc:abstract><ual:supervisor>Jean-Pierre Routy (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/mg74qp335.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/z316q367q</ual:fedora3Handle><dc:subject>Medicine</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A707959882"><dcterms:title>How young Egyptians' interactions with Egypt's master historical narrative shape their social identities and civic attitudes</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Integrated Studies in Education</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Abdou, Ehaab</ual:dissertant><dc:abstract>Cette thèse élucide la manière dont les jeunes Égyptiens interagissent avec le récit historique « dominant » ou « maître » de l'Égypte, présenté dans leurs programmes scolaires officiels. En outre, il cherche à comprendre en quoi l'omission et la fausse représentation des points de vue des minorités et des récits historiques influent sur les identités sociales et les attitudes civiques des étudiants de niveau postsecondaire, telles qu'énoncées dans leurs rôles sociaux actuels et envisagés. Bien que je discute des interactions des étudiants égyptiens du postsecondaire avec les omissions générales et les fausses déclarations dans le récit historique dominant, je me concentre sur les omissions et les représentations largement reconnues, mais sous-étudiées, de l'histoire copte (chrétienne égyptienne) et du programme officiel. Pour aborder ces questions, j'ai travaillé avec un groupe de diplômés récents d'écoles secondaires égyptiennes, d'une grande diversité (n = 39). Adoptant une approche théorique ancrée et guidée par la conscience historique (Historical Consciousness), les théories de représentations sociales (Social Representations), l'analyse critique du discours (Critical Discourse Analysis) et la pédagogie critique (Critical Pedagogy), mes méthodes de collecte de données comprenaient des questionnaires écrits, des entretiens semi-structurés et des méthodes visuelles participatives.L'analyse des données révèle que de nombreux participants - en particulier à cause de la révolution égyptienne de janvier 2011 et de ses conséquences, ainsi que dans certains cas des déclarations erronées de leur communauté minoritaire - ont développé des approches critiques fortes à l'égard du récit dominant construit et propagé par les élites au pouvoir. Cependant, cette approche critique continue à se dérouler dans les limites du récit dominant, laissant ainsi certains des éléments et caractéristiques essentiels du récit dominant intacts et non contestés. Ces caractéristiques incluent la normalisation de la nature primordiale de l'État égyptien ainsi que la nature cyclique de l'histoire.Au sein de ces approches critiques du récit dominant, plusieurs nuances importantes se dégagent. Premièrement, seuls quelques participants ont mis en doute des problèmes structurels ou des dynamiques de pouvoir et d'intérêts qui le récit historique dominant sert. Deuxièmement, les discours dominants qui façonnent la sphère publique en Égypte - y compris le discours nationaliste territorial et le discours fondé sur la religion - ont clairement façonné les récits alternatifs que les participants ont cherché à construire. Par exemple, les participants ancrés dans une compréhension religieuse, tout en critiquant le discours nationaliste, sont revenus sans discernement à un récit transcendantal - et dans certains cas à un récit historique mythique - en tant que fait historique. L'étude montre que les personnes exposées à des cours d'histoire universitaire et engagées dans des initiatives parascolaires liées à l'histoire ont montré une capacité plus développée à adopter de perspectives multiples dans leurs approches de la critique et de la déconstruction du récit dominant.En ce qui concerne les attitudes civiques des participants, on assiste à un changement général vers des activités d'autonomisation économique, d'éducation et de sensibilisation minimisant clairement le contact ou le soutien possible à l'élite dirigeante. En tant que minorité généralement exclue et incompris, plusieurs participants coptes semblaient tenter de canaliser leur sentiment général d'exclusion et d'incompréhension vers l'établissement et le soutien d'initiatives communautaires qui favoriseraient les relations et le respect interreligieux entre chrétiens et musulmans.</dc:abstract><dc:abstract>This dissertation elucidates how young Egyptians interact with Egypt's 'dominant' or 'master' historical narrative, as presented in their formal school curricula. Further, it seeks to understand how the omission and misrepresentation of minority perspectives and historical narratives influences post-secondary students' social identities and civic attitudes, as embodied in their current and envisioned societal roles. While I discuss post-secondary Egyptian students' interactions with general omissions and misrepresentations in the dominant historical narrative, I focus on the widely acknowledged, but understudied, omissions and misrepresentations of Coptic (Egyptian Christian) history and contributions from the formal curriculum. To approach these questions, I worked with a diverse group of recent graduates of Egyptian secondary schools (n=39). Adopting a grounded theory approach and guided by historical consciousness, social representations theories, critical discourse analysis and critical pedagogy, my data collection methods included written questionnaires, semi-structured interviews, and participatory visual methods. The data analysis reveals that many of the participants - especially because of the Egyptian January 2011 revolution and its aftermaths, and their minority community's misrepresentation in some cases - have developed strong critical approaches vis-à-vis the dominant narrative constructed and propagated by the ruling elite. However, this critical approach is still taking place within the confines of the dominant narrative, thus leaving some of its key defining elements and features intact and unchallenged. Those features include normalizing the nature of the Egyptian state as primordial, and the nature of history as cyclical.Within these critical approaches to the dominant narrative, several important nuances emerge. First, only a few participants questioned structural issues or power dynamics and interests that the dominant historical narrative serve. Second, the dominant discourses shaping the public sphere in Egypt - including the territorial nationalist and the religious-based discourse - clearly shaped the alternative narratives that the participants sought to construct. For instance, participants embedded in a religious-based understanding, while critical of the nationalistic discourse, reverted uncritically to a transcendental – and in some cases a mythical historical narrative – as historical fact. The study shows that especially those exposed to academic history courses and engaged in extracurricular history-related initiatives exhibited a more evolved ability to embrace multiple perspectives in their approaches to critiquing and deconstructing the dominant narrative.In terms of the participants' civic attitudes, there is a general shift towards economic empowerment, educational, and awareness building activities clearly minimizing contact or possible support to the ruling elite. As a generally excluded and misunderstood minority, several of the Coptic participants seemed to attempt to channel their general sense of exclusion and being misunderstood towards establishing and supporting community initiatives that would foster Muslim-Christian interfaith relations and respect.</dc:abstract><ual:supervisor>Claudia A Mitchell (Supervisor1)</ual:supervisor><ual:supervisor>Boghos Zanazanian (Supervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/dn39x3597.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/707959882</ual:fedora3Handle><dc:subject>Integrated Studies in Education</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9p290c789"><dcterms:title>Geotechnical hazard assessment of road cuts stability in mountainous areas in Saudi Arabia</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mining and Materials</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Basahel, Hassan</ual:dissertant><dc:abstract>La stabilité des pentes rocheuses est essentielle à la sécurité publique dans les autoroutes flanquées de rochers. L'instabilité de la pente et les effondrements de talus sont attribuables à de nombreux facteurs comme la géométrie défavorable des pentes, les discontinuités géologiques, les matériaux de talus mous ou patinés et les conditions météorologiques extrêmes. Les pressions externes comme les fortes précipitations et les tremblements de terre pourraient jouer un rôle important dans les effondrements de talus. Dans cette thèse, plusieurs systèmes de classification de masse rocheuse permettant d'évaluer la stabilité des pentes rocheuses ont fait l'objet d'un examen critique et ont été comparés aux pentes rocheuses connues dans le sud-ouest de l'Arabie saoudite. On a appliqué certaines méthodes empiriques à 22 tranchées rocheuses et identifié les conditions de stabilité. Les résultats obtenus à partir de ces méthodes ont été comparés les uns aux autres et ont permis de souligner les limites de chaque système de classification empirique.Une analyse probabiliste de la stabilité est ensuite mise au point. L'état de stabilité d'un talus rocheux diaclasé dans l'une des tranchées rocheuses sélectionnée à partir des 22 tranchées rocheuses choisies précédemment a été évalué à l'aide d'approches déterministe et probabiliste, dans des conditions sèches et humides. L'analyse de la stabilité a été effectuée à l'aide du logiciel FLAC3D pour produire un modèle en 3 dimensions, et l'assise est simulée à l'aide d'un modèle à joint systématique, afin de déterminer l'influence de l'orientation dominante et défavorable de l'assise sur la face de la pente. L'analyse déterministe a d'abord été effectuée à l'aide des valeurs moyennes des variables aléatoires choisies, à savoir l'inclinaison, la direction de l'inclinaison et l'angle de friction de l'ensemble de discontinuité dominant, et l'état de stabilité a été évalué. Une approche de conception « Box-Behnken » (BBD) est ensuite adoptée pour créer la fonction de réponse de surface, en tant que polynôme de deuxième degré pour le facteur de sécurité. Quinze modèles FLAC3D ont donc été générés conformément à l'approche BBD. À partir de ces modèles, on a effectué 10 000 simulations de différentes réalisations de pente grâce à la technique de simulation Monte-Carlo, et on a évalué la probabilité de rendement insatisfaisant de la pente rocheuse. Il est démontré que l'approche probabiliste procure une compréhension accrue et une plus grande confiance dans l'état de stabilité de la pente rocheuse, dans des conditions sèches et sous de fortes et constantes précipitations.  On introduit un nouveau facteur, défini dans le présent document comme le facteur d'intensité de l'effondrement, afin de décrire la gravité de l'effondrement du talus par rapport à la largeur de la chaussée, et distinguer les pentes rocheuses ayant la même probabilité d'effondrement, mais pas la même magnitude de matériaux défaillants. Un critère de cisaillement maximum est adopté pour délimiter le volume de matériau de glissement et ses effets sur la chaussée, et 3 niveaux de facteurs d'intensité – faibles, modérés et élevés – sont définis.  Enfin, le facteur d'intensité est utilisé pour calculer la matrice des indices de risque géotechnique. On propose une matrice d'indices de risque géotechnique à 5 niveaux en fonction des niveaux recommandés de probabilités d'effondrement faibles, modérées et élevées. La matrice des indices de risque proposée tient compte à la fois de la probabilité de rendement insatisfaisant de la pente rocheuse et de la gravité de l'effondrement du talus exprimée en facteur d'intensité.  </dc:abstract><dc:abstract>The stability of rock slopes is crucial to public safety in highways passing through rock cuts. Slope instability and failures occur due to many factors such as adverse slop geometries, geological discontinuities, weak or weathered slope materials as well as severe weather conditions. External loads like heavy precipitation and earthquakes could play a significant role in slope failure. In this thesis, several rock mass classification systems for rock slope stability assessment were critically evaluated against known rock slope conditions in the south western region of Saudi Arabia. Selected empirical methods were applied to 22 rock cuts, and the stability conditions were identified. The results obtained from these methods were compared to each other and served to highlight the limitations of each empirical classification system.A probabilistic stability analysis approach is then developed. The stability condition of a jointed rock slope in one of the selected rock cuts taken from the previously chosen 22 rock cuts was assessed using deterministic and probabilistic approaches, under both dry and wet conditions. The stability analysis was carried out using FLAC3D to generate a 3-dimensional model, and bedding is simulated with a ubiquitous joint model, to determine the influence of the dominant, unfavorable bedding orientation with respect to the slope face. The deterministic analysis was implemented first using the mean values of the selected random variables, namely the dip, dip direction and friction angle of the dominant discontinuity set, and the stability condition was assessed. A Box-Behnken design (BBD) approach is then adopted to create the surface response function, as a second order polynomial for the factor of safety. To do so, fifteen FLAC3D models were generated in accordance with BBD. Based on this, 10,000 simulations of different slope realizations were carried out using Monte-Carlo simulation technique, and the probability of unsatisfactory of performance of the rock slope was assessed. It is shown that the probabilistic approach provides more insight and confidence in the stability condition of the rock slope, under both dry and steady state heavy rainfall conditions.  A new factor, herein defined as the failure intensity factor is introduced to describe the severity of slope failure with respect to the roadway width, to distinguish between rock slopes having the same probability of failure, yet not the same magnitude of failed material. A maximum shear strain criterion is adopted to delineate the volume of sliding material and its effect on the roadway, and 3 levels of the intensity factors – low, moderate and high – are defined.  Finally, the intensity factor is used to derive the geotechnical hazard index matrix.  A 5-level geotechnical HI matrix is proposed, based on reported recommended levels of low, moderate and high probabilities of failure.  The proposed Hazard Index matrix considers both the probability of unsatisfactory of performance of the rock slope and the severity of the slope failure expressed as the intensity factor.  </dc:abstract><ual:supervisor>Hani Mitri (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/n583xx10q.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9p290c789</ual:fedora3Handle><dc:subject>Mining and Materials</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Am900nw75z"><dcterms:title>Two faces of revolutionary consciousness: solidarity and ideology in the Arab Spring</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Department of Political Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Ghorayeb, Mila</ual:dissertant><dc:abstract>Twentieth-century theorists of revolution focused on articulating what conditions were necessary for motivating transformative political change. In particular, theorists were concerned with the concept of consciousness raising. V.I. Lenin's theory famously argued that consciousness – an understanding of the inner-workings of one's unfreedom – must be brought to people by revolutionary vanguards. Leninist theory was later challenged when post-colonial theorists such as Ranajit Guha rejected the claim that people require their consciousness to be raised by external actors. Guha's work showed that rebels can be conscious on their own terms by highlighting evidence of oppositional consciousness in peasant insurgents in colonial India. My thesis situates itself in this debate by answering the question: what is it that will make people with legitimate claims against existing powers articulate and act on this claim in a way that radically shifts the current political order? Overlaying my argument on the early uprisings in the Arab Spring, I argue that Lenin and Guha describe two crucial, but conceptually distinct forms of political consciousness. Guha's articulation of oppositional consciousness, I argue, answers the first form of the question: rejecting one's subordinate status through solidarity and self-identification with others of the same status will bring about political contestation. Lenin's conception of revolutionary class consciousness answers the second part: a cohesive and public counter-ideology to that of the status-quo is required to challenge and shift hegemonic political practices. Guha's oppositional consciousness leads a revolution to what Asef Bayat calls 'revolution as movement': the radical mobilization and solidarity of political subjects against existing powers. However, the deciding factor for whether the political order will shift or restore is dependent on which actor(s) cohesively articulate the source of peoples' unfreedom, as well as a blueprint for a freer order. This is what Bayat deems 'revolution as change,' which he argues failed to materialize during the Arab Spring. Just as Bayat argues for a conceptual distinction between movement and change, my thesis complements his concepts through mirroring conceptions of revolutionary consciousness. As a result, my argument shows that Lenin's and Guha's conceptions of consciousness are distinct and complementary rather than rivalling accounts.</dc:abstract><dc:abstract>Les théoriciens de la révolution du XXe siècle ont cherché à définir quelles conditions étaient nécessaires pour motiver un changement politique transformateur. En particulier, les théoriciens se sont intéressés au concept sensibilisation. La théorie de V.I. Lénine affirmait que la conscience - une compréhension du fonctionnement interne de la non-liberté d'une personne - devait être amenée du «dehors» par des avant-gardes révolutionnaires. Par la suite, certains théoriciens post-coloniaux tels que Ranajit Guha ont contesté la théorie léniniste en rejetant l'affirmation selon laquelle les individus exigent que leur conscience soit élevée par des acteurs extérieurs. Les travaux de Guha ont montré que les rebelles peuvent être conscientes selon leurs propres termes, en mettant en évidence des preuves de la conscience opposée chez les paysans insurgés de l'Inde coloniale. Ma thèse se situe dans ce débat en répondant à la question: qu'est-ce qui assurera que ceux qui ont des revendications légitimes contre les pouvoirs existants articulent et agissent sur cette revendication d'une manière qui modifie radicalement l'ordre politique actuel? En reprenant mon argument sur les premières étapes du Printemps arabe, je soutiens que Lénine et Guha décrivent deux formes cruciales, mais conceptuellement distinctes, de la conscience politique. Je soutiens que la formulation par Guha de la conscience oppositionnelle répond à la première forme de la question: rejeter son statut de subordonné par la solidarité et l'auto-identification avec des personnes du même statut suscitera une contestation politique. La conception de Lénine de la conscience de classe révolutionnaire répond à la deuxième partie: une contre-idéologie cohérente et publique qui s'oppose à celle du statu quo est nécessaire pour défier et modifier les pratiques politiques hégémoniques. La conscience oppositionnelle de Guha mène une révolution à ce que Bayat appelle «la révolution en tant que mouvement»: la mobilisation radicale et la solidarité des sujets politiques contre les puissances existantes. Cependant, le facteur décisif pour savoir si l'ordre politique changera ou restaurera dépend de quel (s) acteur (s) énonce de manière cohérente la source de la non-liberté des peuples, ainsi qu'un plan pour un ordre plus libre. C'est ce que Bayat considère comme étant une «révolution en tant que changement», qui selon lui n'a pas réussi à se matérialiser lors du Printemps arabe. Tout comme Bayat plaide pour une distinction conceptuelle entre le mouvement et le changement, ma thèse complète ses concepts en reproduisant cette dualité, cette fois-ci appliquée à conscience révolutionnaire. En conséquence, mon argument montre que les conceptions de la conscience de Lénine et de Guha sont distinctes et complémentaires plutôt que rivales.</dc:abstract><ual:supervisor>William Roberts (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/5712m903t.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/m900nw75z</ual:fedora3Handle><dc:subject>Political Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ac821gn26r"><dcterms:title>Understanding the surface of hemozoin and its synthetic analogue hematin anhydride</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mining and Materials</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Guerra López, Elizabeth</ual:dissertant><dc:abstract>L'hémozoïne est un biocristal largement étudié en raison de sa pertinence en tant que cible antipaludique, biomarqueur de diagnostic potentiel et modulateur immunitaire lors d'une infection paludéenne. De nombreuses études sont effectuées à l'aide de cristaux isolés à partir de modèles in vitro ou in vivo, ou en produisant un analogue synthétique appelé anhydride d'hématine. Bien que la plupart des propriétés structurelles et morphologiques de l'hémozoïne et de l'anhydride d'hématine soient bien comprises, on en sait peu sur leurs surfaces. L'étude de la surface des cristaux est essentielle afin de mieux comprendre les interactions physiques et chimiques intervenant lors de la formation de l'hémozoïne, le mécanisme d'action des antipaludéens actuels, et le rôle de l'hémozoïne en tant que modulateur immunitaire et biomarqueur pour le diagnostic. Pour cette raison, ce travail visait à développer une compréhension critique de certaines propriétés physico-chimiques de la surface de l'hémozoïne et de l'anhydride d'hématine.Dans notre première étude, nous avons étudié la surface de l'anhydride d'hématine produite par deux méthodes de synthèse différentes, l'une en milieu aqueux et l'autre en conditions anhydres. Nous avons montré que la synthèse choisie pour produire les cristaux avait un impact sur leurs propriétés de surface. Les produits issus des deux synthèses sont non-poreux et présentent des valeurs différentes de surface spécifique, composition élémentaire, et quantité d'eau adsorbée à la surface. Nous avons également confirmé la présence de groupes carboxylates à la surface des tous les cristaux; leur quantité diffère également en fonction de la méthode de synthèse utilisée.Dans l'étude suivante, nous avons recueilli l'hémozoïne à partir de cultures de Plasmodium falciparum in vitro. Nous avons éliminé les contaminants organiques de la surface des cristaux en utilisant deux méthodes différentes, l'une impliquant deux étapes de plus que l'autre. Les cristaux présentaient des propriétés de surface différentes en termes de composition et de quantité de contaminants organiques adsorbés à la surface des cristaux. Nous avons montré que la surface de l'hémozoïne peut ne jamais être exempte de contaminants, soit parce qu'ils font partie de la structure cristalline de l'hémozoïne, soit parce que les méthodes de nettoyage les ont introduits. De plus, nous avons montré que les groupes carboxylates jouent un rôle crucial dans l'interaction entre l'hémozoïne ou l'anhydride d'hématine et les biomolécules.Lors de notre dernière étude, nous avons isolé l'hémozoïne à partir de modèles de plasmodium in vitro et in vivo afin d'étudier la composition de la couche organique adhérente à la surface des cristaux. Nous avons montré que la composition élémentaire de la couche organique ne varie pas entre les sources de cristaux. Nous avons reporté pour la première fois la présence d'espèces inorganiques de silicium, calcium et phosphore associées à l'hémozoïne. En évaluant la spéciation de surface et l'origine possible de ces espèces inorganiques, nous avons montré que le silicium pourrait être lié à un mécanisme inconnu exploité par le parasite pour sa défense et sa survie, tandis que le calcium et le phosphore pourraient être liés à des protéines dépendantes du Ca2+ et à des protéines phosphorylées, respectivement. Enfin, nous avons proposé que les éléments inorganiques puissent être impliqués dans la biominéralisation de l'hémozoïne, influencer sur le mode d'action de certains antipaludéens, et jouer un rôle dans les propriétés immunomodulatrices attribuées à l'hémozoïne.</dc:abstract><dc:abstract>Hemozoin is a biocrystal widely studied due to its relevance as an antimalarial target, potential diagnosis biomarker, and immune modulator during malaria infection. Many investigations are carried out using the crystals isolated from in vitro or in vivo models, or by producing a synthetic analogue called hematin anhydride. Although most of the structural and morphological properties of both hemozoin and hematin anhydride are understood, little is known about their surface. Studying the surface of the crystals is critical for gaining insight into the physical and chemical interactions involved during hemozoin formation, the mechanism of action of current antimalarials, and its role as immune modulator and biomarker for diagnosis. For this reason, this work aimed to develop a critical understanding of some physicochemical properties of the surface of hemozoin and hematin anhydride. In our first study, we investigated the surface of hematin anhydride produced by two different synthesis methods, one in an aqueous medium and another under anhydrous conditions. We demonstrated that the synthesis selected to produce the crystals impacts their surface properties. The products obtained from both syntheses are non-porous and show different values of the specific surface area, elemental composition and amount of water adsorbed on the surface. We also confirmed the presence of carboxylate groups on the surface of all crystals; their amount also differs depending on sample preparations. In our next study, we collected hemozoin from in vitro cultures of Plasmodium falciparum. We cleaned the surface of the crystals from its organic contaminants by two purification procedures, one involving two additional steps than the other. The crystals showed differing surface properties in terms of composition and amount of organic contaminants adsorbed on the crystals surface. We showed that the surface of hemozoin may never be free of contaminants, either because they are part of the crystalline structure of hemozoin or because the cleaning methods introduced them. Additionally, we demonstrated that carboxylate groups play a crucial role in the interaction between hemozoin or hematin anhydride and biomolecules. In our final investigation, we isolated hemozoin from in vitro and in vivo models of Plasmodium to investigate the composition of the organic layer adhered onto the crystals surface. We demonstrated that the elemental composition of the organic layer does not vary among the crystals sources. We reported for the first time the presence or inorganic species of silicon, calcium and phosphorous associated with hemozoin. By assessing the surface speciation and possible origin of these inorganic species, we showed that silicon could be related to an unknown mechanism exploited by the parasite for defense and survival, while calcium and phosphorus may be related to Ca2+-dependent proteins and phosphorylated proteins, respectively. Finally, we proposed that the inorganic elements could be involved in hemozoin biomineralization, may influence the mode of action of some antimalarials and could play a role in the immunomodulatory properties attributed to hemozoin.</dc:abstract><ual:supervisor>Marta Cerruti (Supervisor1)</ual:supervisor><ual:supervisor>David Bohle (Supervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/vt150m24m.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/c821gn26r</ual:fedora3Handle><dc:subject>Mining and Materials</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Acj82k9460"><dcterms:title>The helping relationship in social work: an institutional ethnography</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Social Work</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Sinai Glazer, Hagit</ual:dissertant><dc:abstract>The helping relationship is a fundamental building block in social work practice, hence an important concept to be explored, unpacked and better understood. My doctoral research serves this purpose by illuminating the social organization of the helping relationship between social workers and clients who are mothers. Using Institutional Ethnography as my main methodology, my dissertation starts from the everyday experiences of the helping relationship between social workers and their clients who are mothers to explore and analyze how the helping relationships are socially organized to occur as they do. My research question is: How does the social organization of a social services department coordinate the everyday experiences of the helping relationship between social workers and their clients who are mothers? Understanding the mechanisms that shape the helping relationship will enhance practitioners' and policy-makers' ability to deliver services that meet the complexity and nurture the helping relationship between social workers and their core clientele of women who are mothers. My extensive field-work at a social services department in Israel included interviews with all 14 staff members and 20 clients who are mothers. I engaged in daily participant observations in the social services department and I analyzed texts pertaining to institutional and professional policies and regulations that govern the helping relationship in social work. I developed a framework to capture the essential features of the helping relationship based on the perspectives of the social workers and clients. Through textual analysis I exposed the ways in which the helping relationship is textually mediated, and how notions of professionalism held by social workers shape what should and should not be done in the helping relationship. I documented how social workers often subvert professional expectations and regulations in order to meet their clients' needs and nurture the helping relationship. I demonstrated how fear of social services among clients who are mothers and inherent conflicts among social workers are omnipresent prior to the helping relationship and predisposition the helping relationship disadvantageously. Social workers and clients who are mothers take risks and make themselves vulnerable to overcome such barriers and establish meaningful helping relationships. Altogether, my research findings illuminate how the helping relationship—seemingly a private encounter between a social worker and a client—is public and political, coordinated by professional expectations, institutional regulations, and social norms. Moreover, this social organization of the helping relationship often hinders its functioning in social work. Social workers and clients face multiple extra-local barriers in establishing meaningful helping relationships. Paradoxically, the helping relationship—the bedrock of social work practice—is organized in ways that often prohibit it from being effectively established and realized. </dc:abstract><dc:abstract>La relation d'aide est un pilier essentiel de la pratique du travail social, ce qui en fait un concept important à explorer, à décortiquer et à mieux comprendre. Mon projet de recherche de doctorat remplit cette fonction en mettant en lumière l'organisation sociale de la relation d'aide entre des travailleurs sociaux et des clientes qui sont mères. S'appuyant sur l'ethnographie institutionnelle comme méthodologie principale, ma thèse part des expériences quotidiennes de la relation d'aide entre des travailleurs sociaux et leurs clientes qui sont mères afin d'explorer et d'analyser de quelle façon les relations d'aide sont socialement organisées pour se dérouler comme elles le font. Ma question de recherche est la suivante : De quelle façon l'organisation sociale d'un ministère des Services sociaux coordonne-t-elle les expériences quotidiennes de la relation d'aide entre des travailleurs sociaux et leurs clientes qui sont mères? Comprendre les mécanismes qui définissent la relation d'aide améliorera la capacité des professionnels et des décideurs politiques à dispenser des services qui correspondront à la complexité de la relation d'aide et favoriseront cette relation d'aide entre des travailleurs sociaux et leur principale clientèle composée de femmes qui sont mères. Le travail considérable que j'ai accompli sur le terrain au sein d'un ministère des Services sociaux en Israël comprend des entrevues avec les 14 membres du personnel, ainsi qu'avec 20 clientes qui sont mères. J'ai observé quotidiennement des participants du ministère des Services sociaux et j'ai analysé des documents portant sur les politiques et les règlements institutionnels et professionnels qui régissent la relation d'aide en travail social. J'ai élaboré un cadre de travail permettant de saisir les caractéristiques essentielles de la relation d'aide en fonction du point de vue des travailleurs sociaux et de la clientèle. Par l'analyse textuelle, j'ai exposé de quelles façons la relation est textuellement modérée, et comment les notions de professionnalisme des travailleurs sociaux déterminent ce qui doit et ne doit pas être fait dans le cadre de la relation d'aide. J'ai consigné de quelle façon les travailleurs sociaux contournent souvent les attentes professionnelles et les règlements afin de répondre aux besoins de leur clientèle et de favoriser la relation d'aide. J'ai démontré comment la crainte des services sociaux parmi les clientes qui sont mères et le conflit inhérent des travailleurs sociaux sont omniprésents avant la relation d'aide et représentent une prédisposition désavantageuse à la relation d'aide. Les travailleurs sociaux et les clientes qui sont mères prennent le risque de se rendre vulnérables afin de surmonter ces obstacles et d'établir une relation d'aide significative. Dans l'ensemble, mes résultats de recherche mettent en lumière comment la relation d'aide – qui devrait être une rencontre privée entre un travailleur social et un client – est publique et politique, structurée par les attentes professionnelles, les règlements institutionnels et les normes sociales. De surcroît, cette organisation sociale de la relation d'aide compromet souvent la relation d'aide en travail social. Les travailleurs sociaux et la clientèle sont confrontés à des obstacles extérieurs à la mise en place d'une relation d'aide significative. De façon paradoxale, la relation d'aide, la pierre d'assise de la pratique du travail social, est organisée d'une manière qui empêche souvent son établissement et sa réalisation.</dc:abstract><ual:supervisor>Julia Krane (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/2v23vw24p.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/cj82k9460</ual:fedora3Handle><dc:subject>Social Work</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Arv042w15c"><dcterms:title>A data-driven design process including multiphysics for synchronous AC machines using high-performance computing</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Mohammadi, Mohammad Hossain</ual:dissertant><dc:abstract>Over the past century, electric machines have been used in many applications and sizes, ranging from washing machines and other home appliances to large pumps and fans within the industrial sector. Typical procedures for designing them have advanced from analytical formulations to the more recent finite element analysis for running physics simulations. This latter tool allowed motor designers to arrive at design solutions operating close to reality with minimal need of manufacturing hundreds if not thousands of electric machines. Despite the simulation benefits, only the electromagnetic performances are generally considered during a design process. Other physics, such as structural, acoustic and thermal, are usually ignored and only verified for a selected design. This assumption could lead to suboptimal solutions due to the tradeoffs among physical phenomena. Another issue is the increase in simulation time while incorporating multiphysics simulations in the design process. Depending on the modeling complexity, each motor simulation could take minutes or even hours to solve which can be problematic when thousands of designs are to be analyzed for different physics and operating points. Also, previous works often neglect using the simulated data to understand the underlying relationships among design performances and variables. In an optimization problem, only the final set of optimal solutions are analyzed which does not necessarily provide information on how they were achieved for re-use. To address these issues, this thesis proposes a multiphysics design process for synchronous AC machines using a data-driven approach. Each stage of the proposed process is explained using different case studies of a synchronous reluctance machine with a varying number of slots and rotor barriers. Upon setting the initial specifications, thousands of motor geometries are simulated using electromagnetics, structural, acoustics, and thermal analyses in days instead of months with the help of a high-performance computing system. A new methodology known as barrier mapping is then introduced which relates the design spaces of multiple-barrier rotors and systematically reduces their simulation time. Finally, the acquired multiphysics datasets are statistically analyzed for all their performances and variables before recommending various optimal designs for different priorities. Extracting design knowledge and guidelines can help a motor designer arrive at a more informed choice when analyzing results and selecting an optimal design. While this thesis focuses on electric machines, the presented multiphysics design process is applicable to any physical device.</dc:abstract><dc:abstract>Au cours du siècle dernier, les machines électriques ont été utilisées dans de nombreuses applications et de nombreuses tailles, allant des machines à laver et autres appareils ménagers aux grandes pompes et ventilateurs du secteur industriel. Les procédures pour les concevoir sont évoluées des formulations analytiques à la plus récente analyse par éléments finis afin d'exécuter des simulations physiques. Ce dernier outil a permis aux concepteurs des moteurs de parvenir à des solutions de conception fonctionnant au plus près de la réalité avec un besoin minimal de fabrication des centaines, voire des milliers, des machines électriques.Malgré les avantages de la simulation, seules les performances électromagnétiques sont généralement prises en compte lors du processus de conception. Les autres aspects physiques, tels que structurel, acoustique et thermique, sont généralement ignorés et vérifiés uniquement pour une conception sélectionnée. Cette pratique pourrait mener à des solutions sous-optimales en raison des compromis entre les phénomènes physiques. Un autre problème est la durée de simulation augmentée lors qu'on incorpore des simulations multi-physiques dans le processus de conception. Selon la complexité de la modélisation, la résolution de chaque simulation de moteur peut prendre quelques minutes, voire plusieurs heures, ce qui pourrait poser un problème lorsque des milliers de conceptions doivent être analysées pour différents aspects physiques et points de fonctionnement. En plus, les ouvrages précédents négligent souvent l'utilisation des données simulées pour comprendre les relations fondamentales entre les variables et les indices de performance de conception. Dans un problème d'optimisation, seulement les dernières solutions optimales sont analysées, ce qui ne fournit pas nécessairement d'informations sur la façon dont elles ont été réalisées.Pour résoudre ces problèmes, cette thèse propose un processus de conception multi-physique pour les machines synchrones au courant alternatif en utilisant une approche pilotée par les données. Chaque étape du processus proposé est expliquée à l'aide de différentes études de cas d'une machine à réluctance synchrone avec un nombre variable d'emplacements et de barrières de rotor. Lors de la définition des spécifications initiales, des milliers des géométries proposées de moteur sont simulées à l'aide d'analyses électromagnétiques, structurelles, acoustiques et thermiques, en quelques jours au lieu de plusieurs mois, à l'aide d'un système informatique haute performance. Ensuite, une nouvelle méthodologie connue sous le nom de cartographie de barrière est introduite. Elle relie les domaines de conception des rotors à barrières multiples et réduit systématiquement leur temps de simulation. Finalement, les données multi-physiques acquises sont analysées statistiquement pour toutes leurs performances et variables avant de recommander divers modèles optimaux pour différentes priorités. Développer les connaissances et les directives de conception aiderait les concepteurs de moteur à faire un choix plus éclairé lors de l'analyse des résultats et de la sélection d'une conception optimale. Bien que cette thèse se concentre sur les machines électriques, le processus de conception multi-physique présenté sera applicable aux tous dispositifs physiques.</dc:abstract><ual:supervisor>David Alister Lowther (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/c247dv34z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/rv042w15c</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Abc386m64t"><dcterms:title>Incident light modulation by means of power variations and light pulsing reduces photo induced toxicity and bleaching</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Physiology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Mubaid, Firas</ual:dissertant><dc:abstract>The use of fluorescence in microscopy has introduced unprecedented specificity and contrast, paving the way for highly specialized techniques and applications such as correlation mapping or super resolution microscopy for single molecular resolution. However, this powerful tool is curtailed by light damage. Photobleaching of dye molecules causes a gradual loss of signal emission, while phototoxicity causes cell wide disturbances to biological systems and physiological functions. This study introduces novel imaging techniques and recommendations that address these two problems. Chinese Hamster Ovary (CHO-K1) cells complemented with different fluorophores were used to demonstrate the efficiency of the techniques. Phototoxicity levels were evaluated by using the migration and protrusion assays, which measure the speeds of motility and protrusion formation, respectively. The photobleaching assay was used to measure the halftime of signal intensity decay, which provides the speed of photobleaching. This study shows that using low incident light power coupled with long exposure times causes reduced levels of photobleaching and phototoxicity. It also demonstrates that this technique is more efficient at minimizing photodamage than the commercial antifade agent ProLong LiveTM. Finally, this study shows that pulsing incident light significantly reduces photobleaching. Modulations including the decrease of pulse width, the increase of rest time between pulses, and the decrease of pulse amplitude amplify this effect. This project resulted in the development of several novel imaging techniques that not only considerably reduce light-induced damage but are simple and generally applicable on any fluorescence microscope.</dc:abstract><dc:abstract>L'utilisation de la fluorescence en microscopie a introduit un niveau de spécificité en détection et un contraste jamais vue auparavant. Cela a permis le développement de techniques hautement spécialisées, tels que l'imagerie corrélative ou la microscopie à super-résolution. Cependant, l'utilisation de cet outil engendre des effets endommageant depuis les ondes utilisées pour l'activation des fluorochromes. Premièrement, le photo-blanchissement des fluorochromes cause une perte graduelle de la fluorescence d'un spécimen. Deuxièmement, la phototoxicité perturbe les systèmes biologiques et les fonctions physiologiques dans une cellule. Cette étude introduit des nouvelles techniques d'imagerie et des recommandations pour minimiser ces méfaits. Des cellules ovariennes de hamster chinois (CHO-K1) supplémentées avec des fluorochromes variés ont étés utilisé pour démontrer l'efficacité de ces techniques. Les mesures des vitesses de motilité, aussi bien que les vitesses auxquelles se forment des saillies sur les membranes cellulaires ont été utilisées pour tester le niveau de phototoxicité durant l'imagerie de cellules vivantes. Le mi-temps de la perte d'intensité de fluorescence a été la mesure du taux de photo-blanchissement dans des cellule fixés. Pour la première technique, cette étude démontre que l'utilisation de puissances de lumière faible accouplée avec des longues périodes d'exposition lumineuse réduit les niveaux de phototoxicités et de photo-blanchissements. De plus, cette technique est plus efficace que l'utilisation d'un produit commerciale, ProLong LiveTM, à cet effet. Finalement, cette étude démontre que pulser la lumière incidente réduit ses effets endommageant. Cette amélioration est amplifié lorsque la largeur des pulse est réduites, lorsque le temps entre chaque pulse est augmenté, et lorsque l'amplitude (intensité lumineuse) des pulse est réduite. Le résultat de ce projet a été le développement de plusieurs nouvelles techniques d'imagerie qui non seulement réduit les dommages de la lumière, mais qui sont à la fois simple et applicable  sur n'importe quel microscope a fluorescence.</dc:abstract><ual:supervisor>Claire Brown (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/c247dv357.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/bc386m64t</ual:fedora3Handle><dc:subject>Physiology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Abr86b555v"><dcterms:title>Same road, different tracks A comparative study of Edmund Husserl's phenomenology and Chinese Yogācāra Philosophy</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Religious Studies</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Li, Jingjing</ual:dissertant><dc:abstract>Le but de cette thèse est non seulement d'apporter deux traditions intellectuelles à la conversation, mais aussi d'explorer la solution de leur incompatibilité potentielle dans un contexte multiculturel et multilingue. Les protagonistes de cette étude sont Edmund Husserl (1859-1938) et les Yogācārins chinois, deux sont connus pour leurs recherches sur la conscience. Au lieu de leur conceptions similaires de l'intentionnalité au niveau épistémologique, il existe une incompatibilité perçue dans leurs clarifications de la nature de la réalité: pour les Yogācārins chinois, tout est vide de l'essence (svabhāva), alors que Husserl affirme l'existence d'essence et formule la phénoménologie comme une science de l'essence. De cette incompatibilité perçue, le problème de l'essence survient: si Husserl et les Yogācārins tirent des positions incompatibles sur la nature de la réalité de leurs conceptions différentes de l'essence, est-il possible de proposer que le Bouddhisme Yogācāra appartient au même parapluie que celui de la phénoménologie chez Husserl? Profitant des recherches suivantes, cette thèse révèle et résout le problème de l'essence. Cette solution est impérative, parce qu'elle assure la base d'une étude comparative de la phénoménologie et du Yogācāra. De plus, elle approfondit notre compréhension de ce qui est impliqué, quand une comparaison philosophique est faite à travers un clivage culturel et linguistique. Cette solution du problème de l'essence représente comment on pourrait résoudre un conflit entre deux traditions dans un contexte multilingue et multiculturel. Au lieu de faire une conclusion générale, ma thèse soutient que ce que Husserl veut dire par «essence» est diffèrent de ce que les Yogācārins chinois veulent dire par svabhāva. Si Husserl et les Yogācārins chinois n'ont pas de conflit sur l'essence, ce problème de l'essence se trouve finalement PAS un problème. Bien que le problème de l'essence ne soit pas un obstacle, il n'est pas non plus sans importance. L'étude du problème de l'essence présente un nombre d'implications importantes pour une étude comparative des traditions philosophiques dans un contexte multiculturel et multilingue. C'est la contribution majeure de ma thèse au terrain.La relation entre les deux est résumée par la Zen phrase, «notre route est la même mais nous voyageons sur des traces de roues différentes (同軌不同轍)» (ZS 210). Cette analogie de route-trace-destination, qui inspire le titre, fournit également le principe d'organisation de cette thèse. La première partie est centrée sur la route, sur laquelle les deux traditions ont présenté des comptes rendus similaires de l'intentionnalité. La deuxième partie analyse leurs différentes articulations d'essence, qui montrent comment leurs traces commencent à diverger. La dernière partie explore leur destination où ils prescrivent le chemin de la libération. Au cours de son voyage, Husserl s'efforce de remédier à la crise existentielle dans l'Europe moderne. Il expose donc le principe de la libération mais il ne fournit pas les mécanismes pour réaliser cette libération. Différentes de la version naissante de la sotériologie chez Husserl, les Yogācārins chinois ont mis en avant un système élaboré d'entraînement religieux, connu sous le nom de chemin des Bodhisattvas, qui guide tout le monde vers l'éveil.</dc:abstract><dc:abstract>The goal of this dissertation is not only to bring two intellectual traditions into conversation with each other, but also to explore the resolution to their potential incompatibility in a multicultural and multilingual context. The protagonists of the current study are Edmund Husserl (1859-1938) and Chinese Yogācārins, both known for their investigation of consciousness. Aside from their similar views of intentionality, there is a perceived incompatibility in their clarifications of the nature of reality: for Chinese Yogācārins, everything is empty of essence (svabhāva) whereas Husserl affirms the existence of essence and articulates phenomenology as the study of essence. From this conflicting view, there arises the problem of essence: If Husserl and Yogācārins derive incompatible standpoints regarding the nature of reality from their different views of essence, is it possible to make the claim that Yogācāra Buddhism belongs under the same umbrella as Husserl's phenomenology? Drawing on previous research, this dissertation reveals and resolves the problem of essence. This resolution is imperative insofar as it secures the foundation for a comparative study of phenomenology and Yogācāra, which further deepens our understanding of what is involved when a philosophical comparison is made across a cultural and linguistic divide. Solving the problem of essence epitomizes how we can tackle disputes between two traditions in a multilingual and multicultural context. Instead of making an overarching claim, my dissertation argues that what Husserl means by essence differs from what Chinese Yogācārins mean by svabhāva. If Husserl and Chinese Yogācārins do not have a real dispute over essence, the problem of essence is eventually NOT a problem. Although the problem of essence is not an obstacle, it is also not irrelevant. Investigating the problem of essence brings to the forefront a number of important implications for a comparative study of philosophical traditions in a multicultural and multilingual context. This is the major contribution of my dissertation to the field.The relationship between phenomenology and Yogācāra is encapsulated in the Zen capping phrase, "our road is the same but we travel in different wheel tracks (同軌不同轍)" (ZS 210). This road-track-destination analogy, which inspires this dissertation's title, further provides its organizing principle. The first part focuses on the road, on which both traditions present similar accounts of intentionality. The second part analyses their different articulations of essence, which demonstrates how their tracks start to diverge. The last part of the dissertation explores their destination, where they prescribe the path to liberation. On his journey, Husserl strives to cure the existential crisis in modern Europe. He therefore outlines the principle for liberation but does not provide the mechanics for realizing it. Different from the nascent version of soteriology in Husserl, Chinese Yogācārins put forward an elaborate system of religious training known as the Bodhisattvas' path, which guides sentient beings in realizing awakening.</dc:abstract><ual:supervisor>Garth Green (Supervisor2)</ual:supervisor><ual:supervisor>G Hori (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/bk128d19g.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/br86b555v</ual:fedora3Handle><dc:subject>Religious Studies</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9g54xk84j"><dcterms:title>An edge-based galerkin formulation for thermo-chemical non-equilibrium flows</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mechanical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Gao, Song</ual:dissertant><dc:abstract>The present work extends the capabilities of a compressible Navier-Stokes solver into a thermo-chemical non-equilibrium hypersonic flow solver. Finite-rate chemistry and two-temperature thermal non-equilibrium solvers are implemented to account for the additional non-equilibrium processes. The spatial discretization uses an edge-based Finite Element formulation with flow stabilization achieved using a Roe scheme. The governing equations are solved numerically on both structured and unstructured grids. The steady-state solution is obtained by using an implicit integration in time. The present code is comprised of flow, chemistry, and thermal non-equilibrium solvers, developed primarily by Dario Isola, Jory Seguin, and the author, respectively. A loosely-coupled strategy is used in which each of the systems is solved separately via a generalized minimal residual (GMRES) method with an incomplete LU factorization (ILU) preconditioner provided by the PETSc library. Numerical experiments consisting of flows past blunt cones, cylinders, and spheres are performed to assess the accuracy and efficiency of the approach, and good agreement is found with solutions available in the literature. It is observed that mesh distributions are crucial for simulations on unstructured meshes, and anisotropic mesh optimization is successfully applied. A Jacobian-free Newton–Krylov (JFNK) solver with a lower-upper symmetric Gauss-Seidel (LU-SGS) preconditioner is developed for thermal equilibrium flows with frozen chemistry. The traditional LU-SGS formulation is enriched by including the contributions from viscous fluxes and boundary conditions. The performance of the JFNK solver is subsequently assessed and the enriched LU-SGS is found to be more robust and efficient than the Jacobi preconditioner and the original LU-SGS. Comparisons between JFNK with LU-SGS and GMRES with ILU are carried out and the results show that the present method, despite requiring more computation time, significantly reduces the memory footprint by half. </dc:abstract><dc:abstract>Le présent travail étend les capacités d'un solveur Navier-Stokes compressible à un solveur hypersonique thermochimique hors-équilibre. Un solveur chimique à taux fini et un solveur thermique hors-équilibre à deux températures sont mis en œuvre pour tenir compte des processus supplémentaires hors-équilibre. La discrétisation spatiale est réalisée par la méthode des éléments finis avec un flux de stabilisation obtenu à l'aide du schéma de Roe. Les équations sont résolues numériquement sur des maillages structurés et non structurés. La solution à l'état d'équilibre est obtenue par une intégration temporelle implicite. Le présent logiciel comprend un solveur Navier-Stokes, un solveur chimique et un solveur thermique hors-équilibre, développés principalement et respectivement par Dario Isola, Jory Seguin et l'auteur. Une stratégie faiblement couplée est utilisée dans laquelle chacun des systèmes est résolu séparément via la généralisation de la méthode de minimisation du résidu (GMRES) avec un préconditionneur par factorisation incomplète (ILU) fourni par la bibliothèque PETSc. Une série d'expériences numériques sont effectuées pour évaluer la précision et l'efficacité de la méthode, y compris des écoulements autour d'un cône émoussé, d'un cylindre et d'une sphère. Une bonne concordance avec les solutions disponibles dans la littérature est obtenue. On constate que la distribution de mailles est cruciale pour les simulations sur des maillages non structurés et l'optimisation anisotrope du maillage est appliquée avec succès. Un solveur du type Newton – Krylov sans Jacobien (JFNK) avec préconditionneur Gauss-Seidel (LU-SGS) est développé pour les écoulements à l'équilibre thermique sous une condition chimique fixée. La formulation LU-SGS traditionnelle est enrichie par les contributions des flux visqueux et des conditions aux limites. Les performances du solveur JFNK sont ensuite évaluées et la formulation LU-SGS modifiée se révèle plus robuste et efficace que le préconditionneur de Jacobi et la LU-SGS traditionnelle. Des comparaisons entre le JFNK avec la LU-SGS et le GMRES avec l'ILU sont effectuées et les résultats montrent que malgré une légère augmentation de temps de calcul, la méthode actuelle réduit considérablement la mémoire utilisée avec une baisse allant jusqu'à la moitié.</dc:abstract><ual:supervisor>Wagdi George Habashi (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/hh63sz14p.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9g54xk84j</ual:fedora3Handle><dc:subject>Mechanical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Anc580p81c"><dcterms:title>Redundancy for cost, performance, and lifetime improvement in application specific SIMT processors</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Mozafari, Seyyed Hasan</ual:dissertant><dc:abstract>La redondance est maintenant systématiquement attribuée dans les circuits, les structures micro-architecturales ou au niveau du système, afin d àtténuer l'augmentation des pertes de rendement de fabrication et la réduction des coûts de fabrication des circuits intégrés. En outre, la structure des systèmes  à instructions multiples  à threads unique (SIMT) les rend particulièrement adapté pour appliquer une redondance  à plusieurs niveaux de granularité:  à grain grossier (noyau) et  à grain fin (piste). Pour remédier aux pertes de rendement dans les processeurs SIMT, cette thèse présente une économie de froid, de chaud et de froid pour les noyaux chauds et partagés. Le froid épargne est une sorte de redondance qui ne traite que des coûts, alors que le chaud épargne est disponible pour augmenter le rendement (et réduire les coûts) lorsque les composants sont défectueux; sinon, il peut être utilisé pour améliorer performance sur le terrain. L'ajout de redondance  à froid, en particulier  à grain fin, aurait des coûts de synchronisation sur le chemin critique du système. En outre, le remplacement  à chaud peut améliorer les performances d'un système SIMT pour certaines applications. Par conséquent, cette thése caractérise la performance et le coût ensemble pour les systèmes SIMT avec redondance.  à cet égard, il introduit une métrique qui rend compte des coûts et des performances ensemble, performances attendues par coût (E[P]/C). Ensuite, il montre que, pour une étude de cas, l'économie de stockage  à chaud améliore considérablement E[P]/C, alors quélle montre que l'épargne de froid a un surcoût de performance négligeable dans les systèmes SIMT. Ensuite, il introduit deux techniques déstimation E[P], estimated Em[P] et estimated Es[P].Hot-Sparing, complique l'évaluation du système: la présence de défauts affecte les ressources disponibles sur le terrainUne évaluation précise des performances nécessite donc la simulation de la population entière des dés résultants afin de déterminer la performance attendue E[P] du système. Bien que tout simplement coûteux pour une évaluation de système unique, il est insoluble pour léxploration déspace de conception. Par conséquent, il déploie  estimated Em[P] pour réduire la complexité du calcul E[P]. Estimated Em[P] réduit le temps de simulation de 93%. Cela reste coûteux en termes de calcul pour l'éxploration de léspace de conception lorsque des simulations individuelles et détaillées nécessitent des heures. Par conséquent, il introduit estimated Es[P]. Estimated Es[P] réduit la simulation de 98% sans qu'une erreur de plus de 2.6% dans E[P] soit suffisante pour l'éxploration de léspace de conception. Par conséquent, les concepteurs peuvent ajouter de la redondance et évaluer les performances et les coûts du système, sans autre effort de conception que la seule évaluation des performances. Enfin, cette thèse étudie les implications en termes de performances par watt (PPW) et de performances  à la puce sur la durée de vie (LCP) dans les systèmes SIMT avec hot-sparing. Le remplacement  à chaud améliore les performances du système pour certaines applications. Cependant, cela augmente la consommation d'énergie et, par conséquent, la gestion  à chaud peut augmenter la température d'un système, ce qui peut réduire la durée de vie du système et la durée de vie du système. Cela montre que le hot-sparing est efficace pour certains types de configurations de processeurs SIMT (petites et moyennes) en termes de PPW. Sur ces configurations, la sauvegarde  à chaud peut améliorer PPW de plus de 16% en moyenne pour certaines applications. </dc:abstract><dc:abstract>Redundancy is now routinely allocated in circuits, micro-architectural structures, or at the system level, to mitigate mounting manufacturing yield losses and reducing the cost of fabricating ICs. Also, the structure of single-instruction multiple-thread (SIMT) systems makes them particularly suitable for applying redundancy at multiple levels of granularity: coarse-grained (core) and fine-grained (lane).To address yield loss in SIMT processors, this thesis presents cold and hot core-, lane-, and shared-lane-sparing. Cold sparing is a kind of redundancy that addresses cost only, while hot-sparing is available to increase yield (and reduce costs) when the components are defective; otherwise, it can be used to improve performance in the field. Adding cold redundancy, specifically fine-grained one, would have timing overheads to the system's critical path. Also, hot sparing can improve the performance of a SIMT system for some applications. Therefore, this thesis characterizes performance and cost together for SIMT systems with redundancy. In this regard, it introduces a metric that captures cost and performance together, expected performance per cost (E[P]/C). Then, it shows that for a case study system hot-sparing improves E[P]/C significantly, while it shows that cold sparing has a negligible performance overhead in SIMT systems. Next, it introduces two E[P] estimation techniques, estimated Em[P] and estimated Es[P]. Hot-sparing complicates system evaluation: the presence of defects affects what resources are available in the field. Accurate performance evaluation thus requires the simulation of the entire population of resulting dice in order to determine the expected performance E[P] of the system. While simply expensive for single system evaluation, it is intractable for design space exploration. Therefore, it deploys estimated Em[P] to reduce the E[P] calculation complexity. Estimated Em[P] reduces simulation time by 93%. This remains computationally expensive for design space exploration when individual, detailed, simulations require hours. Therefore, it introduces estimated Es[P]. Estimated Es[P] reduces simulation by 98% with no more than 2.6% error in E[P] sufficient for design space exploration. Consequently, designers may add redundancy, and evaluate system performance and cost, with no greater design effort than performance evaluation alone. Lastly, this thesis investigates performance-per-watt (PPW) and lifetime-chip-performance (LCP) implications in SIMT systems with hot-sparing. Hot-sparing improves system performance for some applications. However, it adds power consumption, and consequently, hot-sparing may increase the temperature of a system, which can lead to lower lifetime and LCP. It shows that hot-sparing is effective for specific types of SIMT processor configurations (small and medium sized) in terms of PPW. On these configurations, hot-sparing can improve PPW more than 16%, on average, for some applications. It shows that on the contrary embedding hot-sparing into SIMT processors not only does not damage LCP, rather it increases LCP outstandingly for some specific types of SIMT processor configurations (small and medium systems) and applications (FFT and FILTER), while hot-sparing improves cost and LCP over other configurations and applications as well. For example, hot-sparing can improve LCP more than 75% compared with conventional methods (i.e., cold sparing), on average, for FFT and FILTER applications.</dc:abstract><ual:supervisor>Brett Meyer (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/44558g368.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/nc580p81c</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A2f75r995k"><dcterms:title>Self-interacting dark matter</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Physics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Choquette, Jeremie</ual:dissertant><dc:abstract>Self-interacting dark matter models have grown in influence over the last decade as an alternative to cold dark matter, resolving key problems in the distribution of dark matter structure on sub-galactic scales. In this manuscript-based thesis, we present four papers related to dark matter astrophysics, with an emphasis on self-interacting dark matter. In the first, we consider a dark sector in which a hidden SU(2) gauge symmetry, which breaks to U(1) via a Higgs-like doublet, results in a nonabelian model of atomic dark matter. The model has an interesting phenomenology and can result in the correct relic density. In the second and third papers, we explore a potential signal from annihilating dark matter in the Galactic Center. It has been claimed that the Galactic Center excess is at odds with observations of dwarf spheroidal galaxies that show no signal, however we present two scenarios in which the signal from dwarf galaxies would be diminished by a sufficient margin to alleviate the tension. In the first, the dark matter annihilates with a velocity dependent cross section, decreasing the annihilation rate in dwarf galaxies. In the second, we consider the fact that dwarf spheroidal galaxies likely have density profiles which have a central core of near-constant density (rather than a centrally peaked or 'cuspy' profile), and show that density profiles consistent with self-interacting dark matter as well as observed density profiles would weaken the signal to undetectable levels. In the fourth paper, we explore the formation of high-redshift supermassive black holes from two-component dark matter models, determining the self-interaction cross section and self-interacting dark matter fraction required to provide an alternative explanation to the observed early formation of supermassive black holes.</dc:abstract><dc:abstract>Les modèles de matière noire auto-interagissante ont pris une importance croissante au cours de la dernière décennie en tant qu'alternative à la matière noire froide, résolvant ainsi les principaux problèmes de répartition de la structure de la matière noire à des échelles sous-galactiques. Dans cette thèse par arcticles, nous présentons quatre articles sur l'astrophysique de la matière noire, en mettant l'accent sur la matière noire auto-interagissante. Dans la première, nous considérons un secteur noire dans lequel une symétrie de jauge SU(2) cachée, qui se casse à U(1) par un doublet de type Higgs, donne un modèle non abélien de matière noire atomique. Le modèle a une phénoménologie intéressante et peut aboutir à une densité relique correcte. Dans les deuxième et troisième articles, nous explorons un signal potentiel provenant de la destruction de la matière noire au centre galactique. On a prétendu que l'excès au centre galactique était en contradiction avec l'observation de galaxies naines sphéroïdales qui ne montrent aucun signal. Cependant, nous présentons deux scénarios dans lesquels le signal des galaxies naines serait diminué d'une marge suffisante pour atténuer la tension. Dans le premier cas, la matière noire s'anéantit avec une section efficace dépendante de la vitesse, ce qui diminue le taux d'annihilation dans les galaxies naines. Dans le deuxième, nous considérons le fait que les galaxies naines sphéroïdales ont probablement des profils de densité ayant une région centrale de densité presque constante (plutôt qu'un profil froncé), et nous montrons que ces profils de densité, qui sont cohérents avec la matière noire auto-interagissente et les observations, affaiblissent le signal à des niveaux indétectables. Dans le quatrième article, nous explorons la formation de trous noirs supermassifs fortement décalés vers le rouge à partir de modèles de matiére noire à deux composantes, en déterminant la section efficace auto-interagissante et la fraction de matière noire auto-interagissante requises pour fournir une explication alternative à la formation précoce observée de trous noirs supermassif.</dc:abstract><ual:supervisor>James M Cline (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/n296x138b.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/2f75r995k</ual:fedora3Handle><dc:subject>Physics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A08612q841"><dcterms:title>Computational exploration of rare-earth pyrochlore oxides for thermal barrier coating (TBC)</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mining and Materials</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Lan, Guoqiang</ual:dissertant><dc:abstract>Thermal barrier coatings (TBCs) are materials applied on the surfaces of superalloys of gas-turbine engines to insulating the metallic components from the hot gas stream. TBCs can sustain an appreciable thermal gradient to enable operation of gas-turbine engines at gas temperatures significantly higher than the melting temperature of the superalloy, and thereby improves engine efficiency. The current material of choice for the topcoat of TBC is prevailingly 7 wt.% yttria stabilized zirconia (7YSZ) because it possesses a set of properties that are desirable for the operation conditions of gas-turbine engines. However, the application of 7YSZ is bottlenecked by its thermal conductivity (i.e., ~2 Wm-1K-1) and new compounds with lower thermal conductivities must be sought to further enhance the engine performance. Rare-earth (RE) pyrochlore zirconia has been demonstrated to exhibit lower inherent thermal conductivities than 7YSZ, thus considered as potential new line of materials for the topcoat. However, current research efforts on RE pyrochlore zirconia are predominantly relied on trial-and-error based experimentation, providing little information on the underlying structure-property relationships and thus no reliable guidance for further exploration of the phase space of RE pyrochlore oxides. With the exponential growth in computing power and increasing capabilities in simulation techniques, computational modeling has fostered a paradigm shift in materials development. It can avoid the undesirable trial-and-error, enable systematic investigation, and accelerate quick screening of material systems. In this regard, we develop a computational route to enable accurate predictions of the key thermophysical properties (e.g., thermal conductivity, coefficient of thermal expansion) of the RE pyrochlore oxides on the bases of first-principles calculations, to provide predictive tools to identify structure-property relationships from the fundamental level, and to accelerate and rationalize the  discovery of promising TBC topcoat materials. The thesis is manuscript-based containing four inherently connected articles (Chapters 4-7) already published in referred journals or ready to be published, covering illustration of the structural origin of the low thermal conductivity of the pyrochlore oxides (Chapter 4), methods to predict the coefficients of thermal expansion for the pyrochlore oxides (Chapter 5), strategy to improve the predictions for the pyrochlore oxides containing transition-metal elements (Chapter 6), and complete computational route to evaluate the doping effect on thermal conductivities of complex ceramic oxides (Chapter 7). These studies offer insights of physics into seeking potential materials with extremely low thermal conductivity, and more generally, new knowledge and modeling tools for research in thermal conductivity and coefficient of thermal expansion of complex oxides.</dc:abstract><dc:abstract>Les revêtements de barrière thermique (TBC) sont des matériaux appliqués sur les surfaces de superalliages de moteurs à turbine à gaz pour isoler les composants métalliques du flux de gaz chaud. Les TBC peuvent supporter un gradient thermique appréciable pour permettre le fonctionnement de moteurs à turbine à gaz à des températures de gaz nettement supérieures à la température de fusion du superalliage, ce qui améliore le rendement du moteur. Le matériau actuel de choix pour la couche de finition du TBC est principalement de la zircone stabilisée à 7 wt.% en poids d'yttria (7YSZ) car il possède un ensemble de propriétés souhaitables pour les conditions de fonctionnement des moteurs à turbine à gaz. Cependant, l'application de 7YSZ est gênée par sa conductivité thermique (~2 Wm-1K-1) et de nouveaux composants doivent être recherchés pour améliorer les performances du moteur.Il a été démontré que la zircone pyrochlore des terres rares (RE) exposé contient des conductivités thermiques inhérentes inférieures à 7YSZ, et était donc considérée comme une nouvelle gamme potentielle de matériaux pour la couche de finition. Cependant, les efforts de recherche actuels sur le zircone RE pyrochlore reposent principalement sur des expérimentations basées sur des essais et des erreurs, fournissant peu d'informations sur les relations structure-propriété sous-jacentes et donc aucune indication fiable pour une exploration plus approfondie de l'espace de phase des oxydes de RE pyrochlore. Avec la croissance exponentielle de la puissance de calcul et les capacités croissantes des techniques de simulation, la modélisation informatique a  changé  le paradigme dans le développement des matériaux. elle permet d'éviter les tâtonnements indésirables, permettre une enquête systématique et accélérer le criblage rapide des systèmes de matériaux. À cet égard, nous développons une voie de calcul permettant de prédire avec précision les propriétés thermophysiques essentielles (par exemple, conductivité thermique, coefficient de dilatation thermique) des oxydes de pyrochlore RE sur la base de calculs de principes fondamentaux, afin de fournir des outils de prévision permettant d'identifier la structure fondamentale relations de propriété à partir du niveau fondamental, et d'accélérer et de rationaliser la découverte de matériaux de couche de finition prometteurs TBC.La thèse est contient quatre articles intrinsèquement liés (Chapitres 4 à 7) déjà publiés dans des revues référencées ou prêts à être publiés, qui illustrent l'origine structurelle de la faible conductivité thermique des oxydes de pyrochlore (Chapitre 4), des méthodes permettant de: prédire les coefficients de dilatation thermique pour les oxydes de pyrochlore (Chapitre 5), stratégie pour améliorer les prévisions pour les oxydes de pyrochlore contenant des éléments de métaux de transition (Chapitre 6), et voie de calcul complète pour évaluer l'effet de dopage sur la conductivité thermique d'oxydes céramiques complexes (Chapitre 7). Ces études offrent un aperçu de la physique dans la recherche de matériaux potentiels à très basse conductivité thermique et, plus généralement, de nouvelles connaissances et de nouveaux outils de modélisation pour la recherche sur la conductivité thermique et le coefficient de dilatation thermique des oxydes complexes.</dc:abstract><ual:supervisor>Jun Song (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/6m311r599.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/08612q841</ual:fedora3Handle><dc:subject>Mining and Materials</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Avq27zq80k"><dcterms:title>The cholesteryl ester transfer protein in Alzheimer's disease</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Integrated Program in Neuroscience</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Oestereich, Felix</ual:dissertant><dc:abstract>Alzheimer's disease is a devastating neurodegenerative disease, affecting millions of families every year. Decades of research has widened our understanding of the biochemistry, the pathology, as well as genetic and lifestyle risks factors underlying the disease. Nonetheless, after over 30 years of biochemical research, this knowledge has not transformed into successful therapeutic strategies. Virtually, all drug candidates whether an antibody or small molecule have failed to show clinical efficacy. A potential explanation for these failures could be that a significant proportion of fundamental Alzheimer's disease research has focussed on animal models harboring mutations found only in very rare and aggressive cases of familial Alzheimer's disease. However, most risk factors for sporadic Alzheimer's disease have been disregarded. The logical next step to improve current disease models is to include cardiovascular risk factors as they include some of the strongest genetic and epidemiological risk factors identified thus far. The most striking difference between the human and murine lipid and cholesterol metabolism is the lack of the cholesteryl ester transfer protein (CETP). As a result, mice have negligible levels of low-density lipoproteins, a strong risk factor for both Alzheimer's disease and cardiovascular disease. CETP is a lipid transfer protein responsible for the exchange of cholesteryl esters and triglycerides between lipoproteins and based on its ability to increase the levels of the low-level lipoprotein, is considered pro-atherogenic. In return, decreased CETP activity is associated with longevity, cardiovascular health, and reduced incidence of Alzheimer's disease. With the goal to generate an Alzheimer's disease mouse model with a humanized lipoprotein profile, we studied mice transgenic for human CETP. While there is abundant literature available investigating CETP and its role in the periphery, its role in the central nervous system is poorly defined. Our analysis revealed that CETP transgenic mice had up to 31% higher cholesterol levels in the brain. Further, a microarray using astrocyte-derived mRNA showed that this cholesterol increase is unlikely the result of increased astrocytic de novo synthesis. Additionally, leakiness of the blood-brain barrier was not responsible either. More research effort is required to identify the source of this remarkable cholesterol increase. Nonetheless, we were able to describe downstream effects of the cholesterol increase. Strikingly, both presenilin 1 and presenilin 2 were among the top targets. Presenilins are the catalytic subunits of the γ-secretase complex, a key component in the generation of amyloid-β in the brain. Our data suggest that CETP transgenic mice phenocopy human plasma-cholesterol levels and cholesterol exposure of the brain as a valuable research tool to investigate the impact of the cholesterol metabolism on brain functions in relation to Alzheimer's disease. To further study the effect of CETP on Alzheimer's disease, we generated a novel mouse model combining a humanized cholesterol metabolism with Alzheimer's disease pathology. Our analysis revealed that CETP activity increases soluble and insoluble levels of amyloid-β in a presenilin-dependent manner. The CETP-induced increase of amyloid production was suppressed in mice expressing familial presenilin mutations, underlining problems with most current animal models for Alzheimer's disease. </dc:abstract><dc:abstract>La maladie d'Alzheimer est une maladie neurodégénérative et dévastatrice. Des décennies de recherche ont élargi notre compréhension de la biochimie, de la pathologie, ainsi que des facteurs de risque, soit génétiques soit liés au mode de vie sous-jacents de la maladie. Pourtant, après plus de 30 ans de recherche, ces connaissances ne se sont pas transformées en stratégies thérapeutiques efficaces. Pratiquement, tous les médicaments candidats, qu'il s'agisse d'un anticorps ou d'une petite molécule, n'ont pas démontré d'efficacité clinique. Une explication possible de ces échecs pourrait être qu'une grande partie des recherches fondamentales sur la maladie d'Alzheimer est concentrée sur des modèles animaux portant des mutations découvertes dans des cas très rares et très agressifs de la maladie d'Alzheimer familiale. Pourtant, la plupart des facteurs de risque de la maladie d'Alzheimer sporadique ont été ignorés. Un prochain pas logique pour améliorer les modèles de maladie consiste à inclure les facteurs de risque cardiovasculaires, qui demeurent les facteurs de risque génétiques et épidémiologiques les plus puissants identifiés jusqu'à présent. La différence la plus frappante entre le métabolisme humain et murin des lipides et du cholestérol est l'absence de 'cholesterol ester transfer protein' (CETP). En conséquence, les souris ont des niveaux négligeables de 'low density lipoprotein' (LDL), un facteur de risque important pour la maladie d'Alzheimer et les maladies cardiovasculaires. CETP est une protéine de transfert lipidique responsable pour l'échange d'esters de cholestérol et de triglycérides entre lipoprotéines. Elle est considérée pro-athérogène en raison de sa capacité à augmenter les taux de LDL. En contrepartie, une diminution de l'activité du CETP a été associée à la longévité, à la santé cardiovasculaire et à une incidence réduite de la maladie d'Alzheimer. Dans le but de générer un modèle de souris atteints de la maladie d'Alzheimer avec un profil de lipoprotéines humanisé, nous avons étudié des souris transgéniques pour la CETP humaine. Bien que la littérature disponible sur le CETP et son rôle à la périphérie soit abondante, son rôle dans le système nerveux central est mal défini.Notre analyse a révélé que les souris transgéniques CETP avaient des niveaux de cholestérol dans le cerveau jusqu'à 30% plus élevés. Fait intéressant, une microarray utilisant des ARNm dérivés d'astrocytes a montré que cette augmentation de cholestérol est peu probable du fait d'une augmentation de la synthèse de novo astrocytaire. De plus, la fuite de la barrière hémato-encéphalique n'était pas non plus responsable. Des efforts de recherche supplémentaires sont nécessaires pour identifier la source de cette augmentation spectaculaire du taux de cholestérol. Néanmoins, nous avons pu identifier les effets en aval de l'augmentation du cholestérol. De manière frappante, les présénilines figuraient parmi les principales cibles en aval. Les présénilines sont les sous-unités catalytiques du complexe γ-sécrétase, un composant clé de la génération de β-amyloïde dans le cerveau. Nos données suggèrent que les souris transgéniques CETP phénocopient les taux plasmatiques de cholestérol humain et l'exposition cérébrale au cholestérol comme un outil de recherche précieux pour étudier l'impact du métabolisme du cholestérol sur les fonctions cérébrales et la maladie d'Alzheimer. Pour étudier plus l'effet du CETP sur la maladie d'Alzheimer, nous avons créé un nouveau modèle murin combinant un métabolisme du cholestérol humanisé à une pathologie de la maladie d'Alzheimer. Notre analyse a révélé que l'activité de la CETP augmente les taux d'amyloïde-β de manière dépendante de la préséniline. L'augmentation de la production d'amyloïde induite par la CETP a été supprimée chez les souris exprimant des mutations familiales de la préséniline, soulignant les problèmes rencontrés avec la plupart des modèles animaux actuels de la maladie d'Alzheimer.</dc:abstract><ual:supervisor>Lisa Munter (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/v118rg633.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/vq27zq80k</ual:fedora3Handle><dc:subject>Neuroscience</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Agx41mm142"><dcterms:title>Building solid-state approaches to metal-organic materials: a contribution to green, efficient synthesis</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Chemistry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Li, Shaodi</ual:dissertant><dc:abstract>Les solides hybrides poreux (SHP), autrement appelés « MOFs » ou « metal organic frameworks » en anglais, et les minéraux organiques sont deux genres de matériaux qui ont récemment connu une forte croissance d'intérêt. Ils offrent des opportunités intéressantes pour du design synthétique soigné et sont appréciés pour leur potentiel commercial et industriel. Cependant, leur préparation est typiquement réalisée à l'aide de processus axés sur l'utilisation de solvant, ce qui les rend coûteux, dangereux, et nocif pour l'environnement. Suivant les principes de base de la Chimie verte, il est dans l'intérêt environnemental et commercial d'enquêter sur la possibilité de préparer une large gamme de matériaux synthétiques d'une manière qui réduit de façon substantielle l'utilisation des solvants sans toutefois compromettre l'intégrité de ces produits.Dans cette thèse, une sélection de méthodes synthétiques à l'état solide est présentée en réponse au besoin d'alternatives aux techniques à solution. Parmi celles-ci, on trouve les processus mécanochimiques qui utilisent la force physique pour promouvoir les réactions, les techniques de vieillissement accéléré qui imitent des processus minéralogiques naturels, et également des méthodes plus traditionnelles, dont le chauffage à four. Une véritable pléthore de techniques synthétiques est désormais disponible pour les chimistes en quête d'une manière plus sûre et moins polluante de préparer non seulement des matériaux déjà bien connus, mais également des structures tout à fait neuves.Le premier chapitre présente de nombreux sujets importants relatifs aux recherches qui suivent, dont l'histoire de la mécanochimie, les minéraux organiques synthétiques et naturels, la chimie verte, et les méthodes de synthèse de matériaux. Le deuxième chapitre discute de la viabilité de la mécanochimie et du chauffage afin de préparer les matériaux de type SIFSIX, une classe de solide hybride poreux distinguée à la fois par son potentiel comme filtre de gaz, et par sa stabilité marginale à l'humidité.Le troisième chapitre décrit comment utiliser la mécanochimie pour la préparation d'une gamme de minéraux organiques synthétiques et leurs analogues artificiels. De plus, le vieillissement accéléré comme méthode de synthèse pour ce type de matériaux est discuté.</dc:abstract><dc:abstract>Metal-organic frameworks (MOFs) and organic minerals are two categories of materials that have recently experienced a surge in interest. They offer interesting opportunities for judicious synthetic design and have gained increased attention for their potential commercial and industrial applications. However, their preparation has traditionally been accomplished by means of solution-based methods, which are often expensive, hazardous, and deleterious for the environment. Following the basic Principles of Green Chemistry, it is of environmental and commercial interest to investigate the possibility of preparing a wide range of synthetic materials in a manner that substantially reduces solvent use without compromising the products themselves.In this work, a selection of solid-state synthetic methodologies is discussed in response to the need for an alternative to solution-based techniques. From mechanochemical processes that take advantage of physical force to promote reactions, to accelerated aging techniques that largely mimic natural mineral weathering processes, to more traditionally practiced techniques such as heating, a plethora of synthetic methods have become available to materials chemists in search of safer, less polluting ways to prepare not only extant materials, but entirely novel species as well.Chapter 1 introduces several topics of importance regarding the work as a whole. Background information relating to the history and research of metal-organic frameworks, synthetic and natural organic minerals, green chemistry, and synthetic methodologies is presented.Chapter 2 discusses the viability of mechanochemical ball milling and heating for the preparation of SIFSIX materials, a class of metal-organic framework that is distinguished simultaneously by its potential for gas separations and its intractability with regards to moisture.Chapter 3 proceeds to extend the use of mechanochemistry to the preparation of a selection of synthetic organic minerals and their artificial analogues. In addition, accelerated aging processes are discussed as they relate to the preparation of these organic minerals.</dc:abstract><ual:supervisor>Tomislav Friscic (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/st74cs745.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/gx41mm142</ual:fedora3Handle><dc:subject>Chemistry</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aq524jr06s"><dcterms:title>A decision procedure for continuous quantitative equational logic</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Mathematics and Statistics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Brown, Shael</ual:dissertant><dc:abstract>In [13], a sound and complete proof system, QEL, was developed for quantitative algebras, extending Birkhoff's equational logic. The axiomatizations of certain theories have applications to probability theory [4, 5, 3, 6, 15, 13, 19], and the construction of universal algebras was shown to exhibit similar prop- erties to its classical counterpart [18, 14, 11, 10]. However, we will show that there is also non-trivial proof-theoretic content. The major result of this text is proving that there exists a decision procedure for the "continuous" (i.e. non- equational) part of QEL, and that its decidability is in the complexity class NEXPTIME. Appendix A contains some results on the relationship between QEL and continuous first order logic, which was one of the original motivations to pursue this topic and suggested as a possibly fruitful endeavour in [10].</dc:abstract><dc:abstract>Les auteurs de [13] ont developpé un système de preuve correct et complète pour les algèbres quantitatives, nomme QEL, qui est une extension de la logique équationelle de Birkhoff. L'axiomatisation de certaines théories présente des ap- plications en théorie des probabilités [4, 5, 3, 6, 15, 13, 19], et la construction des algebrès universelles possède des propriétés similaires a son équivalent classique [18, 14, 11, 10]. Néanmoins, nous montrons aussi que l'on y trouve des résultats non-triviaux en théorie de la démonstration. La résultat principale que nous démontrons est l'existence d'une procedure de decision pour la variante continue (i.e. non équationnelle) de QEL, et que sa décidabilité appartient à la classe de complexité NEXPTIME. Notre Appendix A contient une collection de résultats sur les relations entre QEL et la logique continue de premier ordre ; elle a été l'une des motivations initiales pour notre travail et suggérée dans [10] comme une entreprise qui pourrait s'avérer fructeuse.</dc:abstract><ual:supervisor>Prakash Panangaden (Internal/Cosupervisor2)</ual:supervisor><ual:supervisor>Marcin Sabok (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/fq977x156.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/q524jr06s</ual:fedora3Handle><dc:subject>Mathematics and Statistics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9306t143g"><dcterms:title>Time resolved electrostatic force microscopy measurements of ionic transport</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Physics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Mascaro, Aaron</ual:dissertant><dc:abstract>La présente thèse est un recueil d'articles scientifiques publiées ou soumises par l'auteur au cours de la période 2015-2018 dans le cadre des recherches du troisième cycle menées à l'Université McGill. Chacun de ces articles étudie un aspect de la réalisation de mesures quantitatives à l'aide de la microscopie à force atomique (AFM). Le début du premier chapitre présente la motivation du travail. La suite explique le fonctionnement de base d'un accumulateur aux ions lithium et les défis rencontrés pour développer de meilleures batteries. On le chapitre conclut en explorant diverses techniques basées sur la AFM et leur utilisation pour étudier les matériaux de l'accumulateur. Les chapitres restants sont présentés dans un ordre cohérent mais pas nécessairement chronologique pour des raisons de cohésion. Le deuxième chapitre montre l'utilisation de la microscopie à force électrostatique (EFM) dans le domaine temporel pour étudier le transport ionique du lithium fer phosphate, un matériel utilisé actuellement dans certaines batteries commerciales. On sait que le transport ionique dans les solides suit une réponse exponentielle étendue dans le domaine temporel, qui est observée par les mesures directes dans le domaine temporel effectuées sur du lithium fer phosphate. Les descriptions théoriques du transport ionique dans les solides des récentes publications ont conclu que le facteur d'étirement dans la réponse exponentielle étendue contient des informations sur le mouvement collectif des ions (par ex. les interactions ion-ion). En effectuant ces mesures sur une petite région d'un échantillon qui a été directement caractérisé à l'aide d'une variété d'autres techniques, les mesures ont été associées à des calculs théoriques obtenus à l'aide de la théorie de la densité fonctionnelle. Ces mesures démontrent que les grandes barrières de saut en limite de phase constituent le facteur limitant pour le transport ionique, alors que les barrières de saut pour les ions en volume sont nettement plus faibles. Ces connaissances ont été rendues possibles par l'amélioration de la résolution temporelle de la technique du domaine temporel par la mise en oeuvre d'un système de calcul de la moyenne en temps réel. Ceci représente une découverte clé dans la compréhension du transport ionique et une démonstration de la puissance des mesures dynamiques localisées rendues possibles par le AFM. Dans le troisième chapitre, on explore les limites de résolution temporelle pour la EFM dans le domaine temporel. On propose une nouvelle technique tout en démontrant sa résolution temporelle nettement améliorée via une mesure de validation sur un échantillon métallique. Ensuite, les limites et l'application de cette technique au transport ionique sont discutées en détail. De plus, trois autres techniques récemment développées qui utilisent des impulsions de tension pour mesurer les réponses des échantillons résolus dans le temps sont explorées en détail, ainsi que leurs difficultés et leur applicabilité aux mesures de transport ionique.Le quatrième chapitre aborde les problèmes posés par la calibration de la constante de ressort des apex nanométriques situés sur la pointe du levier présent dans les AFMs. Ces mesures précises de la constante de ressort des apex sont nécessaires pour effectuer des mesures de force quantitatives à l'aide de la AFM. Ces constantes de ressort sont généralement réalisées en mesurant les petites oscillations de l'apex dues à l'énergie thermique et en les analysant dans le domaine fréquentiel. On démontre que cela peut être grandement affecté par le bruit acoustique ambiant. Ensuite, un protocole permettant de calibrer les constantes de ressort tout en éliminant l'effet du bruit acoustique est démontré en actionnant activement l'apex du levier; en particulier en effectuant des balayages de fréquence et des mesures optiques.</dc:abstract><dc:abstract>The present thesis is a collection of scientific articles published or submitted by the author during the period from 2015-2018 as part of Ph.D. research conducted at McGill University. Each of these articles explores an aspect of performing quantitative measurements using atomic force microscopy. The first chapter introduces the motivation for the work, explains the basic workings of a lithium ion battery and the challenges faced in developing better batteries, and concludes by exploring a variety of atomic force microscopy-based techniques and how they have been and will be used to study battery materials. The remaining chapters are presented in a coherent order instead of chronologically for cohesiveness. The second chapter demonstrates the use of time-domain electrostatic force microscopy to probe ionic transport on lithium iron phosphate, a material currently used in some commercial batteries. Ionic transport in solids is known to follow a stretched exponential response in the time domain, which is observed in the direct time-domain measurements performed on lithium iron phosphate. Previous theoretical descriptions of ionic trans- port in solids have concluded that the stretching factor in this stretched exponential response contains information regarding the collective motion of the ions (e.g. ion-ion interactions). By performing these measurements on a small region of a sample that was directly characterized using a variety of other techniques, the measurements were related to theoretical calculations obtained using density functional theory. These measurements demonstrate that large phase-boundary hopping barriers are the limiting factor for ionic transport, while the hopping barriers for ions in the bulk are significantly lower. These insights were made possible by the improved time resolution of the time-domain technique by implementation of a real-time averaging system. This represents a key finding in the understanding of ionic transport and a demonstration of the power of localized dynamic measurements enabled by the atomic force microscope.In the third chapter, the time resolution limits for time-domain electrostatic force microscopy are explored. A new technique is proposed and its much improved time resolution is demonstrated via a validation measurement on a metallic sample. Its limitations and application of this technique to ionic transport are then discussed in detail. Three other recently developed techniques that use voltage pulses to measure time-resolved sample responses are then explored in detail, along with their challenges and applicability to ionic transport measurements. The fourth chapter then discusses the challenges in calibrating the spring constant of atomic force microscopy cantilevers. Accurate measurements of the cantilever spring constant are required to perform quantitative force measurements using atomic force microscopy. Cantilever spring constants are typically done by measuring the small oscillations of the cantilever due to thermal energy and analyzing them in the frequency domain. We show that this can be greatly affected by ambient acoustic noise. A proto- col to calibrate the spring constants while precluding the effect of acoustic noise is then demonstrated by actively driving the cantilever; specifically by performing frequency sweeps and ringdown measurements.</dc:abstract><ual:supervisor>Peter H Grutter (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/tt44pq253.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9306t143g</ual:fedora3Handle><dc:subject>Physics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Abk128d207"><dcterms:title>Bisimplicial complexes and shortcut graphs</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mathematics and Statistics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Hoda, Nima</ual:dissertant><dc:abstract>This thesis concerns groups acting on spaces of combinatorial nonpositive curvature.  The first part of this thesis presents a discrete Morse-theoretic method for proving that a regular CW complex is homeomorphic to a sphere.  We use this method to define bisimplices, the cells of a class of regular CW complexes we call bisimplicial complexes.  The 1-skeleta of bisimplices are complete bipartite graphs making them suitable in constructing higher dimensional skeleta for bipartite graphs.  We show that the flag bisimplicial completion of a finite bipartite bi-dismantlable graph is collapsible.  We use this to show that the flag bisimplicial completion of a quadric complex is contractible and to construct a compact K(G,1) for a torsion-free quadric group G.The second part of this thesis introduces shortcut graphs and groups.  Shortcut graphs are graphs in which long enough cycles cannot embed without metric distortion.  Shortcut groups are groups which act properly and cocompactly on shortcut graphs.  These notions unify a surprisingly broad family of graphs and groups of interest in geometric group theory and metric graph theory including: systolic and quadric groups (in particular finitely presented C(6) and C(4)-T(4) small cancellation groups), cocompactly cubulated groups, hyperbolic groups, Coxeter groups and the Baumslag-Solitar group BS(1,2).  Most of these examples satisfy a strong form of the shortcut property.  We show that shortcut groups are finitely presented, have exponential isoperimetric and isodiametric function and are closed under direct products and under HNN extensions and amalgamated products over finite subgroups.  We show that groups satisfying the strong form of the shortcut property satisfy these properties and also have polynomial isoperimetric and isodiametric function.</dc:abstract><dc:abstract>Cette thèse concerne les groupes qui agissent sur des espaces combinatoires à courbure négative ou nulle.  La première partie de cette thèse présente une méthode basée sur la théorie de Morse discrète pour démontrer qu'un CW-complexe régulier est homéomorphe à une sphère.  Nous utilisons cette méthode pour définir les bisimplexes, les cellules d'une classe de CW-complexes réguliers que nous appelons les complexes bisimpliciaux.  Les 1-squelettes des bisimplexes sont des graphes bipartis complets et donc utiles dans la construction des squelettes de haute dimension des graphes bipartis.  Nous montrons que le complété de drapeau bisimplicial d'un graphe biparti fini bi-démontable est collapsible.  Nous utilisons ce résultat pour montrer que le complété de drapeau bisimplicial d'un complexe quadrique est contractile et pour construire un K(G,1) pour un groupe quadrique sans torsion G.La deuxième partie de cette thèse introduit les graphes et les groupes shortcuts.  Les graphes shortcuts sont les graphes dans lesquelles les cycles assez long ne peuvent pas plonger sans distortion métrique.  Les groupes shortcuts sont les groupes qui agissent proprement et cocompactement sur les graphes shortcuts.  Ces notions unifient une famille assez large et intéressante de graphes et de groupes notamment dans la théorie des groupes géométriques et la théorie des graphes métriques comprenant: les groupes systoliques et quadriques (en particulier les groupes à petites simplifications C(6) et C(4)-T(4) finement présenté), les groupes cocompactement cubulés, les groupes hyperboliques, les groupes Coxeter et le groupe Baumslag-Solitar BS(1,2).  La plupart de ces exemples satisfont une forme forte de la propriété shortcut.  Nous montrons également que les groupes shortcuts sont finement présentés, ont des fonctions isopérimétriques et isodiamétriques exponentielles et qu'ils sont fermés par rapport aux produits directes, aux extensions HNN et aux produits libres avec amalgame sur des sous-groupes finis.  Nous montrons que les groupes satisfaisant la forme forte de la propriété shortcut satisfont ces propriétés et ont aussi des fonctions isopérimétriques et isodiamétriques polynomiales.</dc:abstract><ual:supervisor>Daniel Wise (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/qf85nd34q.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/bk128d207</ual:fedora3Handle><dc:subject>Mathematics and Statistics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Az890rw670"><dcterms:title>The effects of a maternal high-fat diet on offspring responsiveness to leptin and afferent projections to the lateral hypothalamus</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Anatomy and Cell Biology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Kelley, Lyla</ual:dissertant><dc:abstract>L'environnement nutritionnel au cours de la periode perinatale est essentiel au développement des circuits neuronaux régulant l'homéostasie énergétique et la prise alimentaire. Cet environnement est modifié par la diète de la mère et en particulier, une alimentation maternelle riche en graisses (60% de gras) peut augmenter les taux de leptine circulante chez les petits et modifier les circuits impliqués dans la prise alimentaire. L'hypothalamus latéral (LH) et en particulier les neurones contenant de l'orexine-A (ORX-A) relie les noyaux hypothalamiques responsables de l'homéostasie aux neurones dopaminergiques mésocorticolimbiques qui modulent la récompense alimentaire. Cette région est sensible à la leptine chez les adultes et une exposition plus élevée à cette hormone chez les nouveau-nés pourrait modifier la sensibilité de ce noyau à la leptine. Notre étude examine l'hypothèse qu'un régime alimentaire maternel riche en graisses augmente la densité des projections neuronales des noyaux ventromédian (VMH) et dorsomédianl (DMH) de l'hypothalamus aux neurones LH ORX-A, ceux-là même qui modulent directement les cellules productrices de dopamine. Nous avons ensuite examiné la réponse intracellulaire des neurones de l'hypothalamus latéral a une injection de leptine (3mg/kg, bw, ip) chez les nouveau-nés issus de mères contrôles (CD) ou recevant une diète riche en gras (HFD). Nous avons également cherché à identifier le phenotype des populations de LH activées par la leptine exogène. Nous avons comparé les réponses des ratons issus de  mères  nourries avec un régime contrôle (CD) ou riche en gras (HFD) à partir du jour de gestation 13-14 jusqu'à la fin de l'allaitement. Pour évaluer les changements induits par le régime alimentaire sur la densité des afférences au LH, nous avons injecté des microbilles rétrogrades fluorescentes dans le champ LH ORX-A a 5-6 de vie postnatale. Quatre jours plus tard, nous observons une augmentation des afférences du VMH (et dans une moindre mesure du DMH) vers le LH chez les ratons HFD comparés à ceux des mères CD.  Pour évaluer la sensibilité à la leptine dans l'hypothalamus latéral, nous avons mesuré la production des seconds messagers pERK et pSTAT3 par double histochimie d'immunofluorescence dans des cellules identifiées phénotypiquement (ORX-A pour pERK, et CART et GABA pour pSTAT3). Chez les ratons HFD, nous observons une augmentation de pSTAT3 dans les neurones de LH, dont certains expriment CART. Il n'y a pas de changement dans l'activation pERK des cellules LH ORX-A. En conclusion, les changements de gras dans la diète maternelle au cours de la période périnatale ont des effets importants sur la progéniture, notamment en  augmentant à la fois la densité des afférences hypothalamiques vers l'hypothalamus latéral, mais aussi en modifiant la sensibilité de ce noyau à la leptine circulante. Cet effet de programmation pourrait influencer les mécanismes de consommation de nourriture à l'âge adulte et conduire à une dysrégulation métabolique à long terme. Tous les travaux ont été financés par la subvention n ° 298195 des IRSC au Dr. Claire-Dominique Walker.</dc:abstract><dc:abstract>The early nutritional environment is critical for the development of neural circuits regulating energy homeostasis and food intake. The lateral hypothalamus (LH) links homeostatic hypothalamic nuclei to the mesocorticolimbic dopamine neurons that modulate food reward. The  LH is sensitive to leptin in adults and higher exposure to this hormone in neonates through maternal high-fat feeding might promote neuronal LH afferent projection growth and leptin sensitivity. Here we examined whether a maternal high-fat diet (HFD) increases the density of projections from the ventromedial (VMH) and dorsomedial (DMH) hypothalamus to LH orexin-A (ORX-A) neurons, which directly modulate dopamine producing cells. We next examined whether elevated leptin exposure enhances second messenger signaling in the LH, by either increasing the activity of leptin-sensitive afferents to this nucleus, or directly activating LH neurons harboring leptin receptors. Finally, we aimed to phenotypically identify neuronal populations in the LH that are activated by leptin during the neonatal period. Mothers were fed either a control diet (CD) or HFD (60% Kcal from fat) from gestation day 13-14 throughout lactation. To evaluate diet-induced changes in the density LH afferents, fluorescent retrograde microbeads were injected stereotaxically into the LH ORX-A field on PND5-6. By PND9-10, HFD pups exhibit a greater density of projections from the DMH and VMH compared to CD pups. To assess leptin-sensitivity in the LH, the production of leptin-induced second messengers (pERK and pSTAT3) was quantified by double immunofluorescence histochemistry in phenotypically identified cells (ORX-A for pERK, and CART and GABA for pSTAT3) following vehicle or leptin treatment (3mg/kg ip). On PND15-16, HFD offspring displayed increased pSTAT3 in LH neurons, some of which expressed CART. Interestingly, there was no change in pERK activation of LH ORX-A cells. Thus, maternal HFD during the perinatal period has an important effect on the offspring, to increase both LH afferent fiber density and LH sensitivity to leptin. This programming effect might influence food intake mechanisms in adulthood and lead to metabolic dysregulation in the long term. All work was funded by CIHR grant #298195 to Dr. Claire-Dominique Walker.</dc:abstract><ual:supervisor>Claire Walker (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/6t053j371.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/z890rw670</ual:fedora3Handle><dc:subject>Anatomy and Cell Biology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A8049g7276"><dcterms:title>An approach towards organic thin film conductivity measurements under ultra-high vacuum conditions</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Physics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Spielhofer, Andreas</ual:dissertant><dc:abstract>Une chambre à ultravide a été construite qui permet de produire des couches minces de molécules pour des applications en photovoltaïque organique. Un système avec une sonde consistant de quatre micro-leviers servant de pointes de contact a été conçu et implémenté dans cet environnement ultravide pour mesurer la résistivité des couches minces lors de l'évaporation. Un système à ultravide portatif, sous forme de valise, a été spécifiquement construit pour réaliser des expériences de pompage optique dans le régime térahertz. Celle-ci permet de transporter et de garder les échantillons dans un ultravide à tout moment. Ses fenêtres en diamant spécialisé ont été conçues pour permettre une transmission optique sur une large plage de fréquence et une plate-forme avec deux axes actués par des éléments piézoélectriques a été implémentée pour permettre un fin positionnement de l'échantillon à l'intérieur de la valise. Le système mesure et enregistre la pression ce qui permet de surveiller la prèsence de gaz à l'intérieur de la valise et de contrôler précisément son niveau de pression. Des mesures préliminaires sur des films de graphène préparés par croissance épitaxiale sur une surface de 6H−SiC montrent que la valise permet de réaliser des mesures térahertz résolues dans le temps.</dc:abstract><dc:abstract>An ultra-high vacuum system was built that can produce molecular films for organic photovolatics applications. A UHV compatible cantilever based four-point probe system was built, implemented and critically analyzed in order to measure the resistivity of thin films during the evaporation. A transportable ultra-high vacuum system, specifically built for an optical pump terahertz-probe was set up that allows to transport and keep the samples under UHVconditions at all times. Special diamond windows were designed that allow ultra-broadband transmission and a home-built two axis piezo stage was implemented that allows to fine position the sample inside the suitcase. The suitcase allows to record the pressure, to monitor therest gases inside the suitcase and to controllably leak gases inside the suitcase. Preliminary measurements on epitaxially grown graphene on a 6H-SiC sample show that the suitcase can be used for time-resolved terahertz measurements.</dc:abstract><ual:supervisor>Peter H Grutter (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/p2676z007.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/8049g7276</ual:fedora3Handle><dc:subject>Physics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aww72bd93w"><dcterms:title>Can primary care and continuity of care prevent asthma- related emergency department use and hospitalizations amongst children?</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Family Medicine</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Cooper, Sarah</ual:dissertant><dc:abstract>Objective Having a primary care provider and a consistent, continuous relationship with said provider may be important for asthma outcomes. In Québec, children are mainly followed by family physicians in family medicine groups (FMGs), family physicians not part of FMGs, or by pediatricians. We sought to determine 1) whether having a usual provider of primary care was associated with asthma-related emergency department (ED) visits and hospitalization in Québec children with asthma and 2) whether continuity of care with a primary care provider was associated with acute asthma outcomes. Methods This was a population-based retrospective cohort study that used Québec provincial health administrative data from 2010-2013. The population was children diagnosed with asthma, aged 2-16 years old (N=39, 341). The main exposure was the primary care model (FMGs, non-FMGs, or pediatricians, compared to no assigned usual provider of care (UPC)). For those with an assigned UPC, continuity of care was measured by the UPC Index (high, medium, low). The main and secondary outcomes were asthma-related ED visits and hospitalizations, respectively. Multivariate logistic regression analyses were used to test associations between exposures and outcomes.ResultsOverall, 17.4% of children diagnosed with asthma in Québec had no assigned UPC. Compared to no assigned UPC, having a UPC was associated with decreased asthma-related ED visits (Pediatrician Odds Ratio (OR): 0.80, 95% confidence interval (CI) [0.73, 0.89], FMGs OR: 0.84, 95% CI [0.75,0.93], non-FMGs OR: 0.92, 95% CI [0.83, 1.02]) and hospital admissions (Pediatrician OR: 0.67, 95% CI [0.59, 0.76], FMGs OR: 0.83, 95% CI [0.73, 0.94], non-FMGs OR: 0.77, 95% CI [0.67, 0.87]). Children followed by a pediatrician were more likely to have high continuity of care. Continuity of care was not significantly associated with asthma-related ED visits but compared to low continuity, medium or high continuity was associated with decreased asthma-related hospital admissions (Medium OR: 0.81, 95% CI [0.73, 0.90], High OR: 0.72, 95% CI [0.63, 0.82])ConclusionHaving a usual provider of primary care was associated with reduced asthma-related ED visits and hospital admissions. For those who had a UPC, high continuity of care was associated with reduced likelihood of asthma-related hospital admissions. </dc:abstract><dc:abstract>Objectif Avoir un fournisseur de soins primaires et une relation stable et continue avec celui-ci peut avoir un effet important sur les complications de l'asthme. Au Québec, les enfants sont principalement suivis par des médecins de famille dans les groupes de médecine de famille (GMF), des médecins de famille ne faisant pas partie des GMF, ou par des pédiatres. Nous avons cherché à déterminer 1) si le fait d'avoir un médecin de soins primaires était associé aux visites à l'urgence et à l'hospitalisation due à l'asthme chez les enfants québécois atteints d'asthme et 2) si la continuité des soins avec un fournisseur de soins primaires était associée à des complications aiguës reliées à l'asthme. Méthodes Ce projet était une étude de cohorte rétrospective à partir de données administratives de santé de la province du Québec pour la période 2010-2013. La population était constituée d'enfants âgés de 2 à 16 ans ayant eu un diagnostic d'asthme (N = 39341). L'exposition principale était le modèle SP (GMF, non-GMF, ou pédiatre, vs. pas de soins primaires). Pour ceux avec un fournisseur de soins primaires, la continuité des soins a été mesurée par l'Indice de Fournisseur Habituel de Soins (élevé, moyen, faible). Les résultats principal et secondaire étaient respectivement les visites à l'urgence et les hospitalisations liées à l'asthme. Des analyses de régression logistique multivariées ont été utilisées pour tester les associations entre les expositions et les résultats, ajusté en fonction des facteurs de confusion potentiels.RésultatsDans l'ensemble, 17,4% des enfants asthmatiques ayant reçu un diagnostic d'asthme n'ont pas été suivis par un fournisseur de soins primaires. Comparé à l'absence de soins primaires, avoir accès aux soins primaires était associé à une diminution du nombre de visites aux urgences liées à l'asthme (OR de pédiatre: 0,80, 95% CI [0,73, 0,89], OR de GMF : 0,84, 95% CI [0,75, 0,93], OR de non-GMF : 0,92, 95% CI [0,83, 1,02] ) et du nombre d'hospitalisations (OR de pédiatre: 0,67, 95% CI [0,59, 0,76], OR de GMF: 0,83, 95% CI [0,73, 0,94], non-FMG: 0,77, 95% CI [0,67, 0,87]). Les enfants suivis par un pédiatre étaient plus susceptibles d'avoir une continuité de soins élevée. La continuité des soins n'était pas significativement associée aux visites aux urgences due à l'asthme, mais comparée à une continuité faible, une continuité moyenne ou élevée était associée à une diminution des hospitalisations due à l'asthme (OR moyen: 0,81, 95% CI [0,73, 0,90], OR élevé : 0,72, 95% CI [0,63, 0,82]).ConclusionAvoir un fournisseur de soins primaires est associé à une réduction du nombre de visites aux urgences et d'hospitalisations liées à l'asthme. Pour ceux qui ont un fournisseur de soins primaires, une continuité de soins élevée est associée à une probabilité réduite d'admissions à l'hôpital due à l'asthme.</dc:abstract><ual:supervisor>Patricia Li (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/8910jw82m.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/ww72bd93w</ual:fedora3Handle><dc:subject>Family Medicine</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Axg94hr952"><dcterms:title>Investigation of the use of CoxSn1-x-oxide as anodes for the oxygen evolution reaction in water electrolysis for hydrogen production</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Chemical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Gharbi, Rihab</ual:dissertant><dc:abstract>This Master's thesis is aimed at developing mixed metal oxide (CoxSn1-x-oxide) coatings on titanium substrates to act as anodes in an attempt to improve the efficiency and decrease the costs of the oxygen evolution reaction in an acidic medium of PEM water electrolysers.  More specifically, the objectives of this thesis is to investigate the influence of CoxSn1-x-oxide composition the resulting activity in the oxygen evolution reaction (OER). Commercially, platinum-group metals and their oxides are used due to their high electrocatalytic activity.  However, these metals are scarce and highly expensive.Through a thermal salt decomposition methodology, the CoxSn1-x-oxides of varying compositions were formed on a titanium substrate.  In order to evaluate the electrochemical behavior and quantify the electrocatalytic activity, electrochemical tests, which consisted of open circuit potential, electrical impedance spectroscopy and Tafel polarization, were performed.  Moreover, material characterization techniques were implemented to verify the formation of the mixed metal oxides as well as characterize their morphology, structure and molar ratio.  These techniques consisted of Scanning Electron Microscopy, Electron-Dispersive X-ray Spectroscopy, X-ray Diffraction Spectroscopy, and X-ray Photoelectron  Spectroscopy. Through material characterization, it was found that the CoxSn1-x-oxide coatings contained a highly crystalline structure with a homogeneous distribution of the Co and Sn on the surface.  Additionally, the coatings were found to be more porous with a structure that resembled the agglomeration of small particles.  For the coating which contained a high molar ratio of Co, the crystals formed were cubic shaped and resembled the structure of Co-oxide (Co3O4).  Through electrochemical characterization, it was found that of the compositions tested, Co0.8Sn0.2-oxide coating was the best performing electrode as it had a significantly higher overall electrocatalytic activity.  The same composition was found to be more corrosion resistive.  It was concluded that the addition of Sn to the well-known good OER electrode, Co-oxide, improved its electrocatalytic activity, with the best performing being at the composition of 80% Co and 20% Sn.  Therefore, CoxSn1-x-oxides are a promising potential candidates for alternative anodes to be used for PEM water electrolysers.  Both Co and Sn are readily available materials and therefore, their use would decrease system costs.</dc:abstract><dc:abstract>Cette thèse examine l'effet de la composition de la couche d'oxyde metallique mixte (CoxSn1-x-oxyde) sur une pièce de titane et sa performance comme anode.  Le but est d'améliorer l'efficacité de l'anode et de réduire les coûts d'électrolyse de l'eau en un milieu acide.  Les objectifs de cette these consistent à étudier l'influence de l'activité résultante de la composition de CoxSn1-x-oxyde dans la réaction de dégagement d'oxygène.  Dans l'industrie, les métaux du groupe du platine et leurs oxydes sont utilisés en raison de leur fortes activités électrocatalytiques. Néanmoins, ces métaux sont rares et coûteux.Les couches de CoxSn1-x-oxyde ont été formées sur un substrat de titane par une méthode de décomposition thermique.  Pour évaluer la performance électrochimique et quantifier l'activité électrocatalytique, des tests électrochimiques ont été réalisés.  Ces tests sont composes d´un potential de circuit ouvert, une spectroscopie d'impédance électrique et une polarisation de Tafel.  De plus, les techniques de caractérisation des matériaux ont été utilisées pour verifier la formation des oxydes métalliques mixtes et pour caractériser leur morphologie, leur structure et leur rapport molaire.  Ces techniques comprenaient de la microscopie électronique à balayage, la spectroscopie des rayons X, et la spectroscopie photoélectronique X. Grâce aux méthodes de caractérisation des matériaux, il a été constaté que les couches de CoxSn1-x-oxyde ont une structure crystalline avec une distribution homogène de cobalt et d'étain sur la surface.  De plus, la structure des couches ressemblaient à l'agglomération des petites particules.  Les couches de CoxSn1-x-oxyde qui contenaient un rapport élevé de cobalt étaient de forme cubique avec la structure de Co-oxyde (Co3O4).  Grâce aux méthodes de caractérisation électrochimique, il a été constaté que la couche avec 80% cobalt représentait l'électrode avec l'activité électrocatalytique la plus supérieure.  Par rapport à Co-oxyde, l'activité de Co0.8Sn0.2-oxyde est 16 fois mieux. Il a été conclu que l'ajout d'étain à Co-oxyde améliorait son activité électrocatalytique.  La meilleure performance était pour la couche de Co0.8Sn0.2-oxyde.  Par conséquent, les CoxSn1-x-oxydes sont des candidats potentiels comme des anodes alternatives à utiliser pour les électrolyseurs à eau en milieu acide.  Le cobalt et l'étain  sont des matériaux facilement disponibles et, donc, leur utilisation réduirait les coûts du système.</dc:abstract><ual:supervisor>Sasha Omanovic (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/br86b5564.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/xg94hr952</ual:fedora3Handle><dc:subject>Chemical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aw3763920h"><dcterms:title>Development of a biosensing strategy for multiplex and dynamic quantification of a secretory fingerprint from human pancreatic islets</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Biological and Biomedical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Castiello Flores, Francisco</ual:dissertant><dc:abstract>Le diabète sucré est une maladie chronique qui arrive quand les taux de sucre dans le sang sont élevés, connu sous le nom d'hyperglycémie, résultant d'une capacité réduite du corps à produire ou à réguler l'insuline. Sans traitement, l'hyperglycémie chronique peut causer des maladies cardiovasculaires, une neuropathie, une néphropathie, et des maladie oculaires, qui peuvent mener à la rétinopathie et la cécité. En 2017, le nombre de personnes affectées par le diabète était de 425 millions à l'échelle mondiale. Cette maladie résulte des carences des voies de sécrétion des îlots du pancréas. Des études récentes montrent que les cellules constituant ces îlots ont un système de communication complexe, dans lequel les hormones qu'elles sécrètent exercent des interactions paracrines sur leurs cellules voisines. Cependant, les conséquences de ces interactions sont encore mal comprises. Jusqu'à présent, beaucoup des recherches dans ce domaine sont concentrées sur la compréhension des procédés de sécrétion d'insuline et de glucagon, qui sont les hormones principales secrétées par les deux principaux types de cellules dans les îlots. Ainsi, le suivi d'une empreinte sécrétoire comprenant plus de deux hormones constitue une opportunité de recherche pour améliorer notre compréhension du diabète.Les biocapteurs constituent une excellente base pour le développement d'outils analytiques pouvant détecter l'empreinte sécrétoire des îlots. L'objectif principal de cette thèse était donc de développer une stratégie de détection biologique pour la détection multiplexe de l'empreinte sécrétoire composée des hormones secrétées par les trois principaux types cellulaires présents dans les îlots pancréatiques. Dans un premier temps, nous avons exploré l'utilisation d'un biocapteur capacitif pour la détection d'insuline. La performance de ce biocapteur dépendant essentiellement de la chimie de surface immobilisant le biorécepteur, une étude comparative a permis d'évaluer les effets des architectures communément décrites dans la littérature. Tandis que le développement de ce biocapteur capacitif nous a fourni une meilleure compréhension du mécanisme de détection de l'insuline et de certains de ses paramètres, la longueur du temps d'analyse  rendait son implémentation pour l'analyse de l'empreintes sécrétoire des îlots difficile. Comme alternative pour réaliser pleinement l'objectif de la thèse, nous avons donc exploré l'imagerie par résonance de plasmons de surface (SPRi).En combinant un immunodosage compétitif avec la SPRi et en optimisant la chimie de surface du biocapteur, il a été possible de détecter simultanément pour la première fois l'insuline, le glucagon et la somatostatine. Cette stratégie de biodétection présentait une limite de détection pour le glucagon et l'insuline comparable aux méthodes reportées antérieurement pour un temps d'analyse court. Néanmoins, la détection de la plus petite hormone, la somatostatine, demeurait un défi en raison des hautes limites de détection obtenues comparées aux autres hormones et en raison de l'absence de rapports offrant une référence convenable pour ses performances.Afin d'éviter cet écueil et de garantir une détection de toutes les hormones ciblées dans une gamme de concentration pertinente sur le plan biologique, nous avons mené une étude comparative de trois différentes stratégies d'amplification de signal basées sur des nanoparticules d'or (GNPs). Ces stratégies comprenaient des GNPs immobilisés sur la surface du biocapteur, des GNPs conjugués avec un anticorps primaire et des GNPs conjugués avec un anticorps secondaire pour la post-amplification d'un immunodosage compétitif. La détection multiplexée des trois hormones a alors été obtenue avec une LOD améliorée de 9 fois pour l'insuline, de 10 fois pour le glucagon et de 200 fois pour la somatostatine comparativement à la détection non-amplifiée, permettant de relever le défi susmentionné. </dc:abstract><dc:abstract>Diabetes mellitus is a chronic disorder occurring when elevated levels of blood glucose, known as hyperglycemia, result from the body's impaired ability to produce or regulate insulin. If left untreated, chronic hyperglycemia can cause cardiovascular disease, neuropathy, nephropathy and eye disease, leading to retinopathy and blindness. In 2017 the number of people with diabetes reached 425 million worldwide. This disease arises from deficiencies in the secretory pathways of the pancreatic islets, a micro-organ constituting 1-2% of the pancreas mass. Recent studies have shown that the cells comprising the islets possess an intricate communication system, in which their secreted hormones exert paracrine interactions on neighbor cells. However, little is understood about the consequences of such communications. Up-to-date most research in the field has focused on understanding the processes related with insulin and glucagon secretion, the main hormones secreted by the two major cell types present in the islets. Thus, monitoring a secretory fingerprint (SF) contemplating more than two hormones, presents a research opportunity to increase our current understanding of diabetes. Due to their simplicity, ease of use, non-invasive and label-free nature, biosensors provide an excellent basis for the development of analytical tools capable of detecting the SF of islets. Therefore, the main objective of the present thesis was to develop a biosensing strategy for the multiplex detection of a SF composed of the hormones secreted by the three major cell types contained in the pancreatic islets. At first, we explored the use of a capacitance-based biosensor for the detection of insulin. This biosensing technique was selected, since it could offer high sensitivity, potential for multiplexing and capabilities for integration with microelectronic technologies. Since the performance of this biosensor critically depends on the surface chemistry design of the bioreceptor immobilization, a systematic study was performed to evaluate the effect of common architectures reported in literature. These chemistries included the covalent immobilization of biomolecules on the electrodes, in the gaps between electrodes and a conformal coating covering both. The development of this capacitive biosensor provided valuable knowledge on the effect of various parameters for the detection of insulin, however its implementation for islet continuous SF analysis proved difficult due to its long analysis time. Thus, we explored surface plasmon resonance imaging (SPRi) as an alternative to fully reach the thesis objective. By combining a competitive immunoassay with SPRi and the optimization of the sensor's surface chemistry it was possible to detect, for the first time, insulin, glucagon and somatostatin simultaneously. This biosensing strategy presented a limit of detection (LOD) comparable to previous reports detecting insulin and glucagon secretions individually with a short analysis time. However, detecting the smallest hormone, somatostatin, remained a challenge due to the obtained high LOD compared to insulin and glucagon and a lack of reports providing a desirable reference for its performance. Thus, to address this pitfall and ensure the detection of all targeted hormones in a biologically relevant concentration range, we performed a study comparing three different signal amplification strategies based on gold nanoparticles (GNPs). These strategies included GNPs immobilized on the sensor surface, GNPs conjugated with primary antibodies and GNPs conjugated with a secondary antibody for post competitive assay amplification. Here, multiplexed detection of the three hormones was achieved with an improved LOD of 9 fold for insulin, 10 fold for glucagon and 200 fold for somatostatin when compared to the SPRi biosensor without GNPs signal amplification, successfully addressing the aforementioned challenge.</dc:abstract><ual:supervisor>Maryam Tabrizian (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/9w032521n.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/w3763920h</ual:fedora3Handle><dc:subject>Computer Science</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Agf06g484g"><dcterms:title>Unifying variational inference and policy optimization</dcterms:title><ual:graduationDate>2019</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>School of Computer Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Ahmed, Zafarali</ual:dissertant><dc:abstract>Stochastic processes are an important theoretical tool to model sequential phenomenon in the natural sciences. Being able to sample trajectories that are consistent with real observations is crucial to understanding the mechanisms underlying natural systems. Sampling from posteriors often depends on hand-crafted importance samplers. In this thesis, we evaluate the reasonability of using reinforcement learning techniques to automatically learn proposals in importance samplers by making a connection with sequential variational inference. We show some encouraging results, highlight failure modes and discuss future work at the intersection of importance sampling, policy optimization and variational inference.</dc:abstract><dc:abstract>Les processus stocastiques sont un outil théorique important pour modéliser les phénomènes séquentiels dans les sciences naturelles. Il est essentiel de pouvoir échantillonner des trajectoires compatibles avec des données observées pour pouvoir comprendre les mécanismes sous-jacent des systèmes naturels. Habituellement, l'échantillonnage postérieur s'appuie sur des techniques manuelles. Dans ce mémoire, nous évaluons la justification d'utiliser des techniques d'apprentissage par renforcement pour apprendre de manière automatique les propositions d'échantillonnage par importance en établissant un lien avec l'inférence variationnelle séquentielle. Nous démontrons des résultats encourageants, soulignons des modes de défaillance et discutons de travaux à venir à l'intersection de l'échantillonnage par importance, l'optimisation de politique, et de l'inférence variationnelle.</dc:abstract><ual:supervisor>Simon Gravel (Internal/Cosupervisor2)</ual:supervisor><ual:supervisor>Doina Precup (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/bc386m653.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/gf06g484g</ual:fedora3Handle><dc:subject>Computer Science</dc:subject></rdf:Description></rdf:RDF>