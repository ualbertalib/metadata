<?xml version="1.0" encoding="UTF-8"?><rdf:RDF xmlns:oai="http://www.openarchives.org/OAI/2.0/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:ual="http://terms.library.ualberta.ca/" xmlns:bibo="http://purl.org/ontology/bibo/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:schema="https://schema.org/" xmlns:etdms="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Apz50h1768"><dcterms:title>Three empirical essays on program evaluation focus on childcare policy and immigration law</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Economics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>He, Ailin</ual:dissertant><dc:abstract>Cette thèse comprend trois essais empiriques concernant l’évaluation de deux programmes d’intérêt. Le premier est un programme de garde scolaire, mise en oevre exclusivement au Québec en 1998. Ce programme offre des services de garde avant et après l’école et durant le repas aux enfants du cycle primaire sur le local de l’école. Ce programme unique a non seulement réduit le coût de services de garde scolaire à 5 dollars par jour, mais a également considérablement augmenté la provision de ce type de garde. En utilisant ce programme comme une expérience naturelle, nous étudions les effets causalsde l’expansion et de la subvention des services de garde scolaire sur les types de garde utilisés par les enfants en classe primaire et sur le développement de ces enfants, et sur les comportements de leurs mères dans le marché du travail.En particulier, chapitre 1 évalue l’impact causal de ce programme sur le développement scolaire et non-cognitive des enfants, et le développement de la santé et la formation d’habitudes. En utilisant l’Enquête Longitudinale Nationale sur les Enfants et les Jeunes (ELNEJ), nous employons la mèthode des doubles différences pour comparer les enfants des écoles primaires au Québec avant et après la réforme aux enfants du cycle primaire dans le reste du Canada. Le programme augmente l’utilisation de services de garde scolaire par 8 points de pourcentage. Le service de garde scolaire remplace principalement les enfants qui restent seuls à la maison ou avec leurs frères et soeurs. Nous trouvons une détérioration dans le développement non cognitif de l’enfant, mais une amélioration dans la santé. Ces effets ne persistent pas à moyen terme.Le chapitre 2 examine l’effet du même programme sur les résultats du marché du travail des mères, étant donné que les options de garde des enfants sont directement liées aux décisions de la mère concernant le marché du travail. Notre groupe de traitement est constitué des mères au Québec dont le plus jeune enfant est âgé de 6 à 11 ans. Les mères dont le plus jeune enfant est du même âge dans le reste du Canada constituent le groupe témoin. En utilisant l’Enquête sur la dynamique du travail et du revenu (EDTR), nous analysons le comportement des mères sur le marché du travail. Nos résultats montrent que la réforme augmente l’emploi maternel sur la marge extensive par 2 à 3 points de pourcentage, mais n’a pas d’effet significatif sur l’intensité de l’emploi. Nous trouvons que l’effet du programme sur l’offre de main-d’oeuvre maternelle est exclusivement dû aux mères éduquées provenant des ménages à faible revenu. En outre, les revenues maternelles ont augmenté significativement.Le chapitre 3 est concerné par les primes de citoyenneté liées au marché du travail. Pour identifier l’effet causal de la citoyenneté, nous utilisons des modifications à la Loi de la citoyenneté canadienne en 2014, qui a changé la règle de présence physique pour la citoyenneté de trois ans de cinq à quatre ans de six. Après avoir régler les problèmes de sélection, nous employons une méthode de différence dans les différences pour comparer les effets de ce règlement sur le marché du travail d’immigrants qui diffèrent les uns des autres seulement en tant d’éligibilité pour la citoyenneté. En utilisant l’Enquête sur la population active du Canada (EPA) ainsi que le Fichier Relatif au Droit d’établissement des Résidents Permanents (FERP), nos résultats suggèrent que le fait de retarder d’un an l’éligibilité à la citoyenneté a des répercussions significatifs sur les marges extensives et intensives de l’offre de travail. Même si les immigrants concernés participent plus activement au marché du travail, leurs revenus salariaux sont négativement affectés. Ce phénomène peut être expliquer par le choix des immigrants affectés de travailler dans des emplois à horaires irréguliers et par l’augmentation des sources alternatives de travail comme le travail indépendant</dc:abstract><dc:abstract>This thesis is comprised of three empirical essays, focusing on evaluating two programs of interest. The first program is a before- and after-school care policy, implemented exclusively in the province of Quebec in 1998. This program provided childcare services to kindergarten and primary-school children on school premises before school time, during lunch and after school. This unique program not only reduced the cost of after-school care to $5 per day, but also increased the provision of care substantially. Using this policy as a natural experiment, we study the causal effects of after-school care’s expansion and subsidization on childcare arrangements and child’s development in Chapter 1 and on maternal labor market outcomes in Chapter 2.In particular, Chapter 1 focuses on studying the causal impact of this program on child’s scholastic achievements, non-cognitive skills, health outcomes as well as habit formation. Using the Canadian National Longitudinal Survey of Child and Youth (NLSCY), we adopt a difference-in-differences methodology to compare primary school children in Quebec beforeand after the reform, to the same school-grade cohorts in the rest of Canada. The policy effectively increases the use of after-school care by 8 percentage points, which mainly substitutes the use of child’s own care and sibling’s care. The intent-to-treat estimates further show a deterioration in child’s overall non-cognitive development but an improvement on their health outcomes. However, these effects do not persist in the medium run. Chapter 2 examines the effect of the same program on maternal labor market outcomes, since childcare options are closely connected to mothers’ labor supply decisions.We define mothers in Quebec with the youngest child aged 6 to 11 as the treatment group, and mothers with the youngest child in the same age cohort in the Rest of Canada as the control group.Using the Survey of Labor Income Dynamics (SLID), we analyze mother’s labor supply on the intensive and the extensive margins, as well as their earning outcomes. Our results show that the after-school care reform increases maternal employment on the extensive margins by 2-3 percentage points, but has no significant effect on employment intensity. Further examination of heterogeneous samples reveal that the policy effect on maternal labor supply is driven exclusively by highly educated mothers from lower non-maternal income households. In the meantime, mothers’ earning profiles have seen significant improvement as well.Chapter 3 turns to another policy targeting immigrants to Canada. This chapter attempts to uncover citizenship premiums on labor market outcomes. To identify the causal effect of citizenship, we make use of changes in the Canadian Citizenship Act of 2014, which extended the physical presence requirement for citizenship from 3 out of 5 years to 4 out of 6 years. After addressing selection issues, a difference-in-differences methodology is employed to compare changes in labor market outcomes of equivalent immigrants, who only differ from each other with respect to their eligibility for citizenship due to the revamped residency requirement. Using the Canadian Labor Force Survey (LFS) along with the Permanent Resident Landing File (PRLF), our results suggest that delaying citizenship eligibility by one year imposes significant impacts on both the extensive and the intensive margins of labor supply. Even though affected immigrants tend to participate more actively in the labor market during the selected periods after the new law has been implemented, their wage earnings are negatively affected. This results may be explained by the increased likelihood or willingness of affected immigrants to engage in jobs with irregular schedules and the rise in finding alternative sources of working opportunities such as self-employment</dc:abstract><ual:supervisor>Fabian Lange (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/5999n753q.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/pz50h1768</ual:fedora3Handle><dc:subject>Economics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Avx021k558"><dcterms:title>Towards an accessible methodology in precision medicine: methods for censored data and non-regular inferences</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Epidemiology and Biostatistics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Simoneau, Gabrielle</ual:dissertant><dc:abstract>A dynamic treatment regime (DTR) formalizes the study of precision medicine in which treatment decisions across multiple stages of clinical intervention are tailored to evolving, patient-level information. Statistical methods for DTR are concerned with identifying an optimal DTR, that is, the sequence of treatment decisions that yields the best expected outcome for a population of (''similar'') individuals. Dynamic weighted ordinary least squares (dWOLS) offers an accessible and theoretically robust framework for estimation and inference of an optimal DTR. However, it suffers from several limitations. First, like other regression-based DTR estimation approaches, dWOLS can yield estimators with non-regular limiting distributions in the sense that the standard asymptotic theory does not hold, in turn leading to incorrect coverage of confidence intervals. A second limitation is that dWOLS only handles uncensored continuous outcomes. It is often the case that the clinical outcome of interest is a survival time, which is typically subject to right-censoring.This thesis is composed of four manuscripts. The first manuscript compares the standard bootstrap to the m-out-of-n bootstrap with dWOLS when estimators suffer from non-regularity. An application to decision rules about an infant’s diet on childhood outcomes six years later, in which estimators are likely to suffer from non-regularity, is presented. In the second manuscript, we propose a novel method called dynamic weighted survival modeling (DWSurv) for estimation and inference of an optimal DTR with survival outcomes subject to right-censoring. An application to rheumatoid arthritis, in which a series of treatments is typically recommended to achieve remission, is presented. The third manuscript describes an extensive simulation study to evaluate the finite sample properties of competing methods for constructing confidence intervals for the DWSurv parameters, including parametric and non-parametric bootstrap as well as methods based on asymptotic theory. The impact of non-regularity is also assessed.The fourth and last manuscript showcases DWSurv in an illustrative example about the treatment of type 2 diabetes, where the objective is to find an optimal sequence of treatments that maximizes the time until the occurrence of a cardiovascular event or death. The first stage compares the addition of sulfonylurea or dipeptidyl peptidase-4 inhibitors to metformin. Extensions to more than one stage are described. Data from a large observational database are used</dc:abstract><dc:abstract>Les plans dynamiques de traitements (PDT) formalisent l'étude de la médecine de précision où les décisions de traitement à travers plusieurs phases d'intervention sont adaptées aux caractéristiques des patients. Les méthodes statistiques pour l'étude d’un PDT cherchent à identifier un PDT optimal, c'est-à-dire la séquence de décisions de traitement qui mène à la meilleure réponse espérée pour une population d'individus similaires.DWOLS est une méthode statistique théoriquement robuste, accessible et facile à appliquer pour l'estimation et l'inférence d'un PDT optimal. Cependant, la méthode comporte plusieurs limitations. Premièrement, comme d'autres approches d'estimation de PDT basées sur la régression, les estimateurs dWOLS peuvent avoir des distributions limites non-regulières dans le sens où la théorie asymptotique standard ne s'applique pas, menant ainsi à des intervalles de confiance avec des couvertures incorrectes. Une deuxième limitation est que dWOLS prend seulement en compte les réponses continues non-censurées. Il arrive souvent que les réponses d'intérêt clinique soient des temps de survie typiquement sujets à la censure.Cette thèse est composée de quatre manuscrits. Le premier manuscrit compare le bootstrap standard au bootstrap m-out-of-n (m parmi n) avec dWOLS lorsque les estimateurs souffrent de non-regularité. Une application concernant des règles de décisions pour la diète d'un nourrisson sur des réponses métaboliques mesurées durant l'enfance six ans plus tard, contexte dans lequel les estimateurs sont probablement non-réguliers, est présentée.Dans le deuxième manuscrit, nous proposons une nouvelle méthode appelée DWSurv pour l'estimation et l'inférence d'un PDT optimal avec des temps de survie sujets à la censure comme réponse. Une application à l'arthrite rhumatoïde, une maladie chronique pour laquelle une séquence de traitements est typiquement recommandée pour atteindre la rémission, est présentée.Le troisième manuscrit décrit une étude de simulation de grande ampleur pour évaluer les propriétés d'échantillon fini de différentes méthodes pour construire des intervalles de confiance pour les paramètres de DWSurv, incluant le bootstrap paramétrique et non-paramétrique ainsi que des méthodes basées sur la théorie asymptotique. L'impact de la non-regularité est aussi étudié.Le quatrième et dernier manuscrit démontre l'utilité de DWSurv dans une étude de cas sur le traitement du diabète de type 2 pour laquelle l'objectif est de trouver une séquence optimale de traitements qui maximise le temps jusqu'à la survenance d'un évènement cardiovasculaire ou la mort. La première phase compare l'addition du sulfonylurea ou des inhibiteurs de la dipeptidyl peptidase-4 à metformin. L'extension à plus d'une phase de traitements est décrite. Des données provenant d'une grande base de données observationnelles sont utilisées</dc:abstract><ual:supervisor>Erica Moodie (Supervisor1)</ual:supervisor><ual:supervisor>Robert William Platt (Supervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/3197xq975.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/vx021k558</ual:fedora3Handle><dc:subject>Epidemiology and Biostatistics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A0g354k73p"><dcterms:title>Energy-efficient silicon photonics switches towards mode-division-multiplexed interconnects</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Priti, Rubana</ual:dissertant><dc:abstract>With sky-rocketing internet traffic and subsequent increase in data-center size and capacity, the escalating trend of power consumption has been a huge challenge in implementing energy-efficient optical interconnection networks. Silicon Photonics is considered to be a potential alternative to the electrical interconnects in high-performance computers (HPC) and data centers due to its high throughput, compatibility with CMOS fabrication, large bandwidth scalability and energy efficiency. To reduce the communication bottleneck in inter- and intra-chip data communications, the aggregated bandwidth and link capacity should be increased by using advanced multiplexing and switching techniques. In the recent years, mode-division-multiplexing (MDM) has gained attention alongside wavelength-division multiplexing (WDM) and polarization-division multiplexing (PDM) to address the challenge of Shannon's limit - the theoretical maximum data rate of a communication channel - by increasing the data transmission capacity of the on-chip optical links in data center interconnects. MDM potentially offers more scalability than both of the other two multiplexing methods by sending multiple modes in a single optical channel, and reduces energy consumption by exploiting only one laser for the transmission of multiple data channels.In this thesis, we present energy-efficient silicon photonics switches using low-loss thermo-optic phase shifters. First, a rearrangable non-blocking broadband 4×4 Beneš switch is investigated in C-band with the detail design of the doped silicon resistive phase shifters and 2×2 Mach-Zehnder interferometer (MZI) elementary building blocks. A 2.4 μs switching time is achieved with -11.0 dB worst case crosstalk. The design methodology leads to the inception of more innovative contribution towards silicon photonics multimode (de)multiplexing and switching. A novel reconfigurable MDM (de)multiplexer/switch is investigated for path reconfigurable switching of multiple parallel optical modulated data signals offering higher bandwidth density and lower power consumption. This multimode component is reconfigurably used in an MDM (de)multiplexer and a mode selecting switch (MSS). Simultaneous transmission of two parallel 10 Gb/s optical data packets exhibits 2.8 dB power penalty with an estimated 1.55 pJ/bit energy efficiency. Next, a scalable multimode switch is proposed using multimode interference (MMI) couplers and metal heater phase shifters. This device is capable of switching either two or three transverse electric (TE) modes increasing footprint efficiency. A detail study on scalability estimates that multimode switches can significantly reduce on-chip power by 63% compared to their single-mode counterparts. Finally, the scalability potential of this device is experimentally verified by switching three TE modes with 3×10 Gb/s aggregated bit rate and 12.0 μs switching time. The investigations of this thesis experimentally demonstrate new promises towards high-throughput data intensive multimode switching for energy-efficient optical interconnects</dc:abstract><dc:abstract>Avec le trafic internet qui monte en flѐche et l'accroissement associé de la taille et de la capacité des centres de données, la tendance à la hausse de la consommation d'énergie est un énorme défi lors de l'implémentation de réseaux d'interconnexions optiques à faible consummation énergétique. La photonique sur silicium (SiP) est considérée comme une plateforme à fort potentiel pour les ordinateurs hautes performances (HPC) et les centres de données en raison du débit binaire plus élevé, de la compatibilité avec la fabrication CMOS, de la plus grande extensibilité de la bande passante et de la haute efficacité énergétique. Pour réduire le goulot d'étranglement de communication dans les communications de données inter et intra-puces, la bande passante agrégée et la capacité de liaison devraient être augmentées en appliquant des techniques avancées de multiplexage et de commutation. Au cours des derniѐres années, le multiplexage par division de mode (MDM) a attiré l'attention parallѐlement au multiplexage par division de longueur d'onde (WDM) et au multiplexage par division de polarisation (PDM) pour relever le dé la limite de Shannon le debit théorique maximal de données d'un canal de communication en augmentant la capacité de transmission de données des liaisons optiques sur puce dans les interconnexions des centres de donnée. MDM offre potentiellement plus d'évolutivité que les deux autres méthodes de multiplexage en envoyant plusieurs modes dans un seul canal optique, et réduit la consummation d'énergie en exploitant un seul laser pour la transmission par le biais de plusieurs canaux de données.Dans cette thѐse, nous présentons des commutateurs photoniques sur Silicium à haute efficacité énergétique utilisant des déphaseurs thermo-optiques à faibles pertes. Tout d'abord, un commutateur Beneš 4×4 à large bande, réarrangeable et non bloquant est investigué dans la bande C. Nous présentons la conception détaillée du déphaseur résistif en silicium dopé et des blocs élémentaires que sont les interféromѐtres Mach-Zehnder (MZI) 2×2. Un temps de commutation de 2.4 μs est atteint avec une diaphonie de -11.0 dB dans le cas le plus défavorable. La méthodologie de conception conduit à des contributions plus innovantes au (dé)multiplexage multimode et à la commutation photonique sur Silicium. Un nouveau multiplexeur/commutateur MDM reconfigurable est étudié pour la commutation à chemin variable de plusieurs débits binaires optiques parallѐles, permettant une plus grande densité de la bande passante et une consommation d'énergie réduite. Ce composant multimode est utilisé dans plusieurs configurations d'un (dé)multiplexeur MDM et d'un commutateur de sélection de mode (MSS). La transmission simultanée de deux paquets de données optiques parallѐles à 10 Gb/s présente une pénalité de puissance de 2.8 dB, avec une efficacité énergétique estimée de 1.55 pJ/bit. Ensuite, un commutateur multimode extensible utilisant des coupleurs d'interférence multimode (MMI) et des déphaseurs optiques chauffants métalliques est proposé. Cet appareil est capable de commuter soit deux ou trois modes électriques transversaux (TE) augmentant l'efficacité de l'empreinte. Une étude détaillée sur l'évolutivité estime que les commutateurs multimodes peuvent réduire significativement la puissance sur puce de 63 % par rapport aux commutateurs monomodes. Ensuite, le potential d'extensibilité de ce dispositif est vérifié expérimentalement en commutant trois modes TE avec une bande passante agrégée de 3×10 Gb/s et un temps de commutation de 12.0 μs. Ce travail démontre expérimentalement de nouvelles avenues vers la commutation multimode à haut débit binaire pour des interconnexions optiques à haute efficacité énergétique</dc:abstract><ual:supervisor>Odile Liboiron-Ladouceur (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/2n49t641n.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/0g354k73p</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3As7526h68v"><dcterms:title>You tube, vlogs, and vlogging to teach and learn about sexual consent: a study of youth practices and perspectives</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Integrated Studies in Education</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Garcia, Chloe</ual:dissertant><dc:abstract>Cette enquête examine l’utilisation et la perception par les jeunes des vlogues YouTube pour s’exprimer et se renseigner sur le consentement sexuel et la violence sexuelle. Plus précisément, cette thèse explore le travail sémiotique de jeunes ‘YouTubers’ abordant ces sujets dans leurs vlogues. J'étudie également les opinions des jeunes sur YouTube, les vlogues et le vlogging pour l'éducation à la sexualité en ligne et dans le contexte de l'enseignement supérieur. Alors qu'un nombre croissant d'études se penchent sur la participation et les discours des jeunes sur YouTube et, dans une certaine mesure, sur leurs négociations sur leur genre et leur sexualité dans ces espaces, rares sont les chercheurs qui se concentrent sur les vlogues et les vidéos liés à la sexualité sur YouTube. Pourtant, la popularité des vlogues portant sur le consentement sexuel au cours des dernières années appelle l’attention sur les discours sexuels qui y circulent. Cela soulève également des questions sur le potentiel pédagogique des vlogues et du vlogging pour l'éducation à la sexualité, qui est un domaine de recherche sous-étudié. Cette thèse aborde cette lacune dans les connaissances en mettant l’accent sur l’éducation au consentement sexuel dans le contexte postsecondaire. Le climat actuel de violence sexuelle dans les campus universitaires et collégiaux et les critiques contemporains sur les initiatives de lutte contre la violence sexuelle dans l'enseignement postsecondaire nécessitent davantage de recherches sur la manière dont le consentement sexuel est enseigné aux jeunes dans ce contexte.Cette étude s’appuie sur le « Constructivist Grounded Theory », les méthodologies artistiques et d’évaluation, et une analyse qualitative multimodale, pour collecter et analyser des données à travers deux phases et contextes: 1) sur YouTube, dans 28 espaces vlogues; et 2) dans des ateliers universitaires avec 18 participants. Je présente mes conclusions à travers deux modèles fondés sur mon analyse des vlogues sur YouTube, et de la voix et de l’expérience des jeunes. Le modèle 1 présente la complexité et la diversité des discours sur le consentement sexuel dans ces espaces sémiotiques et multimodales éclairés par les vloggeurs et leurs publics. Mon échantillon de ‘YouTubers’ a utilisé des stratégies de production attrayantes et partage des expressions de vulnérabilité dans le processus de communication des sentiments et des opinions; promouvoir le dialogue, le changement et le centrage sur les victimes; répondre aux invites; et dans l'ensemble, éduquer et sensibiliser au consentement sexuel et à la violence sexuelle. Le modèle 2 offre un cadre pour comprendre les points de vue des participants sur les vlogues et vlogging pour l'éducation à la sexualité, reflétant des sentiments mitigés au sujet de l'approche. Ceux-ci sont éclairés par leurs perceptions et leurs expériences avec la plateforme, le genre de vlogue, la création et la technologie des médias, les préférences personnelles, ainsi que des sentiments de vulnérabilité et de risque. Enfin, cette étude présente brièvement le retour d’évaluation des ateliers d’éducation au consentement qui ont inspiré cette recherche. Cette thèse propose plusieurs implications pratiques et de recherche de l'étude pour guider les chercheurs et les enseignants en éducation à la sexualité qui sont intéressés par YouTube, les vlogues, et le vlogging</dc:abstract><dc:abstract>This inquiry examines young people’s use and perception of YouTube vlogs (video logs) to express themselves and learn about sexual consent and sexual violence. Specifically, this dissertation explores the semiotic work of young YouTubers addressing these topics in their vlogs. I also investigate youth views of YouTube, vlogs, and vlogging for sexualities education online and in the higher education context. While a growing number of studies inquire about youth participation and discourses on YouTube, and to some extent their negotiations of their own gender and sexuality in these spaces, few scholars focus on YouTube vlogs and videos that tackle sexuality-related themes. Yet, the popularity of YouTube sexual consent vlogs in recent years calls for attention to the sexual discourses circulating within them. It also raises questions about the pedagogical application of YouTube vlogs and vlogging in sexualities education, which is an understudied area of research. This dissertation addresses this gap in knowledge, focusing on postsecondary consent education specifically. The current climate of sexual violence in university and college campuses and contemporary criticisms of anti-sexual violence initiatives in postsecondary education necessitates more research on the ways sexual consent is being taught to youth in this context.This study draws from Constructivist Grounded Theory, arts-based and evaluation methodologies, and multimodal qualitative analysis, to collect and analyze data across two phases and contexts: 1) on YouTube, within 28 vlogs spaces; and 2) in university workshops with 18 participants.  I present my findings through two frameworks grounded in my analysis of the YouTube vlogs and youths’ voices and experiences. Framework 1 presents the complexity and diversity of sexual consent discourses in these multimodal semiotic spaces informed by vloggers and their audiences. My sample of YouTubers used attractive production strategies and share expressions of vulnerability in the process of communicating feelings and opinions; promoting dialogue, change and survivor-centeredness; responding to prompts; and overall educating and raising awareness about sexual consent and sexual violence. Framework 2 presents my participants’ perspectives of YouTube vlogs and vlogging for sexualities education that reflects mixed feelings about the approach. They are informed by their perceptions of and experiences with the platform, the vlog genre, media-making and technology, personal preference, as well as feelings of vulnerability and towards risk. Finally, this study briefly presents the evaluation feedback on the consent education workshops informing this research. This dissertation offers several practical and research implications of the study to guide sexualities education scholars and teachers interested in YouTube, vlogs, and vlogging</dc:abstract><ual:supervisor>Christian Ehret (Supervisor2)</ual:supervisor><ual:supervisor>Shaheen Shariff (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/tt44ps204.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/s7526h68v</ual:fedora3Handle><dc:subject>Education</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A2514nq70c"><dcterms:title>The will not to count: Technologies of calculation and the quest to govern Afghanistan</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Art History and Communications Studies</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Askar, Mohammad</ual:dissertant><dc:abstract>This dissertation explores the technologies of numerical information in Afghanistan. It examines the history and politics of the tools used to produce numeral data for the purpose of governance. Based on archival research and ethnographic fieldwork, this project assesses the critical role of numbers, or lack thereof, in state practices throughout the modern history of Afghanistan. Grounded in the intersection of Media Studies and Science, Technology and Society Studies (STS), it primarily focuses on three technologies that are used for the task of calculation and control: the street sign, the identification document, and the price tag. These are critical tools, both for the state and the market, to exercise power and impose control. In studying these technologies, this research uncovers new aspects of Afghanistan’s long history of attempts to build functioning state institutions and form a stable economy. The literature on quantification has long focused on technologies of surveillance in strong states. This dissertation, however, addresses a critical gap in scholarship by examining the politics of surveillance in a weak state where the rulers have historically used force, instead of knowledge, to govern the population. Despite more than a century of on-and-off efforts, the country still suffers from a lack of reliable information on all aspects of society and the economy. The evidence presented in this research explains how the poverty of information has been contributing to the failure of the state and the economy in Afghanistan</dc:abstract><dc:abstract>Ce travail se veut une étude dur les technologies de l'information numérique en Afghanistan. Elle analyse l'histoire et la politique des outils utilisés pour produire des données numériques en vue de la gouvernance. Basé sur les recherches archivistiques et le terrain ethnographique, ce projet évalue le rôle critique des nombres, ou de leur absence, dans les pratiques de l’États à travers l’histoire moderne de l’Afghanistan. Ancré à l'intersection des études sur les médias et de la science, de la technologie et de la société, le projet se penche principalement sur trois technologies utilisées pour le calcul et le contrôle : la plaque de rue, le document d'identification et l'étiquette de prix. Ce sont des outils essentiels tant pour l’État que pour le marché permettant d’exercer un pouvoir et d’imposer un contrôle. En étudiant ces technologies, cette recherche révèle de nouveaux aspects de la longue histoire des tentatives de l’Afghanistan pour la mise en place des institutions étatiques fonctionnelles et pour la création une économie stable. La littérature sur la quantification a longtemps été axée sur les technologies de surveillance dans les États forts. Cette thèse aborde toutefois une lacune critique dans la littérature académique actuelle en examinant les politiques de surveillance dans un État faible, où les dirigeants ont historiquement utilisé la force au lieu du savoir pour gouverner la population. Malgré plus d'un siècle d'efforts épars, le pays souffre toujours d'un manque d'informations fiables sur tous les aspects de la société et de l'économie. Les preuves présentées dans cette recherche expliquent comment la pauvreté en matière d’informations contribue à l’échec de l’État et de l’économie afghane</dc:abstract><ual:supervisor>William O Straw (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/4x51hp788.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/2514nq70c</ual:fedora3Handle><dc:subject>Art History and Communications Studies</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Azp38wh90h"><dcterms:title>Using the guanosine diphosphate (GDP) cross-linked chitosan sponge as a platform for the co-culture of endothelial and pre-osteoblast cells in attempt to produce osteogenesis and angiogenesis</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Faculty of Dentistry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Agnes, Celine</ual:dissertant><dc:abstract>Despite the ‘self-healing’ properties of bone, non-union bone injuries, often known as critical size bone defects (CSBD), can result from congenital conditions, tumor resections, and trauma(20). Significant effort has been made recently to find a biomaterial that can act as a scaffold to restore native structure and function when placed at the defect site. Many of these biomaterials fail due to the lack of sufficient vascular supply typically present in native bone matrix(23). As a result, the need for a suitable biomaterial continues to grow. The objective of this work is to use the GDP cross-linked chitosan sponge discovered previously as a co-culture platform for osteoblasts and endothelial cells in an attempt to improve osteogenesis and angiogenesis(20,24). The multilayered configuration of this scaffold is expected to be ideal for this purpose because it acts similarly to native tissue and allows for crosstalk between cells in different layers. The rapid gelation of the sponge allows for localization at the site of injury, which is crucial for tissue regeneration(20,24). Various experiments were performed to investigate the viability of co-culture within this scaffold. Immunofluorescent staining and scanning electron microscopy were employed to characterize the multilayered sponge and Alamar Blue was used to evaluate the effect of the co-cultured encapsulation on viability and proliferation. Further experiments were conducted to quantify VEGF, an important pro-angiogenic factor that arises from crosstalk between both cell types and aids in the production of the vascular network(25).  The results found in this thesis provide a successful foundation of the GDP cross-linked chitosan sponge as a platform for the co-culture of pre-osteoblasts and endothelial cells. The cellular co-culture ratio of 1:1 showed increased cellular activity and an organized distribution and arrangement in comparison to mono-cultures. When encapsulated, the cells were found to be homogeneously distributed in the intended layer and the sponge surface was rough and porous with large concentrations of cells present. The cells had similar metabolic activity among the co-culture groups with the highest percent reduction in the single layered group and co-culture multilayered sponge with EOMA in the core and MC3T3 in the shell. The VEGF quantification showed highest secretion for the multilayered co-culture with EOMA in the core, and MC3T3 in the shell but experiment needs to be repeated to determine statistical significance. More research is also needed regarding the viability of the co-culture in-vivo and in measuring angiogenesis and osteogenesis</dc:abstract><dc:abstract>Malgré les propriétés ‘d'autoguérison’ de l'os, les lésions osseuses non soudées, souvent appelées défauts osseux de taille critique (CSBD), peuvent résulter de conditions congénitales, résections tumorales et traumatismes(20). Des efforts marquants ont été récemment mis au point pour trouver un biomatériau pouvant servir d’échafaudage pour restaurer la structure et la fonction originale de l’os lorsqu'il est placé sur le site du défaut osseux. Bon nombre de ces biomatériaux échouent en raison du manque de ressources suffisantes d’approvisionnement vasculaire généralement présent dans la matrice osseuse d’origine(23). Par conséquent, le besoin d’un biomatériau approprié continue de grandir.L’objectif de cette étude est d’utiliser l’échafaudage chitosan /GDP, découvert préalablement dans notre laboratoire, en co-culture pour les ostéoblastes et les cellules endothéliales dans le but d'améliorer l'ostéogenèse et l'angiogenèse(20,24). La configuration multicouche de cet échafaudage devrait être idéale à cet effet car elle agit de la même façon que le tissu natif et permet une relation entre les cellules de différentes couches. La gélification rapide de l'éponge permet de la garder en place à l’endroit de la fracture, ce qui est crucial pour la régénération des tissus(20,24). Diverses expériences ont été menées pour étudier la viabilité de la co-culture dans cet échafaudage. La coloration immunofluorescente et la microscopie électronique à balayage ont été utilisés pour étudier la configuration multicouche et Alamar Blue assay pour évaluer les effets de l'encapsulation en co-culture sur l'activité, la prolifération et la viabilité. Des expériences supplémentaires ont été menées pour quantifier le VEGF, un important facteur pro-angiogénique résultant de la relation entre les deux types de cellules et qui aide à la production du réseau vasculaire(25).Les résultats reportés ci-présent fournissent un fondement servant de plateforme pour la co-culture de pré-ostéoblastes et de cellules endothéliales dans l’éponge chitosan/GDP. Le rapport de co-culture cellulaire de 1: 1 a montré une augmentation de l'activité cellulaire et une distribution structurée par rapport aux mono-cultures. Une fois encapsulées, les cellules ont été réparties de manière homogène dans la couche voulue et la surface de l'éponge était rugueuse avec de grandes concentrations de cellules présentes. On a constaté que les cellules avaient une activité métabolique similaire parmi les groupes de co-culture avec un pourcentage plus élevé de réduction dans le groupe ayant une seule couche et la co-culture multicouche avec EOMA dans le noyau et MC3T3 dans la coque. La quantification VEGF a montrée la plus haute sécrétion pour la co-culture multicouche avec EOMA dans le noyau et MC3T3 dans la coque mais ces recherches doivent être répetes pour la determination de la signification statistique. Des recherches supplémentaires sont nécessaires pour étudier la viabilité de la co-culture in vivo et mesurer l’angiogenèse et ostéogenèse</dc:abstract><ual:supervisor>Maryam Tabrizian (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/qv33s177z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/zp38wh90h</ual:fedora3Handle><dc:subject>Dentistry</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A5138jj915"><dcterms:title>Development of molecular mechanics methods to cover conjugated drug-like molecules for structure based drug design</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Chemistry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Champion, Candide</ual:dissertant><dc:abstract>Afin d’amener un nouveau composé pharmaceutique sur le marché, des investissement considérable (en ressources et en temps) sont nécessaires, dans un processus impliquant biologistes structurels, chimistes organiciens, pharmacologistes, parmi de nombreux autres experts. Il est reconnu depuis un certain temps que l’utilisation d’outils informatiques pourrait réduire le recours à la main d’oeuvre, ainsi que les coûts. Dès à présent, pratiquement chaque étape de la conception de nouveaux médicaments fait appel à ces nouveaux outils. De l’analyse statistique rigoureuse de large jeux de données, ou l’emploi émergent de l’intelligence artificielle permettant de prédire certaines propriétés moléculaires (e.g. absorption, distribution, métabolisme, excrétion et toxicité), jusqu’à l’utilisation de méthodes reposant sur des principes physiques simulant les caractéristiques structurelles et dynamiques de systèmes moléculaires. Dans cette thèse, nous nous intéresserons particulièrement à cette seconde catégorie, en explorant les modèles classiques (et non quantiques) de mécanique moléculaire (MM) auxquels des coûts de calculs avantageux permettent des applications haut-débit et de simuler de larges systèmes biomoléculaires. Les principales applications des modèles MM incluent le criblage virtuel de bibliothèques chimiques afin identifier de potentiels inhibiteurs, ainsi que les simulations par dynamique moléculaire de protéines ou d’acides nucléiques permettant d’éclaircir les mécanismes d’adhésion entre ligand et macromolécule. Dans un premier temps, nous dresserons un bilan sur l’état-de-l’art actuel des méthodes MM, tout en détaillant leur fonctionnement, puis nous décrirons les plus récentes contributions apportées par la communauté scientifique en vue d’améliorer leur performance. Notre groupe de recherche a récemment développé une nouvelle technique appelée H-TEQ, dans laquelle nous omettons la représentation traditionnelle de molécules à travers les « types d’atomes » (liée à plusieurs défaillances majeures), et dont les prédictions sont fondées sur une quantification de principes chimiques qualitatifs établis depuis plusieurs décennies. La conception de nouveaux médicaments fait appel à de nombreux fragments moléculaires privilégiés (e.g. chaines conjuguées, cycles aromatiques) et 84% des composés thérapeutiques approuvés par la FDA contiennent au moins un atome d’azote. Dans ce contexte, l’application des méthodes MM lors de la conception de nouveaux médicaments se doit de porter une attention particulière lors de la paramétrisation des fragments mentionnés ci-dessus, afin d’obtenir des résultats fiables. Dans la deuxième partie de cette thèse, nous rapportons nos plus récentes contributions apportées à H-TEQ, dont nous étendons le domaine d’application aux molécules organiques désaturés. Dans la troisième partie, nous décrivons les défaillances des méthodes MM pour décrire les molécules contenant un azote lie à un système conjugué, puis proposons de potentielles solutions permettant de résoudre ces insuffisances</dc:abstract><dc:abstract>Considerable resources and time are required to bring a new drug to the market, in a multidisciplinary process involving structural biologists, synthetic chemists, pharmacologists, among many other experts. It has long been recognized that computation could alleviate costs and human-involvement; and computational tools are now applied to virtually all stages of the drug discovery process. From rigorous statistical analyses of large sets of data or employment of newly emerging artificial intelligence techniques to predict absorption, distribution, metabolism, excretion and toxicity (ADMET) properties among others, to more physically grounded methods simulating the structural and dynamic features of molecular systems. The focus of this thesis will be directed towards this latter class, as we investigate molecular mechanics (MM) models, in which a computationally affordable classical (as opposed to quantum) description of molecules, allows for high-throughput applications and the simulation of large biomolecular systems. Notable applications include virtual screening of large libraries of potential inhibitors, and molecular dynamics simulations of entire proteins or nucleic acids, which can also provide insights onto macromolecule-ligand binding. First, we will describe the inner-workings of MM, and review the current state-of-the-art, as well as most recent contributions to improve these models. Our research group has recently been involved in the development of a new method called H-TEQ, in which we have challenged the traditional representation of molecules in MM using atom types and base our predictions on quantified implementations of well-established chemical principles. Conjugated chains and aromatic rings are privileged scaffolds in drug design, and 84% of FDA approved pharmaceuticals contain at least one nitrogen atom. In that context, any successful application of MM based methods in drug design is bound to pay particular attention to these moieties. In the second part of this thesis, we report our most recent contributions to H-TEQ, in which we extend its domain of application to small organic molecules containing unsaturations. In the third part of this thesis, we describe current shortcomings of MM to describe molecules in which nitrogen atoms are bound to π-systems and share potential solutions which could resolve those issues</dc:abstract><ual:supervisor>Nicolas Moitessier (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/1831cq122.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/5138jj915</ual:fedora3Handle><dc:subject>Chemistry</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A6d5701915"><dcterms:title>High resolution x-ray imaging and quantitative microanalysis in electron microscopy</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mining and Materials</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Teng, Chaoyi</ual:dissertant><dc:abstract>Scanning electron microscope/energy dispersive spectroscopy (SEM/EDS) systems are popular for X-ray microanalysis due to its simple and fast operation. Recent years, which use cold field emission sources and silicon drift detectors, are highly capable X-ray imaging and quantitative microanalysis tools. However, the analytical efficiency and accuracy of SEM/EDS still have room for improvement. Regarding X-ray elemental mapping in particular, the top concern is to collect sufficient X-ray counts. Unfortunately, high count rates are usually achieved with sacrificing acquisition efficiency or spatial resolution. Regarding the quantitative X-ray microanalysis, the accuracy of EDS is constantly being questioned. Thus, the objective of this study is to optimize the SEM/EDS, making it a more efficient and accurate analytical instrument. In this study, X-ray mapping was performed on rare earth minerals (REMs), which contain various phases and complicated elemental constituents. The data was collected using a cold field emission SEM (CFE-SEM) equipped with an annular SDD (aSDD). The traditional elemental maps usually have excessive noise and limited phase information, so phase map analysis was performed instead. Compared with the conventional SDD, the aSDD has a larger solid angle, which produces high count rate and allows for efficient analysis at a low beam energy. The enhanced spatial resolution enables the accurate identification of REM phases down to one micron. In addition, the multivariate statistical analysis (MSA), i.e. the principal components analysis (PCA) and the blind source separation (BSS), was performed on the phase maps of REMs. This analysis reduces the noise and improves the phase identification accuracy, shortening the necessary acquisition timeThe f-ratio method, which is a recently developed quantitative analysis method for binary systems based on a CFE-SEM/EDS, is used. This method incorporates traditional EDS experiments and Monte Carlo simulations. Standards with known compositions are needed to calibrate the differences between experiments and simulations. In this study, the f-ratio method was applied to multi-element systems, including two Mg-Al-Zn alloys, and three standard minerals [kyanite (Al2SiO5), albite (NaAlSi3O8) and orthoclase (KAlSi3O8)]. The requirement of standard option was extended to any standard containing one or more target elements. The influences of the beam current, beam energy, and the standard composition were investigated. It is shown that the beam current does not have obvious impacts on the quantification results, so the f-ratio method is suitable for long acquisition, even when suffering from current instabilities. In addition, using beam-energy-dependent calibration factors, the f-ratio method can achieve a satisfactory accuracy</dc:abstract><dc:abstract>Les systèmes de microscopie électronique à balayage couplé à la spectroscopie à dispersion d’énergie (MEB/EDS) sont couramment utilisés pour la microanalyse par rayons X en raison de leur fonctionnement simple et rapide. Les systèmes récents, qui utilisent des sources d’émission de champ à froid et des détecteurs à dérive au silicium (SDD), sont des outils d’imagerie par rayons X et de microanalyses quantitatives extrêmement performantes. Cependant, l’efficacité analytique et la précision du MEB/EDS peuvent encore être améliorées. Lors d’une acquisition d’une cartographie d’éléments par rayons X, la principale préoccupation est de récolter un nombre suffisant de rayons X. Malheureusement, des comptes élevés sont généralement obtenus en sacrifiant l’efficacité de l’acquisition ou la résolution spatiale. En ce qui concerne la microanalyse quantitative par rayons X, la précision de l’EDS est souvent remise en question. Ainsi, l’objectif de cette étude est d’optimiser le MEB/EDS afin d’en faire un instrument d’analyse plus efficace et précis.Au cours de cette étude, des cartes de rayons X ont été réalisées sur des minéraux de terres rares (REM), qui contiennent divers constituants élémentaires ainsi que des phases complexes. Les données ont été recueillies à l’aide d’un MEB à émission de champ à froid (CFE-MEB) équipé d’un SDD annulaire (aSDD). Les cartes élémentaires traditionnelles montrent généralement un bruit excessif et des informations de phase limitées. Par conséquent, l’analyse d’une carte de phase a été réalisée à la place d’une carte élémentaire traditionnelle. Comparé au SDD conventionnel, l’aSDD possède une zone de couverture angulaire plus large (grand angle solide) ce qui produit des taux de comptage élevés. Ceci permet donc une analyse efficace à une énergie de faisceau réduite. La résolution spatiale améliorée permet l’identification précise des phases REM jusqu’à un micron. En outre, l’analyse statistique multivariée (MSA), c’est-à-dire l’analyse en composantes principales (PCA) et la séparation aveugle des sources (BSS), a été réalisée sur les cartes de phase des REM. Cette analyse réduit le bruit et améliore la précision de l’identification de la phase, réduisant ainsi le temps d’acquisition nécessaire.La méthode du f-ratio est une méthode d’analyse quantitative récemment mise au point pour les systèmes CFE-MEB/EDS. Jusqu’à présent, cette méthode n’a été utilisée que pour les systèmes élémentaires binaires. Le f-ratio intègre les expériences traditionnelles EDS et les simulations Monte Carlo. Des standards avec des compositions connues sont nécessaires pour calibrer les différences entre les expériences et les simulations. Dans cette étude, la méthode du f-ratio a été appliquée à des systèmes multiéléments, comprenant deux alliages Mg-Al-Zn et trois minéraux standard [cyanite (Al2SiO5), albite (NaAlSi3O8) et orthoclase (KAlSi3O8)]. Les critères de sélection des standards ont été étendus à tous les standards contenant un ou plusieurs éléments cibles. Les influences du courant de faisceau, de l’énergie du faisceau et de la composition standard ont été étudiées. Il est démontré que le courant de faisceau n’a pas d’impact évident sur les résultats de la quantification. La méthode du f-ratio convient donc aux longues acquisitions, même en cas d’instabilité du flux d’électrons. De plus, en utilisant des facteurs de calibration dépendant de l’énergie du faisceau, la méthode du f-ratio peut atteindre une précision satisfaisante</dc:abstract><ual:supervisor>Raynald Gauvin (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/v118rj90d.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/6d5701915</ual:fedora3Handle><dc:subject>Mining and Materials</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ams35td544"><dcterms:title>Fluid-structure interaction: Extended-FEM approach to solidification</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Mechanical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Caraeni, Daniela</ual:dissertant><dc:abstract>The ability of in-flight icing numerical codes to account for the effects of Supercooled Large Droplets (SLD), which are droplets ranging from 50-1000 µm, is still somewhat limited. Solvers that simulate SLD in in-flight icing treat an ensemble of droplets adopting a macroscopic approach, making use of some heuristic correlations based on experiments or extrapolating SLD behavior from smaller non-SLD droplets. Alternatively, taking a microscopic approach and accurately simulating the physics of individual droplets can lead to higher fidelity models when replacing empirical data. Therefore, developing models of a single SLD droplet impingement and solidification can provide a framework that expands the capabilities of current macroscopic solvers to handle water-ice fluid-structure interactions (FSI) and solidification. This presents challenges such as the evolution of the water-ice interface and the jump in fluid/solid properties due to phase change. This thesis proposes a level set method (LSM) to capture the movement of the interface, and the extended finite element method (XFEM) to sharply account for the discontinuities in the material properties without the need for body-conforming meshes. The number of additional degrees of freedom (DOFs) introduced by the XFEM formulation of the incompressible Navier–Stokes (INS) equations can be eliminated by using static condensation as the interface evolves. Furthermore, the Brinkman penalization method is proposed to model fluid-solid boundaries, while the Stefan condition is used to simulate planar solidification. Ultimately, the loose-coupling of the hydrodynamics with the thermodynamics permits the simulation of solidification along with fluid motion. Various test cases are used to evaluate the performance of the present methodology. The Brinkman penalization method is validated with several FSI benchmark lid-driven cavity problems. Subsequently, one-dimensional and two-dimensional solidification and melting cases have been employed to verify the implementation of the Stefan condition, and finally the coupled Stefan-hydrodynamics problem is simulated and compared against other numerical methods. The overall results illustrate that the proposed framework can efficiently solve multiphase flows along with solidification and FSI</dc:abstract><dc:abstract>La capacité des codes numériques à prendre en compte les effets des grosses gouttelettes surfondues (dites SLD), de diamètre compris entre 50 et 1000 µm, lors de simulations du givrage en vol est assez limitée. Les solveurs SLD doivent toutefois faire appel à certaines corrélations heuristiques et macroscopiques basées soit sur des expériences, soit sur une extrapolation du comportement des SLD sur la base de plus petites gouttelettes. Par ailleurs, l'adoption d'une approche microscopique et la simulation précise de la physique d’une seule gouttelette, faisant ainsi abstraction des données empiriques, peuvent conduire à des modèles de plus grande fidélité. Par conséquent, le développement d’un modèle microscopique fournissant des informations sur le comportement lors de l’impact et de la solidification d’une gouttelette pourrait augmenter la fidélité des codes en introduisant les interactions fluide-solide (dites FSI) et le phénomène de solidification de tels écoulements multiphasiques.Ceci présente plusieurs défis tels que le suivi de l'évolution de l'interface et du saut des propriétés fluide/solide dus au changement de phase. La présente thèse propose l’utilisation de la méthode des surfaces de niveau (LSM) pour suivre le mouvement de l'interface, et la méthode des éléments finis étendus (dits XFEM) pour prendre en compte les discontinuités dans les propriétés des matériaux. La méthode XFEM fournit un traitement précis de l'interface sans nécessiter de maillages conformes. Elle entraine cependant davantage de degrés de liberté (DOFs) dans les équations de Navier-Stokes incompressible (INS); éliminées au fur et à mesure par condensation statique. La méthode de pénalisation de Brinkman a été implémentée afin de modéliser les conditions aux frontières fluide/solide et la formulation dite classique du problème de Stefan a été retenue et mise en œuvre pour modéliser la solidification plane. Enfin, le couplage lâche entre hydrodynamique et thermodynamique permet de simuler la solidification du fluide en mouvement.De multiple cas tests sont utilisés pour valider la méthodologie développée dans cette thèse. La méthode de pénalisation de Brinkman est validée à l'aide de plusieurs cas canoniques mettant en jeu une cavité avec ou sans convection forcée. Ensuite, des cas tests unidimensionnels et bidimensionnels ont servi à valider la résolution du problème de Stefan, et finalement, le couplage Stefan-hydrodynamique est simulé et comparé à d’autres approches numériques. Les résultats obtenus montrent que l’approche proposée est capable de traiter de façon efficace les écoulements multiphasiques avec FSI et solidification</dc:abstract><ual:supervisor>Wagdi George Habashi (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/h415pg00x.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/ms35td544</ual:fedora3Handle><dc:subject>Mechanical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aj098zg42t"><dcterms:title>Discrete element modeling and optimization of biological and bioinspired materials</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Mechanical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Abid, Najmul</ual:dissertant><dc:abstract>Les matériaux biologiques durs tels que les coquillages, les os et les dents possèdent une combinaison inhabituelle de solidité, rigidité et ténacité bien que faits de faibles constituants. Jusqu’à présent, de telles performances n’ont pas été reproduites par les matériaux d’ingénierie les plus modernes. Derrière ces performances se trouve l’arrangement unique de blocs de construction rigides collés avec des interfaces faibles qui constituent la microstructure. Ces architectures précises génèrent des déformations non linéaires et concentrent les fractures dans d’efficaces configurations de blocage. Ainsi, la question de comment arranger des blocs de construction cassants avec des interfaces polymériques pour produire des matériaux de haute performance est d’une importance majeure dans le développement des matériaux de demain. Modéliser le comportement de fracture des matériaux biologiques durs est un défi à cause du coût calculatoire associé avec la considération de multiples mécanismes de blocage qui agissent ensembles pendant la propagation de la fracture à différentes échelles ; certains d’entre eux impliquent de larges volumes de matériau. Ainsi, nous avons choisi d’utiliser la méthode par élément discret (DEM) qui a réduit le temps de calcul de plusieurs ordres de grandeur comparativement à la méthode par éléments finis (FEM) conventionnelle. Nous nous sommes tout d’abord concentrés sur la déformation des arrangements échelonnés de tablettes rigides ; un arrangement souvent trouvé dans la nacre, le collagène et la soie d’araignée. Plus précisément, nous avons étudié les effets combinés des statistiques, arrangements de tablettes, et propriétés de l’interface sur les propriétés mécaniques telles que la rigidité, la solidité et l’absorption d’énergie. Nous avons trouvé que les variations statistiques ont un effet négatif sur toutes les propriétés, en particulier sur la ductilité et l’absorption d’énergie car les irrégularités et imperfections précipitent la localisation des déformations. Cependant, les résultats montrent aussi que les effets négatifs des microstructures aléatoires peuvent être compensé par des interfaces avec de larges élongations à la rupture accompagnées d’écrouissage. Nous avons ensuite exploré, à l’aide de larges modèles DEM (450 000 tablettes), les effets des propriétés de l’interface et des variations statistiques sur la mécanique de la fracture : déviation de fracture, fermeture de fracture, zones plastiques volumétriques de différentes tailles et formes, régimes de propagation de fracture transitoires et stationnaires et courbes de résistance à la fracture complètes (courbes-R). Nous avons trouvé que les variations statistiques modérées de la microstructure augmentent la ténacité car la fracture se retrouve coincée dans les régions les plus tenaces. Cependant, des variations statistiques plus hautes génèrent des régions très faibles qui peuvent être activées loin de la fracture principale, menant à des zones cohésives discontinues, des zones plastiques éparses, et une diminution globale de la ténacité. Nous avons ensuite étendu notre cadre de travail en explorant des architectures de matériaux inspirés des tesselations de Voronoi, des polygones réguliers, et des structures de briques et mortier. Nous avons identifié plusieurs mécanismes de renforcement incluant la déflection, la tortuosité ou la fermeture de fractures, et le renforcement des zones de plasticité. Nos résultats ont montré que les architectures périodiques sont plus tenaces que les microstructures à caractère aléatoire, que les architectures les plus tenaces sont aussi les plus anisotropiques, et que les tesselations basées sur des structures de briques et mortier sont les plus tenaces. Nos découvertes sont résumées en recommandations à la conception et en charte de sélection de matériaux pour le développement de matériaux à hautes performances</dc:abstract><dc:abstract>Hard biological materials such as seashells, bone and teeth boast unusual combinations of stiffness, strength and toughness even though they are made of weak constituents. Until now, such high performance cannot be matched in most modern engineering materials. Behind such performance is the unique arrangements of brittle building blocks bonded with weak interfaces that makes up the microstructure. These fine tune architectures generate nonlinear deformation and channel cracks into powerful toughening configurations. Therefore, the question of how to arrange brittle building blocks with polymeric interfaces to produce high performance material is of great importance in developing the ‘next’ generation of materials. In this dissertation, a computational tool is developed to study the deformation and fracture behavior of hard biological materials. Modeling the fracture behavior of hard biological materials is a challenging problem because of the computational cost associated with capturing multiple toughening mechanisms that act together during crack propagation at multiple length scales; some of them involving large volumes of material. Therefore, we chose to use the discrete element method (DEM) which reduced the computational time by several orders as compared to the conventional finite element method (FEM). We first focused on the deformation of staggered arrangements of rigid tablets; an arrangement often found in nacre, collagen and spider silk. Specifically, we studied the combined effects of statistics, tablet arrangement, and interface properties on the mechanical properties such as stiffness, strength and energy absorption. We found that statistical variations have a negative effect on all properties, in particular on the ductility and energy absorption because randomness precipitates the localization of deformations. However, the results also showed that the negative effects of random microstructures can be partially offset by interfaces with large strain at failure accompanied by strain hardening. We also explored, with large DEM models (450,000 tablets), the effects of interface properties and statistical variations on fracture mechanics: crack deflection, crack bridging, volumetric process zones of different size and shapes, transient and steady-state crack propagation regimes and full crack resistance curves (R-curves). We found that moderate statistical variations in the microstructure increase toughness because the crack gets pinned into tougher regions. However, higher statistical variations generate very weak regions which can be activated far from the main crack, leading to discontinuous cohesive zones, sparse process zones, and an overall decrease in toughness. We finally expanded the design space by exploring material designs inspired from Voronoi tessellations, regular polygons, and brick and mortar structures. We identified several toughening mechanisms including crack deflection, crack tortuosity, crack pinning and process zone toughening. Our results showed that periodic architectures can achieve higher toughness compared to random microstructures, the toughest architectures are also the most anisotropic, and tessellations based on brick and mortar structure are the toughest. Our findings were summarized as design guidelines and material selection charts for developing high performance materials</dc:abstract><ual:supervisor>Francois Barthelat (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/9c67ws13z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/j098zg42t</ual:fedora3Handle><dc:subject>Mechanical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Axd07gz06k"><dcterms:title>The role and effectiveness of advertising creative strategy</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Desautels Faculty of Management</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Dall'Olio, Filippo</ual:dissertant><dc:abstract>Le cœur de cette thèse repose sur trois chapitres qui traitent de la nature et des effets du contenu créatif en publicité. Dans le premier chapitre, je propose un cadre d’analyse novateur pour évaluer le contenu créatif en publicité, dit Stratégie Créative en Publicité (SCP), lequel est compréhensif, parcimonieux et ancré dans la littérature sur la commercialisation et la publicité. Ce cadre est composé de deux éléments : la Fonction de la publicité, qui est le message transmis aux consommateurs (c’est-à-dire le contenu publicitaire), et la Forme de de la publicité, qui est la manière dont le message est transmis aux consommateurs (à savoir l’exécution). La composante Fonction est construite sur l’idée que la publicité encourage les consommateurs de trois manières : par l’expérience, l’affect et le cognitif (l’espace EAC). La composante Forme évalue la complexité d’exécution de la publicité et détermine si les éléments d’exécution sont structurés selon des modèles créatifs précis. Dans le deuxième chapitre, je propose une analyse empirique des effets du SCP sur la réaction des consommateurs à la publicité, soit l’élasticité publicitaire. Les résultats démontrent que les signaux expérientiels et cognitifs sont les principaux vecteurs de l’élasticité publicitaire, et que les publicités structurés selon des modèles créatifs précis fonctionnent mieux pour les produits à forte implication. L’interprétation géométrique de l’espace EAC permet des dérivés de mesures synthétiques d’interaction, tant contemporaines que dynamiques, entre les dimensions du contenu publicitaire. Dans le troisième chapitre, j’examine la manière dont le SCP affecte la nature informative et persuasive de la publicité. Les résultats prouvent que les signaux cognitifs sont à la base du caractère informatif de la publicité, alors que son caractère persuasif émerge des signaux expérientiels et de la structuration des éléments d’exécution selon le modèle créatif. Ce dernier point est d’un grand intérêt considérant que l’effet persuasif de la publicité est habituellement identifié par élimination, c’est-à-dire en l’absence de contenu informatif</dc:abstract><dc:abstract>The core of this thesis comprises three chapters and investigates the nature and the effects of the advertising creative. In the first chapter, I propose a novel framework for evaluating advertising creative, Advertising Creative Strategy (ACS), that is comprehensive, parsimonious, and grounded in the marketing and advertising literature. This framework consists of two elements: The Function of the advertisement, that is what message the advertisement is conveying to consumers (i.e. its content), and the Form of the advertisement, that is the way the message is conveyed to consumers (i.e. its execution). The Function component is based on the notion that advertisements nudge consumers along three dimensions: experience, affect, and cognition (the EAC space). The Form component evaluates the executional complexity of the advertisement and assesses whether its executional elements are structured according to specific creative templates. In the second chapter, I empirically analyze the effect of ACS on consumers response to advertising, i.e. advertising elasticity. Results show that experiential and cognitive cues are the main drivers of advertising elasticity, and that advertisements structured according to creative templates fare better in high involvement product categories. The geometric interpretation of the EAC space also allows for the derivation of contemporaneous and dynamic synthetic measures of interaction among content dimensions. In the third chapter, I assess the way ACS affects the informative or persuasive nature of an advertisement. Results show that cognitive cues drive advertising informativeness, while persuasiveness stems from experiential cues and the structuring of executional elements according to creative templates. This latter result is of particular importance since advertising persuasiveness has been usually identified in the literature by elimination, i.e. by the absence of informative content</dc:abstract><ual:supervisor>Demetrios Vakratsas (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/0z709212d.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/xd07gz06k</ual:fedora3Handle><dc:subject>Management</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aws859k91h"><dcterms:title>Cellular stress responses elicited by gold nanourchins in mammalian cells</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Physiology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Abou Samhadaneh, Dana</ual:dissertant><dc:abstract>Le cancer est une maladie mondiale associée à des options restreintes de traitement, entraînant des situations difficiles telles que la pharmaco-résistance et la défaillance d'organes. En effet, des méthodes de traitement plus spécifiques sont nécessaires pour une efficacité maximale et des effets secondaires minimaux. Les nanoparticules d'or possèdent un excellent potentiel pour les applications théranostiques parce qu’elles peuvent être conçues pour cibler des cellules cancéreuses et établir des fonctions spécifiques. À ce jour, leur impact sur les cellules vivantes n’est que partiellement compris. Bien que, plusieurs études montrent l’effet des nanoparticules d'or sur le noyau, les mitochondries et le cytosquelette de la cellule, leur effet sur le réticulum endoplasmique (RE) et sur la réponse protéique dépliée (UPR) est peu connue. Cette étude est importante car l'UPR peut favoriser la survie des cellules. De plus, une UPR sévère peut déclencher la mort cellulaire. Mon étude explore l'UPR, les granules d'ARN cytoplasmique et la protéostase, parce qu’ils sont des indicateurs établis du stress cellulaire et des régulateurs clés de l'homéostasie cellulaire. En utilisant des cellules cancéreuses (HeLa) et les cellules du tubule proximal rénal non-cancéreuses (LLC-PK1), j'ai montré que les nanourchines d'or réduisent légèrement la prolifération cellulaire, induisent le stress du RE et altèrent l'homéostasie des protéines et de l'ARN. Spécifiquement, les nanourchins d'or activent la branche PERK de l'UPR, augmentent la production de chaperonnes du RE, améliorent la formation de corps P, favorisent l'oxydation de l'ARN et accumulent les protéines antioxydantes Nrf2 et NFκB dans les noyaux. En conclusion, les résultats suggèrent que les nanourchines d'or induisent plusieurs réponses de stress qui affectent le RE et d'autres compartiments subcellulaires. Cette connaissance est cruciale pour les applications thérapeutiques liées aux nanoparticules d'or, ce qui propose une nouvelle stratégie potentielle pour éliminer les cellules tumorales</dc:abstract><dc:abstract>Cancer is a worldwide problem with poor treatment options that lead to difficulties such as drug resistance and organ failure. Therefore, more specific methods of treatment are required for maximum efficacy and minimal side effects. Gold nanoparticles have excellent potential for theranostic applications because they can be engineered to target cancer cells and perform specific functions. To date, their impact on living cells is only partially understood. Different studies have shown how gold nanoparticles affect nuclei, mitochondria and the cytoskeleton; however, little is known about their effect on the endoplasmic reticulum (ER) and the unfolded protein response (UPR). This information is important, because the UPR can promote cell survival. On the other hand, a severe UPR may trigger cell death. My study focuses on the UPR, cytoplasmic RNA-granules and proteostasis, because they are established indicators of cell stress and key regulators of cellular homeostasis. Using cancer (HeLa) and non-cancer renal proximal tubule (LLC-PK1) cells as model systems, I showed that gold nanourchins slightly reduce cell proliferation, induce ER stress, and impair protein and RNA homeostasis. Specifically, gold nanourchins activate the PERK-branch of the UPR, increase production of ER chaperones, enhance P-body formation, promote RNA oxidation and accumulate the antioxidant proteins Nrf2 and NFκB in nuclei. Taken together, the results suggest that gold nanourchins induce several stress responses that affect the ER and other subcellular compartments. This knowledge is crucial for gold nanoparticle-related therapeutic applications and a promising new strategy to eliminate tumor cells</dc:abstract><ual:supervisor>Ursula Stochaj (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/05741x11w.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/ws859k91h</ual:fedora3Handle><dc:subject>Physiology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9306t360j"><dcterms:title>High-throughput electrochemistry of high-voltage Li-Ion cathode materials</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Chemistry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Potts, Karlie</ual:dissertant><dc:abstract>In the ongoing search for higher energy Li-ion batteries, the discovery of new cathode materials is of critical importance. Therefore, employing high-throughput methodologies to greatly increase the number of compositions studied is an attractive solution to rapidly screen and optimize next-generation battery materials.  While the combinatorial investigation of cathode materials has been reported in the literature, studies have been limited to structural characterization, with electrochemical characterization having only been performed on select compositions. Herein, we adapt a high-throughput electrochemical testing system in order to test 64 different mg-scale cathode compositions simultaneously. As the ~2 mg samples are synthesized in the same way as commercial materials, the results scale-up very well to that observed in bulk samples. Excellent agreement with literature is observed for both the peak potentials and the capacities obtained for the two test materials: LiCoO2 and Li[Ni1/3Mn1/3Co1/3]O2. Standard deviations in the 64-cell plate is about 7% for capacity and energy density. This level of precision and accuracy is mostly limited by 5% uncertainty on mass such that the system is considered optimized. We thus have the capability to study important electrochemical performance metrics including energy and voltage fade between cycles.  The system is then used to study 192 samples in the Li-Ni-Mn-O ternary system, synthesized and cycled in a one week period, and the results are then correlated to the structural phase diagram. This represents the first application of high-throughput cathode screening on powder electrode materials and enables the development of meaningful structure-property relations that are needed for the rational design of next-generation battery materials</dc:abstract><dc:abstract>Pour la recherche actuelle de batteries Li-ion à plus haute énergie, la découverte de nouveaux matériaux cathodiques revêt une importance cruciale. Ainsi, l’utilisation de méthodologies à haut débit (combinatoire) pour augmenter considérablement le nombre de compositions étudiées est une solution attrayante pour le criblage et l’optimisation rapides des matériaux de batterie de pointe. Bien que l'étude combinatoire des matériaux de cathode ait été rapportée dans la littérature, les études ont été limitées à la caractérisation structurale, dont la caractérisation électrochimique n'ayant été effectuée que sur des compositions sélectionnées. Ici, nous adaptons un système de test électrochimique à haut débit afin de tester simultanément 64 compositions différentes de cathodes à l'échelle du mg. Étant donné que les échantillons de ~2 mg sont synthétisés de la même manière que les matériaux commerciaux, les résultats correspondent très bien à ceux observés dans des échantillons échantillons à l'échelle du g et kg. Un excellent accord avec la littérature est observé pour les potentiels de pointe et les capacités obtenues pour les deux matériaux à tester: LiCoO2 et Li[Ni1/3Mn1/3Co1/3]O2. Les écarts types dans la plaque à 64 cellules sont d’environ 6% pour la capacité et la densité énergétique. Ce niveau de précision et d’exactitude est généralement limité à 5% d’incertitude sur la masse, de sorte que le système est considéré comme optimisé. Nous avons ainsi la possibilité d’étudier d’importantes mesures de performance électrochimique, notamment la perte d’énergie et de tension entre les cycles. Le système est ensuite utilisé pour étudier 192 échantillons dans le système ternaire Li-Ni-Mn-O, synthétisés et cyclés en une semaine, puis les résultats sont corrélés au diagramme de phase structurel. Ceci représente la première application du criblage cathodique à haut débit sur des matériaux d'électrode en poudre et permet de développer des relations structure-propriété nécessaires à la conception rationnelle de matériaux de batterie de nouvelle génération</dc:abstract><ual:supervisor>Eric Russell McCalla (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/9593v0807.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9306t360j</ual:fedora3Handle><dc:subject>Chemistry</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Arr1722534"><dcterms:title>The law-spacetime-justice nexus: International boundary disputes and indigenous territorial claims</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Geography</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Braiden, Michelle</ual:dissertant><dc:abstract>This dissertation analyzes international law, the International Court of Justice (ICJ) and indigenous territorial claims from the perspective of legal geography, a field of study that investigates the mutually constitutive relationship between law, space and power in society. I use the term “international law-spacetime-justice nexus” to signify that the analysis is concerned with the spatiotemporal foundations of international law and their relationship with concepts of international political justice. The dissertation analyzes matters pertaining to the spatial and structural (in)justices associated with the international legal principle of uti possidetis, a doctrine that holds that decolonizing or seceding states are entitled to retain the boundaries of the previous colonial or state power. The concept of spatiotemporalities is used to analyze questions about how the ICJ produces and reproduces the spatial hierarchies and structural injustices associated with colonial legalism in contemporary contexts. The dissertation also examines the theoretical foundations of indigenous self-determination and self-government as articulated in the United Nations Declaration of the Rights of Indigenous Peoples (2007). It counters the statist spatial paradigm of traditional international border disputes by examining indigenous territorial claims based on historical entitlement, remedial claims such as corrective justice and treaty violations, as well as non-remedial claims based on cultural integrity arguments. These themes are explored in three case studies, including the 1992 boundary dispute between El Salvador and Honduras, the 1994 dispute case between Libya and Chad, and the 1995 response by the Grand Council of Cree of northern Quebec and James Bay to the Quebec government’s proposals for separation</dc:abstract><dc:abstract>Ce mémoire étudie le droit international, la Cour Internationale de Justice (CIJ) et les revendications territoriales indigènes envisagées sous l'angle de la géographie légale, un champ d'études qui explore les relations entre la loi, l'espace et le pouvoir dans la société. J'utilise le terme "nexus international loi-espace temps-justice" pour signifier que l'analyse se concentre sur les fondations spatiotemporelles du droit international et leurs relations avec les concepts de justice politique internationale. Le mémoire étudie les questions des (in)justices spatiales et structurelles liées au principe légal international du uti possidetis, une doctrine qui affirme que les états décolonisés ou en sécession ont le droit de garder les frontières de l'état colonial qui précédait. Le concept de spatiotemporalités est utilisé pour analyser la manière dont la CIJ produit et reproduit les hiérarchies spatiales et les injustices structurelles liées au colonialisme juridique à l'époque contemporaine. Le mémoire examine aussi les fondations théoriques de l'auto-détermination et de l'auto-gouvernement indigène tels qu'ils sont définis dans le texte United Nations Declaration of the Rights of Indigenous Peoples (2007) (Déclaration des Nations-Unies sur les droits des peuples indigènes). Il s'oppose au paradigme spatial étatique des conflits de frontières traditionnels, en examinant les revendications territoriales indigènes basées sur les droits historiques, les mesures de réparation telles que la justice correctrice et les violations des traités, et aussi les revendications non liées aux réparations mais basées sur les arguments d'intégrité culturelle. Ces thèmes sont explorés dans trois études de cas, notamment le conflit de frontière de 1992 entre le Salvador et le Honduras, le conflit de 1994 entre la Lybie et le Tchad, et la réponse de 1995 du Grand Conseil des Cris du Québec du Nord et de James Bay aux propositions du gouvernement du Québec pour une séparation</dc:abstract><ual:supervisor>Benjamin Forest (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/3n204351d.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/rr1722534</ual:fedora3Handle><dc:subject>Geography</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A1r66j552j"><dcterms:title>Comparison of three point-based techniques for fast rigid US-CT intraoperative registration for  lumbar fusion</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Bioengineering and Biomedical Engineering Program</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Brat, Benjamin</ual:dissertant><dc:abstract>Le guidage par imagerie médicale pour l'implantation de vis pédiculaires lors de la fusion vertébrale réduit considérablement le taux de complications chez les patients. Les images préopératoires nécessitent un recalage image-patient afin de fournir des informations et un guidage précis. Le recalage automatique basé sur une échographie peropératoire localisée est une méthode prometteuse mais nécessite un algorithme robuste et précis. Cette thèse présente la comparaison de trois algorithmes différents susceptibles de satisfaire toutes les exigences cliniques: l'algorithme Iterative Closest Point (ICP), une variante plus robuste de l'ICP et l'algorithme de Coherent Point Drift (CPD).L’exactitude, la précision, la rapidité, la robustesse vis-à-vis du désalignement initial et du bruit ont été évaluées pour chaque algorithme sur trois jeux de données au réalisme croissant. Ils ont d'abord été évalués sur le recalage de nuages de points artificiels sous la forme d'un ellipsoïde. Après cela, ils ont été testés sur le recalage avec lui-même d'une surface segmentée de la tomodensitométrie (TDM) d'un fantôme lombaire en plastique Sawbones. Enfin, ils ont été évalués sur le recalage des surfaces osseuses segmentées d'un scanner d'un cadavre de porc avec la surface osseuse segmentée d'une échographie localisée du même cadavre.La précision a été mesurée par rapport à l'étalon-or actuel généré par des repères fiduciaires. Les résultats ont montré que même si les trois algorithmes étaient équivalents sur les deux premiers ensembles de données, seul le CPD pouvait rester robuste et précis sur les données porcines.L'algorithme CPD est un bon candidat pour une utilisation peropératoire automatique, rapide et robuste. À l’avenir, les méthodes de test présentées ici pourraient être étendues à d’autres algorithmes</dc:abstract><dc:abstract>Medical image guidance for pedicle screw implantation during spinal fusion reduces the complication rate for patients drastically. Preoperative images require image-to-patient registration to provide accurate information and afterwards, accurate guidance. Automatic registration based on tracked intraoperative ultrasound is a promising method but requires a robust and accurate algorithm. This thesis presents the comparison of three different algorithms that could potentially satisfy all clinical requirements in terms of image-to-patient registration: the Iterative Closest Point algorithm, a more robust variant of ICP and the Coherence Point Drift (CPD) algorithm.The accuracy, precision, speed, robustness regarding initial misalignment, and robustness regarding noise were evaluated for each algorithm on three datasets of increasing realism. First, they were evaluated on the registration of artificial point clouds in the shape of an ellipsoid. Second, they were tested on the registration of a segmented bone surface of the Computed Tomography (CT) scan of a plastic Sawbones lumbar phantom. Finally, they were assessed on the registration of the segmented bone surfaces of a CT scan of a porcine cadaver with the segmented bone surface of a tracked ultrasound scan of the same cadaver.Accuracy was measured relative to the current gold standard generated with fiducial landmarks. The results demonstrated that even though the three algorithms were equivalent on the two first datasets, only CPD could stay robust and precise on porcine datasets.The CPD algorithm is a good candidate for automatic, fast and robust intraoperative use. In the future, the testing methods presented here could be expanded to other algorithms</dc:abstract><ual:supervisor>Louis Collins (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/44558j55w.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/1r66j552j</ual:fedora3Handle><dc:subject>Bioengineering and Biomedical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Agb19fb323"><dcterms:title>Carbon and water cycle reconstructions across the Cretaceous-Paleocene boundary through plant wax lipids</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Earth and Planetary Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Bourque, Robert</ual:dissertant><dc:abstract>L'extinction crétacé-paléogène est l'un des phénomènes d'extinction de masse les plus étudiés parmi les archives fossiles. De nombreux aspects de la cause et de l'effet de l'extinction ont été bien étudiés, mais certaines zones n'ont pas encore été explorées de manière très détaillée. Après une longue période de réchauffement, il s'est produit un refroidissement avant l'extinction de la K-Pg qui a duré environ 100 000 ans. L'impact du Chicxulub a été suivi d'une période d'instabilité pendant laquelle les communautés végétales se rétablissaient. Bien que ceux-ci aient été bien étudiés, le cycle du carbone n'a été exploré que par des méthodes limitées et le cycle de l'eau encore moins.En étudiant la composition isotopique des lipides de cire végétale conservés dans les sédiments situés au-delà de la limite K-Pg, il devient possible de donner une image plus claire de la façon dont les cycles du carbone et de l’eau se comportaient jusqu’à l’événement d’extinction de masse. Ces sédiments ont été collectés dans le sud de la Saskatchewan, au Canada, les lipides ayant été extraits des sédiments et l’abondance de chaînes lipidiques individuelles et de compositions isotopiques d’hydrogène et de carbone a été mesurée. Le fractionnement moderne des cires de plantes a servi de base pour ramener les valeurs isotopiques aux valeurs attendues de CO2 atmosphérique et d’eau de pluie, en fournissant une base de référence pour les cycles du carbone et de l’eau et en observant leur évolution dans le temps. Les inférences sur ces cycles montrent que la région est inondée d'eaux de pluie isotopiquement légères mais sans tendance à long terme, tandis que les valeurs de carbone montrent une cyclicité rappelant les cycles orbitaux de Milankovitch, aucun enregistrement isotopique ne montrant des effets à long terme causés par l'extinction, ce qui suggère une reprise relativement rapide de 10 000 ans pour les deux cycles</dc:abstract><dc:abstract>The Cretaceous-Paleogene extinction has been one of the most heavily studied mass extinction events from the fossil record, with many aspects from the causation and effect of the extinction having been well studied, but with some areas still not having been explored in great detail. After an extended period of warming, there was a cooling event before the K-Pg extinction occurred that lasted for around 100,000 years, with the Chicxulub impact having been followed by a period of instability where plant communities were recovering. While these have been well studied, changes to the global carbon cycle remain uncertain and changes to the global water cycle have received little attention.By studying the isotopic composition of plant wax lipids preserved in sediments from across the K-Pg boundary, it becomes possible to put together a clearer image of how the carbon and water cycles were behaving up to and across the mass extinction event. These sediments were collected from southern Saskatchewan, Canada, with the lipids having been extracted from the sediments and had the abundance of individual lipid chains and isotopic compositions of hydrogen and carbon measured. Modern plant wax fractionation was used as a basis to relate the isotopic values back to the expected values of atmospheric CO2 and rain water, providing a baseline for the carbon and water cycles and observing how they change across time. Inferences on these cycles show the region was characterized by isotopically light rain water but with no long term trends in water isotope composition, while carbon values show cyclicity reminiscent of Milankovitch orbital cycles, with neither isotope record showing long term effects caused by the extinction, suggesting a relatively rapid recovery of within 10,000 years for both cycles</dc:abstract><ual:supervisor>Peter Douglas (Supervisor1)</ual:supervisor><ual:supervisor>Hans Carl Larsson (Supervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/3n204352p.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/gb19fb323</ual:fedora3Handle><dc:subject>Earth and Planetary Sciences</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9306t361t"><dcterms:title>Model predictive control of electric building energy and heating systems</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Seal, Sayani</ual:dissertant><dc:abstract>The residential building sector is a major consumer of electricity. In cold countries like Canada, heating systems contribute to a large share of the total building energy demand. Smart and efficient solutions to reduce building energy consumption are continually being explored through multi-disciplinary fields of research. This thesis focuses on the development and analysis of a central model predictive control (MPC) strategy for both comfort as well as energy management. In this work, the building simulations are done using a well-established simulation software TRNSYS (Transient System Simulation Tool) and the controllers are designed and simulated in MATLAB using standard computation routines. The thesis is conceptually divided into two parts. A simulation-based analytical study of two typical Canadian residential heating systems, namely electric baseboard  heaters (BB) and hydronic radiant floor heating (RFH) systems is presented in the first part. The study focuses on the modelling and characterization of the RFH and BB heating systems, highlighting the differences between their respective heating dynamics and related effects on their performance. A closed-loop MPC is designed and implemented for indoor temperature control. Various control scenarios are simulated to study the individual and cooperative performances of the two heating systems. Two novel MPC schemes are proposed to design a collaborative heating system using RFH and BB. These cooperative control schemes, namely the Sequential and Simultaneous approaches, have the identical goal of improving the performance of the slow-reacting RFH in maintaining the indoor operative temperature within predefined bounds while reducing the energy cost. In the Sequential approach, separate MPCs successively perform the optimizations for the RFH and BB whereas for the Simultaneous approach a single MPC optimizes the two heating systems. The MPC optimization also considers a thermal energy storage unit, incorporated as a part of the RFH system, for optimal energy usage based on variable Time-of-Use rate.The HVAC system constitutes an integral part of the building energy management system. In the second part of the thesis, a novel centralized MPC based building energy management system (BEMS) is proposed. The new residential setup used here is equipped with a photovoltaic (PV) solar system and a battery storage unit. An air-to-air multi-split heat pump (HP) is used as the primary heating system. The electric baseboard (BB) unit in each zone is used as a secondary system. The MPC is simultaneously responsible for controlling the heating inputs of the HP and BB units for comfort management, as well as for the control of energy flow between the PV, the home-battery and the bidirectional grid system. Variable Time-of-Use (ToU) rates are considered for the energy cost calculation and Feed-in-Tariff (FiT) is considered for selling energy to the grid. The MPC strategy is further modified to incorporate an electric vehicle (EV). The EV is considered as an intermittently available extended battery storage unit. Uncertainties related to the EV availability schedule, home-work-home trip discharges and state of charge (SOC) of the battery are considered. A Monte Carlo based uncertainty analysis is presented to estimate the reliability of MPC performance against the disturbances introduced into the system. Performance of the MPC is studied considering EV to home (V2H) communication, in an example simulation scenario</dc:abstract><dc:abstract>Le secteur de l'immobilier résidentiel est un grand consommateur d'électricité. Dans les pays froids comme le Canada, les systèmes de chauffage contribuent pour une large part à la demande énergétique totale des bâtiments. Des solutions intelligentes et efficaces pour réduire la consommation d'énergie des bâtiments sont constamment explorées dans des domaines de recherche multidisciplinaires. Cette thèse porte sur le développement et l'analyse d'une stratégie de commande par modèle prédictif (CMP) central pour le confort et la gestion de l'énergie. Dans ce projet, les simulations des bâtiments sont effectuées à l'aide d'un logiciel de simulation bien établi TRNSYS (Transient System Simulation Tool) alors que les contrôleurs sont conçus dans MATLAB à l'aide de routines de calcul standard.  La thèse est conceptuellement divisée en deux parties.Une étude analytique basée sur la simulation de deux systèmes de chauffage résidentiels canadiens typiques, à savoir les systèmes de chauffage par plinthes électriques (BB) et les systèmes de chauffage par plancher hydronique (RFH), est présentée dans la première partie. L'étude se concentre sur la modélisation et la caractérisation des systèmes de chauffage RFH et BB, en soulignant les différences entre leurs dynamiques de chauffage respectives et leurs effets sur la performance. Un CMP en boucle fermée est conçu et mis en œuvre pour le contrôle de la température intérieure. Divers scénarios de contrôle sont simulés pour étudier les performances individuelles et coopératives des deux systèmes de chauffage.Deux nouveaux systèmes CMP sont proposés pour concevoir un système de chauffage collaboratif utilisant RFH et BB. Ces systèmes de contrôle coopératifs, à savoir les approches Séquentielle et Simultanée, ont le même objectif d'améliorer les performances du RFH à réaction lente en maintenant la température à l'intérieur des limites prédéfinies tout en réduisant le coût énergétique. Dans l'approche Séquentielle, des CMP distincts effectuent successivement les optimisations pour RFH et BB, alors que pour l'approche Simultanée, un seul CMP optimise les deux systèmes de chauffage. L'optimisation CMP prend également en compte une unité de stockage d'énergie thermique, intégrée au système RFH, pour une utilisation optimale de l'énergie basée sur des taux de temps d'utilisation variables.Le système de HVAC fait partie intégrante du système de gestion de l'énergie du bâtiment. Dans la deuxième partie de la thèse, un nouveau système de gestion de l'énergie du bâtiment centralisé basé sur la CMP est proposé. La nouvelle configuration résidentielle utilisée ici est équipée d'un système solaire photovoltaïque (PV) et d'une unité de stockage à batterie. Ici, une pompe à chaleur multi-split air-air (HP) est utilisée comme système de chauffage principal. La plinthe électrique (BB) de chaque zone sert de système secondaire. Le CMP est simultanément responsable du contrôle des entrées de chauffage des unités HP et BB pour la gestion du confort, ainsi que du contrôle du flux d'énergie entre le PV, la batterie domestique et le système de réseau bidirectionnel. Des taux de temps d'utilisation variables sont pris en compte pour le calcul du coût de l'énergie et le tarif de production est pris en compte pour la vente d'énergie au réseau. La stratégie de CMP est encore modifiée pour incorporer un véhicule électrique (VE). Le VE est considéré comme une extension l'unité de stockage à batterie disponible par intermittence. Les incertitudes liées à l'horaire de disponibilité des VE, aux décharges domicile-travail-domicile et à l'état de charge de la batterie sont prises en compte. Une analyse d'incertitude basée sur la méthode de Monte Carlo est présentée pour estimer la fiabilité des performances du CMP par rapport aux perturbations introduites dans le système. Les performances du CMP sont étudiées en tenant compte de la communication VE vers domicile, dans un exemple de scénario de simulation</dc:abstract><ual:supervisor>Vahid Raissi Dehkordi (Supervisor2)</ual:supervisor><ual:supervisor>Benoit Boulet (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/p2677093m.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9306t361t</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A2b88qh614"><dcterms:title>Victor Horta’s Hôtel Tassel: The symbolism of the glasshouse and vegetal life</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Architecture</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Guxholli, Aniel</ual:dissertant><dc:abstract>L’Hôtel Tassel, conçu par l’architecte belge Victor Horta, est un des bâtiments clefs dans l’histoire de l’art et de l’architecture. Son décor intérieur est célèbre. L’escalier est particulièrement évocateur avec sa peinture murale représentant des lianes s’élevant du bas vers la lumière en haut. Sa signification n’a jamais été expliquée par Horta ou son client Émile Tassel. Elle hante l’imagination des historiens depuis lors. Cette thèse vise à saisir les significations de l’espace en établissant une série de connexions avec l’histoire des idées, de l’architecture et de l’art : les serres au dix-neuvième siècle, les visions changeantes de la nature, et l’espace du rituel maçonnique. Horta, l’architecte, avait déjà travaillé sur des projets de serres. Tassel, le client et l’homme de science, suivait de près les découvertes et théories récentes dans les sciences de la vie et dans l’art contemporain. On retrouve les mêmes thématiques dans des domaines différents qui étaient, à leur tour, connus d’Horta et de Tassel. Leur intérêt pour la philosophie de l’art et de la nature en « Extrême-Orient » facilita la transition de ces idées en des images poétiques pour le décor de la maison Tassel, évoquant l’expérience d’une vie en contact avec les forces de la nature</dc:abstract><dc:abstract>The Hôtel Tassel (1893-1895) by Belgian architect Victor Horta is an iconic building in the history of architecture. Its interior décor, especially its sky-lit staircase and a mural depicting plant-like motifs rising from a dark bottom towards the light, has a unique evocative power, yet what it attempts to convey is not immediately evident. The dissertation seeks to grasp this space by identifying a series of connections to elements in the history of ideas, architecture, and design which may be brought to bear upon its meaning. These are contexts in which a new environmental awareness arises in the nineteenth century and they include real and imaginary glasshouses, evolving visions of nature and even the space of masonic rituals. The dissertation traces them in their historical development and establishes their pertinence to the work and its agents. Horta had previously been involved in the design of glasshouses, but it is Émile Tassel, the client and a scientist, who followed most closely recent discoveries, theories and images of nature conjured in the sciences of life and in modern art. Horta’s and Tassel’s involvement in disciplines exploring similar themes, their fondness for the art and the philosophy of nature in what was then called the “Far East” facilitated the creation of poetic images in the interior of a house where life unfolds in communion with the forces of nature</dc:abstract><ual:supervisor>Martin Bressani (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/vx021k56j.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/2b88qh614</ual:fedora3Handle><dc:subject>Architecture</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Akh04dt83z"><dcterms:title>Autonomous wireless sensor network in a substation area using energy harvesting and wireless transfer of energy</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Hajikhani, Mohammadjavad</ual:dissertant><dc:abstract>La surveillance d’état des réseaux électriques peut réduire les coûts d’entretien, améliorer la fiabilité de l’approvisionnement et fournir un aperçu en temps réel des conditions d’exploitation. Dans le secteur de l’électricité, les systèmes de surveillance en continu ont conduit à privilégier l’entretien conditionnel aux programmes d’entretien préventif. Cependant, l’installation de capteurs dans un poste ou sur une ligne électrique, à proximité d’appareils à haute tension (HT), peut s’avérer très coûteuse. L’utilisation de réseaux de capteurs sans fil (RCSF) réduirait les coûts d’installation, mais les capteurs finiraient néanmoins par manquer d’énergie. Autre facteur à prendre en considération : le changement des batteries des capteurs situés à proximité d’appareils à haute tension peut être coûteux, peu pratique et présenter un risque élevé pour la sécurité des personnes. C’est pourquoi la surveillance d’état doit absolument passer par un réseau autonome.  Avec un réseau autonome, le problème du remplacement des batteries ou de leur recharge manuelle disparaît. Si les nœuds d’un réseau pouvaient récolter l’énergie de leur environnement, le réseau autonome susmentionné pourrait fonctionner. Heureusement, les postes sont des environnements propices à la récupération d’énergie ambiante. Mais tous les capteurs de surveillance ne sont pas nécessairement assez près des bornes à haute tension.  Cette thèse étudie la possibilité de mettre en place un réseau de capteurs sans fil autonome dans un poste. Un modèle hiérarchique de récupération d’énergie, dans lequel l’énergie est récupérée à deux niveaux différents, est proposé. Dans un premier temps, il s’agirait d’utiliser les récupérateurs d’énergie situés à proximité des bornes à haute tension. Ensuite, une partie de l’énergie récupérée serait distribuée aux nœuds de capteurs avoisinants au moyen de signaux radioélectriques. L’aspect hiérarchique de notre modèle nous permet de gérer la distribution irrégulière des sources de récupération d’énergie.   Le principal objectif de cette thèse est d’évaluer et d’optimiser l’efficacité du modèle hiérarchique de récupération d’énergie proposé. À cette fin, nous avons étudié notre système au moyen d’analyses mathématiques et de simulations informatiques. Notre analyse nous permet de prédire les risques de panne d’un système composé d’un nombre variable de capteurs à différentes configurations initiales. Les paramètres peuvent donc être modifiés pour atteindre les résultats voulus. Nous proposons également des techniques d’alimentation électrique permettant de maximiser l’efficacité de notre RCSF autonome. Cette étude démontre que malgré les pertes liées aux transferts d’énergie sans fil, il est possible d’utiliser un réseau de capteurs sans fil autonome dans un poste</dc:abstract><dc:abstract>Condition monitoring in electrical power networks can reduce maintenance costs, improve supply reliability and provide a real-time measurement of equipment operating conditions. For example, around a transformer we need to control parameters like KV, tap position, temperature of the transformer, oil temperature, buck trip, buck alarm, oil temperature trip, winding temperature alarm, auto tap position control etc.  However, wiring monitoring sensors in a substation environment or on a power line and around High Voltage (HV) devices can be very expensive. Using Wireless Sensor Networks (WSNs) would reduce installation costs. Nonetheless, sensors would eventually run out of energy. Another factor to consider is that changing the batteries of sensors located around HV devices can be costly, inconvenient and could potentially involve high personal safety risks. Consequently, for the purpose of condition monitoring, providing an autonomous WSN is of utmost importance. An autonomous WSN eliminates the problem of replacing batteries or recharging them manually. Enabling nodes in the network to harvest energy from their ambient environment may provide us with a self-powering WSN. Fortunately, a substation area is a rich environment for energy harvesting purposes. However, not all the monitoring sensors are necessarily close enough to HV terminals to take advantage of energy harvesting potential.This thesis studies the possibility of deploying an autonomous wireless sensor network in a substation environment. To this end, a hierarchical energy-harvesting model is proposed in which energy is scavenged in two levels. In the first step, energy is scavenged using harvesters that are located close to high voltage terminals. Then a portion of the scavenged energy will be distributed to nearby sensor nodes by radiating radio frequency signals to them. The hierarchical aspect of our model allows us to deal with the non-uniform distribution of the energy harvesting resources.The main goal of this thesis is to evaluate and optimize the performance of the proposed hierarchical energy-harvesting model. To this end, we have studied our system through mathematical analysis and computer simulations. %Our analysis allows us to predict the outage probability for a system, consisting of a number of sensors and with different initial settings. Therefore, the involved parameters can be adjusted to deliver the targeted performance. We also have proposed power allocation techniques to maximize the efficiency of our autonomous WSN.In this work we show that despite the wasteful nature of wireless transfer of energy, a self-sustainable wireless sensor network in a substation area can be accomplished</dc:abstract><ual:supervisor>Fabrice Labeau (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/rx913v193.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/kh04dt83z</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Av118rj91p"><dcterms:title>Comparison of the performance of laser-induced breakdown spectroscopy and color, visible, near-infrared and mid-infrared spectroscopy in the prediction of various soil properties</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Bioresource Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Marmette, Marie-Christine</ual:dissertant><dc:abstract>La spectroscopie peut prédire certaines propriétés physiques et chimiques du sol. C'est donc un moyen prometteur de complémenter les analyses de laboratoire traditionnelles, qui peuvent être coûteuses en temps et en argent. Dans cette recherche, la performance de sept instruments est comparée pour la prédiction de neuf propriétés du sol: phosphore (P), potassium (K), calcium (Ca), magnésium (Mg) et aluminium (Al) extractibles, pH tampon (BpH), pH, matière organique du sol (MO) et capacité d’échange de cations (CEC). Au total, 798 échantillons de sol séchés à l'air et compressés, représentant différentes conditions agro-climatiques du Québec (Canada), ont été analysés à l'aide de ces instruments, dont la résolution, le domaine spectral et les techniques optiques variaient. Les spectres visibles (Vis) ont été recueillis à l'aide d'un microscope numérique – bandes rouge, vert, bleu- (Vis-1) et d'un spectromètre visible couvrant une plage de 425 à 725 nm (Vis-2). Les spectres visibles et proche infrarouge (Vis-NIR) de tous les échantillons de sol ont été recueillis à l'aide d'une installation de laboratoire avec un spectromètre de terrain opérant dans la plage de 350 à 2 200 nm (Vis-NIR-1) et d'un autre instrument Vis-NIR de résolution supérieure allant de 350 à 2500 nm (Vis-NIR-2). Les spectres dans l'infrarouge moyen (MIR) ont été recueillis à l'aide d'un spectromètre infrarouge par réflexion diffuse à transformée de Fourier (DRIFT) portable avec une plage spectrale de 5 500 à 11 000 nm (MIR-1) et d'un spectromètre MIR utilisant la réflectance totale atténuée (ATR-FTIR) couvrant une plage de 2 500 à 17 000 nm (MIR-2). Enfin, les spectres de spectroscopie par claquage induit par éclair laser (LIBS) ont été acquis avec la technologie LaserAg développée par Logiag (Québec, Canada). Les performances de prévision des instruments, des domaines spectraux et des résolutions spectrales ont été comparés. Les résultats ont été obtenus par régression partielle des moindres carrés (PLSR) et les performances des instruments ont été évaluées en termes d’erreur de prédiction quadratique moyenne (RMSEP), du coefficient de détermination (R2) de la régression linéaire entre les valeurs mesurées et prédites et du rapport écart interquartile / performance (RPIQ). LIBS a conduit aux meilleurs résultats de prédiction pour P, K, Mg, Ca, pH, BpH et MO. Vis-NIR-1 donnait la meilleure prédiction pour Al et Vis-NIR-2 donnait la meilleure prédiction de CEC. La prédictibilité globale des propriétés du sol étudiées peut être classée comme suit: la prédiction était «excellente» pour Ca, «bonne» pour Mg, Al, MO et CEC, «modérée» pour P, pH et Bph et «médiocre» pour K. Dans cette étude, il a été constaté que le domaine spectral avait une influence sur la précision de la prédiction. La tendance générale était que LIBS fournissait la meilleure prédiction, suivi de Vis-NIR, puis de MIR qui était mieux ou comparable à Vis. Il a également été constaté que la résolution spectrale avait une influence sur la prédiction. Dans tous les cas autres que Al où Vis-NIR-1 donnait de meilleurs résultats que Vis-NIR-2, les instruments les plus sophistiqués surpassaient leurs homologues à résolution inférieure pour un domaine spectral donné. Les erreurs absolues moyennes (MAE) de prédiction de certains modèles pour K (Vis-NIR-2), Ca (Vis-NIR-1, Vis-NIR-2, MIR-2, LIBS), Al (Vis-NIR-1, Vis-NIR-2, LIBS), BpH (tous les instruments sauf Vis-1) et CEC (tous les instruments) respectaient les standards des analyses de sol en laboratoire</dc:abstract><dc:abstract>Spectroscopy can predict soil chemical properties; thus, it is complementary to traditional laboratory analysis, which is more costly and time-consuming. The objective of this thesis was to evaluate the ability of seven spectroscopic instruments to predict nine soil chemical properties: available phosphorus (P), exchangeable potassium (K), calcium (Ca), magnesium (Mg) and aluminum (Al), buffer pH (BpH), pH, soil organic matter (SOM) and cation exchange capacity (CEC). In total, 798 air dried and compressed soil samples, representing different agro-climatic conditions across Québec (Canada), were analyzed with these instruments, which have variable resolution, spectral range and optics. For instance, visible (Vis) spectra were collected with RGB bands from a digital microscope (Vis-1) and a visible spectrometer that scanned wavelengths from 425 – 725 nm (Vis-2). The visible and near-infrared (Vis-NIR) spectra was collected from the range of 350 – 2,200 nm (Vis-NIR-1) with low-resolution field equipment and from 350– 2,500 nm (Vis-NIR-2) with a high-resolution laboratory scanner. Mid-infrared (MIR) spectra were collected from 5,500 – 11,000 nm (MIR-1) with a custom portable diffuse reflectance infrared Fourier-transform (DRIFT) spectrometer and with a benchtop attenuated total reflectance Fourier-transform infrared (ATR FTIR) spectrometer covering 2,500 – 17,000 nm (MIR-2). Finally, laser-induced breakdown spectroscopy (LIBS) spectra were acquired with the LaserAg technology developed and owned by Logiag (Chateauguay, Quebec, Canada). Performances of instruments, spectral ranges and spectral resolutions were compared using partial least squares regression (PLSR) with relevant test statistics, such as the root mean squared error of prediction (RMSEP), the coefficient of determination (R2) for the linear regression between measured and predicted values and the ratio of performance to interquartile distance (RPIQ). The best fit lines between measured and predicted values of P, K, Mg, Ca, pH, BpH and SOM were obtained with the LIBS spectra, while Vis-NIR-1 gave the best prediction for Al and Vis-NIR-2 gave the best prediction of CEC. Overall, the prediction was “excellent” for Ca, “good” for Mg, Al, SOM and CEC, “moderate” for P, pH and Bph and “poor” for K. Prediction MAEs of models for K (Vis-NIR-2), Ca (Vis-NIR-1, Vis-NIR-2, MIR-2, LIBS), Al (Vis-NIR-1, Vis-NIR-2, LIBS), BpH (all intruments except Vis-1) and CEC (all intruments) respected soil laboratory analysis standards</dc:abstract><ual:supervisor>Viacheslav Adamchuk (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/z029p9117.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/v118rj91p</ual:fedora3Handle><dc:subject>Bioresource Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A0p096c27b"><dcterms:title>Coded-Engagement: data-driven participation in the smart city</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Geography</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Tenney, Matthew</ual:dissertant><dc:abstract>Cette thèse explore les concepts de l'engagement codé en relation avec le rôle de la capacité des citoyens à participer aux villes intelligentes.  L'engagement codé est défini comme la perspective du citoyen- capteur dans la ville intelligente, dans un monde où capteurs interconnectés et grandes données sont omniprésents.  En raison des difficultés perçues dans l'exploitation de la participation publique et des promesses hyperboliques associées aux villes intelligentes, l'engagement codé est un mode d'engagement entre les gouvernements locaux et les citoyens en développement.  Le principal mode d'engagement consiste souvent à recueillir des données à partir des médias sociaux, des caméras ou des citoyens eux-mêmes pour alimenter des méthodes axées sur les données afin que les villes intelligentes puissent offrir des services axés sur les citoyens.  L'analyse de la littérature présente la participation du public en tant que pierre angulaire de la démocratie et suit son évolution vers une méthode d'engagement des citoyens axée sur la technologie.  La discussion met en lumière la façon dont les pratiques d'engagement du public ont suivi l'intérêt croissant pour l'identification des communautés cibles et l'utilisation de méthodes axées sur la technologie pour attirer des participants plus nombreux et plus représentatifs.  Cependant, à mesure que le rôle des technologies du Web 2.0 grandit, la nature et la qualité des acteurs et des données exploitées par ces médias deviennent de plus en plus préoccupantes.Un système de détection de foule a été conçu pour " supprimer la boîte noire " de nombreux algorithmes propriétaires conçus pour capter l'opinion publique à l’aide du contenu généré par les utilisateurs.  Une nouvelle approche méthodologique est fournie combinant de multiples attributs des médias sociaux pour identifier les communautés de pratique et d'intérêt dans des contextes géographiques spécifiques.  Grâce à l'utilisation de réseaux sociaux spatialement situés, à l'analyse spatio-temporelle et au traitement du langage naturel, des liens inférés pourraient être explorés au moyen d'une plateforme de visualisation 3D dans le but de prendre des décisions éclairées.  Deux études de cas sont également fournies sur la ville de Toronto.  La première met en contexte le contrôle exercé par l'entreprise sur une collectivité du secteur riverain de Toronto.  Cette étude analyse l’implémentation défective d’une collectivité intelligente, où les citoyens pouvaient fournir une rétroaction en temps réel aux dirigeants communautaires au moyen de plates-formes virtuelles, financée par de vastes quantités de ressources publiques.  La deuxième étude de cas fournit un contexte plus approfondi des efforts de la ville de Toronto pour devenir une ville intelligente avant-gardiste.  Cependant, les mécanismes de gouvernance et de contrôle des intérêts des entreprises privées ont une emprise sur le rôle futur de la participation publique dans la ville intelligente.  Il s'agit notamment de la propriété privée de services aux citoyens qui sont alimentés par la collecte des contributions passives du public</dc:abstract><dc:abstract>This dissertation explores the concepts of coded engagement as they relate to the role of citizens’ ability to participate in smart cities.  Coded engagement is what we call the perspective of citizen-as-sensor in the smart city, as it meets the ubiquitous world of interconnected sensing-technologies and big data.  Because of the perceived difficulties in harnessing public participation and the hyperbolic promises of smart city technologies, coded engagement stands as a developing mode of engagement between local governments and citizens.  The primary method of realizing coded engagement is often through the harvesting of data from social media, cameras, or citizens themselves to fuel data-driven methods for smart cities to offer citizen-centric services.  The literature review presents public participation in the context of its role as a cornerstone of democracy and follows its evolution to a technologically enabled method for engaging with citizens.  The discussion highlights how public engagement practices have followed increasing interests in identifying communities of interest and using technologically-driven methods to capture wider and more representative participants.  However, as Web 2.0 technologies become more increasingly involved, so does the increased concern for the nature and quality of actors and data harnessed through these mediums.A crowd-sensing system has been designed to “remove the black-box” of many proprietary algorithms designed to harvest public opinion from user-generated content.  A novel methodological approach is provided that conflates multiple attributes from social media to identify communities of practice and interest within local geographic contexts.  Through the use of spatially situated social networks, space-time analysis, and natural language processing, inferred links could be explored through a 3-D visualization platform in the pursuit of informed decision-making.  Two case studies are also provided on the City of Toronto.  The first contextualizes corporate control on a Toronto Waterfront community as vast amounts of public resources were leveraged for a failed implementation of an intelligent community where citizens could provide real-time feedback to community leaders through crowd-platforms.  The second case study provides a deeper context into the City of Toronto’s pursuits to become a smart city leading the 21st century.  However, the mechanisms of governance and control of corporate interests have a grip on the future role of public participation in the smart city.  Namely, participation converges through the private ownership of citizen-centric services that are fueled by collecting the passive contributions of the public</dc:abstract><ual:supervisor>Renee Sieber (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/z603r2948.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/0p096c27b</ual:fedora3Handle><dc:subject>Geography</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aqr46r537z"><dcterms:title>Adaptation in natural populations: Integrating phenotypic and genetic perspectives</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Biology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Thurman, Timothy</ual:dissertant><dc:abstract>Un des objectifs de la biologie est de comprendre comment les organismes s’adaptent à leur environnement. Bien que beaucoup d’avancées aient été faites à ce propos, de nombreux aspects du processus d’adaptation restent mystérieux. Cela est particulièrement vrai en ce qui concerne notre compréhension des bases génétiques de l’adaptation en populations naturelles. Ma thèse intègre des perspectives phénotypiques et génétiques pour développer notre compréhension de la sélection et de l’adaptation au sein de populations naturelles. Dans mon premier chapitre, j’explore les conséquences de la sélection naturelle sur les variantes génétiques. Dans de nombreux modèles de génétique des populations, la sélection est décrite par le coefficient de sélection, s. À travers une méta-analyse de plus de 3000 coefficients de sélection, je révèle des généralités sur la sélection naturelle en matière de génétique. Je relie ces résultats aux théories de la génétique des populations et aux études de sélection phénotypique, et formule des recommandations pour le calcul et l’interprétation de les coefficients de sélection. Dans mon deuxième chapitre, je considère une zone hybride dynamique qui se déplace rapidement entre deux races de papillons Heliconius erato de couleurs différentes. Étant donné que les locus génétiques responsables de la variation du motif de couleur chez H. erato sont bien caractérisés, je considère simultanément la sélection aux niveaux phénotypique et génétique. Je développe de nouvelles méthodes statistiques pour quantifier la position et la forme de la zone hybride et les applique pour montrer que la zone hybride d’H. erato s’est élargie, alors que son déplacement a ralenti. Je montre que cela est dû à une diminution de la sélection du motif de couleur des ailes de ce papillon et de l’allèle sous-jacente au motif de couleur.  Dans mon dernier chapitre, j’examine si les changements phénotypiques et génétiques sont prédictibles. Je choisis une approche expérimentale réalisée sur des lézards Anolis sagrei. Les «anoles»  sont un exemple d’évolution parallèle, et leurs interactions avec des espèces compétitrices et prédatrices ont été bien étudiées. Cela fournit des prévisions claires sur la manière dont ces interactions écologiques pourraient conduire à une évolution adaptative. Je teste ces prévisions en manipulant la présence et l’absence d’espèces prédatrices et compétitrices dans un plan factoriel sur 16 iles des Bahamas. Je mesure les changements dans une série de traits morphologiques et utilise le séquençage ADN pour caractériser les changements de fréquence d’allèles. Malgré les effets forts et constants des prédateurs et des compétiteurs sur le comportement, le régime alimentaire et la taille de la population chez A. sagrei, j’ai trouvé que les changements phénotypiques et génétiques étaient difficiles à prédire. Le changement phénotypique était lié à la variation de la structure de la végétation et aux densités de lézards entre les îles, rendant la prévision difficile. Les changements génétiques, en revanche, étaient imprévisibles et sans lien avec nos manipulations expérimentales, aux changements phénotypiques ou aux différences environnementales. Mon travail révèle la nécessité d’obtenir des données écologiques et des connaissances en histoire naturelle pour prédire les effets de la sélection naturelle, et montre comment les expériences en milieu naturel peuvent être utilisées pour tester et clarifier des hypothèses sur le fonctionnement de la sélection naturelle. Dans l’ensemble, ma thèse démontre que l’intégration de perspectives phénotypiques et génétiques peut aider les biologistes à comprendre le fonctionnement de la sélection naturelle en milieu naturel. En particulier, mes études montrent l’intérêt de combiner ces perspectives avec des données écologiques détaillées, des techniques statistiques novatrices et des expérimentations pour tester directement des hypothèses sur l’évolution de populations naturelles</dc:abstract><dc:abstract>A central goal of evolutionary biology is to understand how organisms adapt to their environment. Though much progress has been made in answering this question, many aspects of the process of adaptation remain mysterious. This is especially true for biologists’ understanding of the genetic basis of adaptation in natural populations of organisms. My dissertation integrates phenotypic and genetic perspectives to advance our understanding of selection and adaptation in natural populations of organisms. I take multiple approaches to this question, combining meta-analysis, population surveys, and manipulative experiments in the field. In my first chapter, I explore the consequences of natural selection on genetic variants. In many population genetic models, selection is parameterized as the selection coefficient, s. Through a meta-analysis of over 3000 selection coefficients from 79 studies, I reveal generalities about how natural selection operates at the genetic level. I relate these results to population genetic theory and studies of phenotypic selection, and provide recommendations for the calculation, interpretation, and reporting of selection coefficients. In my second chapter, I consider natural selection and adaptation within a rapidly moving hybrid zone between two races of Heliconius erato butterfly that differ in colour pattern. Because the genetic loci responsible for variation in colour pattern in H. erato are well characterized, I consider selection at the phenotypic and genetic levels simultaneously. I develop new statistical methods for quantifying hybrid zone position and shape and apply these to show that over the last 15 years the H. erato hybrid zone has grown wider while its movement has slowed. I show that this is due to a decrease in the strength of selection on colour pattern and the underlying colour-pattern allele. I then use remotely-sensed data on forest loss and productivity to test hypotheses about the ecological forces that influence hybrid zone dynamics. In my final chapter, I examine whether phenotypic and genetic change are predictable. I take an experimental approach, using a large-scale, long-term, eco-evolutionary field study with Anolis sagrei lizards. Anoles are an exemplar of parallel evolution across an adaptive radiation, and their interactions with competitor and predator species have been well-studied in within-generation experiments. This provides clear predictions for how these ecological interactions might drive adaptive evolution over multiple generations. I test these predictions by manipulating the presence and absence of predator and competitor species in a factorial design across 16 small islands in the Bahamas. I measure changes in a suite of morphological traits relevant to habitat use and performance, and use DNA sequencing to characterize changes in allele frequency across the genome. Despite strong and consistent effects of predators and competitors on behavior, diet, and population size in A. sagrei, I found that phenotypic and genetic change were difficult to predict in advance. Phenotypic change was related to variation in vegetation structure and lizard densities across islands, making a priori prediction challenging. Genetic change, on the other hand, was unpredictable and unrelated to either our experimental manipulations, phenotypic change, or environmental differences. My work reveals the necessity of ecological data and knowledge of natural history for predicting natural selection, and shows how field experiments can be used to test and clarify hypotheses about how natural selection operates. Overall, my dissertation demonstrates that integrating phenotypic and genetic perspectives can help biologists understand how natural selection operates in the wild. In particular, it shows the value of combining these perspectives with detailed ecological data, novel statistical techniques, and experimentation to directly test hypotheses about evolution in natural populations</dc:abstract><ual:supervisor>William McMillan (Supervisor2)</ual:supervisor><ual:supervisor>Rowan Barrett (Supervisor1)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/h702qb769.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/qr46r537z</ual:fedora3Handle><dc:subject>Biology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aqb98mk61s"><dcterms:title>Making environmental governance work in emerging economies: a case study of China</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Laws</schema:inSupportOf><dc:contributor>Faculty of Law</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Li, Ruo Ying</ual:dissertant><dc:abstract>It has long been recognized that the resolution of global environmental problems is closely connected to differences in levels of economic development. While international law and policy tends to assert the compatibility of economic growth and environmental protection, developing countries continue to grapple with the challenge of growing their economies and combating poverty without causing significant harm to the environment. Given the unsatisfactory solutions offered by influential concepts that are predicated on the notion that economic development can be reconciled with improvements in environmental performance, such as sustainable development, green growth or the Environmental Kuznets Curve, the task of carrying out economic growth while avoiding ecological breakdown has become the pathway emerging countries must pursue. However, important questions arise against the backdrop of this new challenge. What are the challenges and opportunities for improving environmental governance in emerging economies? How and to what extent can emerging economies develop laws and policies that will enable them to decouple environmental degradation from the pursuit of economic growth? China is one country where the complex relationship between economic development and environmental quality offers a rich context for examining the opportunities and challenges of improving environmental governance in a rapidly industrializing economy. Since 2007, there have been clear signs that China has continuously made efforts to reverse environmental degradation and improve the environment’s quality. This thesis seeks to distill key findings from China’s experience with environmental governance during the past two decades with a view to understanding how to strengthen environmental governance in emerging economies. The general context in which China’s environmental governance arose, reasons for its recent pro-environmental intentions, and its unique features can all provide lessons for other emerging countries. At the same time, this thesis shows that there are limitations in replicating China’s politically, economically, and geographically unique experience in environmental governance</dc:abstract><dc:abstract>Il est reconnu depuis longtemps que le problème de la résolution des problèmes environnementaux mondiaux est étroitement lié aux différences des niveaux de développement économique. Alors que les lois et politiques internationales tendent à affirmer la compatibilité de la croissance économique et de la protection de l’environnement, les pays en développement continuent à faire face aux défis de la croissance de leur économie et de la lutte contre la pauvreté tout en évitant les dommages environnementaux. Compte tenu des solutions peu satisfaisantes offertes par des concepts influents fondés sur la notion que le développement économique peut être concilié avec l’environnement, telles que le développement durable, la croissance verte ou l'hypothèse de la courbe environnementale de Kuznets, la poursuite de la croissance économique sans dégradation de l'environnement est attendue des pays émergents. Cependant, d'importantes questions se posent dans le contexte de ce nouveau défi. Quels sont les défis et les opportunités pour améliorer la gouvernance environnementale dans les économies émergentes? Comment et dans quelle mesure les économies émergentes peuvent-elles élaborer des lois et des politiques leur permettant de dissocier la dégradation de l'environnement de la poursuite de la croissance économique?La Chine est un pays où la relation complexe entre développement économique et qualité de l'environnement offre un contexte riche pour examiner les opportunités et les défis liés à l'amélioration de la gouvernance environnementale dans une économie en voie d'industrialisation rapide. Depuis 2007, il apparaît clairement que la Chine n’a cessé de poursuivre des efforts afin d’inverser la dégradation de l’environnement et améliorer sa qualité. Cette thèse cherche des conclusions tirées de l’expérience de la Chine en matière de gouvernance environnementale au cours des deux dernières décennies afin de comprendre comment renforcer la gouvernance environnementale dans les économies émergentes. Le contexte général dans lequel la gouvernance environnementale de la Chine a été créée, les raisons de ses récentes intentions en faveur de l’environnement et ses caractéristiques uniques peuvent tous fournir des enseignements pour les autres pays émergents. Cette thèse montre aussi qu’il est difficile de reproduire l’expérience unique de la Chine sur le plan politique, économique et géographique en matière de gouvernance environnementale</dc:abstract><ual:supervisor>Sébastien Jodoin Pilon (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/1544bt728.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/qb98mk61s</ual:fedora3Handle><dc:subject>Law</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A5x21tk86m"><dcterms:title>Production and Characterization of Cardboard Biochar using a Commercial Gasifier</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Bioresource Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Fricke, Sebastian</ual:dissertant><dc:abstract>Biocharbon a été suggéré pour utilisation dans de nombreuses applications, allant de l’amendement du sol au stockage du carbone, en passant par l’utilisation dans les piles comme électrode. Par conséquent, le biocharbon doit être produit pour favoriser la formation de traits pour l’application spécifique. La recherche visait à produire et à caractériser physiquement, chimiquement et biologiquement un biocharbon produit à l’aide d’un système de microgazéification automatique (MAGS) fourni par Terragon Environmental Technologies, Montréal, Canada. Le mode opératoire du MAGS devait être modifié pour augmenter la teneur en carbone du produit carbonisé et être qualifié de biocharbon selon les normes de l’International Biochar Initiative (IBI) et du European Biochar Certificate (EBC). La production de biocharbon est survenue à 3 rapports d’équivalence (ER) de (0,15, 0,2 et 0,25). Le ER a été utilisé en raison de la nature couplée de la régulation de la température et de l’oxygène dans MAGS. Trois kilogrammes de carton déchiqueté ont été chargés dans le MAGS dans un sac en plastique et le débit d’air souhaité a été ajusté pour répondre aux exigences des urgences. Le temps de réponse lent des contrôleurs entraîne des difficultés pour atteindre ces ER et atteindre au lieu de 0,14, 0,17 et 0,20 respectivement. Le temps de résidence chaud était maintenu constant à 16 minutes et le temps de résidence du temps de recharge, à 20 heures. Le rendement en biocharbon diminue avec l’augmentation de ER. La teneur en cendres du ER ciblé = 0,2 a augmenté par rapport aux deux autres conditions de fonctionnement (57,76 % à la base sèche à @ 105 °C contre 29,85 % et 31,28 % pour le ER ciblé = 0,15 et 0,25). Cela pourrait être dû à un mauvais mélange dans la chambre de gazéification et à un mauvais contrôle des conditions de refroidissement (exposition extrême à la chaleur et à l’air entraînant une combustion spontanée de carbonisation). La teneur plus élevée en cendres de ce traitement a probablement contribué à une surface spécifique plus basse de 7,22 m2kg-1 comparés à 17,01 m2kg-1 et 22,70 m2kg-1 pour un ER ciblé de 0,15 et 0,25, respectivement. La caractérisation a permis de conclure que le biocharbon ne présentait aucun risque pour le sol, car les concentrations de métaux lourds étaient conformes aux directives IBI et EBC. Une étude sur la germination a été réalisée et rien n’indique que le biocharbon ait des effets phytotoxiques sur la germination de Lactuca sativa cv. Green Towers M. I. (laitue romaine). Le taux d’application de 3 % et 5 % en masse/masse avait un taux de germination, un poids sec et une longueur de pousse significativement plus élevés que 1 % (p = 0,05). Basé sur les résultats, le type de biocharbon utilisé (ER = 0,15, 0,2, 0,25) n’a pas eu d’effet statistiquement significatif sur ces paramètres de germination</dc:abstract><dc:abstract>Biochar has been suggested for use in many applications such as soil amendment, long-term carbon storage and use in batteries as an electrode. Certain applications require a particular set of traits thereby necessitating the careful selection of production parameters to meet the end user’s needs. This research aimed to produce biochar using a Micro Auto Gasification Systems (MAGS) provided by Terragon Environmental Technologies (Montreal, Canada)  then conduct physical, chemical and biological characterization. The MAGS’ operating procedure was adapted  to increase the carbon content of the charred product and qualify it as biochar, as per the International Biochar Initiative (IBI) and European Biochar Certificate (EBC) standards. The production of biochar occurred at three equivalence ratios (ER) of 0.15, 0.2 and 0.25. The ER was used due to the coupled nature of the temperature and oxygen control in MAGS. Approximately 3.0 kg of shredded cardboard was loaded into the MAGS and the desired airflow was adjusted to meet the ERs. Airflow controllers within the MAGS exhibited a slow response time thus caused difficulties in attaining the desired ERs, only reaching: 0.14, 0.17 and 0.20, respectively. The residence time was held constant at 16 minutes (min) and the cooldown time was held constant at 20 hours (h) . The biochar yield decreased with increasing ER. With an equivalence ratio of 0.2, the ash content spiked relative to the other two operating conditions (57.76% d.b. @ 105 °C compared to 29.85% and 31.28% for target ER = 0.15 and 0.25, respectively). This could be due to poor mixing in the gasification chamber and poor control of the cooldown conditions, whereby excess heat and air exposure could have caused spontaneous combustion of char. The higher ash content in this treatment likely contributed to the sample’s lower specific surface area of 7.22 m2kg-1, compared to 17.01 m2kg-1 and 22.70 m2kg-1 for target ER = 0.15 and 0.25, respectively. The heavy metal concentrations were within the IBI and EBC guidelines. A germination study was performed and no evidence was found to indicate that the biochar had phytotoxic effects on the germination of Lactuca sativa cv. Green Towers M.I. (romaine lettuce). The application rate of 3% and 5% mass/mass had statistically significantly higher germination rate, shoot dry mass and shoot length than 1% (p=0.05). Based on the results, the type of biochar (ER = 0.15, 0.2, 0.25) had no significant effect on the studied germination parameters. The biochar produced in this thesis is thus suitable for use as a soil amendment</dc:abstract><ual:supervisor>Shiv Prasher (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/xk81jr00c.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/5x21tk86m</ual:fedora3Handle><dc:subject>Bioresource Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ath83m377f"><dcterms:title>The role of cognitive reserve in protecting cognitive ability in people with HIV</dcterms:title><ual:graduationDate>2020</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>School of Physical and Occupational Therapy</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Kaur, Navaldeep</ual:dissertant><dc:abstract>The landscape of HIV-1 disease has radically improved with the contemporary anti-retroviral therapy. However, HIV-associated neurocognitive disorders (HAND) continue to be a challenge even among those with an optimal drug regime. Although neurocognitive deficits are typically of mild nature, the burden associated with HAND is an emerging public health concern due to unrelenting repercussions on everyday functioning including medication adherence, employment, and overall survival. Could people with HIV somehow protect themselves from cognitive impairment? Cognitive reserve is a theoretical entity which has been put forward to offset the deleterious impact of brain pathology on cognitive performance. This thesis investigated the impact of cognitive reserve on cognitive performance in individuals with HIV.The thesis commenced with a systematic review and a meta-analysis of the published literature (N=10) to estimate the strength of association between cognitive reserve and cognitive performance in HIV (Manuscript I). The association between the two constructs was found to be moderately strong. This work showed that discrepant indicators have been employed to operationalize cognitive reserve in HIV. It was found that participation in leisure and social activities was infrequently employed as an indicator of cognitive reserve in these HIV studies. This review produced a framework which hypothesized the mediating role of cognitive reserve on brain pathology and its consequences from the context of rehabilitation.The second manuscript quantified cognitive reserve into a single numeric value utilizing various indicators which have been proposed to build cognitive reserve. Pertinent data (including education, occupation, social network, number of spoken languages, and participation in other cognitively stimulating activities) were acquired from a Canadian longitudinal study (N=856) with HIV+ individuals to devise an index of cognitive reserve based on the differential impact of each indicator. This work involved a standard methodology where the regression coefficients for each contributing indicator were used in the scoring algorithm. A correlation coefficient of 0.3 between the developed index and a measure of cognitive performance at follow-up was expected given its hypothesized role as a mediator of cognitive performance rather than having a direct effect. Manuscript III documented the feasibility and efficacy potential of a 12-week combined aerobic and resistance training program in improving cognitive performance in HIV (N=27). This study was a subset of a cohort multiple randomized controlled trial which provided access to the participants who were eligible for the exercise program. As expected, physical performance measures showed improvements post-training. No positive influence was observed on the primary efficacy potential outcome of cognitive performance.Lastly, a systematic review was carried out to address some questions which surfaced at the outset of the aforementioned feasibility study. For instance, are pilot and feasibility studies the same? What should be the key objectives of a pilot study? What happens to a pilot study once it is completed? Does an effect size observed in a pilot study predict its follow-up? These questions were addressed based on 191 feasibility studies published in a specific rehabilitation journal since its inception. This work demonstrated that feasibility outcomes were often disregarded in the measurement strategy of these studies. Only a minor proportion of the studies got followed-up in a full-strength clinical trial and effect size did not drive this follow-up. This manuscript generated key areas of focus relevant to feasibility studies designed in rehabilitation research.</dc:abstract><dc:abstract>Le paysage de la maladie du VIH-1 s'est radicalement amélioré avec les thérapies antirétrovirales contemporaines. Cependant, les troubles neurocognitifs associés au VIH (HAND) continuent de poser problème, même parmi ceux qui suivent un régime de traitement optimal. Bien que les déficits neurocognitifs soient généralement légers, le fardeau associé au HAND est une préoccupation émergente pour la santé publique en raison des répercussions persistantes sur le fonctionnement quotidien, notamment l'adhérence au médicament, l'emploi et la survie globale. Les personnes vivant avec le VIH pourraient-elles se protéger d'une manière ou d'une autre contre les troubles cognitifs? La réserve cognitive est une entité théorique qui a été mise en avant pour compenser l’impact néfaste de la pathologie cérébrale sur les performances cognitives. Cette thèse portait sur l'impact des réserves cognitives sur les performances cognitives chez les personnes vivant avec le VIH.La thèse a débuté par une revue systématique et une méta-analyse de la littérature publiée (N=10) pour estimer la force de l’association entre réserve cognitive et performance cognitive chez les personnes atteintes du VIH (Manuscrit I). L’association entre les deux concepts s’est avérée modérément forte. Ces travaux ont montré que des indicateurs divergents ont été utilisés pour opérationnaliser la réserve cognitive liée au VIH. Il a été constaté que la participation aux loisirs et aux activités sociales était rarement utilisée comme indicateur de réserve cognitive dans ces études sur le VIH. Le deuxième manuscrit a quantifié la réserve cognitive en une valeur numérique unique en utilisant divers indicateurs proposés pour créer une réserve cognitive. Des données pertinentes (comprenant l’éducation, la profession, le réseau social, le nombre de langues parlées et la participation à d’autres activités stimulantes sur le plan cognitif) ont été obtenues à partir d’une étude longitudinale canadienne (N = 856) auprès de personnes séropositives afin de concevoir un indice de réserve cognitive basé sur l’impact différentiel de chaque indicateur. Ce travail a impliqué une méthodologie standard dans laquelle les coefficients de régression pour chaque indicateur de contribution ont été utilisés dans l'algorithme de notation. Un coefficient de corrélation de 0,3 entre l'indice développé et une mesure de la performance cognitive au suivi était attendu compte tenu de son rôle hypothétique de médiateur de la performance cognitive plutôt que d'avoir un effet direct. Le manuscrit III a documenté la faisabilité et le potentiel d'efficacité d'un programme combiné de 12 semaines d'aérobie et de musculation visant à améliorer les performances cognitives des personnes vivant avec le VIH (N = 27). Comme prévu, les mesures de performance physique ont montré des améliorations après l'entraînement. Aucune influence positive n’a été observée sur le principal critère d’efficacité de la performance cognitive.Enfin, une revue systématique a été réalisée pour répondre à certaines questions apparues au début de l’étude de faisabilité susmentionnée. Par exemple, les études pilotes et les études de faisabilité sont-elles identiques? Quels devraient être les objectifs principaux d'une étude pilote? Qu'advient-il d'une étude pilote une fois qu'elle est terminée? Est-ce que l’ampleur de l'effet observé dans une étude pilote prévoit sa suite? Ces questions ont été traitées sur la base de 191 études de faisabilité publiées depuis sa création dans un journal spécifiquement de réadaptation. Seule une petite proportion des études a fait l'objet d'un suivi dans le cadre d'un essai clinique complet et l'ampleur de l'effet n'a pas motivé ce suivi. Ce manuscrit a généré des domaines d’intervention clés pertinents pour les études de faisabilité conçues dans le cadre de la recherche en réadaptation</dc:abstract><ual:supervisor>Nancy Mayo (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/p2677094w.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/th83m377f</ual:fedora3Handle><dc:subject>Physical and Occupational Therapy</dc:subject></rdf:Description></rdf:RDF>