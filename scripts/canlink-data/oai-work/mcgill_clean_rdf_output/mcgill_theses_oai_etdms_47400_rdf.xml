<?xml version="1.0" encoding="UTF-8"?><rdf:RDF xmlns:oai="http://www.openarchives.org/OAI/2.0/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:ual="http://terms.library.ualberta.ca/" xmlns:bibo="http://purl.org/ontology/bibo/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:schema="https://schema.org/" xmlns:etdms="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A9c67wq684"><dcterms:title>The Raman effect.</dcterms:title><ual:graduationDate>1931</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Physics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Howlett, Leslie Ernest.</ual:dissertant><dc:abstract>On March 16, 1928, Sir C. V. Raman ( 1) announced, at a meeting of the south Indian Science Association, that if a transparent substance is illuminated by monochromatic light, new rays, differing in frequency from the incident beam, are to be found in the scattered radiation. This result was soon confirmed by other investigators (2) and it became evident that tbe phenomenon was a very general one. This in brief, is the effect which bears the name of the man who discovered it after years of painstaking work on the problem of light scattering by material media. [...]</dc:abstract><ual:supervisor>Eve, A. (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/fx719q088.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/9c67wq684</ual:fedora3Handle><dc:subject>Physics.</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A7p88ck51f"><dcterms:title>Economic relations of the Maritime provinces to Central Canada.</dcterms:title><ual:graduationDate>1931</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Department of Economics and Political Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Woods, H. D.</ual:dissertant><dc:abstract>In 1927 the people of Canada, with due pomp and ceremony celebrated the diamond jubilee of the union of the four provinoes of Nova Scotia, New Brunswick, Quebec, and Ontario into a new and greater community within the British Empire than had before existed overseas. Into that federation there have come since 1867 five other provinces, so that the young nation now stretches frem sea to sea. In many ways the prophecies and hopes that were expressed for the Dominion at its inception have come true; in some ways, unfortunately, the warnings of the pessimists who in 1867 foresaw ill in the scheme, have borne fruit. One of the most unfortunate circumstances found within the federation is the backwardness of the Maritime provinces. Certainly these provinces have not kept pace with the rest of the country. The outcome has been a series of unofficial mutterings, finally assuming the form of an official protest from the three little provinces for redress of supposed grievances attributed to Confederation. [...]</dc:abstract><ual:supervisor>Hemmeon, J. &amp; Goforth, W. (Supervisors)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/mg74qq102.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/7p88ck51f</ual:fedora3Handle><dc:subject>Maritime Provinces -- Economic conditions.</dc:subject><dc:subject>Economics &amp; Political Science.</dc:subject><dc:subject>Canada -- Economic conditions.</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ak930c121c"><dcterms:title>Agricultural credit in Western Canada.</dcterms:title><ual:graduationDate>1931</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Department of Economics and Political Science</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Willis, Stewart Wesley.</ual:dissertant><dc:abstract>The use of credit has increased rapidly in recent years. Whereas, formerly, credit was mainly extended for consumptive purposes to spendthrift heirs by professional money-lenders, its chief use, at present, is to finance productive enterprises. Banks and loan companies extending credit are thoroughly conversant with the needs and requirements of businessand their policy of giving loans only for legitimate purposes greatly reduces the hazard of non-payment. Unlike their predecessors, the present extenders of credit wish to make the burden of repayment as light as possible. Large financial institutions render valuable assistance and advice to their creditors in making their business ventures successful. [...]</dc:abstract><ual:supervisor>Culliton, J. (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/0r967657v.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/k930c121c</ual:fedora3Handle><dc:subject>Economics &amp; Political Science.</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Adn39x426w"><dcterms:title>The nature and properties of Aqueous solutions of hydrogen sulphide.</dcterms:title><ual:graduationDate>1931</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Chemistry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Wright, Robert Hamilton.</ual:dissertant><dc:abstract>That solutions exist is undeniable. why, or better still, how they exist, and the reasons for the variety and complexity of the phenomena they exhibit, are still largely obscure despite the most diverse researches. On the one hand, attempts have been made to adapt the beautifully exact methods of the kinetic theory of gases to the liquid state, but, in the face of experimental and mathematical difficulties, the attack has become limited in its objective to a qualitative picture based on the extrapolation of the properties of gas molecules. While undoubtedly a profitable and generally illuminating procedure, it cannot give other than the barest outline of the true nature of things. [...]</dc:abstract><ual:supervisor>Maass, O. (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/gq67jv11z.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/dn39x426w</ual:fedora3Handle><dc:subject>Chemistry.</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3An870zt47m"><dcterms:title>The petrology of some crystalline rocks of the Perth sheet, Ontario.</dcterms:title><ual:graduationDate>1931</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Geological Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Wykes, Eric Robinson.</ual:dissertant><dc:abstract>During the field season of 1930, the writer acted as an assistant on a Dominion Government Geological Survey party which was engaged in making a detail outcrop map of the Perth district covering parts of Lanark and Leeds counties, Southeastern Ontario. Towards the close of the season he was given the opportunity to collect specimens for detailed study. The award of the LeRoy Memorial Fellowship to the writer made it possible for him to carry on the laboratory work at McGill University during the last six months. [...]</dc:abstract><ual:supervisor>Osborne, F. (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/12579w12c.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/n870zt47m</ual:fedora3Handle><dc:subject>Geological Sciences.</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ap5547v374"><dcterms:title>Barriers to personalized medicine in pediatrics: The implementation of a novel pharmacogenomic test for pediatric neuroblastoma</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Family Medicine</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Crimi, Laura</ual:dissertant><dc:abstract>Contexte: Les tumeurs cérébrales primaires, en particulier d’ haute qualité astrocytome (HQA), sont l'une des principales causes de décès par cancer pour les enfants sous l'âge de 20. Une nouvelle analyse pharmacogénomique a été mis au point lorsque les enfants peuvent stratifier la diagnostique avec l'HQA pédiatrique. Ce nouveau analyse est bien placé d’être utilisé comme une norme de soins, on peut facilement identifier si oui ou non un enfant avec HQA est porteuse d'une mutation génétique qui est responsable de la résistance à tous les traitements disponibles. La connaissance de cette mutation va déplacer les patients pédiatriques directement dans les soins palliatifs; cela lui permet de les empêcher de recevoir les traitments severe et inutile. La médecine personnalisée en pédiatrie est souvent imbu en controverse; les obstacles pratiques et éthiques associés à ce test doivent être explorées avant la mise en œuvre.Objectif: Le but de cette enquête était d'évaluer les obstacles pertinents associés à l'utilisation de ce nouvelle PGx pharmacogénomique comme une norme de soins en oncologie pédiatrique clinique.Méthodes: Un design de méthodes mixtes intégré a été utilisé pour évaluer les obstacles anticipés perçus par les utilisateurs finaux de l'essai pharmacogénomique, incluant les professionnels de la santé travaillant dans les deux soins palliatifs pédiatriques ou l'oncologie pédiatrique. Consultations des parties prenantes délibératifs ont été utilisés pour étudier les obstacles associés à ce test pharmacogénomique, les intervenants ont été invités à diffuser l'utilisation de ce nouvelle analyse pharmacogénomique comme une norme de soins. Délibérations de qualité sont uniques en ce qu'ils favorisent un espace de discussion ouvert, avec un objectif de générer des échanges et un dialogue riche avec les parties prenantes informées. Les délibérations ont été suivis par une évaluation quantitative en utilisant un outil destiné à évaluer la survenance de la délibération et de mesurer le succès de la délibération. Des consultations ont été enregistrées et transcrites, analyse thématique a été menée et la sortie délibérative généré a été évaluée.Cadre: L'étude a eu lieu à l'Hôpital de Montréal pour enfants à Montréal, Québec.Participants: Les participants recrutés pour délibération incluant utilisateurs finaux de l'essai, y compris les oncologues pédiatriques et pédiatriques médecins en soins palliatifs pédiatriques.Résultats: Une meilleure compréhension des obstacles liés à l'utilisation de ce nouvelle analyse pharmacogénomique a été atteint. Les obstacles à la mise en œuvre pertinents identifiés par les intervenants: le rôle des soins palliatifs dans la gestion des patients, la communication, l'impact du test sur les soins et l'existence de cultures conflictuelles des soins entre l'oncologie pédiatrique et les soins palliatifs. Plusieurs thèmes mineurs ont également été identifiés, notamment: la nécessité d'une formation pour éviter l'expérience de l'abandon du patient, difficultés pour maintenir l'espoir, fournissant un soutien inter-professionnelle et les médias comme barrière. Les résultats de l'enquête quantitative auto-administré corroborées résultats qualitatifs et ont montré que la délibération a eu lieu. Sortie délibérante a également été généré et il a été conclu qu'une réunion hebdomadaire pré-clinique conjointe entre l'oncologie pédiatrique et les soins palliatifs pédiatriques faciliterait l'introduction de ce analyse comme une norme de soins en oncologie pédiatrique pour les patients atteints HQA.Conclusions: L'étude identifie des nouvelles barrières qui existent lorsque la mise en œuvre avec ce ne nouveau analyse pharmacogénomique, capable de fournir un diagnostic terminal, comme une norme de soins en oncologie pédiatrique clinique. Il est à espérer que ce cadre pour l’exploration de l'utilisation d'un analyse pharmacogénomique PGx dans les soins cliniques peut être généralisé et utilisé pour d'autres tests.</dc:abstract><dc:abstract>Background: Primary brain tumours, specifically high-grade astrocytoma (HGA), are one of the leading causes of death from cancer in children under the age of 20. A novel laboratory derived pharmacogenomic (PGx) test has been developed and is well placed for use as a standard of care. The test is able diagnostically stratify the disease, it can readily identify whether or not a child is a carrier of a genetic mutation causing resistance to all available curative treatments. Knowledge of this mutation will move pediatric patients directly into palliative care; doing so will prevent the child from receiving harsh, ineffective treatments. Personalized medicine in pediatrics is often met with controversy; the practical and ethical barriers associated with this test must be explored prior to implementation. Objective: The purpose of the investigation was to evaluate relevant barriers associated with the use of the novel PGx test as a standard of care in clinical pediatric oncology. Methods: A mixed methods embedded design was used to explore the barriers perceived by the end users of the novel pharmacogenomic test; end users included healthcare professionals working in either pediatric palliative care or pediatric oncology. Deliberative stakeholder consultations were used to explore barriers associated with the test. Stakeholder deliberations are unique in that they promote a space for open discussion, with a goal of generating meaningful exchange and rich dialogue with informed stakeholders. The deliberations were followed by quantitative assessment using a tool aimed at evaluating the occurrence of deliberation and to measure the deliberation success. Consultations were recorded and a thematic analysis was conducted. Setting: The study took place at Montreal Children’s Hospital in Montreal, Quebec.  Participants: End users of the test, including: pediatric oncologists, pediatric palliative care physicians, pediatric palliative care nurses, pediatric oncology nurses, bioethicists and a social worker, were recruited as stakeholders to participate in the deliberations. Results: A better understanding of the barriers surrounding the use of this novel PGx test was attained. Relevant barriers to implementation identified by stakeholders included: the role of palliative care in patient management, communication, the impact of the test on care and the existence of conflicting cultures of care between pediatric oncology and palliative care. Several minor themes were also identified, including: the need for training to prevent the patient’s experience of abandonment, difficulties with maintaining hope, providing inter-professional support and media as barrier. Results from the self-administered quantitative survey corroborated qualitative results showing that deliberation occurred; deliberative output was generated and it was concluded that a joint pre-clinic weekly meeting between pediatric oncology and pediatric palliative care would facilitate the introduction of this test as a standard of care. Conclusions: The study identified barriers that exist when implementing a pharmacogenomic test, capable of delivering a terminal diagnosis, as a standard of care in clinical pediatric oncology. It is hoped that this framework for exploring the implementation of a PGx test in clinical care can be generalized and used for other tests.</dc:abstract><ual:supervisor>Gillian Bartlett-Esquilant (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/9k41zh30p.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/p5547v374</ual:fedora3Handle><dc:subject>Family Medicine</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Am039k8011"><dcterms:title>The CLEAR toolkit pilot study: an educational intervention for helping health workers address the social causes of poor health</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Family Medicine</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Naz, Anila</ual:dissertant><dc:abstract>INTRODUCTIONSocial determinants of health are widely recognized as root causes of health disparities between and within countries. Health care workers can play a significant role to improve the health of individuals and populations if provided proper training on how to address the social causes of poor health in day-to-day clinical practice. The objectives of this study were: (1) To determine whether the CLEAR toolkit, a clinical decision-aid designed to help frontline health workers ask about and act upon the social determinants of health, is considered useful and applicable to health workers at a large university-affiliated family medicine teaching centre serving a highly ethnically diverse population in inner city Montreal, (2) To elicit what changes should be made to this toolkit to make it even more useful for frontline health workers, (3) To understand how the toolkit should be adapted to the local setting and how to train health workers to use it in practice, and (4) To better understand the kind of organizational support available to frontline health workers to address the social causes of poor health of their patients, and what more could be done to further support health workers in taking on a social determinants of health approach. METHODSWe conducted a multi-method study involving: (1) an online survey of frontline health workers to assess current practices and collect feedback on the feasibility of using the CLEAR toolkit in clinical practice, (2) in-depth interviews to understand why health workers consider certain patients to be more vulnerable and how to best help such patients, (3) focus groups to explore barriers to asking about social determinants of health during routine clinical practice, and (4) key informant interviews with high-level administrators to identify organizational levers for promoting widespread change in health workers’ practices. RESULTSOf the 100 health workers surveyed, fifty health workers responded to the questionnaire (Response Rate 50%). We continued the in-depth interviews until data saturation was reached (15 in-depth interviews). We conducted two focus groups of 6-8 health workers in each group, and three key informant interviews with senior health administrators. There was a high level of agreement that it is the role of frontline health workers to address the underlying social issues that are the root causes of their patient’s health problems (n=44/50, 88%,). The majority of health workers found the CLEAR toolkit easy to understand (n=36/37, 97.3%), relevant to their work (n=33/37, 89.2%), and can help them to address the social causes of poor health (n=32/37, 86.5%). Health workers who already had specific ways of asking their patients about social issues were twice as likely to report having helped their patients with social issues (n=15/16, 93.7%, vs. n=9/17, 52.9%; p=0.003). Organizational barriers to asking about the social determinants of health in clinical practice included lack of role modeling, training and time. Facilitators for adopting a social determinants of health approach included having access to clinical practice tools and a short list of local referral resources.CONCLUSIONSFrontline health workers appreciate the value of taking action to address the social determinants of health. However, there is a need to provide health workers more education, training and organizational support. The CLEAR toolkit has the potential to contribute to reducing health disparities by training the frontline health workers to ask about and act upon the social causes of poor health.</dc:abstract><dc:abstract>INTRODUCTIONLes travailleurs de la santé peuvent jouer un rôle important pour améliorer la santé des individus et des populations s’ils reçoivent une formation adéquate sur la façon d'aborder les déterminants sociaux de la santé dans leur pratique clinique courante. Les objectifs de cette étude étaient: (1) de déterminer si la boîte à outils CLEAR, un outil d’aide à la décision clinique sur les déterminants sociaux de la santé, est considéré comme utile et applicable par des travailleurs de la santé provenant d’un grand centre universitaire d'enseignement de la médecine familiale servant une population ethnique diversifiée dans la région de Montréal, (2) d’identifier les changements qui devraient être apportés à cette boîte à outils pour la rendre encore plus utile pour les travailleurs de la santé de première ligne, (3) de comprendre comment la boîte à outils doit être adaptée au contexte local et comment former les travailleurs de la santé pour l'utiliser dans leur pratique courante, et (4) de mieux comprendre le soutien organisationnel mis à la disposition des travailleurs de la santé de première ligne dans leur pratique courante pour mieux soutenir leurs patients à agir sur les déterminants sociaux.METHODE  Nous avons mené une étude multi-méthode incluant: (1) un sondage en ligne avec des travailleurs de la santé de première ligne afin d'évaluer les pratiques actuelles et de recueillir des commentaires sur la faisabilité de l'utilisation de la boîte à outils CLEAR dans la pratique clinique, (2) des entrevues en profondeur pour comprendre pourquoi les travailleurs de la santé considèrent certains patients comme plus vulnérables et comment les aider, (3) des groupes de discussion pour explorer les obstacles pour aborder les déterminants sociaux de la santé dans leur pratique clinique courante, et (4) des entretiens avec des informateurs-clés (cadres supérieurs) afin d'identifier les leviers organisationnels pour promouvoir un changement dans la pratique des travailleurs de la santé.RESULTATSSur les 100 travailleurs de la santé interrogés, 50 ont répondu au questionnaire (taux de réponse : 50%). Les entretiens en profondeur ont été effectués jusqu'à saturation de données (n=15). Nous avons effectué deux groupes de discussion avec respectivement 6 et 8 travailleurs de la santé dans chaque groupe, et trois entretiens avec des cadres supérieurs de la santé. Il y avait un niveau d'accord élevé que les travailleurs de la santé de première ligne devraient assumer la tâche  d’identifier les déterminants sociaux qui causent des problèmes de santé à leurs patients (n=44/50, 88%). La majorité des travailleurs de la santé ont trouvé la boîte à outils CLEAR facile à comprendre (n=36/37, 97,3%), pertinente à leur travail (n=33/37, 89,2%), et utile pour aborder les déterminants sociaux de la santé (n=32/37, 86,5%). Les travailleurs de la santé qui posaient des questions sur les déterminants sociaux étaient deux fois plus susceptibles d’indiquer avoir aidé leurs patients avec des problèmes sociaux (n=15/16, 93,7%, par rapport à n=9/17, 52,9%; valeur p = 0,003). Les obstacles organisationnels à poser des questions sur les déterminants sociaux de santé dans la pratique clinique incluent l’absence de modèles, la formation et le temps. Les facilitateurs de l'adoption de déterminants sociaux dans la démarche de santé incluent avoir accès à des outils de pratique clinique et à une courte liste de ressources  locales.CONCLUSION Les travailleurs de la santé de première ligne reconnaissent l’importance d’aborder les déterminants sociaux de la santé. Cependant, il y a un besoin de fournir aux travailleurs de la santé plus d'éducation, de formation et de soutien organisationnel. La boîte à outils CLEAR peut contribuer à réduire les disparités en santé par la formation des travailleurs de la santé de première ligne pour agir sur les déterminants sociaux de la santé.</dc:abstract><ual:supervisor>Anne Adina Andermann (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/v692t923s.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/m039k8011</ual:fedora3Handle><dc:subject>Family Medicine</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Apc289m89f"><dcterms:title>Modeling surface runoff and subsurface tile drainage under regular drainage and controlled drainage with sub- irrigation in Southern Ontario</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Bioresource Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Lu, Cheng</ual:dissertant><dc:abstract>Controlled drainage with subirrigation has been applied as a strategy in southern Ontario to mitigate nutrient loss from subsurface drained cropland to surface water bodies.   The Root Zone Water Quality Model (RZWQM2) has been widely used for simulating management effects on crop production and soil and water quality, and a subirrigation component was recently developed. The objective of this study is to model surface runoff, subsurface tile drainage, and crop yield under two water management practices: regular drainage (DR) and controlled drainage with subirrigation (CDS) in southern Ontario. Field observed hydrological and yield data under those two water management practices near Harrow, ON from June 2008 to December 2011 were used to evaluate RZWQM2.  The measured surface and subsurface water discharges were monitored continuously year round in a corn-soybean rotation field. Subirrigation was not measured but was estimated assuming it met the daily crop ET computed by the model. RZWQM2 was calibrated and validated against tile drainage and yield data from regular drainage and controlled drainage with subirrigation, respectively.  For the calibration against runoff and tile drainage data under regular drainage, percent bias (PBIAS) was within ±15%, Nash-Sutcliffe efficiency (NSE) &gt; 0.50, and index of agreement (IoA) &gt; 0.80; however, for the validation under the controlled drainage with subirrigation, PBIAS &gt;±15%, NSE &lt; 0.22, and IoA &lt; 0.78. This RZWQM2 was capable of predicting tile drainage and surface runoff under the regular drainage, but was not as precise for the controlled drainage with subirrigation treatment. This may be attributable to a poor estimation of sub-irrigation amount as model input.</dc:abstract><dc:abstract>Le drainage contrôlé avec sous-irrigation a été appliqué en tant qu’une stratégie au sud d’Ontario afin de mitiger la perde des éléments nutritifs, des terres agricoles avec un système drainage souterrain aux eaux pluviales. Le modèle de qualité de l’eau de la zone de racine  (RZWQM2) a été utilisé à grand échelle pour simuler les effets de la gestion sur la production de culture ainsi que la qualité de l’eau et le sol, et un composant de sous-irrigation a été récemment développé.  L’objectif de cette étude est de modeler le ruissellement, le drainage souterrain et le rendement de culture sous l’effet de deux pratiques de gestion d’eau tel que le drainage régulier et le drainage contrôlé avec sous-irrigation au sud d’Ontario. Les données hydrologiques et de rendement observées et ramassées sur le terrain du Juin 2008 au Décembre 2011 pour un champ du maïs-soja sous ceux deux pratiques de gestion d’eau à proximité de Harrow, ON, ont été utilisées afin d’évaluer le RZWQM2.  Les débits mesurés de l’eau en surface et de l’eau souterraine, ont été surveillés en continu toute au longe de l’année. La sous-irrigation n’était pas mesurée mais elle était estimée en supposant que l’évapotranspiration journalière estimée a été atteint par le modèle. Le RZWQM2 a été calibré and validé contre les données du drainage et du rendement respectivement par le drainage régulier et le drainage contrôlé avec sous-irrigation.  Pour le calibrage contre les données du ruissellement et drainage sous le drainage régulier, le pourcentage de biais (PBIAS) était entre ±15%, le rendement de Nash-Sutcliffe (NSE) était plus de 0.5 et l’indice d’accordance (IOA) était plus de 0.80. Cependant, pour la validation sous le drainage contrôlé avec sous-irrigation, le PBIAS était au-delà de ±15%, NSE &lt; 0.22, et IOA &lt; 0.78. Cette étude de modélisation a fait prouve que le RZWQM2 est capable de prédire le drainage et le ruissellement sous le drainage régulier, mais ce n’est pas aussi précise pour le drainage contrôlé avec sous-irrigation. La raison pourrait être une estimation incorrecte du teneur de sous-irrigation comme la donnée du modèle.  </dc:abstract><ual:supervisor>Zhiming Qi (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/4f16c551c.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/pc289m89f</ual:fedora3Handle><dc:subject>Bioresource Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A5x21tj29s"><dcterms:title>Improving physical methods of cell permeabilization by reducing plasma membrane stiffness in HeLa cells</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Chemical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Kanzo, Sarah</ual:dissertant><dc:abstract>Lorsque des forces mécaniques sont appliquées sur une cellule, un transfert de gènes peut s’effectuer amenant à une perméabilité de la membrane sur un temps limité. C’est ainsi que  l’acide nucléique ou autres macromolécules peuvent pénétrer l’entité cellulaire dans le cadre d’une thérapie génique. Un transport moléculaire naturel ne pourrait pas s’effectuer sans aide additionnelle extérieure.La perméabilité de la cellule peut être contrôlée par plusieurs paramètres, notamment le temps de l’opération, la fréquence et l’amplitude de la force appliquée. Cette dernière est déterminée grâce à la connaissance du comportement viscoélastique et non-linéaire de la membrane. Les  propriétés mécaniques jouent donc un rôle important dans le processus de perméabilité cellulaire.La relation entre la rigidité de la membrane et le transfert des gènes a été étudiée en modifiant l’environnement de la cellule et en les soumettant à un jet de gaz inerte provoquant une rupture temporaire. Cette nouvelle méthode mécanique permet le transfert des gènes en appliquant des forces de cisaillement sur la membrane, causant ainsi des micropores qui serviront aux macromolécules de pénétrer à l’intérieur de la cellule. Plusieurs débits d’écoulement de gaz ont été utilisés avec trois tubes capillaires en verre de différentes tailles.Les résultats montrent que la modification de la rigidité du substrat, sur lequel les cellules sont cultivées, et  l’incorporation des médicaments de cellules médicamenteuses altérant la rigidité de la membrane plasmique, modifient significativement le taux de perméabilisation.</dc:abstract><dc:abstract>Physical methods of gene transfer seek to make the cell permeable by creating temporary imperfections in the cell membrane with physical forces. These imperfections allow nucleic acid and other macromolecules to be transferred into the cell for which would not naturally transport across the cell membrane. The cell’s plasma membrane is a  non-linear viscoelastic structure that can disrupt and reseal itself depending on the time scale, frequency and amplitude of force applied. Therefore the mechanical properties of the cell and how much stress it can absorb are important for optimizing and predicting the forces needed to cause temporary permeability of the membrane. The relationship between cell stiffness and cell permeability was explored by altering the cell stiffness with drugs and substrate stiffness. A physical method that involves the use of an inert jet that imparts shear stress on the membrane was used to permeabilize the cells. Tuning of jet dynamics to cell stiffness may improve the efficiency of this physical method of gene transfer. Results show that changing the rigidity of the substrate the cells are grown on and the incorporation of stiffness altering drugs significantly modify the permeabilization efficiency.</dc:abstract><ual:supervisor>Richard L Leask (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/bn999949p.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/5x21tj29s</ual:fedora3Handle><dc:subject>Chemical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Atm70mz451"><dcterms:title>Linking deformation and diffusion to develop a strain speedometer</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Earth and Planetary Sciences</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Barshi, Naomi</ual:dissertant><dc:abstract>Je présente la première étude qui évalue en détail l’effet  de la déformation sur l’augmentation des taux de diffusion des éléments dans la plagioclase grâce à une combinaison d’analyses théoriques et physiques  à diverses échelles spatiales. En  théorie, la déformation et la diffusion sont des processus qui requièrent que des liaisons atomiques soient rompues et reformées et dépendent donc d’un apport d’énergie. La déformation peut toutefois augmenter la vitesse de diffusion par  l’addition d’énergie élastique,  facilitant la rupture des liaisons atomiques et la mobilisation des dislocations qui favorisent la migration atomique. Les géochronomètres et géothermobaromètres sont des outils couramment appliqués sans inclure  l’effet de la déformation sur la  diffusion atomique, qui peut diminuer les températures de fermeture et remettre au zéro les horloges isotopiques. Des valeurs erronées de température et pression peuvent résulter de calculs basés sur le postulat d’une diffusion strictement statique ou négligeable sous les conditions estimées de temperature et pressions estimées.Si l’augmentation de la mobilité des éléments par déformation est mesurable, nous pouvons en principe en corriger les effets sur les calculs de température et de pression. Le lien entre déformation et diffusion a également une dépendence temporelle qui nous permet de déterminer le taux de déformation des roches. Evaluer ce taux est essential à notre compréhension de la déformation crustale mais il est rarement possible de le déduire directement à partir des observations de terrain. Pour évaluer l'augmentation des taux de diffusion due à la déformation d’un minéral crustal commun, j’ai combiné levés de terrain et analyses en lames minces pour décrire la déformation et la composition chimique (éléments  majeurs et en trace) de phénocristaux de plagioclase provenant du pluton San José, dans le batholite Peninsular Ranges au Mexique. J’ai développé une méthodologie me permettant de comparer le degré de déformation subi par ces grains individuels à celui de la roche qui les contient. J’ai évalué la déformation des grains par leur changement de forme et la mobilité de leurs éléments par la variation spatiale de leur composition chimique,  mesurée à haute résolution par microsonde électronique et par spectrométrie de masse couplée à un plasma inductif avec prélèvement par ablation laser. Etant donné les modestes différences de composition observes entre les zones de croissance primaires des phénocristaux, mon modèle prédit que l’effet de la faible déformation subie par ces roches sur les taux de diffusion d’éléments dans la plagioclase est trop subtil pour être détecté par les techniques analytiques disponibles. Sous les conditions de déformation inférées pour ces roches, un modèle de diffusion statique est donc suffisant et les résultats d’outils comme les géothermobaromètres et géochronomètres sont valides. Il serait intéressant dans le futur d’explorer les relations entre divers facteurs, dont l’histoire thermique, le taux de déformation et la composition initiale de roches crustales, qui sont susceptibles d’influencer l’effet de la déformation sur la mobilité des éléments.</dc:abstract><dc:abstract>I present the first comprehensive study of strain-enhanced diffusion in plagioclase through a multiscale, theoretical and physical approach. First I argue from first principles that diffusion and deformation are both processes of breaking and reforming bonds that take energy in order to proceed. Deformation can enhance diffusion by supplying strain energy that facilitates breaking bonds and by mobilizing dislocations, which can act as pathways for atomic migration. Widely-used geologic tools such as geochronometers and geothermobarometers neglect deformation-enhanced diffusion, which may lower closure temperatures and reset ages, temperatures, and pressures, leading to erroneous values calculated based on static diffusion or no diffusion after conditions of interest. If deformation enhancement of element mobility is measurable, we may be able to calculate and account for the effects of deformation on these calculations. The connection between deformation and diffusion also provides an opportunity to link deformation with a time-dependent process and infer strain rate. Strain rate is an integral parameter in understanding crustal deformation and cannot be directly determined from the rock record in most cases. To evaluate strain enhanced diffusion in a common rock forming mineral, I combined field-scale, thin-section scale, and grain-scale strain and chemical measurements for major and trace elements in naturally deformed plagioclase phenocrysts from the San José Pluton, Peninsular Ranges Batholith, México. The multiscale approach made it possible to connect grain-scale strain with bulk-rock deformation through methods I developed. I used high spatial resolution, sensitive, in situ analyses using electron probe micro-analysis (EPMA) and Laser Ablation Inductively Coupled Plasma Mass Spectrometry (LA-ICP-MS) to correlate changes in shape (strain) and chemical composition (element mobility). I conclude that at low strain and low compositional contrast between primary growth zones measured, the effect of strain-enhanced element mobility, as predicted from my model, is too subtle to quantify using available analytical techniques. This means that for such a set of conditions, a static diffusion model is sufficient, and geothermobarometers and geochronometers can still be applied to yield accurate results. Future work should explore the relationships between different variables affecting strain enhanced element mobility, such as thermal history, strain, strain rate, and starting compositions, to refine the applicability of existing tools and the potential for new ones.</dc:abstract><ual:supervisor>Vincent Johan van Hinsberg (Internal/Cosupervisor2)</ual:supervisor><ual:supervisor>Christen Danielle Rowe (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/0z709068v.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/tm70mz451</ual:fedora3Handle><dc:subject>Earth and Planetary Sciences</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A3t945t77p"><dcterms:title>On computing optimal thresholds for sequential hypothesis testing problem</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Cui, Can</ual:dissertant><dc:abstract>Sequential hypothesis testing problem, after firstly being successfully used in the second world war, has developed its own research field and been applied to many other fields. How to numerically solve this problem and develop an automatic decision strategy has become an academic topic since 1950's. For centralized sequential hypothesis testing, the threshold-based structure is well developed, and for decentralized sequential hypothesis testing, there has been a lot of recent progress in understanding the structure of optimal control strategies, but very little is known about computational methods in both cases.  In this work, we looked at different methods to numerically find the optimal stopping rule for centralized and decentralized sequential hypothesis testing problem. We first implemented the classic Sondik's algorithm. Then, we seek approximation methods to simplify the process and improve the efficiency. To do this, we first introduced zeroth-order and first-order discretization methods to discretize the continuous state. After discretizing the state, we propose two main approaches to find the optimal threshold: one is value iteration, the other is policy evaluation combined with non-convex optimization function. For policy evaluation, we tried three approximation methods: Monte Carlo sampling, asymptotic expression and absorption probability of Markov chain. The performance of these methods are compared. Finally, for decentralized hypothesis testing problem, we choose one approximation method and compare the person-by-person optimal strategy with the optimal strategy based on global search.</dc:abstract><dc:abstract>Le problème de test d'hypothèse séquentielle, après avoir tout d'abord été utilisé avec succès dans la seconde guerre mondiale, a développé dans son propre domaine de recherche et a été appliquée à de nombreux autres domaines. Comment résoudre numériquement un problème de ce type et élaborer une stratégie automatique de décision est devenu un sujet académique depuis 1950. Pour les test d'hypothèse séquentielle centralisées, la structure à seuil est bien développé, et pour les tests d'hypothèse séquentielle décentralisées, il y a eu beaucoup de progrès récents dans la compréhension de la structure des stratégies de contrôle optimales, mais on sait très peu sur les méthodes de calcul dans les deux cas. Dans ce travail, nous avons examiné de différentes méthodes pour trouver numériquement la règle d'arrêt optimale pour le problème de test d'hypothèse séquentielle centralisée et décentralisée. Nous avons d'abord mis en œuvre l'algorithme classique de Sendik. Ensuite, nous cherchons des méthodes d'approximation pour simplifier le processus et améliorer l'efficacité. Pour ce faire, nous avons introduit des méthodes de discrétisation d'ordre zéro et de premier ordre pour discrétiser l'état continu. Après avoir discrétiser l'état, nous proposons deux approches principales pour trouver le seuil optimal: l'itération de valeur, et l’évaluation stratégique, combiné à une fonction d'optimisation non convexe. Pour l'évaluation stratégique, nous avons essayé trois méthodes d'approximation: l’ échantillonnage Monte Carlo, l’expression asymptotique et la probabilité d'absorption de chaîne Markov. La performance de ces méthodes sont comparées. Enfin, pour le problème de test d'hypothèse décentralisée, nous choisissons une méthode d'approximation et nous comparons la stratégie optimale personne par personne avec la stratégie optimale en fonction de recherche globale.</dc:abstract><ual:supervisor>Mahajan, Aditya (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/n009w5215.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/3t945t77p</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A5q47rr744"><dcterms:title>Characterization of the air flow in a scale model of a hydrogenerator by means of particle image velocimetry</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Mechanical Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Bach, Emmanuel</ual:dissertant><dc:abstract>Au sein des installations hydro-électriques, les alternateurs sont des composants essentiels et génèrent de la chaleur par pertes. Le moyen le plus courant d’évacuer cette chaleur est de faire circuler un fluide refroidissant (en règle générale de l’air) dans les différents composants de l’alternateur. En raison de sa complexité géométrique, il est très difficile de simuler cet écoulement afin de prédire l’efficacité du refroidissement. Par ailleurs, les mesures in situ sont coûteuses et difficiles à réaliser en raison de l’accès limité. Pour ces raisons, un modèle à l’échelle 1:4 d’un alternateur hydro-électrique a été réalisé à l’institut de recherche d’Hydro-Québec (IREQ). Dans ce mémoire, nous allons présenter des mesures par Vélocimétrie par Image de Particules (PIV) dans les fosses alternateurs, dans l’espace entre les sorties radiateurs et la paroi de l’enceinte, à la sortie du radiateur, dans les capots ainsi que dans l’entrefer et l’interpôle. Les aspects experimentaux de l’ensemencement, de la calibration et de la méthode de mesure ainsi que les aspects théoriques de la PIV sont étudiés. Par ailleurs, une comparaison avec des résultats de simulation CFD ("Computational Fluid Dynamics") réalisée à l’aide  d’ANSYS-CFX est également présentée. La sensibilité de la simulation aux modifications géométriques a été mise en avant grâce aux mesures réalisées dans la fosse alternateur. Les mesures conduites dans l’espace entre la sortie radiateur et la paroi de l’enceinte ainsi que dans l’entrefer et l’interpôle ont permis de valider qualitativement la CFD. Enfin, le calcul du débit d’air massique à travers la sortie radiateur et dans les capots ont permis de valider quantitativement les résultats de la simulation.</dc:abstract><dc:abstract>In hydroelectric power plants, generators are essential components and, like all machines, generate heat due to losses. The most common way to evacuate this heat is by circulating a cooling fluid (generally air) through the generator components. Due to their geometrical complexity, it is quite challenging to numerically simulate the flow to predict the cooling in a generator. Furthermore, in situ measurements are costly and difficult to perform due to the limited access. For this reason, a 1:4 scale model of a hydroelectric generator was built at the research institute of Hydro-Quebec (IREQ). In this thesis, particle image velocimetry (PIV) measurements of the flow in the opening of the generator pit, in the space between the enclosure wall and the cooler exit of the scale model, at the cooler exit, in the covers, and in theair gap and interpole region are presented. Experimental aspects pertaining to the seeding of the flow, calibration targets, experimental method and PIV theory are also discussed. Furthermore, a comparison of the experimental data with the results of CFD (Computational Fluid Dynamics) simulations using ANSYS-CFX is given. The results in the pit opening region have shown the sensibility of the simulation results to small modifications to the geometry. The measurements in the space between the cooler exit and the enclosure wall and those in the air gap and interpole region have qualitatively validated the CFD. Finally, computation of the mass flow rate through the cooler exit and in the cover has also quantitatively validated the simulation results.</dc:abstract><ual:supervisor>Laurent B Mydlarski (Internal/Supervisor)</ual:supervisor><ual:supervisor>Federico Torriano (Internal/Cosupervisor2)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/3197xp55f.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/5q47rr744</ual:fedora3Handle><dc:subject>Mechanical Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Abc386n438"><dcterms:title>Hidden sector dark matter</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Physics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Choquette, Jeremie</ual:dissertant><dc:abstract>Hidden sector dark matter models provide a possible solution to several key problems in the study of dark matter related to its structure and distribution on galactic and sub-galactic scales. This thesis is the combined result of two research projects related to hidden sector dark matter. The first stud- ies dark matter freeze-out in a hidden sector, showing that in some cases the standard approach to freeze-out is insufficient. This is due to the propensity of a dark sector to resist decreases in temperature upon freeze-out, slowing the process and thereby relaxing the bounds on hidden sector dark matter self-interaction cross sections from freeze-out. The second research project involves constructing a complete model of hidden sector dark matter with an additional SU(2) gauge group to explore the possibility of dark matter as vector boson bound states.</dc:abstract><dc:abstract>Les modèles de matière noire du secteur caché fournissent une solution possible à plusieurs problèmes clés dans l’étude de la matière noire liée à sa structure et à sa répartition sur des échelles galactiques et sous-galactiques. Cette thèse est le résultat combiné de deux projets de recherche liés à la matière noire du secteur caché. La première section étudie le découplage de la matiére noire dans un secteur caché, montrant que dans certains cas, l’approche standard de découplage est insuffisan étant donné que le secteur sombre peut se réchauffer pendant le découplage. Ceci ralentit le processus et détend ainsi les bornes sur les sections d’auto-interaction dans le secteur caché. Le deuxiéme projet de recherche consiste à construire un modèle complet de matière noire du secteur caché avec un groupe de jauge supplémentaire SU(2). Cette section sert a explorer la possibilité d’états liés de bosons vecteurs.</dc:abstract><ual:supervisor>James M Cline (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/76537391v.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/bc386n438</ual:fedora3Handle><dc:subject>Physics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aqn59q7014"><dcterms:title>Heat transfer characterization during reciprocating agitation thermal processing of canned particulates in a Newtonian fluid</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Food Science and Agricultural Chemistry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Pratap Singh, Anubhav</ual:dissertant><dc:abstract>Une nouvelle méthode d’agitation à mouvement alternatif a été investiguée pour améliorer le transfert de chaleur au cours du traitement thermique d’un fluide Newtonien (glycérine) contenant des matières particulaires. Dans un premier temps, un système d’agitation à mouvement alternatif à petite échelle a été développé en modifiant un système conventionnel pour y ajouter un mécanisme réciproque dont la fréquence d’agitation est de 0 à 5 Hz et l’amplitude est de 0 à 30 cm. Une étude de la distribution des températures à l’intérieur de conserves traitée par agitation à mouvement alternatif révéla une meilleure uniformité des températures par rapport au témoin. L’agitation réciproque a augmenté U et hfp d’un facteur de 2 à 7, contribuant à une réduction de 52 à 87% du temps d’équilibre au point froid. Ceci a pour effet une réduction de 46 à 62% et par conséquent une réduction de 26 à 36% de l’index de perte de qualité (valeur de cuisson / léthalité) par rapport à un procédé conventionnel sans agitation. Il a ainsi été observé que l’agitation peut produire des aliments de bonne qualité.  L’effet de l’orientation de la conserve sur le transfert de chaleur au cours du traitement thermique à agitation à mouvement alternatif a ensuite été étudié en plaçant la conserve sur l’une de trois axes: horizontalement selon l’axe de mouvement de va-et-vient (HA); horizontalement perpendiculaire à l’axe de mouvement de va-et-vient (HP); et verticalement (V). L’orientation HA produit le transfert de chaleur le plus rapide, suivi par les orientations HP et V. L’influence de plusieurs variables de traitement (température d’opération, fréquence de mouvement, amplitude de mouvement, espace de tête, viscosité/concentration du liquide, densité des particules &amp; concentration des particules) sur U et hfp a été estime pour deux cas: i) une conserve contenant une seule particule (entièrement liquide) et ii) une conserve contenant plusieurs particules.  Par la suite, une étude d’optimisation multiobjective des coefficients de transfert de chaleur (U et hfp pour maximiser le transfert de chaleur) et de l’intensité d’agitation (RI pour minimiser les pertes d’agitation) a été effectuée. Un RI élevé (37 à 45 ms-2) maximise U et hfp alors qu’un RI plus faible (16 à 19 ms-2) permet l’optimisation simultané des paramètres U, hfp et RI. Les conditions optimales pour des températures, viscosités, concentrations particulaires et densités particulaires ont également été obtenes. Des corrélations sans dimension ont ensuite été développées pour la modélisation prédictive et changement d’échelle du processus thermique en utilisant des régressions non-linéaires des groupes significatifs sans dimension. La dimension de la conserve selon l’axe de mouvement d’agitation joue un rôle important dans le phénomène de transfert de chaleur et s’inclut dans la longueur caractéristique servant au calcul du nombre de Nusselt. En comparant avec la littérature, il est clair que les effets de convection forcée et de la turbulence durant l’action d’agitation par mouvement alternatif était plus important que les autres modes d’agitation rotatoire. Toutefois, l’effet de la convection est non-négligeable pour une RI faible. Enfin, une étude de visualisation du mouvement des fluides a permis la caractérisation du mouvement des particules et des interactions liquide-particule par enregistrement vidéo de conserves transparentes sous traitements différents. Le mouvement des particules s’est révélé très différent pour des orientations et densités de particule différentes. Le temps d’agitation était affecté par la fréquence, par la viscosité du liquide, par l’orientation de la conserve, par la concentration des particules et par la densité des particules.</dc:abstract><dc:abstract>A novel method of reciprocating agitation for enhancing heat transfer during thermal processing of canned particulates in Newtonian fluid (glycerin) was investigated in this study. First, a lab-scale reciprocating agitation retort was developed by modifying an existing conventional still retort to include a reciprocating mechanism providing reciprocations at a frequency of 0-5 Hz and amplitude of 0-30 cm. Temperature distribution studies inside the modified retort and inside the reciprocating cans revealed better temperature uniformity during reciprocating agitation as compared to still mode. Reciprocating agitation resulted in 2-7 times enhancement in U &amp; hfp leading to 52-87% reduction in equilibration time of the cold spot. This enabled 46-62% reduction in process time resulting in 26-36% reduction in quality-deterioration index (cook-value/lethality) as compared to conventional still mode of thermal processing. Thus, reciprocating agitation was shown to have potential to deliver high quality products.The effect of container orientation on heat transfer during reciprocation agitation thermal processing was then studied by placing experimental cans in one of the three possible orientations viz. horizontally along axis of reciprocation (HA), horizontally perpendicular to axis of reciprocation (HP) &amp; vertically (V). HA orientation provided most rapid heat transfer followed by HP and V orientations, respectively. Influence of various process variables (operating temperature, reciprocation frequency, reciprocation amplitude, container headspace, liquid viscosity/concentration, particle density &amp; particle concentration) on U and hfp were estimated for two situations: i) cans filled with single particle (liquid-only situation) and ii) cans filled with multiple particles. Based on the results of these studies, simultaneous multi-objective optimization of heat transfer coefficients (U &amp; hfp - to maximize heat transfer) and reciprocation intensity (RI - to minimize agitation losses) was conducted. High RI (37-45 ms-2) was recommended to maximize U &amp; hfp, whereas lower RI (16-19 ms-2) was found optimal for simultaneous optimization of U, hfp &amp; RI. Optimal conditions were also reported for different operating temperatures, liquid viscosities, particle concentrations and particle densities.Dimensionless correlations were then developed for predictive modeling and scale-up of reciprocating agitation thermal process using multiple non-linear regressions of significant dimensionless groups. Dimension of can along the axis of reciprocation was found to play a dominant role in the heat transfer phenomenon and was included in the characteristic length formulated for Nusselt number at can wall (from U). On comparing results of this study with literature, it was clear that forced convection effects and turbulence levels during reciprocating agitation were significantly larger than other modes of rotary agitation. Yet, effect of natural convection could not be ignored at lower RI.Finally, flow visualization studies were carried out to characterize particle motion/mixing behavior of liquid/particle mixtures by videotaping particle motion/mixing in transparent containers under various conditions. The nature of mixing and particle motion was found markedly different for different orientations and particle densities. Mixing time was affected by frequency, liquid viscosity, container orientation, particle concentration and particle density.</dc:abstract><ual:supervisor>Hosahalli Ramaswamy (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/3x816q39h.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/qn59q7014</ual:fedora3Handle><dc:subject>Food Science and Agricultural Chemistry</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ac821gn866"><dcterms:title>Assessing mean diffusivity in grey matter regions in a sample of fibromyalgia patients</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Integrated Program in Neuroscience</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Feier, Natasha</ual:dissertant><dc:abstract>Fibromyalgia is a widespread pain disorder lacking detectable pathology making it extremely difficult to diagnose and treat. Recently, researchers have turned to neuroimaging in order to search for morphological changes in the brain related to fibromyalgia. These magnetic resonance imaging studies have revealed widespread gray matter decreases in a variety of brain regions thought to be involved in pain processing. A previous study from our group used voxel-based morphometry to characterize these gray matter decreases in several prefrontal areas, including the middle frontal gyrus, the anterior cingulate cortex, and the premotor cortex in a sample of 28 female fibromyalgia patients compared to 28 matched controls. In this study, we aimed to further examine the nature of these gray matter decreases by assessing mean diffusivity in the same prefrontal regions where our group found decreases. The posterior cingulate cortex and anterior insula were also assessed because these regions have shown volumetric decreases in previous fibromyalgia studies. The lateral occipital cortex served as a control region due to the fact there are no reports of changes in this region in the literature. Mean diffusivity is characterized from diffusion images and represents a measure of average water diffusion, most commonly examined in white matter. We developed a technique that allowed the assessment of mean diffusivity in gray matter regions. This technique prioritized the alignment of diffusion and anatomical images to ensure that measurements were only in gray matter and were not corrupted by cerebrospinal fluid. Mean diffusivity was increased in the premotor cortex and anterior insula, indicating that tissue density within these regions is decreased. Decreases in mean diffusivity were observed in the posterior cingulate cortex and lateral occipital cortex, indicating an increase in tissue density despite no changes in volume. Because diffusion changes were detected in regions lacking a volumetric change, the suggestion is that diffusion measurements may be more sensitive measures to gray matter alterations than volumetric measurements alone. In addition, changes in mean diffusivity were correlated across many of these pain relevant regions in patients only, suggesting that diffusion alterations may be a consequence of continuous nociceptive input in chronic pain. Additionally, measures of body mass index were related to diffusivity measurements in the anterior cingulate cortex in healthy controls and the middle frontal gyrus in patients, indicating the fact that behavioral variables may also have a role in the observed diffusivity changes. The study of gray matter diffusion is a valuable first step in understanding the underlying mechanisms that contribute to volumetric changes and chronic pain symptoms in fibromyalgia.</dc:abstract><dc:abstract>La fibromyalgie est un trouble de la douleur largement répandu manquant d’une pathologie détectable le rendant difficile a diagnostiquer et a soigner. Récemment, des chercheurs se sont tournes vers la neuroimagerie pour trouver des changements morphologiques dans le cerveau lies à la fibromyalgie. Ces études d’images a résonance magnétique ont révèle de larges diminutions de matière grise dans une variété de régions du cerveau considère implique dans le traitement de la douleur. Une étude précédente de notre groupe utilisait la morphometrie par contours pour caractériser ces diminutions de matière grise dans plusieurs zones pre-frontal, y compris dans le gyrus frontal du milieu, la cortex cingulate antérieur, et le cortex pre-moteur dans un échantillon de 28 patientes atteintes de fibromyalgie comparées a 28 patients assortis.Dans cette étude, nous cherchons a examiner la nature de ces diminutions de matière grise en évaluant la diffusion moyenne dans les mêmes régions pre-frontales ou notre groupe trouva des diminutions. Le cortex cingulate postérieur et l’insula antérieur étaient aussi évalue car ces régions ont montre des diminutions volumétriques dans de précédentes études sur la fibromyalgie. Le cortex occipital latéral sert comme région de contrôle grâce au fait qu’il n’y a aucun changement dans cette région selon différentes recherches.La diffusion moyenne est caractérisée depuis les images de diffusion et représente la mesure de la diffusion moyenne de l’eau, plus communément vu dans la matière blanche. Nous avons développe une technique qui permet la validation de la diffusion moyenne dans les régions de matière grise. Cette technique priorise l’alignement de la diffusion et des images anatomiques pour assurer que les mesures étaient seulement en matière grise et n'étaient pas corrompues par le fluide cérébro-spinal. La diffusion moyenne augmentait dans le cortex pre-moteur et l’insula antérieur, indiquant que la densité des tissus à l'intérieur de ces régions est diminuée. Diminutions en diffusion moyenne étaient observées dans le cortex cingulate postérieur et le cortex occipital latéral indiquant une augmentation de la densité de tissus malgré aucun changement de volume. Parce que ces changement de diffusion étaient détectes dans des régions sans changement volumétrique, il est suggère que les mesures de diffusion peuvent être plus sensible aux altérations de matière grise que des changements volumétriques seuls. De plus, les changements en diffusion moyenne étaient corrélés à travers nombreuses régions du cerveau liées à la douleur chez les patients seulement, suggérant que les altérations de diffusion peuvent être une conséquence de l’apport nociceptif continue en douleur chronique. En outre, des mesures de l’index de masse corporelle étaient relatif aux mesures de diffusions dans le cortex cingulate antérieur chez les patients témoins et le gyrus frontal du milieu chez les patients, indiquant le fait que des variables comportementales peuvent aussi avoir un rôle dans les observations de changement de diffusion. L'étude de la diffusion dans la matière grise est une première étape importante dans la compréhension des mécanismes sous-jacent qui contribuent aux changements volumétriques et aux symptômes de douleur chronique dans la fibromyalgie.</dc:abstract><ual:supervisor>Petra Schweinhardt (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/1831cn723.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/c821gn866</ual:fedora3Handle><dc:subject>Neuroscience</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Adv13zx312"><dcterms:title>The functional interplay between eIF2alpha phosphorylation and mTOR signaling pathways: implications in Tuberous Sclerosis Complex disorder</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Medicine</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Tenkerian, Clara</ual:dissertant><dc:abstract>The mammalian target of rapamycin (mTOR) nucleates two complexes, namely mTOR complex 1 (mTORC1) and mTORC2, which are implicated in cell growth, survival, metabolism and cancer. The phosphorylation of the alpha subunit of the eukaryotic initiation factor eIF2 at serine 51 (eIF2αP) is a key regulator of mRNA translation and an important mechanism of adaptation of cells to various forms of environmental stress frequently associated with cancer formation. eIF2αP can act either as a promoter of cell survival or as an inducer of cell death in response to different forms of stress. Increased eIF2αP is mediated by a family of four kinases consisting of PKR, PERK, GCN2 and HRI, each of which becomes activated by distinct stimuli. In this study, we show that disruption of mTORC2, but not of mTORC1, induces eIF2αP through the activation of PERK. mTORC2 deficiency increases PERK activity owing to the ineffecive activation of AKT, which negatively controls PERK by phosphorylation at threonine 799. Moreover, pharmacological inhibition of mTOR with either rapamycin or the new generation of catalytic inhibitors also increases PERK activity and eIF2αP. Interestingly, rapamycin treatment induces eIF2αS51P through a mechanism that is independent of mTORC1 inhibition.The physiological relevance of our findings was substantiated in cells deficient in tuberous sclerosis complex (TSC), which have impaired mTORC2/AKT function, but increased mTORC1 activity. Our research shows that TSC-deficient cells exposed to ER stress exhibit increased levels of the PERK-eIF2αP arm, which functions as a compensatory mechanism to substitute for the loss of AKT and facilitate cell survival. TSC-deficient cells subjected to oxidative stress on the other hand, downregulate PERK-eIF2αP but activate the PKR-eIF2αP arm instead in an mTORC1-S6K1-mediated mechanism to promote cell death. Furthermore, we show that TSC-null cells deficient in eIF2αP have a greater tolerance to oxidative stress, leading to an increase in their tumorigenic potential and therefore allowing an earlier tumor incidence. Our study reveals that eIF2αP acts downstream of either mTORC2-AKT or mTORC1-S6K1 to promote either the survival or death of TSC-mutant cells in response to different stress-inducing drugs.</dc:abstract><dc:abstract>La cible de la rapamycine chez les mammifères (mTOR) forme deux complexes, à savoir les complexes mTOR 1 (mTORC1) et 2 (mTORC2), lesquels sont impliqués dans la prolifération cellulaire, la survie, le métabolisme et le cancer. La phosphorylation de la sous-unité alpha du facteur d’initiation eucaryote eIF2 au niveau de la sérine 51 (eIF2αP) est un régulateur essentiel de la traduction des ARNm et un mécanisme important de l’adaptation des cellules face aux diverses formes de stress environnementaux fréquemment associés à la formation du cancer. eIF2αP peut agir comme promoteur de la survie cellulaire ou bien comme inducteur de la mort cellulaire en réponse aux différentes formes de stress. L’augmentation d’eIF2αP est médiée par une famille de quatre kinases constituée par PKR, PERK, GCN2 et HRI, chacune d’entre elles étant activée par des stimuli distincts. Dans cette étude, nous montrons que la perte de mTORC2, et non pas de mTORC1, induit eIF2αP via l’activation de PERK. L’absence de mTORC2 augmente l’activité de PERK en raison de l’activation insuffisante d’AKT, lequel contrôle négativement PERK par phosphorylation à la thréonine 799. De plus, l’inhibition pharmacologique de mTOR par la rapamycine ou par la nouvelle génération d’inhibiteurs catalytiques augmente aussi l’activité de PERK et eIF2αP. Fait intéressant, le traitement à la rapamycine induit eIF2αS51P par le biais d’un mécanisme indépendant de l’inhibition de mTORC1.La pertinence physiologique de nos résultats a été mise en évidence dans les cellules déficientes en sclérose tubéreuse complexe (TSC), lesquelles ont une fonction altérée de mTORC2/AKT mais qui présentent une augmentation de l’activité de mTORC1. Nos recherches montrent que les cellules déficientes en TSC exposées à un stress du réticulum endoplasmique (ER) augmentent les niveaux de la voie PERK-eIF2αP qui sert alors de mécanisme compensatoire pour remplacer la perte d’AKT et faciliter la survie cellulaire. D’autre part, les cellules déficientes en TSC assujetties à un stress oxydatif diminuent la voie PERK-eIF2αP mais active par contre celle de PKR-eIF2αP par un mécanisme médié par mTORC1-S6K1 pour promouvoir la mort cellulaire. Par ailleurs, nous montrons que les cellules TSC-nulles déficientes en eIF2αP ont une plus grande tolérance au stress oxydatif, ce qui conduit à augmenter leur potentiel tumorigène et donc permettre une incidence plus précoce des tumeurs. Notre étude révèle qu’eIF2αP agit en aval de mTORC2-AKT ou de mTORC1-S6K1 pour promouvoir la survie ou la mort des cellules déficientes en TSC en réponse aux différents traitements induisant le stress.</dc:abstract><ual:supervisor>Antonis E Koromilas (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/8623j186v.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/dv13zx312</ual:fedora3Handle><dc:subject>Medicine</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3At722hc72w"><dcterms:title>Bounding the heat trace of a Calabi Yau manifold</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Department of Physics</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Fiset, Marc-Antoine</ual:dissertant><dc:abstract>La borne SCHOK offre un contrôle sur le nombre d'opérateurs marginaux de certaines théories conformes des champs bidimensionnelles en termes des opérateurs pertinents suffisamment légers. Lorsqu'interprété dans le contexte de la théorie des cordes, ce résultat suggère une avenue qui pourrait permettre de borner supérieurement le nombre de Hodge total h11+h21 uniformément sur les types topologiques de 3-variétés compactes de Calabi-Yau. Une telle borne serait significative en physique, car elle suggèrerait la finitude de la collection des vides de la théorie des cordes. Il est soutenu que l'existence d'une borne sur la trace du noyau de chaleur scalaire sur les variétés de Calabi-Yau est le prochain jalon majeur à investiguer. L'expansion asymptotique standard de cette trace indique que les singularités de courbure à même la variété pourraient empêcher l'existence d'une borne. Une étude de cas dans cette direction est effectuée: la dégénérescence d'une région dans la variété compacte assimilée au conifold résolu. La conclusion majeure est que la limite vers le conifold singulier est vraisemblablement continue au niveau du spectre, ce qui implique que de telles régions de courbure ne sont, en fait, pas problématiques vis-à-vis l'existence d'une borne. Cette conclusion est atteinte suite à un traitement analytique du problème aux valeurs propres scalaire sur le conifold résolu et à une analyse asymptotique des fonctions spectrales.</dc:abstract><dc:abstract>The SCHOK bound gives control on the number of marginal operators in certain 2-dimensional conformal field theories in terms of light enough relevant operators. Interpreted in the context of string theory, this result suggests a route that could be used to bound from above the total Hodge number h11+h21 uniformly over topological types of compact Calabi-Yau 3-folds. Such a bound would be relevant in physics as it would suggest finiteness of the landscape of string vacua. The major remaining issue is argued to be the existence of an upper bound on the trace of the scalar heat kernel on Calabi-Yau manifolds. The standard asymptotic expansion of this trace indicates that curvature singularities in the manifold might preclude the existence of a bound. As a case study in this direction, the degeneration of a resolved conifold-like patch in a compact manifold is investigated. It is argued that the limit to the singular conifold is continuous at the spectral level, meaning that such regions of extreme curvature are actually not problematic in bounding attempts. This conclusion is reached after analytic treatment of the scalar eigenvalue problem on the resolved conifold as well as an asymptotic analysis of the spectral functions.</dc:abstract><ual:supervisor>Johannes Walcher (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/d504rp11r.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/t722hc72w</ual:fedora3Handle><dc:subject>Physics</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aj3860976m"><dcterms:title>Full-duplex wireless communications: Capacity analysis, low-density parity-check coding and feasibility</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Engineering</schema:inSupportOf><dc:contributor>Department of Electrical and Computer Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Fu, Tong</ual:dissertant><dc:abstract>Nowadays wireless frequency spectrum becomes more crowded and expensive due to emergence of data intensive wireless communication applications. Consequently, it is imperative to introduce new technologies that can enhance both spectrum efficiency and data transmission rates. Recently, full-duplex wireless transmission has emerged as one such promising technology that enables data transmission and reception in the same frequency band simultaneously. Full-duplex wireless transmission can potentially double the data rate and spectrum usage efficiency compared to existing frequency-division duplex (FDD) and time-division duplex (TDD) technologies, which use two separate frequency bands (or time slots) for up-link and down-link data transmission, respectively. However, in full-duplex transmission, due to the omnidirectional propagation of wireless signals, the received signal is corrupted by self-interference caused by the reflections of transmitted signal from the same transceiver. In wireless systems, the self-interference is orders-of-magnitude stronger than the desired received signal that is attenuated during transmission. This makes full-duplex wireless systems more susceptible to self-interference than wire-line systems. This thesis explores the achievable data rates and potential applications of full-duplex wireless transmission systems. This thesis first gives a detailed literature review of recent results of full-duplex wireless communication. It then discusses the three self-interference cancellation methods proposed for full-duplex wireless systems, namely antenna cancellation, analog cancellation and digital cancellation. For each of the three cancellation methods, the thesis summarizes the strengths, shortcomings and achievements of the cancellation methods from the key papers. The second part of this thesis introduces a specific, practically relevant full-duplex wireless communication system model from literature. Using this model it analyzes the capacity limits of full-duplex transmission in different scenarios. The analysis shows that good analog cancellation is essential to achieve capacity gains over traditional half-duplex wireless systems. Digital cancellation can further improve the performance after good analog cancellation, but has limited effects when analog cancellation is ineffective. This section also presents the MATLAB implementation and bit error rate simulation results of the considered full-duplex system model that uses LDPC codes and OFDM modulation. The simulation results prove that error control coding will play a key role in any future full-duplex wireless communication system. The last part of this thesis gives a system-level feasibility study of full-duplex transmission in existing cellular networks. It shows that full-duplex transmission works best in balanced up-link/down-link traffic, which means the up-link and down-link data volumes are about equal. However, the majority of today's wireless cellular networks have data traffic that is severely biased towards down-link transmission. Under such conditions, the benefits of full-duplex transmission are severely limited. Nevertheless, in relay networks, wireless back-haul transmission and some multiple-input multiple-output (MIMO) systems with balanced up-link/down-link traffic, full-duplex transmission technology can be advantageous to traditionally used half-duplex transmission technology.</dc:abstract><dc:abstract>Aujourd'hui spectre de fréquences sans fil devient plus encombré et coûteux en raison de l'émergence d'applications de communication sans fil. Par conséquent, il est impératif d'introduire de nouvelles technologies qui peuvent améliorer les taux de transmission de données d'efficacité et de spectre. Récemment, la transmission sans fil full-duplex a émergé comme une telle technologie prometteuse qui permet la transmission de données et la réception dans la même bande de fréquence simultanément.  La liaison full-duplex sans fil peut potentiellement doubler le débit de données et de l'utilisation du spectre d'efficacité par rapport aux technologies de duplex existants division de fréquence duplex et temporelles, qui utilisent deux bandes distinctes de fréquence (ou intervalles de temp) pour la transmission de données de liaison montante et de liaison descendante respectivement. Cependant, dans la transmission en duplex intégral, en raison de la propagation de signal sans fil, le signal reçu est corrompu par des interférences provoquées par les réflexions des signaux transmis à partir du même émetteur-récepteur. Dans les systèmes sans fil, l'auto-interférence est beaucoup plus fort que le signal reçu souhaité. Cela rend les systèmes sans fil full-duplex plus sensibles aux interférences que les systèmes filaires. Cette thèse explore les débits de données réalisables et applications potentielles des systèmes full-duplex transmission sans fil. Cette thèse donne d'abord une revue de la littérature détaillée des résultats récents en communication sans fil full-duplex. Il examine ensuite les trois méthodes d'annulation d'interférence proposées pour les systèmes full-duplex sans fil, à savoir l'annulation antenne, l’annulation analogique et l’annulation numérique. Pour chacune des trois méthodes d'annulation, la thèse résume les points forts, les lacunes et les réalisations des méthodes des documents clés d'annulation. La deuxième partie de cette thèse présente un modèle full-duplex pratiquement spécifique pertinente système de communication sans fil de la littérature. En utilisant ce modèle, il analyse les limites de capacité de transmission full-duplex dans différents scénarios. L'analyse montre que bon annulation analogique est essentiel de réaliser des gains de capacité plus de systèmes sans fil traditionnels half-duplex. Annulation numérique peut améliorer davantage la performance après un bon annulation analogique, mais a des effets limités lorsque l'annulation analogique est inefficace. Cette section présente également la mise en œuvre et de simulation de taux d'erreur binaire MATLAB résultats du modèle de système full-duplex considéré que utilise des codes LDPC et la modulation OFDM. Les résultats de simulation prouvent que le codage de contrôle d'erreur va jouer un rôle clé dans tout futur système de communication sans fil full-duplex. La dernière partie de cette thèse donne une étude de la transmission full-duplex dans les réseaux cellulaires existants faisabilité au niveau du système. Il montre que la transmission full-duplex qui fonctionne le mieux dans le trafic de liaison montante / liaison descendante équilibrée, ce qui signifie la liaison montante et de liaison descendante volumes de données sont à peu près égale. Cependant, la majorité des réseaux cellulaires sans fil d'aujourd'hui ont le trafic de données qui est sévèrement biaisé vers la transmission de liaison descendante. Dans de telles conditions, les avantages de la transmission en duplex sont très limitées. Néanmoins, dans les réseaux de relais, transmission arrière-courriers sans fil et un multiple-input multiple-output (MIMO) systèmes avec liaison montante équilibrée trafic / liaison descendante, la technologie full-duplex transmission peut être avantageux d'traditionnellement utilisé la technologie half-duplex transmission.</dc:abstract><ual:supervisor>Jan Bajcsy (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/9s161912n.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/j3860976m</ual:fedora3Handle><dc:subject>Electrical and Computer Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A1z40kw736"><dcterms:title>Perception of Timbre Intervals</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Schulich School of Music</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Wood, Christopher</ual:dissertant><dc:abstract>An experiment tested participants’ ability to recognize and reproduce timbre intervals. Listeners performed an analogy completion task in which they were presented with a change in either attack time or spectral centroid (A:B interval) and were instructed to adjust tone D in a second interval (C:D) until the two intervals were perceptually equivalent in terms of both magnitude and direction. The two intervals were related by transposition, where point A was transposedto C and D was a point selected by participants using a slider. There were two possible interval sizes (small and large) and three possible sizes of transpositions (small, medium, and large).Both intervals and transpositions were either upward or downward. Configurations of these directions were categorized as either congruent (both in the same direction) or incongruent (opposite directions). Responses in the spectral-centroid block showed significantly higher accuracy and consensus among participants than did the responses in the attack-time block. Listeners were likely to correctly reproduce interval directions in their responses, an effect that increased in large interval conditions and incongruent conditions. Participants responded with smaller than expected C:D intervals in congruent conditions and larger than expected intervals in incongruent conditions, suggesting a tendency to be biased in the direction of point B. This effect was seen in both blocks, but to a greater extent in the attack-time block. The effect increased with transposition size (i.e., distance from B to D), though less reliably for conditions with downward interval directions. Similarities found between trends and interactions in both blocks suggest that the same cognitive functions are used in the completion of each task, regardless of dimension.</dc:abstract><dc:abstract>Nous avons testé la capacité des participants à reconnaître et à reproduire des intervalles de timbre. Les auditeurs effectue une tâche d’achèvement d’analogie dans laquelle on leur présente un changement du temps d’attaque ou du centre de gravité spectral formant un intervalle A:B; ils doivent ajuster le deuxième son d’un autre intervalle C:D jusqu’à ce que les deux intervalles soient équivalents en grandeur et en direction du changement. Le deuxième intervalle est censé être une transposition du premier: le point A est transposé à C et l’auditeur doit trouver le point D approprié en ajustant un curseur. Il y a deux grandeurs d’intervalle (petite et grande) et trois grandeurs de transposition (petite, moyenne, grande). Les intervalles et les transpositions pouvaient être ascendants ou descendants. Les configurations de ces directions sont catégorisées comme congruentes (les deux dans la même direction) ou incongrues (directions opposées). Les réponses pour le centre de gravité spectral présentent une précision et consensus significativement plus élevés que celles pour le temps d’attaque. Les auditeurs reproduisent souvent la bonne direction de l’intervalle, un effet qui augmente dans les conditions avec des grandes intervalles et dans les conditions incongrues. Les participants répondent avec des intervalles C:D plus petits que prévus dans les conditions congruentes et plus grands que prévus dans les conditions incongrues, suggérant ainsi un biais vers le point B dans les réponses. Cet effet se produit pour les deux dimensions, mais est plus grand pour le temps d’attaque. Cet effet augmente également avec la grandeur de transposition (c’est-à-dire la distance entre le point B et le point D idéal), bien que de façon moins fiable pour les intervalles descendants. Des ressemblances trouvées entre les tendances et les interactions pour les deux dimensions du timbre suggèrent que les même fonctions cognitives sont en jeu dans la réalisation de la tâche, indépendamment de la dimension testée.</dc:abstract><ual:supervisor>Stephen McAdams (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/nz8062919.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/1z40kw736</ual:fedora3Handle><dc:subject>Music</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ah415pd464"><dcterms:title>Musical engagements with technology: analytical explorations of spectral mixed music</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Arts</schema:inSupportOf><dc:contributor>Schulich School of Music</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Friesen, Lesley</ual:dissertant><dc:abstract>Ce mémoire est une enquête sur les interactions entre les composantes acoustiques etélectroniques dans les pièces de la musique mixte, et comment ces interactions peuvent être à l’origine des nouveaux effets de perception ainsi que des dialectiques sémantiquement riches. Dans ces cas, la technologie n’est pas utilisée seulement pour produire les sons, néanmoins son incorporation est intégrale à la formation du sens musical. Les pièces mixtes qui font partie de la tradition spectrale (crée par Gérard Grisey et Tristan Murail dans les années 1970) sont particulièrement aptes pour ces enquêtes grâce à une préoccupation esthétique, élaborant ainsi les sons musicaux ambiguës exploitant les limites des méchanismes de la perception. Deux pièces qui sont spectrales et mixtes, "Tombeau de Messiaen" (1994) par Jonathan Harvey pour piano et bande électronique, et "The Texture of Time" (2006) par Joshua Fineberg pour flûte et dispositif électronique, fournissent deux études analytiques sur les effets de perception et du sens musical des interactions entre les sons acoustiques et électroniques. Les analyses incorporent une méthodologie diverse, incluant à la fois des techniques poïétiques (concernant le processus de création) et d’esthésiques (concernant le processus de réception), dont le dernier est plus rare dans la littérature savante sur la musique mixte. Je soutiens que les revendications analytiques esthésiques, lorsqu’elles sont basées sur les théories de la perception, peuvent aider à élucider le potentiel de la musique mixte à créer des nouvelles perceptions et de nouveaux sens musicaux.</dc:abstract><dc:abstract>This thesis is an examination of how the interactions between acoustic and electronic components in mixed music pieces can be the source of both novel perceptual effects in listeners as well as semantically rich dialectics. In such instances, technology is not used solely as a means of sound production but its incorporation is crucial to the shaping of musical meaning. Mixed works of the spectralist tradition, pioneered by Gérard Grisey and Tristan Murail in the 1970s, are especially well-suited to such investigations because of an aesthetic interest in creating ambiguous musical sounds that exploit certain limitations of the human auditory system’s perceptual mechanisms. Two spectral mixed works, Jonathan Harvey’s "Tombeau de Messiaen" (1994) for piano and tape, and Joshua Fineberg’s "The Texture of Time" (2006) for flute and electronics, form the basis of two analytical case studies on the perceptual and semantic effects of acoustic and electronic interactions in live music. The analyses incorporate a diverse methodology, including both poietic (creation-based) and esthesic (reception-based) approaches, the latter of which is more rare in scholarly literature on mixed music. I argue that esthesic analytical claims, when rooted in theories of music perception, can help to elucidate the perceptual effects of mixed works as well as new musical meanings created by these effects.</dc:abstract><ual:supervisor>Robert Hasegawa (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/8623j1874.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/h415pd464</ual:fedora3Handle><dc:subject>Music</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3A3t945t818"><dcterms:title>The role of the locus coeruleus norepinephrine system in mediating resilience to social defeat stress</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Master of Science</schema:inSupportOf><dc:contributor>Integrated Program in Neuroscience</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Perret, Lea</ual:dissertant><dc:abstract>In the last decade the mouse social defeat model has allowed research on the pathophysiology of a chronic stress induced depression. This model further allowed the understanding of the molecular mechanisms underlying susceptibility and resilience to chronic stress. The ventral tegmental area dopamine pathways have been established as determining factors of resilience and susceptibility to the social defeat stress. In this study, we focus on the locus coeruleus norepinephrine system (LC-NE) as a potential facilitator of resilience through a hypothesized direct influence on the ventral tegmental  area dopaminergic system (VTA-DA). Our research combines behavioral, molecular, and electrophysiological techniques to suggest a pro-resilience role of NE transmission from the LC onto the VTA-DA system. Increased NE transmission induced resilience while decreased NE transmission was linked to susceptibility after a chronic stress. These results further validate the hypothesis of LC-NE system hypoactivation in pathological anxiety and depression conditions after a chronic stress.</dc:abstract><dc:abstract>Depuis les dix dernières années, le modèle de défaite sociale chez la souris a permit d’étudier la pathophysiologie de la dépression en réponse à un stress chronique. Ce modèle a également permit une investigation plus ample des mécanismes moléculaires sous jacents aux comportements de susceptibilité et de résilience en réponse à un stress chronique. Les voies dopaminergiques de l’aire tegmentale ventrale (AVT) ont été établies comme ayant un role déterminant dans la résilience et la susceptibilité au stress social chronique. Ce projet consiste à définir le système noradrenergique du locus coeruleus (LC) comme facilitateur de la résilience au stress par son influence sur l’AVT. Cette recherche combine des approaches comportementales, moléculaires, et électrophysiologiques dans le but de démontrer le role pro-résilient de la transmission noradrénergique du LC sur l’AVT. Plus spécifiquement, une hausse de la transmission noradrénergique induit la résilience, tandis qu’une absence de transmission noradrénergique est liée à la susceptibilité après un stress social chronique. Ces résultats valident l’hypothèse d’une hypoactivation du système noradrénergique du LC dans la pathophysiologie de la dépression après un stress chronique.</dc:abstract><ual:supervisor>Bruno Giros (Internal/Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/3f4628253.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/3t945t818</ual:fedora3Handle><dc:subject>Neuroscience</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Akk91fp669"><dcterms:title>Nanocrystalline cellulose: Surface chemical transformations and applications</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Chemistry</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Morse, Timothy</ual:dissertant><dc:abstract>Nanocrystalline cellulose (NCC) is a crystalline derivative of biomass. Its physical and chemical properties have made it the focus of intense research in the disciplines of science and engineering. This thesis explores a number of aspects that link the two disciplines. We introduce a new method to make NCC that avoids hydrolysis with concentrated sulfuric acid, which is widely used in the current industrial process to produce sulfated NCC. Moreover, our new process introduces a carboxylic acid group to the surface. Compared with sulfated NCC, carboxylate offers an improved "handle" for chemical transformations at the surface of NCC. Accordingly, we show that hydrogen peroxide, with or without ultra-violet light activation, can convert biomass directly into carboxylated NCC. With a theoretical by-product of water, the reaction has a better "green quotient" than does the current industrial process. In a series of chemical transformations, the carboxylic acid group is converted to link β-cyclodextrin to the surface of the nanocrystallites. This modification is then used to create cyclodextrin-based host–guest supramolecular nanoparticles of cellulose. Surface transformations are reduced in complexity to a one-step procedure that binds a guest adamantyl derivative in the hydrophobic interior of the cyclodextrin. Thus surface modifications of NCC can be conducted "off line" at the bench where adamantane is derivatized. This "plug and play" supramolecular chemistry and other elaborations at the NCC surface are illustrated in several instances in order to improve and expand the design of NCC: fluorescence probes are used study solvent interactions at the NCC surface through the solvatochromic effect; polarized fluorescence emission is shown to reveal information about the structural order in NCC in the region of the photonic crystal stop band; water diffusion is used to adjust the stop band by tuning the chiral nematic pitch of NCC; trafficking of NCC in live cells can be followed by fluorescence imaging; spin resonance from an organic free radical spin probe (adamantyl-TEMPO) establishes some of the dynamical properties associated with the guest-host supramolecular platform.   Polymer nanocomposites based on NCC were prepared. Real-time infrared spectroscopy was used to follow photopolymerization as a function of NCC loading. The transparent nanocomposites show changes in mechanical and thermal properties that are consistently related to the NCC content in the polymer host. The propensity of NCC to self-associate to form a chiral nematic liquid crystal phase is exploited to make a polymer composite in which the ordered texture of NCC is photo-locked in place. A technique was developed to obtain individual nanoparticle dispersions of positively charged NCC in water. Binding of poly(diallyldimethyl ammonium chloride) inverts the negative surface charge to positive. The resulting hybrid structure was used to bind a series of negatively charged dye molecules. A process to make solid powders of NCC was scaled from the laboratory to the pilot plant. Spray dried NCC pigments and pristine (no dye) material exhibit spherical morphology. The behaviour of these particles in various solvent media relevant to cosmetic and printing applications was examined. Trials were conducted to evaluate the potential of the new materials in coloured cosmetics and in ink jet printing.</dc:abstract><dc:abstract>La cellulose nanocristalline (NCC) est un dérivé cristallin d’une biomasse. Ses propriétés physico-chimiques l’ont mise au centre d’intenses recherches dans le domaine des sciences et de l’ingénierie. Cette thèse traite un certain nombre d’aspects reliant ces deux domaines. On présente ici une nouvelle méthode pour synthétiser du NCC en évitant  une hydrolyse avec de l’acide sulfurique concentré, celle-ci étant très utilisée dans les procédés industriels actuels pour produire du NCC sulfatée. De plus, ce nouveau processus implique l’apparition d’un groupement carboxylique à la surface. En comparaison avec la NCC sulfatée, le carboxylate propose une méthode simplifiée pour les transformations chimiques à la surface de la NCC. Par conséquent, on montre que le peroxyde d’hydrogène, avec ou sans activation aux UVs, peut directement convertir la biomasse en NCC carboxylée. Avec l’eau comme produit dérivé théorique, la réaction est plus «eco-friendly»  que les procédés industriels actuels. Le groupement acide carboxylique est converti pour lier la β-cyclodextrine à la surface des nanocristallites au travers d’une chaîne réactionnelle. Cette modification est ensuite utilisée pour créer une base de cyclodextrine qui joue le rôle d’hôte/récepteur pour les nanoparticules de cellulose supramoléculaires. Les transformations de surface sont réduites en complexité à une procédure en une étape. Celle-ci lie un dérivé d’adamantyle à l’intérieur hydrophobe de la cyclodextrine. L’adamantine est modifiée grâce à des méthodes standards pour faire la molécule cible. Cette chimie supramoléculaire de type «plug and play» et autres organisations à la surface de la NCC sont illustrées dans une série d’exemples afin d’améliorer et d’étendre la structure de la NCC : des sondes de fluorescence sont utilisées pour l’étude des interactions du solvant avec la surface de la NCC par l’effet solvatochromique ; l’émission de fluorescence polarisée est utilisée pour donner une information sur l’ordre structural de la NCC dans la région des bandes d'énergie interdites photonique du cristal ; la diffusion de l’eau est utilisée pour ajuster les l'énergie des bandes interdites en réglant le pas «cholestérique» de la NCC ; la circulation de la NCC dans les cellules vivantes peut être suivie par imagerie de fluorescence ; et la résonance de spin d'une sonde organique radical libre (adamantyle - TEMPO ) établit certaines des propriétés dynamiques associées à la plate-forme supramoléculaire de la NCC. Les nanocomposites polymères ont été préparés à base de CNC. La spectroscopie infrarouge en temps réel a été utilisée pour suivre la photopolymérisation en fonction du chargement du CNC. Les nanocomposites transparents montrent des changements dans les propriétés mécaniques et thermiques qui sont toujours liées à la quantité de NCC dans la matrice du polymère. La prédisposition de la CNC à s'auto-associer pour former une phase chirale de cristal liquide nématique est exploitée pour faire un composite polymère dans lequel la texture ordonnée de la CNC est fixé par photoréticulation. Une technique a été développée pour obtenir des dispersions de nanoparticules individuelles du CNC chargées positivement dans l'eau. La liaison du poly(chlorure de diallyldiméthyl ammonium) permet le passage d’un potentiel négatif à un potentiel positif à la surface du CNC. La structure hybride résultante a été utilisée pour relier une chaîne de molécules de colorant chargées négativement. Un processus visant à rendre la CNC sous forme d’une poudre solide a été dimensionné de l'échelle du laboratoire à l’échelle pilote. Le mélange des pigments de CNC séchés par atomisation et de la poudre pure (sans colorant) adopte une morphologie sphérique. Le comportement de ces particules dans divers solvants a été analysé dans le cadre d’applications cosmétiques et d'impression.</dc:abstract><ual:supervisor>Mark P Andrews (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/w66346628.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/kk91fp669</ual:fedora3Handle><dc:subject>Chemistry</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Aqz20sw67k"><dcterms:title>Outer-membrane proteases at the forefront of the host-pathogen evolutionary arms race</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Microbiology and Immunology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Brannon, John</ual:dissertant><dc:abstract>Avec leur capacité à hydrolyser les liaisons peptidiques, les protéases sont des éléments essentiels des systèmes biologiques. Les protéases de la membrane externe (ME) bactérienne font partie de l'arsenal de défense à l'interface des interactions hôte-pathogène. Les omptines sont des protéases de la ME impliquées dans la virulence bactérienne. Les omptines se retrouvent principalement chez des pathogènes tels que Escherichia coli (OmpT), Yersinia pestis (Pla), et Citrobacter rodentium (CroP). Les pathotypes d’E. coli entérohémorragiques, entéropathogènes, et uropathogènes (EHEC, EPEC et UPEC) sont d’importants pathogènes qui affectent la santé humaine. Ces bactéries doivent survivre aux peptides antimicrobiens (PAMs) du système immunitaire inné afin de coloniser l’hôte et d'établir une infection. Auparavant, nous avons montré que OmpT de EHEC et EPEC dégrade le PAM humain de la famille des cathélicidines, LL-37. EHEC dégrade LL-37 plus rapidement que EPEC en raison de l’expression accrue du gène ompT. Nous avons cherché à approfondir le rôle de OmpT dans la résistance aux PAMs. Nous émettons l'hypothèse que la contribution de OmpT à la résistance aux PAMs est spécifique à certains pathotypes. Comme ompT est impliqué dans la virulence d’UPEC, nous avons cherché à déterminer l'activité protéolytique et la contribution de OmpT à la résistance au LL-37 chez ce pathotype. Nous avons montré que la lignée UPEC CFT073 et la lignée EPEC E2348/69 ont des niveaux d’expression et d’activité de OmpT similaires. En outre, l’activité protéolytique de OmpT envers LL-37 était similaire parmi les souches UPEC étudiées et les EPEC. Nos résultats indiquent que tout comme OmpT de EPEC, la protéase OmpT de UPEC contribue de façon marginale à la résistance au LL-37. Ceci est probablement le résultat du niveau élevé de similitude entre le contexte génomique du gène ompT entre les deux pathotypes. Nous avons également étudié le rôle de la protéase CroP, un orthologue de OmpT, dans la résistance aux PAMs chez Citobacter rodentium. Considérant qu’EHEC est confrontée au LL-37 dans le gros intestin humain, C. rodentium doit faire face à la cathélicidine murine CRAMP dans le côlon murin. CroP dégrade CRAMP et empêche l'activation du système à deux composants PhoPQ chez C. rodentium. Nous avons purifié CroP afin de caractériser son activité protéolytique et sa spécificité de substrat. Comme OmpT, CroP active peu le plasminogène, CroP est actif de façon optimale à un pH neutre et est non-affecté par la présence d'inhibiteurs usuels de protéases. Nous avons montré que l'aprotinine, un inhibiteur des protéases à sérine, inhibe CroP de façon compétitive. En outre, l'aprotinine inhibe non seulement l'activité de CroP, mais aussi celles de OmpT et de la protéase Pla. À ce jour, l'aprotinine est le premier inhibiteur identifié qui puisse inhiber plusieurs omptines. Notre modèle moléculaire du complexe omptine-aprotinine révèle la formation d'un pont salin entre la Lys15 de l'aprotinine et le Glu27 de la poche de spécificité S1 de CroP. L’analyse des produits de dégradation de LL-37 a révélé que CroP partage avec OmpT la même spécificité de séquence pour les motifs dibasiques. Bien que OmpT de EHEC dégrade LL-37 et CRAMP à des vitesses similaires, CroP de C. rodentium dégrade CRAMP bien plus rapidement que LL-37. Une analyse plus poussée avec CroP purifié a confirmé que CroP dégrade les deux PAMs à des vitesses différentes. Nous avons montré que CroP dégrade plus rapidement les peptides adoptant une conformation non structurée par rapport à ceux adoptant une conformation α-hélicoïdale. La spécificité structurelle stricte de C. rodentium CroP, qui n’existe pas chez EHEC OmpT, suggère que la spécificité de substrat des omptines de la sous-famille OmpT a divergé. Collectivement, toutes nos données indiquent que l’activité protéolytique des omptines de ces divers pathotypes et espèces bactériennes s’est adaptée aux PAMs présents aux divers sites d’infection.</dc:abstract><dc:abstract>With their ability to hydrolyze peptide bonds, proteases are vital components of biological systems. Bacterial outer-membrane (OM) proteases are amongst the arsenal of mechanisms significant at the interface of host-pathogen interactions. Omptins are OM proteases often implicated in bacterial virulence. Omptins are found in noteworthy pathogenic members of Enterobacteriaceae such as Escherichia coli (OmpT), Yersinia pestis (Pla), and Citrobacter rodentium (CroP). Enterohemorrhagic, enteropathogenic, and uropathogenic E. coli (EHEC, EPEC, and UPEC respectively) are important E. coli pathotypes that affect human health. These pathogens must overcome antimicrobial peptides (AMPs) of the innate immune system to colonize their host and establish infection. Previously, we found OmpT from EHEC and EPEC cleaves the sole human cathelicidin AMP, LL-37. EHEC cleaves LL-37 more swiftly than EPEC as a result of its superior expression of ompT. We sought to further investigate the role of OmpT in AMP resistance. We hypothesize OmpT’s contribution to AMP resistance is specific to different E. coli pathotypes. As ompT is connected with UPEC virulence, we sought to determine OmpT’s proteolytic activity and contribution to LL-37 resistance within this pathotype. The prototypical UPEC strain CFT073 and EPEC strain E2348/69 had a similar level of OmpT and protease activity. Additionally, OmpT proteolytic activity and OmpT mediated LL-37 resistance was similar amongst UPEC isolates and EPEC. Our results indicate UPEC OmpT makes a marginal contribution to LL-37 resistance similar to EPEC OmpT. This is likely a result of the high level of similarity between the ompT genomic context between the two pathotypes. We further investigated omptin mediated AMP resistance with C. rodentium CroP, an OmpT orthologue. Whereas EHEC encounters a high concentration of LL-37 within the human large intestine, C. rodentium confronts the solitary murine cathelicidin CRAMP within the murine colon. CroP degrades CRAMP and prevents activation of the master two-component regulator PhoPQ in C. rodentium. We purified CroP in order to characterize its proteolytic activity and substrate specificity. As OmpT, CroP is a poor plasminogen activator, optimally active at a neutral pH, and unaffected by the presence of typical class-specific protease inhibitors. We found the serine protease inhibitor aprotinin competitively inhibited CroP; furthermore, aprotinin inhibited the activity of not only CroP but also Pla and OmpT. To date, aprotinin is the first inhibitor described to inhibit multiple omptins. Our computational model supports that the omptin-aprotinin complex is directed through the formation of a salt-bridge between Lys15 of aprotinin and Glu27 of the omptin S1 specificity pocket. Digestion of LL-37 revealed that CroP shares the same sequence specificity for preferentially cleaving at dibasic motifs with OmpT. Curiously, EHEC OmpT cleaved LL-37 and CRAMP at similar rates, but C. rodentium CroP cleaved CRAMP more rapidly than LL-37. Further analysis with purified CroP confirmed that CroP cleaved the two AMPS at different rates. We found CroP more rapidly degrades peptides in a unstructured state compared to those with an α-helical conformation. The stringent structural specificity of C. rodentium CroP, which EHEC OmpT does not display, suggests that the substrate specificity of omptins in the OmpT sub-family have diverged. Collectively, all of our data indicate that omptin activity has developed as a specific adaptation in response to their respective bacteria’s niche.</dc:abstract><ual:supervisor>Herve Le-Moual (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/mp48sg53d.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/qz20sw67k</ual:fedora3Handle><dc:subject>Microbiology &amp; Immunology</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3Ahq37vr488"><dcterms:title>Fractionation of sweet sorghum bagasse using steam-assisted and microwave-assisted methods</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Bioresource Engineering</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Kudakasseril Kurian, Jiby</ual:dissertant><dc:abstract>The production of biofuels and biomaterials in the centralized biorefineries is challenged not only by the low bulk density material but also due to its variation in the availability and composition of the lignocellulosic part. Hence, on-farm production of denser and more uniform material for biorefinery operations is one of the solutions to address these problems. Under this circumstance, this study investigated the potential of steam-assisted and microwave-assisted treatments for the on-farm processing of lignocelluloses. Sweet sorghum bagasse was subjected to steam-assisted hydrothermal treatment (SAHT) for the extraction of hemicellulose. The optimized conditions of 12.5% (g/g) substrate concentration and 90 min of treatment at 121°C were conducive to the extraction of 72.69% (g/g) of the hemicelluloses and produced a hydrolysate containing 59.80 g/L reducing sugars. The SAHT-SSB was composed of 56.36% (g/g) cellulose and 31.42% (g/g) lignin. The hemicellulose-extracted residue was subjected to a steam-assisted lime treatment (SALT) to extract the lignin. The optimized conditions of 10% (g/g substrate) Ca(OH)2 and 106 min of treatment at 121°C extracted 69.67% (g/g) of the lignin from the SAHT-SSB, and produced a residue composed of 68.29% (g/g) cellulose and 13.26% (g/g) lignin. The extracted lignin was present in the yellow colored liquor. The CO2 treatment of the yellow liquor precipitated 58.85% (g/g) of the lignin present.Next, the SSB was subjected to microwave (MW)-assisted hydrothermal treatment (MAHT) at 121°C for the extraction of hemicellulose. The optimized conditions of 10% (g/g) substrate and 65 min of treatment time extracted 70.83% (g/g) of the hemicellulose from SSB. The hydrolysate contained 60.82 g/L reducing sugars and 2.98 g/L furfural. The solid residue was composed of 53.38% (g/g) cellulose and 30.19% (g/g) lignin. The MAHT-SSB was subjected to the MW-assisted lime treatment at 121°C for the extraction of lignin. Under the optimized conditions, lime at a concentration of 10% (g/g) of the substrate and 78 min of treatment residence time, 68.27% (g/g) of the lignin was extracted from MAHT-SSB. The MALT-SSB was composed of 69.41% (g/g) cellulose and 12.59% (g/g) lignin. The yellow-liquor was treated with CO2 and 60.26% (g/g) of the dissolved lignin was recovered.While comparing, the steam-assisted and MW-assisted methods have not exhibited differences in the overall recovery of solids, extraction of hemicellulose and lignin, and recovery of lignin and lime. The difference was found to be significant (p-value &lt; 0.05) only in the concentration of total reducing sugars in the hydrolysates and in the yellow liquors. The compositional and morphological changes to SSB samples were compared using FTIR spectroscopy, differential scanning calorimetry and scanning electron microscopy analyses. The MW-assisted process increased the total crystallinity index of the cellulose in the treated-SSB and also increased the concentration of guaiacyl content in the recovered lignin.A life cycle assessment (LCA) of the processing of SSB, on-farm versus at a biorefinery, was conducted to compare the energy balance and greenhouse gas (GHG) emissions of the two systems. The total energy required for ethanol production in the system of on-farm treatment was 3.2 GJ/Mg SSB which is higher than the total energy required (1.6 GJ/Mg SSB) for the ethanol production through the centralized treatment of SSB at the biorefinery. However, the use of bioenergy for the processing of SSB at the farm can reduce the fossil energy input required for cellulosic ethanol production by a factor of 71%. Overall, the total GHG emissions from the on-farm processing system was 7-22% less than that of the centralized processing system. Both the systems were sensitive to the type of technology and energy source used in operation.</dc:abstract><dc:abstract>La production des biocarburants et des biomatériaux dans les bio-raffineries centralisées est ardue non seulement en raison du fait que les intrants ont une faible densité, mais aussi parce que la disponibilité et la composition de la partie ligno-cellulosique sont très variables. Il serait donc avantageux de transformer en  partie, à la ferme, la matière première pour la rendre plus dense et plus uniforme et du fait, faciliter sa transformation dans les bio-raffineries centralisées. Cette étude présente les résultats de travaux qui ont évalué le potentiel des traitements à la ferme, soit à l’aide de vapeur ou de micro-ondes pour la transformation primaire de matériaux ligno-cellulosiques. La première partie de l’étude a porté sur la transformation de la bagasse de sorgho (SSB) par traitement hydro-thermique à la vapeur (SAHT) pour en extraire l'hémicellulose. Les résultats ont indiqué que les conditions optimales pour l’extraction des hémicelluloses étaient lorsque la concentration du substrat était de 12,5% et avec un traitement thermique de 90 min. à 121°C. Dans ces conditions, 72,7% des hémicelluloses ont été extraites et l’hydrolysat contenait près 59,8 g/L de sucres réducteurs. Par la suite, l’extrait d'hémicellulose a été soumis à un traitement à la chaux et à la vapeur (SALT) pour en extraire la lignine. Dans ces essais, les conditions optimales ont été obtenues avec 10% de chaux et un temps de traitement de 106 min à 121°C. La quantité de lignine récupérée (liquide jaunâtre) était de 69,7% et le résidu contenait 68,3% de cellulose et 13,3% de lignine. Le traitement de la liqueur jaunâtre au gaz carbonique a permis de précipiter près de 58,9% de la lignine qu’elle contenait. Dans la deuxième partie de l’étude, un traitement hydro-thermique assisté par micro-ondes (MAHT) a été utilisé pour l'extraction de l'hémicellulose de la SSB. Les résultats ont indiqué que les conditions optimales pour l’extraction permis d’extraire près de 70% de l'hémicellulose. L'hydrolysat contenait 60,82  g/L de sucres réducteurs et près de 3 g/L de furfurals. Quant à lui, le résidu solide était composé de 53,4% de cellulose et de 30,2% de lignine. Par la suite, la lignine a été extraite du MAHT-SSB à l’aide d’un traitement micro-onde à 121°C et en présence de chaux (MALT). Les meilleurs résultats ont été obtenus avec un dosage de chaux de 10%) du substrat et avec un temps de traitement de 78 min. Dans ces conditions,  68,3% de la lignine a été récupérée. Quant  à lui, le MALT-SSB était composé de 69,4% de cellulose et de 12,6% de lignine. La liqueur jaune a été traitée au gaz carbonique et 60,3% de la lignine dissoute a été récupérée. L’étude comparative des deux méthodes d’extraction utilisées a indiqué qu’il n’y avait pas de différences dans la récupération de matières solides, dans l'extraction de l'hémicellulose et de la lignine, dans la concentration résiduelle de cellulose et des cendres, et dans la récupération de la lignine et de la chaux. La différence a été significative uniquement dans les concentrations de sucres réducteurs totaux présents dans les hydrolysats et dans les liqueurs jaunâtres. Les modifications morphologiques et la composition des échantillons de SSB ont été comparées à l’aide de la spectroscopie FTIR, de la calorimétrie différentielle à balayage et par microscopie électronique à balayage. L’extraction assistée par micro-ondes a eu pour effet d’augmenter l'indice de cristallinité totale de la cellulose et d’augmenter les concentrations de la teneur en guaïacyle dans les lignines récupérées. Une analyse du cycle de vie du traitement du SSB effectué à la ferme plutôt qu’à la bio-raffinerie a été réalisée pour comparer le bilan énergétique et les émissions de gaz à effet de serre (GHG). Les émissions totales de GHG du système de transformation à la ferme étaient de 7 à 22% moindre que celles du système de transformation centralisé dans une bio-raffinerie.</dc:abstract><ual:supervisor>G S Vijaya Raghavan (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/td96k5435.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/hq37vr488</ual:fedora3Handle><dc:subject>Bioresource Engineering</dc:subject></rdf:Description><rdf:Description rdf:about="http://example.org/oai%3Aescholarship.mcgill.ca%3An870zt50x"><dcterms:title>The structure and functioning of metacommunities in changing environments</dcterms:title><ual:graduationDate>2015</ual:graduationDate><dcterms:language>eng</dcterms:language><schema:inSupportOf>Doctor of Philosophy</schema:inSupportOf><dc:contributor>Department of Biology</dc:contributor><dcterms:publisher rdf:resource="http://dbpedia.org/resource/McGill_University"/><ual:dissertant>Thompson, Patrick</ual:dissertant><dc:abstract>La fragmentation du paysage limite la capacité des espèces à se disperser d’un habitat à l’autre et à modifier leurs distributions en fonction des variations dans les conditions environnementales. En raison de la fragmentation des habitats, plusieurs espèces seront incapables de suivre le rythme des changements climatiques, ce qui pourrait entraîner des conséquences importantes au niveau de la diversité, du fonctionnement, et de la stabilité des écosystèmes futurs. Bien que la conservation de la connectivité des habitats devrait permettre de mitiger certains de ces impacts, notre capacité à prévoir la structure et le fonctionnement des communautés futures pourrait être compromise par la complexité des réponses écologiques. L’hypothèse d’assurance spatiale approfondit la science de la connectivité en démontrant comment la dispersion entre les habitats locaux maintient la biodiversité et le fonctionnement de l’écosystème lors de changements dans les conditions environnementales. Néanmoins, certains enjeux complexes persistent : la dispersion des espèces peut-elle simultanément stabiliser l’éventail complet des fonctions écosystémiques générées par une communauté, et comment les changements climatiques régionaux influenceront-ils la force de l’assurance spatiale engendrée par la diversité biologique? Cette thèse approfondit la recherche au sujet de l’hypothèse d’assurance spatiale à travers une combinaison d’études de terrain, d’expérimentation, et de simulations théoriques. Premièrement, j’ai réalisé une étude de terrain afin de déterminer comment les conditions environnementales locales ont structuré la composition et le fonctionnement des communautés de zooplancton dans des étangs du Mont St-Hilaire, QC. Mes résultats démontrent que les mesures de diversités fonctionnelle et phylogénétique du zooplancton étaient plus performantes que celles de diversité des espèces pour expliquer la variation dans deux types de fonctions des écosystèmes de zooplancton. Je me suis alors demandé si la dispersion permettait de préserver la diversité et d’assurer la stabilité de métacommunautés sous conditions ambiantes et sous une hausse de température. Mes résultats démontrent que la dispersion préserve la biodiversité et stabilise la biomasse de la métacommunauté sous conditions ambiantes, mais que cet avantage se perd avec une hausse de température. En portant un second regard sur le modèle de l’assurance spatiale, j’ai approfondi la théorie en y incorporant plusieurs fonctions écosystémiques. J’ai démontré que lorsque la fréquence de dispersion des espèces varie, le nombre, l’identité, et la stabilité des fonctions produites de façon locale et régionale sont altérés considérablement. Finalement, j’ai utilisé une simulation théorique afin de tester comment les interactions biotiques et la fréquence de dispersion des espèces interagissent et affectent la prévisibilité des déplacements multiespèces des aires de répartition en réponse à un changement climatique directionnel. J’ai démontré comment les interactions biotiques entraînent des différences dans la capacité des espèces à suivre les changements climatiques, ce qui entraîne de nouvelles compositions de communautés imprévisibles. Pourtant, lorsque les fréquences de dispersion ne sont pas limitantes, ces différences sont minimisées et les espèces suivent les changements climatiques à la même vitesse que leurs voisins, entraînant ainsi des déplacements prévisibles des aires de répartitions. Dans son ensemble, ma thèse teste et approfondit l’hypothèse d’assurance spatiale en démontrant les conditions sous lesquelles la dispersion maintient la composition, le fonctionnement, la stabilité et la prévisibilité des communautés écologiques. Ces résultats se portent à l’appui d’une stratégie de gestion des paysages visant à maintenir la connectivité dans le but de mitiger les impacts conjoints de la fragmentation des habitats et des changements climatiques.</dc:abstract><dc:abstract>Landscape fragmentation limits the ability of species to disperse between habitats and shift their distributions in response to changing environmental conditions. Because of habitat fragmentation, many species will be unable to keep pace with climate change, and this is expected to greatly impact the diversity, functioning, and stability of future ecosystems. Conserving habitat connectivity is expected to mitigate some of these impacts. But there are also concerns that the complexity of ecological responses will compromise our ability to predict future community structure and functioning. The spatial insurance hypothesis extends connectivity science to show how dispersal between local habitats maintains biodiversity and ecosystem functioning when environmental conditions are changing. However, complex issues remain, such as whether dispersal can simultaneously provide stability for the full range of ecosystem functions produced by a community, and how regional climate warming will impact the strength of spatial insurance provided by biological diversity.In this thesis, I extend research on the spatial insurance hypothesis with a combination of field surveys, experimentation, and theoretical simulations. I first conducted a field survey to determine how the composition and function of pond zooplankton communities was structured by the local environmental conditions of ponds on Mont St. Hilaire, QC. I found that measures of zooplankton functional and phylogenetic diversity outperformed species richness in explaining variation in two types of zooplankton ecosystem functions. Furthermore, the composition of these communities was determined by the local environmental conditions in the ponds, suggesting that dispersal could potentially provide spatial insurance if these conditions were to change. I then tested this experimentally, asking whether dispersal could preserve diversity and provide stability to metacommunities under ambient and warmed conditions. I found that dispersal preserved biodiversity and stabilized metacommunity biomass in ambient conditions, but that this benefit was lost with warming. This suggests that the stabilizing effects of dispersal may be eroded by directional environmental change, such as climate warming. I then returned to the spatial insurance model, extending the theory by incorporating multiple ecosystem functions. I showed that changing the rate at which species disperse dramatically alters the number, identity, and stability of functions that are produced both locally and regionally. Intermediate dispersal rates result in the greatest simultaneous production of functions across spatial scales and stabilize the temporal production of each function at the regional scale. However, this results in great local variability of each function, which differs from the stabilizing effect previously reported when only one function is considered. Finally, I used a theoretical simulation to test how biotic interactions and the rate of species dispersal interact to affect the predictability of multispecies range shifts under directional climate change. I showed how biotic interactions result in differences in the ability of species to track changes in climate, resulting in novel and unpredictable community compositions. Yet, when dispersal rates are not limiting, these differences are minimized and species track changes in climate at the same speed as their neighbours, leading to predictable range shifts.As a whole, my thesis tests and extends the spatial insurance hypothesis, demonstrating the conditions under which dispersal maintains the composition, functioning, stability, and predictability of ecological communities. These findings give support to the strategy of managing landscapes to maintain connectivity as a way to mitigate the joint impacts of habitat fragmentation and climate change.</dc:abstract><ual:supervisor>Andrew Gonzalez (Supervisor)</ual:supervisor><dc:identifier>https://escholarship.mcgill.ca/downloads/4f16c5546.pdf</dc:identifier><ual:fedora3Handle>https://escholarship.mcgill.ca/concern/theses/n870zt50x</ual:fedora3Handle><dc:subject>Biology</dc:subject></rdf:Description></rdf:RDF>