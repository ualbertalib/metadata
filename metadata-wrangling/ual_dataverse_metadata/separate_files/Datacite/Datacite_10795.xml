<?xml version="1.0" encoding="UTF-8"?>
<resource xsi:schemaLocation="http://datacite.org/schema/kernel-4 http://schema.datacite.org/meta/kernel-4/metadata.xsd"
          xmlns="http://datacite.org/schema/kernel-4"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <identifier identifierType="DOI">10.7939/DVN/10795</identifier>
    <creators><creator><creatorName>Cannaviccio, Matteo</creatorName><affiliation>(Roma Tre University)</affiliation></creator><creator><creatorName>Barbosa, Denilson</creatorName><affiliation>(University of Alberta)</affiliation></creator><creator><creatorName>Merialdo, Paolo</creatorName><affiliation>(Roma Tre University)</affiliation></creator></creators>
    <titles>
        <title>Accurate Fact Harvesting from Natural Language Text in Wikipedia with Lector.</title>
    </titles>
    <publisher>UAL Dataverse</publisher>
    <publicationYear>2016</publicationYear>
    <resourceType resourceTypeGeneral="Dataset"/>
    
    <descriptions>
        <description descriptionType="Abstract">Many approaches have been introduced recently to auto- matically create or augment Knowledge Graphs (KGs) with facts extracted from online resources such as Wikipedia, par- ticularly from its structured components like the infoboxes. Although these structures are valuable, they represent only a fraction of the actual information expressed in the articles. In this work, we quantify the volume of highly accurate facts that can be harvested with high precision from Wikipedia text articles using information extraction techniques boot- strapped from the entities and relations already in a KG. Our experimental evaluation, performed on facts about en- tities in the domain of people, reveals we can augment such Freebase relations by more than 10%, with facts whose ac- curacy are over 95%. Moreover, that vast majority of these facts are missing from the infoboxes, YAGO and DBpedia.</description>
    </descriptions>
    <contributors></contributors>
</resource>
